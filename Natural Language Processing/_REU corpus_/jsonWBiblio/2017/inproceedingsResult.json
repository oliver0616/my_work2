{"index":{"_id":"1"}}
{"datatype":"inproceedings","key":"Kovanovic:2017:DME:3027385.3027398","author":"Kovanovi'c, Vitomir and Joksimovi'c, Sre'cko and Katerinopoulos, Philip and Michail, Charalampos and Siemens, George and Gavsevi'c, Dragan","title":"Developing a MOOC Experimentation Platform: Insights from a User Study","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"1--5","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027398","doi":"10.1145/3027385.3027398","acmid":"3027398","publisher":"ACM","address":"New York, NY, USA","keywords":"A/B testing, Coursera, MOOCs, analysis platform, controlled experiments, technology acceptance model, user study","Abstract":"In 2011, the phenomenon of MOOCs had swept the world of education and put online education in the focus of the public discourse around the world. Although researchers were excited with the vast amounts of MOOC data being collected, the benefits of this data did not stand to the expectations due to several challenges. The analyses of MOOC data are very time-consuming and labor-intensive, and require and require a highly advanced set of technical skills, often not available to the education researchers. Because of this MOOC data analyses are rarely done before the courses end, limiting the potential of data to impact the student learning outcomes and experience. In this paper we introduce MOOCito (MOOC intervention tool), a user-friendly software platform for the analysis of MOOC data, that focuses on conducting data-informed instructional interventions and course experimentations. We cover important design principles behind MOOCito and provide an overview of the trends in MOOC research leading to its development. Although a work-in-progress, in this paper, we outline the prototype of MOOCito and the results of a user evaluation study that focused on system's perceived usability and ease-of-use. The results of the study are discussed, as well as their practical implications","pdf":"Developing a MOOC experimentation platform: Insights from a user study  Vitomir Kovanovic Srecko Joksimovic Philip Katerinopoulos School of Informatics Moray House School of Education School of Informatics  The University of Edinburgh The University of Edinburgh The University of Edinburgh Edinburgh, UK Edinburgh, UK Edinburgh, UK  v.kovanovic@ed.ac.uk s.joksimovic@ed.ac.uk f.katerinopoulos@sms.ed.ac.uk  Charalampos Michail George Siemens Dragan Gaevic School of Informatics LINK Research Lab Moray House School of Education  The University of Edinburgh University of Texas at Arlington and School of Informatics Edinburgh, UK Arlington, USA The University of Edinburgh  c.michail@sms.ed.ac.uk gsiemens@uta.edu Edinburgh, UK dragan.gasevic@ed.ac.uk  ABSTRACT In 2011, the phenomenon of MOOCs had swept the world of edu- cation and put online education in the focus of the public discourse around the world. Although researchers were excited with the vast amounts of MOOC data being collected, the benefits of this data did not stand to the expectations due to several challenges. The anal- yses of MOOC data are very time-consuming and labor-intensive, and require and require a highly advanced set of technical skills, often not available to the education researchers. Because of this MOOC data analyses are rarely done before the courses end, lim- iting the potential of data to impact the student learning outcomes and experience.  In this paper we introduce MOOCito (MOOC intervention tool), a user-friendly software platform for the analysis of MOOC data, that focuses on conducting data-informed instructional interven- tions and course experimentations. We cover important design prin- ciples behind MOOCito and provide an overview of the trends in MOOC research leading to its development. Although a work-in- progress, in this paper, we outline the prototype of MOOCito and the results of a user evaluation study that focused on systems per- ceived usability and ease-of-use. The results of the study are dis- cussed, as well as their practical implications.  CCS Concepts Human-centered computing  User studies; User interface design; Applied computing  Distance learning; E-learning; Education; Information systems Clustering; Data mining;  Keywords MOOCs, A/B testing, controlled experiments, analysis platform, user study, technology acceptance model, Coursera  1. INTRODUCTION The introduction of Massive Open Online Courses (MOOCs) to  the landscape of online learning was welcomed with great enthusi- asm. With MOOC reaching the unprecedented number of students, they have been seen as a panacea for a broad range of issues, such as increasing access to higher education, student debt crisis, pro-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027398  viding means for lifelong learning, and the overall democratization of learning [15]. In addition to the potential of MOOCs to solve a broad range of practical challenges, they also offer great opportu- nities for improving the understanding of learning processes [6, 9], given the vast amounts of data being collected and made available to the researchers [21]. Although significant quantities of data are being collected, there are several issues related to its use.  One of the main challenges of MOOC research is the format of the data delivered by major MOOC providers. Such data usu- ally requires extensive pre-processing before it can be used for the analysis. Due to the lack of MOOC-specific data analysis tools, studies are typically conducted using traditional software packages (e.g., R, Weka, SPSS). Aside from technical challenges, MOOC data analyses are generally correlational in nature, which  due to inability to eliminate the effect of confounding factors  has a sig- nificant impact on the external validity of their results. As with the rest of the educational research, the findings from MOOC stud- ies are also very localized to a particular context (e.g., particular course design, pedagogy, subject domain, student population), and data analysis procedures (e.g., construct operationalization and the analysis process) [13] making it hard to generalize across a variety of educational settings. Due to the high costs of MOOC data analy- sis, they are also typically conducted after courses are over, limiting the potential of MOOC instructors to alter and adjust their instruc- tional approach based on the data generated during the course. Fi- nally, at present, popular MOOC platforms provide very limited in- sights into student learning activities [2, 8], focusing primarily on demographic data, cumulative statistics related to course content, and student satisfaction with the course (i.e., upvotes and down- votes, star-ratings, and stories about student learning experiences).  This paper is the first step towards resolving some of the is- sues identified above. We present our work-in-progress on a novel MOOC analytics platform, that explicitly focuses on enabling in- structors with little technical background to conduct MOOC anal- yses, to gain better insight into student learning. Also, instructors should be able to act and experiment based on the analysis results during the course, leading to the improved student learning expe- rience and the better understanding of the learning processes. In this paper, we present a prototype of the proposed system and out- line the design principles guiding its design. We also present a user evaluation study which sought to examine the perceived usefulness and ease-of-use by the target user group. Results and the implica- tions of our findings are further discussed.  2. BACKGROUND WORK With the growing popularity of MOOCs, it became apparent  that there is a need for a common platform for the analysis of the MOOC data [25]. One of such efforts is MOOCdb project [25],  http://dx.doi.org/10.1145/3027385.3027398   which focuses on developing a standardized format for MOOC data so that analytics and visualizations projects (e.g., MOOCviz project1) built on top of it can be used for the analysis of data coming from the different MOOC platforms. However, one signifi- cant challenge of MOOCdb is that it requires considerable technical competence, limiting its use for the majority of MOOC instructors.  Another major trend in MOOC research is the growing interest in controlled experiments and A/B testing, which has supported by several MOOC platforms and used by an increasing number of re- searchers [3, 8, 21]. Currently, experimental studies in MOOC do- main primarily focus on examining the differences in instructional practices, such as serving random sub-populations of students dif- ferent learning materials [16, 5, 27], instructional approach [12, 24], or platform interfaces [1] without the regard for their individ- ual differences. As pointed out by Lamb et al. [16], this raises sev- eral issues related to the estimation of the actual effectiveness of the particular instructional measure, in large part by the significant stu- dent attrition. As shown by Winne [28], individual differences are also limiting the validity of findings across different settings. In- stead of observing students as homogeneous groups and randomly assigning them to groups, there is a need to account for the speci- ficity of different individuals need to be considered.  An additional challenge with the existing support for MOOC ex- perimentation is that they do not enable instructors 1) to alter and adjust their instructional approach for various groups of students during the course, nor 2) to test the effectiveness of different in- structional methods. For instance, it is not possible to identify stu- dents who are inactive in the discussions and then send a certain instructional email to this particular group of students in order to elicit more active participation in online discussions. Similarly, the instructor might want to examine the effectiveness of a particu- lar instructional message for different groups of students, or the effectiveness of several messages (e.g., collectivist vs. individual- ist encouragement as done by Kizilcec et al. [12]) on one particular group of students (by sending each intervention message to a subset of groups students). Due to the high costs of MOOC data analy- sis, the intervention approaches have to be planned in advance and assigned to students at random, without examination of their learn- ing activities. Although some systems, such as MOOClet [27] or Bazaar [22], can be used to dynamically assign students to groups, they require substantial technical expertise and infrastructural plan- ning to be successfully utilized in the course.  Given the need to adjust instruction by understanding student behavior during the course, the proposed system focuses on clus- ter analysis of students based on the different indicators of their engagement. Cluster analysis is a commonly adopted method in MOOC research, with a significant number studies using it to ex- amine the student behavior (e.g., [7, 19, 10, 14, 17, 4, 23]). For example, a study by Kizilcec et al. [10] used clustering to iden- tify four groups of students in MOOCs based on their course en- gagement (i.e., completing students, auditing students, disengaged students, and sampling students). In a similar manner, MOOC instructors should be able to identify different subgroups of stu- dents and then conduct various instructional interventions on those subgroups. Through identification of groups of students based on individual differences in their behavior, we can examine the ef- fects these differences have on the success of different intervention strategies. From the practical perspective, identification of student sub-populations in the real time enables the provision of person- alized and focused interventions to students in various subgroups which can improve their course success and learning experience.  3. METHOD 3.1 Design principles 1moocviz.csail.mit.edu  As the first step in our process, we focused on developing a pro- totype of an envisioned analytics system. The process was guided by the following set of design criteria which were based on the im- portant issues identified above.  No technical prerequisites. Given that many of MOOC in-  structors and researchers do not have advanced IT training, the system should not require substantial technical knowl- edge from its target users.  Incremental data import. As MOOC platforms typically  provide data as daily or weekly archives on the cloud storage (e.g., Amazon S3), the systems should support incremental import of the data when the new batch becomes available, as this is necessary to enable data analysis during the course.  Support cluster analysis. Given the need to understand  different patterns of student engagement, the system should support clustering of students based on their interaction with the course materials and other learners.  Support class interventions. With the need for the more  proactive use of the MOOC data, the system should sup- port quick analysis during the course. Hence, the data pre- processing and the extraction of important indicators of stu- dent engagement should be automated as much as possible.  Enable follow-up data analysis. After an instructor imple-  ments an intervention, the system should enable the analysis of the effects of that intervention. As such, the system should store information about conducted interventions in a format which is suitable for follow-up analysis by the popular sta- tistical packages (e.g., SPSS, R, Matlab).  Support study replication. With the goal of the platform  to advance the state of MOOC research, the system should support pre-configured analyses based on the previously published research. Not only would this limit the need for a manual analysis by the researchers, but it would also help validate previous research findings and examine their gener- alizability. By supporting analysis templates, it would be possible to investigate the replicability of the published re- search and the effect of study context on its findings.  Based on the defined design principles, we developed a proto- type of the user interface following an iterative design cycle. Using paper-based prototypes and Axure RP platform2, we incrementally developed a final version of the system which was then used in our user study. At present, the focus of the platform development is on supporting cluster analysis and the Coursera platform, in order to test the effectiveness of the proposed approach in practice. If MOOCito proves useful to instructors and researchers, our goal is to extend its support to other types of analyses (e.g., social network analysis and discourse analysis), additional MOOC platforms (e.g., edX and FutureLearn). Given the similarity between the different MOOC providers, support for other platforms would be most tech- nical in nature and require importing the data from slightly different MOOC data export format. Similarly, we would also enable certain automation of course interventions through a simple rule-building interface (e.g., if a student has not logged in into the discussions after five days, send him a particular email message). Finally, the import of the data should be done manually at the moment, but that also could be automated in the later versions of the system.  3.2 Prototype overview The final version of the developed prototype is shown in Fig. 1.  The system consists of four main application tabs corresponding to the main steps of the analysis process.  The Overview tab (Fig. 1a) provides details of all engagement indicators which are available in the system. The particular list of indicators is defined following the review of MOOC literature on predicting student learning and persistence [9]. For each indicator,  2www.axure.com  http://moocviz.csail.mit.edu http://www.axure.com   (a) Overview tab (b) Details tab (c) Analyze/indicators tab  (d) Analyze/Run analysis tab (e) Analyze/evaluate tab (f) Intervene tab Figure 1: MOOC Analytics prototype  minimum, maximum, average and standard deviation are provided, alongside its score distribution and weekly mean values. Due to space limitations, we will not be going in depth over the list of extracted indicators, as they are not the focal point of this report.  The details tab (Fig. 1b) enables instructors to examine and com- pare engagement indicators between specific students. The upper part provides a table with the values of all indicators for all stu- dents so that values of engagement indicators for each student can be seen. Given that the table is sortable, it is easy to identify stu- dents with extreme values for all engagement indicators. The lower portion of the window enables instructors to compare values of all engagement indicators for a selected list of students (e.g., compare students with messages posted and most submitted assignments).  The analysis tab represents a central component of the system which can be used to identify different clusters of students. The first step (Fig. 1c) is a selection (and limited pre-processing) of engagement indicators which are used for clustering (i.e., feature selection in data mining terminology), together with a predefined indicator configurations based on the published literature [18, 11]. Next, users can conduct a cluster analysis (Fig. 1d) using some of the popular algorithms (i.e., k-means, hierarchical clustering, em-clustering) which can be configured with very basic parameters (e.g., distance measure and K, the number of clusters). From the analysis results, several of visualizations are produced (Fig. 1e): vi- sualization of cluster centroids, silhouette evaluation plot, and pro- jections of clusters in two principal component space. For K-means and hierarchical clustering, it a range of values for K is selected, an elbow evaluation plot is also produced.  Finally, after analysis (or several analyses) are performed, in- structors can send intervention messages to the desired group of students (Fig. 1f). Those can be whole clusters of students or some parts of the identified student groups. For instance, it is possible to send one message to 50% of cluster one, and another message to remaining 50% of that cluster (or no message at all, for a control group case). Instructional messages can be saved as templates and contain several variables in the email body, as commonly done in mail-merge applications. In the end, the instructor can save a CSV file with the intervention summary which can be later used for comparing the effectiveness of different interventions.  Table 1: Study participant experience.  Question Mean SD Mdn IQR  Years of experience with online learning 9.10 4.53 9.50 7.75 Years of experience with MOOCs 3.38 1.12 4 0.75 Years of experience with Coursera 3.28 1.11 4 1.00  3.3 Evaluation study design The study was conducted with eleven experienced MOOC in-  structors/researchers/designers from three large research-intensive universities from the UK, USA, and South America. All partici- pants had several years of experience with online learning, MOOCs and Coursera platform (Table 1). The focus of the examination was on obtaining rich qualitative insights about the developed prototype to evaluate the impressions of its targeted end-user population.  After signing the informed consent form, all participants were shown a fifteen-minute presentation of the study scenario which focused on a hypothetical MOOC instructor halfway through an introductory programming course delivered on the Coursera plat- form. The first part of the presentation provided an overview of the current Coursera dashboard, with the actual data from the Code Yourself MOOC offered by the University of Edinburgh, followed then by the detailed description and overview of the functionali- ties of the proposed MOOCito system. Finally, participants were presented a Web-based survey3 with twenty five-item Likert-scale (from 1: strongly disagree to 5: strongly agree) and nine open- ended questions on the perceived usability and ease-of-use of the proposed system, and general impressions of the proposed system.  The design of the study instrument was based on the technol- ogy acceptance model (TAM) [26] which is a psychometric tool commonly used for assessing the suitability of software systems. TAM can be used to evaluate the determinants of 1) the perceived usefulness (i.e., perceived ease-of-use, subjective norm, image, job relevance, output quality, and result demonstrability) and 2) per- ceived ease-of-use (i.e., computer self-efficacy, perception of ex- ternal control, computer anxiety, computer playfulness, perceived enjoyment, and objective usability) which influence the adoption of a new software system. As our participants were presented the sys-  3goo.gl/EjLc8o  http://goo.gl/EjLc8o   Table 2: Five-item Likert-scale user study responses.  Q# Question M SD Mdn IQR  Dashboard: Perceived usefulness D1 The tool enables me to get an insight into the students engagement within the learning environment. 4.64 0.67 5.00 0.50 D2 The information the tool provides helps me identify students that might need assistance. 4.18 0.75 4.00 1.00 D3 The tool helps me to generate interesting questions related to course design/students worth exploring in more detail. 4.73 0.47 5.00 0.50 Dashboard: Perceived ease-of-use D4 Conducting analyses with the tool is easy and intuitive. 3.55 0.93 4.00 1.00 D5 Graphical user interface of the tool prototype is intuitive enough. 4.18 0.75 4.00 0.59 D6 Graphical user interface of the tool prototype is overburdened with information. 2.18 1.08 2.00 1.09 D7 Information about indicators of student engagement are adequately presented. 3.82 0.98 4.00 1.50 Intervention: Perceived usefulness I1 The tool enables me to intervene on the student learning during the course. 4.18 0.87 4.00 1.50 I2 It is important to be able to provide different interventions/instructions/guidances to different groups. 4.73 0.47 5.00 0.50 I3 It is important to be able to provide several different interventions/instructions/guidances to the same student group. 4.55 0.82 5.00 0.50 Intervention: Perceived ease-of-use I4 Selecting students for intervention is intuitive. 3.82 1.08 4.00 0.50 I5 Information on the Intervene tab is adequately laid out. 4.09 0.83 4.00 1.04 I6 Specifying different intervention/instruction/guidance messages for different groups of students is intuitive. 3.82 1.08 4.00 0.50 General opinion G1 I would like to be able to use the tool in my courses. 4.36 0.81 5.00 1.00 G2 I intend to use the tool in my future courses. 4.00 0.89 4.00 1.50 G3 I am willing to use the tool in my future course. 4.36 0.81 5.00 1.00 G4 I would recommend using the tool to my colleagues. 4.40 0.52 4.00 0.85 G5 I would use the tool frequently. 3.91 1.04 4.00 1.50 G6 I would use the tool for research purposes. 4.55 0.82 5.00 0.50 G7 I would use the tool for improving course design & teaching, and improving student learning. 4.30 0.67 4.00 1.00  tem prototype, we focused on the qualitative investigation of the systems perceived usefulness and ease-of-use and used only the relevant subset of dimensions defined by TAM.  4. RESULTS AND DISCUSSION The summary of the Likert scale questions is given in Table 2.  Overall, this indicates that participants were satisfied with the pro- posed system and its perceived usability and ease-of-use. The most positive responses were related to the ability to generate interesting questions and hypotheses (4.73), the ability of the tool to provide insights into students engagement with the learning environment (4.64), and the capacity to provide different interventions to sub- groups of students (4.55). A similar sentiment was also reflected in the open-ended responses where participants emphasized the se- lection of engagement indicators, weekly plots, and the clear pre- sentation of the interface, despite the vast amounts of data being presented. Regarding the ability to conduct instructional interven- tions, participants highlighted the flexibility and ease of performing meaningful interventions without the dangers of providing blan- ket information inappropriately to a particular target subset of stu- dents, and the use of templates and variables in the message body.  Although the overall impressions were positive, participants in- dicated several important issues which should be resolved before the system is fully developed. Despite our best efforts, there were several concerns regarding the accessibility of the tool to the gen- eral population of MOOC instructors. First of all, the participants stressed that instructors, users of MOOCito, might not be familiar with some of the adopted terminology, extracted engagement in- dicators, statistical analyses, or studies used as study templates. Thus, additional information should be provided, for example in the form of short summaries and video tutorials. Also, full bibliograph- ical information and summaries of the reference studies should be provided. Some participants indicated that they would not know what would be the best way to analyze the data, and in this regard, additional information on the standard approaches should be pro- vided. The need for simplification of the analysis steps is also seen in responses to Question D4, which measured how much the tool was intuitive and how easy was to conduct the analysis. As such, one of the future directions is related to enabling more streamlined analysis, with better guidelines and support for instructors with lit- tle background and experience in statistics.  Another set of concerns relates to the ability to provide meaning-  ful insights and instructional interventions. First of all, the partici- pants indicated that instructors should be able to go from indicators of engagement to the actual student content. For instance, if a stu- dents message cohesion is low, instructors should be able to easily see messages that have low cohesion and through their examina- tion get a better understanding of the students engagement. More- over, some participants indicated the challenge of knowing when or how to intervene, highlighting the need for more interpretable results, summarized in a form which is easy to act upon. For ex- ample, besides the detailed description of cluster centers, a simpler description using low, moderate, and high for describing en- gagement indicators could be employed (e.g., cluster one charac- terizes low cohesion, high volume of message postings, and mod- erate viewing of online lecture videos). In this regard, some of the data-to-text techniques [20] could be used to provide a one- paragraph description of each cluster which are easier to interpret by the MOOC instructors.  Overall, the participants indicated a strong willingness (4.36) to use MOOCito and would like to be able to use it in their courses (4.00). Although the participants reported the eagerness to use it for improving both teaching and research, the willingness to use it for research was higher (4.55) than for course design and teaching pur- poses (4.30). As nicely summarized by one study participant This has the potential to be a powerful tool in educational research as well as a means of tracking learner engagement day-to-day.. This and similar comments indicate the real need for an MOOC analysis platform and we hope that MOOCito can fill this gap which hinders the advancement in the MOOC teaching and research.  4.1 Study limitations There are several limitations of our current approach. As the fo-  cus of our efforts is on supporting MOOC instructors on the Cours- era platform, the results of our evaluation study might be affected by Courseras current analytics capabilities and user satisfaction. Hence, despite Coursera being the MOOC platform with the high- est user base, it might not be indicative of the MOOC domain as a whole. As such, one important area of future work is to support additional MOOC platforms. This is especially important given the Courseras recent shift towards corporate training4, which might impact its future adoption in the higher education sector. Secondly,  4video.cnbc.com/gallery/video=3000547421  http://video.cnbc.com/gallery/video=3000547421   although we provided a detailed prototype of MOOCitos graphical interface, it is still work-in-progress, and with its development un- derway, the final usability of MOOCito is yet to be seen when it is finally used in the real-world. Finally, in our study we had eleven participants in total which  although common in prototype testing  might be a small number to measure the usability of the proposed system reliably. Still, their qualitative feedback offers much helpful guidance that can inform future development of MOOCito.  5. CONCLUSIONS AND FUTURE WORK While it is true that MOOCs generated vast amounts of data  about student learning, the lack of analytics support for MOOC instructors and researchers significantly reduced the potential of this data to improve student learning. Not only this, but the com- plexities of analyzing MOOC data leading to its disconnect to the teaching practice also affects our ability to use the same data to understand the learning phenomena better.  In this paper, we introduced MOOCito, a novel MOOC analyt- ics platform which focuses on enabling MOOC instructors to gain insights about student learning and perform instructional interven- tions based on the collected student data. With the goal of helping ordinary MOOC instructors with little or no advanced techni- cal knowledge, the system focuses on conducting in-course analy- ses and interventions based on those analyses so that students can be given proper instructional support. Also, the tool provides the ability to experiment with the instructional approach, with the goal of providing more reliable evidence on the success of different in- structional interventions. Although experimentation in MOOCs is already happening [21], the lack of dedicated analytics support and pre-experimentation analysis results in experiments that have to be pre-planned and have significant methodological challenges [16].  Although still a work-in-progress, in this paper we present re- sults of the user study which investigated perceived usefulness and ease-of-use of the proposed MOOC platform. The study findings confirmed our intuition on the necessity of providing analytical support for MOOC instructors beyond what is currently available. As one participant concluded: I believe this is an excellent tool that will significantly improve teaching and learning within the MOOCs. It will also expand the way the MOOCs data are used and enable the instructors to conduct research and publish data.  Based on our findings, we identified several important directions for the future work on MOOCito platform. There is a need for more guidance and support in using the system (through detailed descrip- tions, summaries, and tutorial materials) which will be included in the next system prototype. Also, there is a need for better connec- tion to the MOOC platform itself (e.g., study materials), and ability to go from engagement indicators to the associated learning content (e.g., discussion messages, video lectures, quizzes). Finally and most importantly, a more interpretable and actionable cluster de- scriptions must be provided, and this a critical advancement which will be the center of our future work on the platform. By focusing on the issues identified by the actual MOOC instructors, we hope to provide analytics toolkit which will significantly advance the cur- rent state of MOOC teaching and research and ultimately enhance student learning experience.  REFERENCES [1] A. Anderson, D. Huttenlocher, J. Kleinberg, and J. Leskovec. Engaging  with Massive Online Courses. In Proceedings of the 23rd International Conference on World Wide Web, pages 687698. ACM, 2014.  [2] C. Coffrin, L. Corrin, P. d. Barba, and G. Kennedy. Visualizing Pat- terns of Student Engagement and Performance in MOOCs. In Proceed- ings of the Fourth International Conference on Learning Analytics And Knowledge, pages 8392. ACM, 2014.  [3] P. Diver and I. Martinez. MOOCs as a massive research laboratory: opportunities and challenges. Distance Education, 36(1):525, 2015.  [4] R. Ferguson and D. Clow. Examining Engagement: Analysing Learner Subpopulations in Massive Open Online Courses (MOOCs). In Pro- ceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 5158. ACM, 2015.  [5] W. W. Fisher. HLS1x: CopyrightX course report, 2014. [6] D. Gaevic, V. Kovanovic, S. Joksimovic, and G. Siemens. Where is  research on massive open online courses headed A data analysis of the MOOC Research Initiative. The International Review of Research in Open and Distributed Learning, 15(5), 2014.  [7] T. Hecking, S. Ziebarth, and H. U. Hoppe. Analysis of Dynamic Re- source Access Patterns in Online Courses. Journal of Learning Analyt- ics, 1(3):3460, 2014.  [8] F. M. Hollands and D. Tirthali. MOOCs: Expectations and Reality. Full Report, Center for Benefit-Cost Studies of Education, Teachers College, Columbia University, 2014.  [9] S. Joksimovic, O. Skrypnyk, V. Kovanovic, N. Dowell, C. Mills, D. Gaevic, S. Dawson, A. C. Graesser, and C. Brooks. How do we Model Learning at Scale A Systematic Review of the Literature. submitted.  [10] R. F. Kizilcec, C. Piech, and E. Schneider. Deconstructing disengage- ment: analyzing learner subpopulations in massive open online courses. In Proceedings of the Third International Conference on Learning An- alytics and Knowledge, pages 170179. ACM, 2013.  [11] R. F. Kizilcec, C. Piech, and E. Schneider. Deconstructing disengage- ment: analyzing learner subpopulations in massive open online courses. In Proceedings of the Third International Conference on Learning An- alytics and Knowledge, pages 170179. ACM, 2013.  [12] R. Kizilcec, E Schneider, G. Cohen, and D. McFarland. Encouraging Forum Participation in Online Courses with Collectivist. eLearning Pa- pers, 37:1322, 2014.  [13] V. Kovanovic, D. Gaevic, S. Dawson, S. Joksimovic, and R. Baker. Does time-on-task estimation matter Implications on validity of learn- ing analytics findings. Journal of Learning Analytics, 2(3):81110, 2016.  [14] V. Kovanovic, S. Joksimovic, D. Gaevic, J. Owers, A.-M. Scott, and A. Woodgate. Profiling MOOC Course Returners: How Does Student Behavior Change Between Two Course Enrollments In Proceedings of the Third (2016) ACM Conference on Learning @ Scale, pages 269 272. ACM, 2016.  [15] V. Kovanovic, S. Joksimovic, D. Gaevic, G. Siemens, and M. Hatala. What public media reveals about MOOCs: A systematic analysis of news reports. British Journal of Educational Technology, 46(3):510 527, 2015.  [16] A. Lamb, J. Smilack, A. Ho, and J. Reich. Addressing Common Ana- lytic Challenges to Randomized Experiments in MOOCs: Attrition and Zero-Inflation. In Proceedings of the Second (2015) ACM Conference on Learning @ Scale, pages 2130. ACM, 2015.  [17] N. Li, L. Kidzinski, P. Jermann, and P. Dillenbourg. MOOC Video In- teraction Patterns: What Do They Tell Us In Proceedings of the 10th European Conference on Technology Enhanced Learning, pages 197 210. Springer International Publishing, 2015.  [18] N. Li, . Kidzinski, P. Jermann, and P. Dillenbourg. Mooc video in- teraction patterns: what do they tell us In Design for Teaching and Learning in a Networked World, pages 197210. Springer, 2015.  [19] S. F. J. Mak, R. Williams, and J. Mackness. Blogs and Forums as Com- munication and Learning Tools in a MOOC. In pages 275284, 2010.  [20] A Ramos-Soto, B Vzquez-Barreiros, A Bugarn, A Gewerc, and S Barro. Evaluation of a data-to-text system for verbalizing a learning an- alytics dashboard. International Journal of Intelligent Systems, 2016.  [21] J. Reich. Rebooting MOOC Research. Science, 347(6217):3435, 2015. [22] C. P. Ros, O. Ferschke, G. Tomar, D. Yang, I. Howley, V. Aleven, G.  Siemens, M. Crosslin, D. Gasevic, and R. Baker. Challenges and op- portunities of dual-layer moocs: reflections from an edx deployment study. In Proceedings of the 11th International Conference on Com- puter Supported Collaborative Learning (CSCL 2015), volume 2, 2015.  [23] T. Sinha.  Your click decides your fate : Leveraging clickstream pat- terns from MOOC videos to infer students information processing & attrition behavior. arXiv:1407.7143 [cs], 2014. arXiv: 1407.7143.  [24] J. H. Tomkin and D. Charlevoix. Do Professors Matter: Using an a/B Test to Evaluate the Impact of Instructor Involvement on MOOC Stu- dent Outcomes. In Proceedings of the First ACM Conference on Learn- ing @ Scale Conference, pages 7178. ACM, 2014.  [25] K. Veeramachaneni, S. Halawa, F. Dernoncourt, U.-M. OReilly, C. Taylor, and C. Do. MOOCdb: Developing Standards and Systems to Support MOOC Data Science. arXiv:1406.2015 [cs], 2014.  [26] V. Venkatesh and H. Bala. Technology Acceptance Model 3 and a Re- search Agenda on Interventions. Decision Sciences, 39(2):273315, 2008.  [27] J. J. Williams, J. Kim, and B. Keegan. Supporting Instructors in Col- laborating with Researchers Using MOOClets. In Proceedings of the Second (2015) ACM Conference on Learning @ Scale, pages 413416. ACM, 2015.  [28] P. H. Winne. Minimizing the black box problem to enhance the validity of theories about instructional effects. Instructional Science, 11(1):13 28, 1982.    Introduction  Background work  Method  Design principles  Prototype overview  Evaluation study design   Results and discussion  Study limitations   Conclusions and future work   "}
{"index":{"_id":"2"}}
{"datatype":"inproceedings","key":"Hlosta:2017:OEI:3027385.3027449","author":"Hlosta, Martin and Zdrahal, Zdenek and Zendulka, Jaroslav","title":"Ouroboros: Early Identification of At-risk Students Without Models Based on Legacy Data","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"6--15","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027449","doi":"10.1145/3027385.3027449","acmid":"3027449","publisher":"ACM","address":"New York, NY, USA","keywords":"imbalanced data, learning analytics, predictive analytics, self-learning, student retention","Abstract":"This paper focuses on the problem of identifying students, who are at risk of failing their course. The presented method proposes a solution in the absence of data from previous courses, which are usually used for training machine learning models. This situation typically occurs in new courses. We present the concept of a self-learner that builds the machine learning models from the data generated during the current course. The approach utilises information about already submitted assessments, which introduces the problem of imbalanced data for training and testing the classification models. There are three main contributions of this paper: (1) the concept of training the models for identifying at-risk students using data from the current course, (2) specifying the problem as a classification task, and (3) tackling the challenge of imbalanced data, which appears both in training and testing data. The results show the comparison with the traditional approach of learning the models from the legacy course data, validating the proposed concept","pdf":"Ouroboros: Early identification of at-risk students without models based on legacy data  Martin Hlosta1, 3 Zdenek Zdrahal1,2 Jaroslav Zendulka 3  Knowledge Media Institute 1 The Open University, Walton Hall  Milton Keynes, MK7 6AA, UK {martin.hlosta;  z.zdrahal}@open.ac.uk  CIIRC, 2 Czech Technical University  Zikova street 1903/4 Prague, 166 36 Czech Republic  Faculty of Information Technology3 Brno University of Technology Bozetechova 2, Brno, 61266  Czech Republic {ihlosta; zendulka}@fit.vutbr.cz  ABSTRACT This paper focuses on the problem of identifying students, who are at risk of failing their course. The presented method proposes a solution in the absence of data from previous courses, which are usually used for training machine learn- ing models. This situation typically occurs in new courses. We present the concept of a self-learner that builds the machine learning models from the data generated during the current course. The approach utilises information about al- ready submitted assessments, which introduces the problem of imbalanced data for training and testing the classification models.  There are three main contributions of this paper: (1) the concept of training the models for identifying at-risk stu- dents using data from the current course, (2) specifying the problem as a classification task, and (3) tackling the chal- lenge of imbalanced data, which appears both in training and testing data.  The results show the comparison with the traditional ap- proach of learning the models from the legacy course data, validating the proposed concept.  CCS Concepts Information systemsData analytics; Computing methodologies  Supervised learning by classification;  Keywords Student Retention, Predictive Analytics, Self-Learning, Im- balanced data, Learning Analytics  1. INTRODUCTION Student dropout is a critical problem which is being tack-  led by various educational institutions, i.e. universities, high- schools or various platforms for Massive Open Online Courses (MOOCs). According to [21, 25] the number of students not  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027449  finishing university in Europe is between 20 and 50 %. In USA 20 % of high-school students fail to finish their stud- ies in time [14]. For distance education, these numbers are even more pessimistic with 78% of students not finishing the degree [22]. And even worse, for MOOCs, the percent- age of students who registered and successfully completed the course is only 15% on average [12] or even 5% reported by [16]. The problem of identifying students likely to fail the course has been in recent history intensively investigated by the research community [9, 27, 13, 15, 8]. It was also the topic of the KDDCUP 2015 competition, which mainly fo- cused on predicting students withdrawing from courses in XuetangX, thr Chinese MOOC learning platform1.  Identifying students, who are at risk of failing or with- drawing from their course, is the first step in the process of providing them with the remedial support. Typically, inter- ventions are mediated by a tutor who receives the results of the predictions. [9, 27]. Alternatively, the prediction system may generate email messages that are sent directly to the student [6]. The primary goal is to improve the students learning, to retain them in the course, and to help them finish the study programme.  In distance education, most courses are delivered through a Virtual Learning Environment (VLE). In this case, the stu- dents interactions with the VLE are recorded and stored. Besides, student data include demographic information, as- sessment results, etc. Together, these sources provide a large amount of student data for analysis. After cleaning and pre-processing the data, machine learning techniques are commonly used to build predictive models. These are then utilised to provide the predictions of at-risk students.  A typical approach is to train the models using legacy data from a previous presentation of the course[27]; the models are then applied to the current presentation. However, this approach cannot be used for new courses which have no history. In such a case, it is necessary to find a different solution.  The highest level of dropout typically happens in the first years courses, and many students drop out even during the first few weeks of the course presentation. This finding has been confirmed by the analysis of both distance Higher Ed- ucation (HE) courses [27] and MOOCs [23]. One of the explanation is that the drop-out might happen due to the possible fee reimbursement. The withdrawal rate is shown in  1KDD CUP 2015  http://kddcup2015.com    Figure 1: Number of withdrawn students for 7 courses in days relative to the start of the course (day=0).  Figure 1 for the HE OULAD dataset2. Therefore, the goal is to identify at-risk students as early as possible. Its worth noting that the same pattern might not necessarily hold for all the educational institutions - based on the course design, significant student dropout may sometimes happen later in the course [15].  2. IDENTIFYING AT-RISK STUDENTS: RE- LATED WORK  The results of the state-of-the-art solutions are highly de- termined by the data available for analysis, which is depen- dent on the type of the educational institution (i.e. whether it is a high school, university, distance learning university or MOOCs). Nevertheless, the main idea is usually the same, i.e. to use legacy data to train predictive models. The fea- tures can be selected either by tutors experience or by ma- chine learning algorithms. Then, the models are used to provide predictions for the current students - current pre- sentation of the course, current cohort of high-school, etc.  2.1 High schools High schools in the USA are usually interested in forecast-  ing whether students will finish their studies in time. Using the data from the previous cohort, models are trained and applied to the current cohort. The precision of the models increases when students approach the final grade and thus the prediction time frame decreases [18, 11]. In a recent study GPA3 was found to be the feature with the highest predictive power [18].  2.2 Higher Education In [8, 19, 13], students previous results were used to train  the models for identifying success or failure of the current co- hort. In contrast, [26] extracted the predictive features from demographic data only. Most of the recent work in Higher Education makes use of demographic, performance and be- havioural data extracted from the VLE. The key issue ad- dressed by different educational institutions and researchers is how the concept of an at-risk student is defined. It can be a student with the grade lower than C [9] or even B- [2], less than 60% [3], not submitting the following assessment [27], or dropping out in the following days [15, 23].  2.3 MOOCs 2https://archive.ics.uci.edu/ml/datasets/ Open+University+Learning+Analytics+dataset 3GPA = Grade Point Average  In MOOCs, the main source of information is usually the click-stream data, as the current performance and demo- graphic may not be available. The key point is to tackle the high dimensionality of the data. Students daily inter- act with a lot of study material. The activity types include viewing videos, reading study texts, posting in forums etc. The predictions are usually based on the summary of clicks, possibly grouped by activity types [28]. MOOCs differ from HE courses: students are often not motivated to finish the MOOC course and they may register only to have access to videos or text materials, typically provided for free [28].  The current research also differs in defining the perfor- mance measure used to evaluate the methods. Frequently used are ROCAUC4 [18, 1, 7], Precision and Recall [27, 9, 2], less often Accuracy [10], but mostly its a combination of AUC and Precision/Recall.  Sometimes, various constraints were posed to the problem, e.g. focusing on obtaining smoothed probabilities across the predicted weeks [7] or limiting the predictions to most at- risk students. Top-K-Precision and Top-K-Recall were used in [18] where K defines the percentage of students selected as at-risk ordered by probability of failing and used for cal- culating Precision and Recall. This study discussed the pos- sibility of limited resources to assist students. In this case, the schools were able to provide support to at least 5% of the student population. Due to different problem specifications, data used, and evaluation metrics, a comparison of existing solutions is not an easy task.  2.4 Early identification of at-risk students When all data are available, the best predictor makes use  of actual performance either by: (a) student study history measured (e.g. by GPA) or (b) by evaluating the progress in the current course from assessment results. However, stu- dent study history - is not available for entry-level courses, or more generally courses, which are taken at the beginning of the study programme. Moreover, these courses require in- creased attention because student dropout is typically high.  The progress in the current course is unavailable before the first assessment (denoted as A1) is evaluated, though A1 is often important for early predictions. This issue has been addressed in [27] by predicting submissions of the A1 from demographic and pre-A1 VLE activities with models trained on the previous presentation of the same course. To early identify at-risk students, [10] used behaviour in the first week, by evaluating quiz results as the most important attribute. Similarly, [28] has found quizzes to be most infor- mative for the predictions.  3. PROBLEM SPECIFICATION  3.1 Importance of the first assessment It is important to investigate whether A1 is a good predic-  tor of the overall success in the course. We assume that the student succeeds in A1 if he/she submits A1 and achieves a score higher than 50% of the points. This has been in- vestigated for courses A to G, see table 1. The results are divided into 4 columns: (a) probability of failing the course given student failing A1 (scoring less than 50 %), (b) proba- bility of failing in the course given that the student did not submit A1, (c) number of students who failed the course and  4Receiver Operation Characteristic Area Under Curve    submitted but failed A1, and (d) number of students failing the course and not submitting A1. The numbers in the table are means calculated across all presentations available in the OULAD dataset.  Based on the previous presentation, its possible to extrap- olate the probability of failing based on the A1 in the next presentation5. If no presentation is given we can extrapolate from all courses.  The probability of failing the course if a student hasnt submitted A1 is almost 90%, making A1 a strong predictor of future failure. If no data from the previous presentation of the course is available, its impossible to use the results from the assessment before they are marked. However, even the assessment submission is a good predictor for identifying at-risk students. On average, there is 95% probability that a student will not finish the course given that he/she hasnt submitted A1. When limiting the approach only to predict submissions instead of predicting the failures, we are not able to identify some of the at-risk students. According to the 3rd and 4th column in Table 1, we are missing 1392 students (i.e. 15% of those that fail), but at the same time, the probability of failing the course is 5% higher, making the predictions more accurate.  Table 1: Probability of failing the course (F) if failed (F A1) or not submitted (NS A1) A1 for courses averaged for all available presentations and count of students.  Course P (F | F A1)  P (F | NS A1)  CNT (F  F A1  S)  CNT (F  NS A1)  A 0.8004 0.9421 23 50 B 0.8809 0.9905 434 1888 C 0.8381 0.8252 255 1710 D 0.9651 0.9876 431 1436 E 0.9808 0.9932 49 643 F 0.9805 0.9930 122 1527 G 0.8300 0.9157 78 453 AVG / SUM  0.8966 0.9496 1392 7707  3.2 Assessment description Each assessment has a cut-off date before which the stu-  dents have to submit their assignment. Four types of data are available:  1. students demographic information (age, gender, etc.),  2. students interactions with the VLE system,  3. information about students date of registration and  4. a flag indicating student assessment submission.  The latest available data always come from the previous day, no information is available for the current day, i.e. we know students activities and whether she/he has submitted by the end of the previous day.  5The values for each presentation has been omitted for space but they are similar across presentations.  Figure 2: Time line with the current day and cut-off date.  3.3 Dealing with lack of legacy data There are two straightforward possibilities how to deal  with the lack of data from previous presentations. It is possible to build a prediction model based on all  available courses. However, it has been shown in [27] that the identification of at-risk students is more accurate when the predictions are tailored for each course separately.  Another option is to use data from the students previous study results. Unfortunately, these data are not available for the courses at the beginning of the study programme. At the same time, these level-1 courses usually have lower retention [27], therefore they are more important to consider them for analysis.  3.4 Ouroboros: Self-Learning approach This paper proposes a new Self-Learning approach, i.e.  to use only data from the running presentation for train- ing predictive models. The underlying idea is to use the data about students who have already submitted the next assignment and exploit the patterns of their behaviour to identify the students who might be at risk of not submit- ting. Its expected that the behaviour of learners who are about to submit will follow a similar pattern as those who have already submitted and differs from students who will not submit.  There are several options how to make use of these pat- terns. In this paper, we define the task as a binary classifica- tion problem: Given the current day, which is n days before the cut-off date, the goal is to construct a binary classifi- cation model that will predict whether the student (1) will submit or (2) will not submit the next assessment in time, i.e. today or within the next n days. If n = 0, predictions are made on the cut-off day. Only students that are reg- istered in the course and havent submitted the assessment yet are subject to the prediction. The figure 2 depicts the problem for n = 3.  4. OUROBOROS FRAMEWORK Lets denote the cut-off date as cutoff date and the date  when the prediction is made, which is n days before the cut- off day, as prediction date. In order to be able to create the prediction model for interval [prediction date; cutoff date] we need labelled examples for interval of the same size [d prediction date; d cutoff day ] such that d cutoff date = prediction date. The d prediction date and d cutoff day can be considered to be a dummy prediction and cut-off day, respectively.  The example of the problem is depicted in Figure 3 in the top part a). Here, the cut-off date is within 3 days from the current day and we want to predict if students submit either today or within the next 3 days. The data for the current day are unavailable, so the training data will come from the    Figure 3: Classification framework for self-learning and testing predictions of at-risk students  days [presentation start;now + 5] = 8 with the labels of submission in [now + 4;now + 1] = [7; 4].  The bottom part of Figure 3 b) shows the relative view of the days for training and testing data, day = 0 denotes the current day, negative indexes relate to known data and positive indexes to unknown. Thanks to this view its visible that though we have more days available when applying the predictive model, some older days cannot be used since they were not present in the training phase.  4.1 Extending labelling window Based on the described concept, going back in history  means the window for labels is growing. The more days before the cut-off date, the more days for training labels we need. The situation for the current day being 0 to 3 days be- fore the cut-off date is depicted in the Figure 4. For n days before the cut-off, the size of the window both for training and testing labels is n + 1.  Figure 4: Extending window for training and testing labelled data. Day = n denotes that the current day is n days from the cut-off date, (day=0 cut-off day is today, day=1 cut-off day being tomorrow, etc.)  4.2 Features for learning The available data for learning include information about  student demographics and activities in the VLE. As the de- mographic data is static, it is only necessary to perform transformations, such as vectorisation of categorical data and standardisation/normalisation for numerical data.  On the other hand, the VLE data are very rich containing daily click summary activities grouped by specific activity, for example student A viewed 10 times the specific PDF resource study material.pdf . All the activities are grouped into activity types, so all the PDF resources are grouped as a resource. There are approximately 30 different activity  types such as forum, video, resource, etc. Given the current day when the model is learned, the VLE  features are aligned backwards in time on this day, i.e. day 0 is the current day, day 1 is referring to yesterday etc. The oldest day used for training is the day that the course starts.  In addition to VLE daily counts, its possible to extract various summarising statistics about student behaviour in the VLE, such as the number days that the student was active in the VLE (i.e. when he/she at least logged in). These statistics and all the features are described in Table 2.  5. PREDICTIVE MODELLING For training the models and for the whole evaluation frame-  work, the Python Scikit-learn library [20] was used, which provides a large number of existing implementations of clas- sification algorithms and preprocessing routines.  For training the models, we chose models that support probabilistic predictions. This enables us to order students according to their likeliness to fail, and then apply the re- sources limitation. Also, the existing results from the re- search in the identification of at-risk students were taken into consideration. The selected algorithms included: Logis- tic Regression (LR), Support Vector Machines (SVM), Ran- dom Forest (RF), Naive Bayes (NB), and the Tree Boosting XGBoost. The last one was selected due to its success in many Kaggle7 competitions. According to [4] 17 out of 29 winning solutions in 2015 used XGBoost. Moreover in the KDD-CUP15 focused on predicting students dropout, all top 10 solutions used this algorithm 8.  6. TACKLING IMBALANCED DATA Machine learning algorithms are usually designed to learn  concepts from data when the classes in the training data are balanced. However, in many real-world problems, the dataset includes a class with a significantly lower number of instances than the others. Without any changes, these algo- rithms perform poorly and therefore new approaches have been developed [6].  The basic approaches to deal with imbalanced data ad- dress the problem at the following two levels:   Data level  using various sampling methods to modify the class distribution in a way that the training data are balanced.   Algorithmic level  cost-sensitive learning, One-class classification methods, and various ensemble methods are among those mostly used.  The key idea of cost-sensitive learning is to penalise the cost of error on the minority class, which is in- curred during the training phase. This can be achieved by specifying a cost matrix. However, for the binary classification problem, its usually good enough to set the weight for the minority class (with the assumption that the weight for the majority class remains 1).  The problem of imbalanced data appears in the exist- ing research in predicting at-risk students. In [9] a data  7https://www.kaggle.com 8https://www.linkedin.com/pulse/ present-future-kdd-cup-competition-outsiders-ron-bekkerman    Table 2: Features used for learning the model. No. Type Dim. Description Examples  1 Demographic 8 Static demographic data  Age, IMD6, Qualification, Region, Gender, Declared Disability, Number of previous at- tempts, number of currently studied credits  2 Registration info  1 The registration day relative to start of the course - positive or negative number  -  3 VLE statistics 28 Various statistic measures about stu- dent behaviour in VLE  1) Num. of consecutive days that the stu- dent is currently active, 2) first and last day he/she was active or indication of never logged in, 3) average/median of clicks and number of materials visited per day normalised either by all days or only days when he/she was active in the VLE, 4) total number of active days in the VLE  4 VLE statistics before presen- tation start  19 Same as 3), measured before only the start of the presentation  Same type of statistics as for previous feature type (4) but only limited to 3) and 4) features mentioned in the examples.  6 VLE daily counts per activity type  50-560 Number of clicks in the VLE grouped by activity type per day  Number of clicks in resources/forum in day 0, 1, ...  level approach was used, combining random over-sampling and under-sampling. Similarly, [24] improved the AUC and F1-Score by over-sampling the dataset using SMOTE algo- rithm. Moreover, they examined which algorithm best copes with different cost ratios specified to False Negatives and False Positive errors. On the other hand, in [7] the problem was tackled by focusing only on active students and com- pletely omitting those who havent shown interest in doing assignments.  6.1 Problem and solution The specificity of this problem comes from the fact that  the ratio between the majority and the minority class is changing in time. The more we move backwards from the cut-off date, the higher is the imbalance ratio in the train- ing data, because there are fewer students who have already submitted the assessment and also more students that with- draw later in the course meaning they dont appear in the training data. Most important, the majority class in the training data is minority class in the testing data. The ratio between the classes is depicted in the Figure 5.  For the algorithms such as SVM and Logistic Regression, its possible to use cost-sensitive learning by specifying the weights of the classes during the training. The most suit- able way proved to be to set the weights proportionally to the ratio of the cardinality of minority and majority classes. Moreover, several ensemble based algorithms, which are able to cope with the imbalance data. for learning were used.  The important question, when dealing with imbalanced data is the selection of the performance metric for algo- rithms comparison. The area under ROC curve (ROCAUC) and area under the Precision-Recall curve (PRAUC) are the most suitable measures. The latter is giving more informa- tion about the algorithm performance on the target class, especially when the data are imbalanced and the target class is more important [5]. Moreover, this metric suits more the  Figure 5: Ratio for NotSubmit class for training and testing data.  problem of identification at-risk students when Precision is more important metric to measure than FPR, which is used in ROC. For these reasons, we chose PRROC as our eval- uation metric.  7. EXPERIMENTAL RESULTS The proposed framework for learning the student dropout  model has been evaluated using various experiments with all the data and code publicly available.  7.1 Experimental setup The experiments were conducted on four level-1 university  courses with 1200 to 2500 students on the publicly available OULAD - Open University Learning Analytics Dataset [17].    For all the courses, the goal was to predict the submission for A1, with the cut-off ranging from day 19 to 33. More information about the courses is available in the Table 3. We narrowed the focus on the most recent 2014 presentation, but the numbers dont differ much.  The courses cover wide range fields such as maths, engi- neering, history or social care. They last between 20 and 30 weeks and they are organised in logical blocks, each of them completed by an assessment. In order succeed in a course, students have to achieve minimum scores in the assessments and then pass the final exam.  Three more courses are available in the dataset, A, C and G, A being level-3 course and G being a preparatory course. These courses have different properties and we omitted them from the comparison. Course A and C have a lower number of enrolled students, and the cut-off date of A1 for course G is late in the course, on day 61. Surprisingly, despite only 8% of students submitted A1 in the level-3 course A, the course has the lowest retention out of all courses. The course C was withdrawn from the experiments because this course doesnt have the previous presentation 2013J and we wanted these experiments to be comparable.  Table 3: Information about the courses under anal- ysis - 2014 presentation  Course Num. of students  Pass Rate [%]  A1 S/NS[%]  Class Ratio for cut- off  Cut- off date  A 365 30.69 92.23 12.04 19 B 2292 49.74 77.31 3.41 19 C 2498 59.37 57.04 1.33 32 D 1803 56.07 78.48 3.65 20 E 1188 42.42 78.20 3.59 33 F 2365 52.77 77.12 3.37 24 G 749 40.72 77.97 3.54 61  The experiments were focused on the following goals:  1. Daily analysis of classification performance across days and machine learning algorithms.  2. Tackling the problem of imbalanced data.  3. Compare Ouroboros against models trained on legacy data, i.e. previous presentation of the same course.  4. Analysis of Precision-at-K for various K to see these metrics for a limited resources for interventions.  5. Feature importance for the best algorithm from the first experiment to see the change in time and across courses.  The source code of the Ouroboros framework together with all the performed experiments and scripts for the pre- sented statistics are available on GitHub9.  7.2 Daily results This experiment focused on comparing the performance of  the algorithms with each other for various days relative to  9https://github.com/hlostam/ouroboros paper/  the cut-off. The main goal was to observe a change of perfor- mance when moving further back to the history and which machine learning models are coping best with the given data.  The Table 4 shows the PRAUC for the used classification methods. The value for the classifier is an average computed over the four courses. The table shows the performance in the cut-off date and up to 11 days before the cut-off date.  The performance is better for the cut-off date and then drops down when going back in time, especially in the day 1 and 2. The highest PRAUC was achieved by three models - XGB, RF and weighted SVM with RBF kernel. While SVM performed best in the cut-off date, RF in day 1 and 2, XGB gave better results from day 3 to 11.  Figure 6: PRAUC for days 0 to 11 before the cut- off. LR stands for Logistic Regression, with W indicating weighted LR used for imbalanced data, same for SVM. SVM-R and SVM-W-R = SVM with RBF kernel, RF = Random Forest, XGB=XGBoost, NB=NaiveBayes  7.2.1 Dealing with imbalanced data Both the Figure 6 and the Table 4 reveals how impor-  tant might be setting class weights for the machine learning algorithm to handle the imbalanced data. The difference in performance between weighted and unweighted versions of LR and SVM becomes visible when moving further from the cut-off date and having higher imbalance ratio. The performance of the weighted version doesnt suffer from the change that much because the error made on the minority class influences the model. Given highly imbalanced data, the model might not be able to underpin minority class and classify all the data to the majority class.  Moreover, we utilised several sampling methods for mod- ifying the class distribution of the data from the Imbal- ancedLearn10 but even using sophisticated sampling meth- ods didnt lead to better results than for the class weighting and using ensemble methods.  7.3 Comparing with learning from legacy data The aim of this experiment was the comparison of the  self-learning approach with training on the legacy data. In the real world, there might not be any previous course to  10ImbalancedLearn  github.com/scikit-learn- contrib/imbalanced-learn    Table 4: PRAUC values for different days trained on the same presentation. Day SVM-W-R SVM-R LR LR-W NB RF XGB 0 0.7790 0.7435 0.7561 0.7682 0.6779 0.7748 0.7442 1 0.6161 0.4081 0.5267 0.5944 0.4587 0.6184 0.5965 2 0.5436 0.3138 0.3852 0.4934 0.3673 0.5353 0.5315 3 0.4726 0.2629 0.3019 0.4164 0.3412 0.4960 0.5225 4 0.4596 0.2547 0.2866 0.3954 0.3577 0.4796 0.5079 5 0.4289 0.2363 0.2569 0.3870 0.3453 0.4600 0.4920 6 0.4171 0.2185 0.2195 0.3610 0.3475 0.4234 0.5200 7 0.4024 0.2027 0.2072 0.3263 0.3456 0.4309 0.4959 8 0.4118 0.1948 0.2272 0.3350 0.3487 0.4378 0.5309 9 0.3850 0.2031 0.2120 0.3260 0.3809 0.4820 0.5737 10 0.3677 0.2074 0.1967 0.3225 0.4011 0.4785 0.5669 11 0.3440 0.2033 0.1879 0.3039 0.3985 0.4569 0.5652  Figure 7: PR AUC for days 0 to 11 before the cut-off using training on the previous presentation.  compare with, but the OULAD dataset has them. The ex- periments were run for the same courses and same days as in the previous experiment.  Looking at the previous Figure 6 and the Figure 7, the results show that more data provided to the algorithms using training on the previous presentation helped the algorithms to have more stable results. However, when predicting in the cut-off date, the performance of Ouroboros based models was significantly better, around 10%.  7.3.1 Comparing with existing solution Moreover, we were able to compare our solution with the  existing work for predicting next assessment submission in [27], denoted as PREV MODEL. F1-Score, Precision and Recall was computed in the selected days before the cut-off date because these predictions were computed weekly not daily. F1-score was selected as an evaluation metric as it represents a harmonic mean between Precision and Recall. Both self-learning and learning using legacy data were com- pared and again using the 4 courses as previously.  Based on the previous experiment, we selected the best al- gorithm, which was XGBoost and optimised the probability threshold of the predictions on the training data in order to maximise the F1-score. This threshold was used to compute the evaluation metrics.  Table 5 shows that both Ouroboros and our solution trained  on the previous presentation outperform the PREV MODEL in F1-score. Ouroboros performs better in the day 0 while training on legacy data in the other days. The only situation when the PREV MODEL performed better is course E on the cut-off date.  7.4 Top-K-Precision Although PR-AUC is a suitable measure for comparing  classifiers performance, the target users are sometimes in- terested how confident are the classifiers for the top ranked students in terms of their probability to Not Submit the A1. As mentioned, this might be useful for determining the qual- ity of predictions given a limited resources for interventions.  For this experiment, the results are compared with two baseline models. Base[Nonactive] model classifies all the students that havent accessed the VLE so far as NotSubmit and all the others as Submit. The Base[NotSubmit] assigns all the students to NotSubmit class, meaning that we would intervene with all the students. Those were not used in the previous experiment as they dont provide probabilis- tic prediction and their performance was otherwise overly optimistic.  The Figure 8 contain 3 sub-figures of precision for the first 5,10 and 25%, and similarly Figure 9 contain Top-K-recall. Its clearly visible that as the k increases the precision de- creases, especially when moving from top 10% to top 25%. Again, as the daily gap towards the cut-off date increases the performance goes down. The drop is greatest from day 0 to day 2, thanks to very high precision achieved in day 0. The decrease continues only until the day 7 and 8. We can observe a drop of precision from day 8 to 7 and in some of the models a peak from 9 to 8. This drop can be explained looking at the performance of the baseline classifier, because these days are typical for students with completely no ac- tivity so far to withdraw from the course, meaning that this low-hanging fruit disappears from the data and classifiers focused on them drop in performance.  7.5 Intervention strategy recommendation The predictions are being used by tutors to spot at-risk  students and make an appropriate intervention if necessary. Based on the results, we also suggest when might be the right time to intervene.  Given the graphs from 8, we tried to find the most suitable k and day for predictions and interventions with students. Because the drop from top 10 to top 25 %, k=10 seems like a reasonable choice. Intervening in day 0 might be very    Table 5: Ouroboros vs training on the legacy data vs PREV MODEL using F1-score, Precision and Recall. Course Days to cut-off PREV MODEL Ouroboros best Prev. presentation  F1 Prec. Recall F1 Prec. Recall F1 Prec. Recall B 5 0.1741 0.5124 0.1049 0.2808 0.1635 0.9949 0.3592 0.3034 0.4400 B 0 0.1633 0.7031 0.0924 0.6724 0.5751 0.8093 0.4503 0.3725 0.5692 D 6 0.3072 0.3615 0.2670 0.2596 0.1495 0.9847 0.3109 0.1843 0.9924 D 0 0.3740 0.5476 0.2840 0.5534 0.3986 0.9048 0.3026 0.1784 0.9960 E 5 0.5678 0.6505 0.5038 0.3511 0.2139 0.9792 0.5792 0.6752 0.5072 E 0 0.6857 0.7579 0.6261 0.6528 0.5044 0.9247 0.6718 0.6804 0.6633 F 3 0.3931 0.4191 0.3701 0.3366 0.1858 0.9898 0.5618 0.5303 0.1711 F 0 0.5134 0.5583 0.4752 0.7131 0.6698 0.7624 0.5979 0.6170 0.5800  Figure 8: Top-K-Precision for k = 5,10,25  Figure 9: Top-K-Recall for k = 5,10,25  accurate but intuitively its too late to provide students with any help. There is a drop from day 3 to 4 and from day 6 to 7 in precision but peak for 3 a 6 in Recall for XGBoost.  Given this information, one reasonable strategy might be to use the XGBoost model for Top-5-Precision 6 days before the cut-off and Top-10-Precision model on day 3. Using Ouroboros approach both should provide average precision around 0.4 and Recall 0.6.  7.6 Feature importance Apart from SVM, most of the used models enable to ex-  tract importance of the features used for prediction easily. We selected XGBoost as the best performing classification model on average, especially further from the cut-off. Then, we extracted top 5 ranked features for the analysed courses  in days 0, 3 and 7. Table 6 shows that across all the courses the most impor-  tant factors are coming from the specific usage of the VLE and the VLE statistics. While VLE statistics prevail across all the selected days, the specific VLE activity type impor- tance varies. On the cut-off date, login information appears among the most important factors and in two of the courses visiting forum becomes important predictor.  8. DISCUSSION Though the analysed courses come from different fields,  the relatively small number of courses under analysis do not allow us to investigate the dependence of the performance on the discipline. For example, Table 5 shows that the lowest    Table 6: Most important features for XGBoost  # Course Day0 Day3 Day7 1 B first.login days.fromvleopen days.fromvleopen 2 B clicks.fromstart is.click.7.subpage resources.fromvleopen 3 B max.mat.beforestart is.click.6.oucontent clicks.9.oucontent 4 B clicks.fromvleopen clicks.2.forum clicks.9.subpage 5 B last.login.rel avg.mat.cnt.fromstart.peractive clicks.13.oucontent 1 D days.fromstart clicks.fromvleopen clicks.30.oucontent 2 D last.login.rel clicks.1.oucontent clicks.5.glossary 3 D studied.credits resources.fromvleopen resources.fromvleopen 4 D clicks.6.forum clicks.38.oucontent clicks.4.glossary 5 D clicks.9.oucontent clicks.7.glossary clicks.2.glossary 1 E last.login.rel clicks.34.oucontent clicks.36 2 E min.click.fromstart.peractive min.click.fromstart.peractive clicks.20.quiz 3 E clicks.3 clicks.11.quiz clicks.31 4 E median.mat.cnt.beforestart.peractive clicks.18.quiz clicks.6.url 5 E clicks.fromvleopen clicks.12.subpage min.click.beforestart.peractive 1 F last.login.rel clicks.fromvleopen days.fromvleopen 2 F days.fromstart min.click.fromstart.peractive clicks.8.htmlactivity 3 F clicks.fromvleopen clicks.33.subpage is.click.1.oucontent 4 F days.fromvleopen clicks.4 is.click.22.ouwiki 5 F clicks.6.forum clicks.10.resource is.click.9.forum  F1 score is for course D, however, a deeper analysis would be required to support the claim that the field of the course D influenced the classifier performance.  8.1 Usage in different contexts The proposed method is not limited only to A1 at OU  and given several conditions it can be used without adapta- tion for further assessments and in other contexts, such as other distant educations or MOOCs. There need to be (1) a task/event with specified cut-off date and also (2) students that fulfil it in advance. It can also be used to different kind of tasks such as whether students will register for a course. Although we didnt examine this yet, we expect that this approach can be used for a wide class of problems outside the Learning Analytics field, given that they satisfy the con- ditions mentioned above.  8.1.1 Limitations When there is no deadline specified, the method would  need to be adapted to treat the window for training the model differently. Then we would be able to use the same approach for predicting dropout of students or potentially if students will register for paid certificate in MOOCs. Sim- ilarly, for the second condition, the approach wouldnt be suitable for High Schools scenario predicting whether the students will finish the studies in time because the students are not expected to complete it in advance.  9. CONCLUSIONS AND FUTURE WORK This paper introduced Ouroboros, the novel approach to  early identification of at-risk students in the courses with- out legacy data, i.e. data from previous presentations. Our method utilises the importance of the first assessment be- ing a critical milestone in the progress of the course. The key idea is that the learning patterns can be extracted from the behaviour of students who have already submitted their assessment earlier.  We defined the problem as a binary classification task  with the goal being able to learn and predict daily using the widening window. The approach was evaluated on the publicly available OULAD dataset using 4 level one courses. The experiments showed that the method can successfully predict at-risk students, for the day 0 and 1 it gives better results than training using the legacy data.  Analysis of feature importance of XGBoost as the best performing algorithm showed that specific VLE activities are important for predicting at-risk students together with statistical information about VLE usage.  In the further work, we want to explore how the self- learning model is performing in the later phases of the cour- ses. Also, we want to combine both models to improve pre- dictions even for the training using the legacy data. The large-scale analysis might reveal the influence of the learn- ing objective or field on the classifier.  Acknowledgement This work was partially supported by the institutional re- sources for research by the Czech Technical University in Prague, Czech Republic and The Ministry of Education, Youth and Sports from the National Programme of Sus- tainability (NPU II) project IT4Innovations excellence in science - LQ1602.  10. REFERENCES [1] E. Aguiar, H. Lakkaraju, N. Bhanpuri, D. Miller,  B. Yuhas, and K. L. Addison. Who, when, and why: A machine learning approach to prioritizing students at risk of not graduating high school on time. In LAK 15, 93102, New York, NY, USA, 2015. ACM.  [2] J. Bainbridge, J. Melitski, A. Zahradnik, E. Laura, S. M. Jayaprakash, and J. Baron. Using Learning Analytics to Predict At-Risk Students in Online Graduate Public Affairs and Administration Education. The JPAE Messenger, 21(2):247262, 2015.    [3] R. S. Baker, D. Lindrum, M. J. Lindrum, and D. Perkowski. Analyzing early at-risk factors in higher education e-learning courses. In Proceedings of the 8th International Conference on Educational Data Mining, EDM 2015, Madrid, Spain, June 26-29, 2015, 150155, 2015.  [4] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. CoRR, abs/1603.02754, 2016.  [5] J. Davis and M. Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd international conference on Machine learning, 233240. ACM, 2006.  [6] H. He and E. A. Garcia. Learning from imbalanced data. IEEE Trans. on Knowl. and Data Eng., 21(9):12631284, Sep 2009.  [7] J. He, J. Bailey, B. I. Rubinstein, and R. Zhang. Identifying at-risk students in massive open online courses. In AAAI, 17491755, 2015.  [8] S. Huang and N. Fang. Predicting student academic performance in an engineering dynamics course: A comparison of four types of predictive mathematical models. Comput. Educ., 61:133145, Feb 2013.  [9] S. M. Jayaprakash, E. W. Moody, E. J. M. Lauria, J. R. Regan, and J. D. Baron. Early Alert of Academically At-Risk Students: An Open Source Analytics Initiative. Journal of Learning Analytics, 1(1):647, 2014.  [10] S. Jiang, M. Warschauer, A. E. Williams, D. ODowd, and K. Schenke. Predicting mooc performance with week 1 behavior. In EDM14, 273275, 2014.  [11] R. A. Johnson, R. Gong, S. Greatorex-Voith, A. Anand, and A. Fritzler. A data-driven framework for identifying high school students at risk of not graduating on time. Bloomberg Data for Good Exchange Conf., 5, 2015.  [12] K. Jordan. Mooc completion rates: The data. http://www.katyjordan.com/MOOCproject.html, 2015. Accessed: 2016-06-10.  [13] R. R. Kabra and R. S. Bichkar. Performance prediction of engineering students using decision trees. International Journal of Computern Applications, 36(11):812, December 2011.  [14] G. Kena, J. W. X. R. A. Musu-Gillette, Laurenand Robinson, J. Zhang, S. Wilkinson-Flicker, A. Barmer, and E. D. V. Velez. The condition of education 2015. Technical Report 2015-144, NCES, May 2015.  [15] M. Kloft, F. Stiehler, Z. Zheng, and N. Pinkwart. Predicting mooc dropout over weeks using machine learning methods. In Proceedings of the EMNLP 2014 Workshop on Analysis of Large Scale Social Interaction in MOOCs, 6065, 2014.  [16] D. Koller, A. Ng, C. Do, and Z. Chen. Retention and intention in massive open online courses: In depth. EDUCAUSE, http://www.educause.edu/ero/article/ retention-and-intention-massive-open-online-courses-depth-0, Jun 2013. [Online; posted 3-June-2013].  [17] J. Kuzilek, M. Hlosta, and Z. Zdrahal. Open university learning analytics dataset. In Data literacy for Learning Analytics workshop at LAK16, 26th April 2016, Edinburgh, UK, 9, 2016.  [18] H. Lakkaraju, E. Aguiar, C. Shan, D. Miller,  N. Bhanpuri, R. Ghani, and K. L. Addison. A machine learning framework to identify students at risk of adverse academic outcomes. 19091918, 2015.  [19] M. Pandey and V. K. Sharma. A decision tree algorithm pertaining to the student performance analysis and prediction. International Journal of Computern Applications, 61(13):15, January 2013.  [20] F. Pedregosa, G. Varoquaux, and e. Gramfort. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011.  [21] J. Quinn. Drop-out and completion in higher education in europe among students from under-represented groups. Technical report, European Commission, Oct 2013.  [22] O. Simpson. 22% - can we do better In The CWP Retention Literature Review, 47, 2010.  [23] C. Taylor, K. Veeramachaneni, and U. OReilly. Likely to stop predicting stopout in massive open online courses. CoRR, abs/1408.3382, 2014.  [24] N. Thai-Nghe, A. Busche, and L. Schmidt-Thieme. Improving academic performance prediction by dealing with class imbalance. In Ninth International Conference on Intelligent Systems Design and Applications, ISDA 2009, Pisa, Italy , November 30-December 2, 2009, 878883, 2009.  [25] H. Vossensteyn, A. Kottmann, B. Jongbloed, and F. Kaiser. Drop-out and completion in higher education in europe executive summary. Technical report, European Commission, 2015.  [26] C. Wladis, A. C. Hachey, and K. M. Conway. An investigation of course-level factors as predictors of online STEM course outcomes. Computers & Education, 77:145150, 2014.  [27] A. Wolff, Z. Zdrahal, D. Herrmannova, J. Kuzilek, and M. Hlosta. Developing predictive models for early detection of at-risk students on distance learning modules. In Machine Learning and Learning Analytics workshop at LAK14, 24-28 March 2014, Indianapolis, Indiana, USA, 4, 2014.  [28] C. Ye and G. Biswas. Early prediction of student dropout and performance in moocs using higher granularity temporal information. Journal of Learning Analytics, 1(3):169172, 2014.    "}
{"index":{"_id":"3"}}
{"datatype":"inproceedings","key":"Agnihotri:2017:ISC:3027385.3027437","author":"Agnihotri, Lalitha and Essa, Alfred and Baker, Ryan","title":"Impact of Student Choice of Content Adoption Delay on Course Outcomes","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"16--20","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027437","doi":"10.1145/3027385.3027437","acmid":"3027437","publisher":"ACM","address":"New York, NY, USA","keywords":"content adoption delay, effect size, performance, procrastination","Abstract":"It is difficult for a student to succeed in a course without access to course materials and assignments; and yet, some students delay up to a month in obtaining access to these essential materials. Students delay buying material required for their course due to multiple reasons. Out of a concern for students with limited financial resources, some publishers offer a period of free courtesy access. But this may lead to students having access later in the course but then having a lapsed period until they pay for the materials after the courtesy access period ends. Not having key course materials early on probably hurts learning, but how much? In this paper, we investigate the question, Does lack of access to instructional material impact student performance in blended learning courses? Specifically, we analyze students who purchased and obtained access to online content at different points in the course. We determine that both types of failure to obtain access to course materials (delaying in signing up for the product, or signing up for a free trial and letting the trial period lapse without purchasing the materials) are associated with substantially worse student outcomes. Students who purchased the product within the first few days of class had the best scores (median 77). Those who waited two weeks before accessing the product did the worst (median 56, effect size Cliff's Delta","pdf":"Impact of Student Choice of Content Adoption Delay on Course Outcomes  Lalitha Agnihotri McGraw-Hill Education  2 Penn Plaza New York, NY, USA lalitha.agnihotri  @mheducation.com  Alfred Essa McGraw-Hill Education  281 Summer Street, 7th Floor Boston, MA, USA  alfred.essa @mheducation.com  Ryan Baker University of Pennsylvania  3700 Walnut St. Philadelphia, PA, USA  rybaker@upenn.edu  ABSTRACT It is dicult for a student to succeed in a course without access to course materials and assignments; and yet, some students delay up to a month in obtaining access to these essential materials. Students delay buying material required for their course due to multiple reasons. Out of a concern for students with limited financial resources, some publish- ers oer a period of free courtesy access. But this may lead to students having access later in the course but then hav- ing a lapsed period until they pay for the materials after the courtesy access period ends. Not having key course mate- rials early on probably hurts learning, but how much In this paper, we investigate the question, Does lack of ac- cess to instructional material impact student performance in blended learning courses Specifically, we analyze stu- dents who purchased and obtained access to online content at dierent points in the course. We determine that both types of failure to obtain access to course materials (delay- ing in signing up for the product, or signing up for a free trial and letting the trial period lapse without purchasing the materials) are associated with substantially worse stu- dent outcomes. Students who purchased the product within the first few days of class had the best scores (median 77). Those who waited two weeks before accessing the product did the worst (median 56, eect size Clis Delta=0.31 1). We conclude with a discussion of possible interventions and actions that can be taken to ameliorate the situation.  CCS Concepts Information systems!Data mining; Applied com- puting ! Education; E-learning;  1For Clis Delta a small eect size is around 0.147, a medium eect size around 0.33, and a large eect size around 0.474.  Permission to make digital or hard copies of all or part of this work for personal  or classroom use is granted without fee provided that copies are not made or  distributed for profit or commercial advantage and that copies bear this notice  and the full citation on the first page. Copyrights for components of this work  owned by others than the author(s) must be honored. Abstracting with credit is  permitted. To copy otherwise, or republish, to post on servers or to redistribute to  lists, requires prior specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-4870-6/17/03$15.00 DOI: http://dx.doi.org/10.1145/3027385.3027437  Keywords eect size, procrastination, content adoption delay, perfor- mance  1. INTRODUCTION There is evidence that students benefit when they start  their work early rather than waiting to start or procrasti- nating [4, 6]. However, despite the evidence for benen  ,  Ats from getting a prompt start to a course, there is emerg- ing evidence that many learners do not even purchase their course materials until one or even two weeks after the course has started. In this paper, we study the impact of this stu- dent choice on course outcomes, and propose interventions that may have the potential to reduce negative outcomes stemming from this choice.  More specifically, in this paper, we investigate the ques- tion Does lack of access to textbooks and digital instruction resource significantly aect learning performance Students delay getting access to material required for their course for all sorts of reasons. Not having key course materials early on probably hurts learning, but how much There are mul- tiple reasons to look into this question: Many instructors believe that it has an eect when a student delays getting the materials required for their course. But how much do their grades suer And how long can a student delay on this before there is a detrimental enAect And if it does impact outcomes, what interventions can we apply in order to insure that student indeed do get access to material in a timely fashion We will discuss some ways that it may be possible to intervene and address the issue.  2. LITERATURE REVIEW There are multiple reasons why students may delay in  purchasing their course materials. For some students it may simply be procrastination [10]. 87% of the 13,000 high school and college students surveyed by StudyMode.com admitted to procrastinating. 45% of students surveyed reported that they believe that their procrastination negatively impacts their grades on at least a fairly regular basis. Other students may be trying to decide what course to take. For some students, it simply comes down to the fact that they do not receive their financial aid check until two weeks in the semester, and they can not aord to buy materials before then. Research has shown that even opening the text book prior to the start of course is predictive of success in the course [4].    Agnihotri and Ott also determined that another form of procrastination, late registration, is associated with lower fall-to-fall student retention [2]. In addition, Levy and Rahm [8] found that students who procrastinated performed signif- icantly worse than those who completed their work in a more timely fashion. Results of this study indicate that when it comes to online exams, over half (58%) of the students tend to procrastinate, while the rest (42%) started the exam well before the deadline and avoided procrastination.  Jayaprakash et al., determined that course success can be predicted from the students interaction with the learning management system [7]. Predictive models have also been developed by Civitas and deployed at a range of instituions [9]. Their predictive models were able to identify with 83% accuracy on the first day of a course the students who would successfully complete a course based on ACT scores, SAT scores and economic factors.  3. METHOD In order to estimate the impact of student choice about  when to obtain access to content, and when students pur- chase access to content, we look at the dierences between dierent groups of students. Specifically, we dierentiated groups of students from each other in terms of how long they chose to go without access to course materials.  We study this in the context of the Connect system [1]. Connect is an open learning environment for students and instructors in the higher education market. In this analy- sis, we examine this utilized about 2.6 million students who used Connect in 2015. These students were in 145,115 course sections taught by 14,000 instructors, who created 89 mil- lion assignments using about 2000 textbooks/course mate- rial packets. The majority (75%) of the students who obtain access to Connect purchase access outright. However, there is an option for students to try it for free for two weeks (termed Courtesy Access) and then convert it to full access at a later date. Of the students who opt for Courtesy Access, 80% convert to full access.  For all the students we obtained data about when the got access to Connect. Additionally we obtained the start date of the class. We use this information to compute two vari- ables: Start delay is defined as how many days after the start of the course the student first obtained access to the online content, whether by purchasing the course or obtain- ing courtesy access. Conversion delay, defined only for those students who obtained courtesy access and then eventually purchased access, is the number of days between when their courtesy access period started and when they converted to full access. Since the courtesy access period is two weeks, students with a conversion delay of two weeks or less have a conversion delay of zero. We also obtained data on students assignment scores and quiz scores, and computed their final scores for the class based on this data. We compute these scores in two ways. The first, termed ScoreCompleted, is a strict average of all the scores students have received on the assignments/quizzes etc. that they submitted. The second one, termed ScoreAll, shows the score with the impact of missed assignments factored in. In other words, if a student failed to do two assignments due to not having the materi- als for two weeks, ScoreAll will directly penalize them but ScoreCompleted will not.  A quick analysis of our data showed that these variables were largely non-normal. As such, we compared the scores  Figure 1: Histogram of aquiring access relative to start of semester.  of students who obtain access of the book at dierent points using the Clis Delta eect size measure [5]. The Clis Delta statistic is a non-parametric eect size measure that quantifies the amount of dierence between two groups of observations. This eect size measure is used for non-normal distributions; an analogue for normal distributions is Co- hens D. Clis Delta was chosen for its particularly high robustness to unusual data distributions; other alternatives such as Alginas D control for outliers but not for bimodality or extremely high skew.  4. CONVERSION DATES AND PERFORMANCE Our data set consisted of:   2.6 million students in 145,115 sections in 2015, who made a total 3.2 million purchases   2.4 million (75%) outright purchases (i.e. without first signing up for a Courtesy Access period)   818k (25%) Courtesy Access trials: 633k (77% of 818k) purchases after trial, 185k (23% of 818k) trials without purchases  Figures 1 and 2 show the histogram of getting access to the course material relative to the start of the semester (start delay on the X axis vs. counts on the Y axis) and the con- versions relative to the start of courtesy access (conversion delay on the X axis vs. counts on the Y axis). We track up to 30 days after the start of courtesy access in our data. 47% of the student get access to content (full or courtesy access) in the first 4 days of the semester. Another 38% happens between the 5th-12th days and finally 14% occur 12 or more days after the semester starts. Very few students obtain access to course materials prior to the ocial start date of the course, a contrast to the results presented in [4]. This is largely because the way the courses are set up; students typically receive the link to obtain course materials on the first day of class from the instructor.  In terms of conversions from Courtesy Access to full (paid) access, 54.0% of conversions happen in less than 14 days, a time window where the student has no lapse in their ac- cess to content. Another 20.0% conversions happen in 14-16 days, suggesting fairly limited time lapse and fairly limited disruption to the students studies. In fact, 14 days is the modal day for conversion. However, a sizable 26.0% of con- versions happen after 16 days of the start of Courtesy Ac- cess. And 7% of conversions occur more than 21 days after    Figure 2: Histogram of conversion dates relative to start of Courtesy Access.  Figure 3: ScoreCompleted and ScoreAll relative to start of semester.  the start of Courtesy Access, indicating that the student is without access for the whole week (see Figure 2).  Figures 3 and 5 show the student assignment/quiz scores relative to the two delays we have talked about. In each figure, the first plot shows the ScoreCompleted vs. the time delay and the second one shows the ScoreAll which considers the missed assignments as well. The graphs in figure 3 show that performance on the completed assignments (ScoreCom- pleted) drops a bit for students who delay in getting ac- cess once the semester starts. Figure 5 shows the same but with respect to getting full access (conversion delay) to the product after starting free courtesy access. A student who obtains access on the first day of the course and immedi- ately purchases access will have an median ScoreCompleted of 89%. By contrast, a student who waits 14 days to ob- tain access will have a median ScoreCompleted of 84%. The ScoreAll for students who get acess on the first day of the class is 81%. A student who waits 14 days to obtain ac- cess will have an average ScoreAll of 67.5%, and a student who waits a full week or more to convert to full accesss after their 2-week Courtesy Access period ends (i.e. 21 days after obtaining Courtesy Access) will have an average ScoreAll of 64%.  These results suggest that students who choose (for what- ever reason) to not have access to course materials for a period of time have worse outcomes, but that much of this dierence (81% to 67%) is due to missing assignments rather than worse performance on the assignments they complete.  Figure 4: ScoreCompleted and ScoreAll relative to the start of the Courtesy Access.  Figure 5: Heat map of scores for dierent start and purchase delays.  This is reassuring, because it suggests that encouraging stu- dents to purchase or obtain their materials in a timely fash- ion has the potential to ameliorate the missed assignments problem, providing students with a chance to perform bet- ter in the course (and learn all the material). Of course, encouraging students to purchase or obtain their materials in a timely fashion will not benefit all students; for exam- ple, students who cease participation in the course for a week due to a personal or family emergency are unlikely to be benefitted. But positive impact may be possible for the students who fail to purchase or obtain their materials due to simple procrastination [6, 8]. After all, no matter how bright a student is, he or she cannot successfully complete an assignment that he or she does not have access to.  Of course, many of the students who have a start delay will also have a purchase delay. The same factors that lead to one may lead to the other. The relationship between start delay and conversion delay, and the associated scores, are shown in figure . The x-axis shows the start delay for getting access to the online content. The y-axis shows the conversion delay, the time the student delayed between obtaining Courtesy Access and purchasing full access. The size of the circle indicates the number of people in that group. The color indicates the median score of the students in that cohort. As can be seen, students have relatively better scores when the start delay is less than 4 days and the conversion delay is less than 14 days. When the start delay or conversion delay go above these numbers, the student is likely to obtain a lower score.    5. MAGNITUDE OF DIFFERENCES In order to quantify the eects of start delay and con-  version delay after signing up for courtesy access, we com- puted Clis Delta eect sizes on ScoreAll between groups of students who delayed for dierent amounts of time. Clis delta or d [5] is a measure of how often one the values in one distribution are larger than the values in a second distribu- tion. Crucially, it is non-parametric and does not require any assumptions about the shape or spread of the two dis- tributions. The sample estimate d is given by:  d = #(xi > xj)#(xi < xj)  mn  (1)  where the two distributions are of size n and m with items xi and xj , respectively, and # is defined as the number of times. d is linearly related to the Mann-Whitney U statistic, however it captures the direction of the dierence in its sign which is important to us in this study. Clis delta ranges from +1 when all the values in one group are higher than the values of the other group, in the expected directionand -1 when the reverse is true. Two completely overlapping distri- butions will have a Clis delta of 0. ClinAaAZs delta eval- uates the degree of overlapping between two vectors of ob- servations. A less raw interpretation, is to use conventional descriptors like Cohens d (small, medium, large), which are explicitly conventional according to Cohen. For Clis Delta absolute value you have a small eect size around 0.15, a medium eect size around 0.33, and a large eect size around 0.50.  We computed the Clis delta for each of the combinations of the start delay and conversion delay. More specifically, for the start delay, we computed Clis delta measure for all the students scores with start delay less than or equal to a vs. start delay greater than a, where a takes values from 2 to 25. So in the above equation, we set xi to be student scores whose start delay is less than or equal to a and xj is the student scores for the rest of the students. We then found the start delay, a that resulted in the maximum Clis delta. Also, we computed the eect size for students with start de- lay less than or equal to a vs. students with start delay greater than b, where b takes all possible values from a to 25. So in the above equation, we set xi to be student scores whose start delay is less than or equal to a and xj is the stu- dent scores for students with start delay greater than b. We repeated the procedure for delay in converting to full access after obtaining 2 week courtesy access, as well. We want to find automatic cuto points where there was maximum impact on the students scores. We finally repeated the pro- cedure with dierent combinations of start and conversion delays. To get the results we ran about 4000 dierent com- binations of dierent start and conversion times to get all the dierent Clis delta.   For students with start delay less than or equal to 12 days, the median score is 74.4% vs. students with a start delay of more than 12 days, the median score was 62.7%. Clis delta was 0.17.   For students with start delay less than 3 days, the median score is 76.7% vs. students with a start delay of more than 12 days, the median score was 62.7%. Clis delta was 0.20.   For students with conversion delay less than 19 days, the median score is 73.5% vs. students with a conver-  Figure 6: Score distributions of students with start delay less than 3 days and conversion delay less than 15 days.  Figure 7: Score distributions of students with start delay greater than 15 days and conversion delay greater than 23 days.  sion delay of 19 days or more, the median score was 63.9%. Clis delta was 0.14.   For students with conversion delay less than 16, the median score is 73.6% vs. conversion delay greater than 22, the median score was 60.4%. Clis delta was 0.19.  We then found automatic cut-os for combinations of both the start delay and conversion delay:   The Clis delta students with start delay less than 3 days and conversion delay less than 23 days (Me- dian score 76.9%) vs. all other students (Median score 60.3%) is 0.25   For varying start and conversion delays, students with start delay less than 3 days and conversion delay less than 15 days do much better (Median score 77.3%) than students who get access 15 days of the start of the semester and have a conversion time greater than 23 days (Median score 56.4%). Clis delta is 0.31.  Overall, then, the students who have the highest perfor- mance in their courses access the course materials within the first few days after the start of the class. If they opt for the free courtesy access, then they are more successful if    they convert to full access before they lose access to content. The worst choice is to wait for two weeks or more to obtain access to content and then let the courtesy access lapse for a week or more before converting to full access. Figures 6 and 7 show the distribution of scores for these two extremes. The odds ratio of the second group getting a score less than 60 is 2.44 and the risk ratio of getting this score and possibly failing the course is 1.68.  6. INTERVENTION POSSIBILITIES While our results are correlational, they nonetheless show  large dierences in student outcomes based on when stu- dents access course materials. These findings therefore war- rant intervention studies that can both validate whether these findings are causal, while testing interventions that may be able to improve student outcomes. The findings presented here suggest that there is the opportunity for im- proving student outcomes if we can convince students to access course materials from the beginning, and to avoid lapses in access.  One clear intervention is to simply give free access to every student. Unfortunately, as the Connect product team and project researchers need to earn money in order to eat, this solution is probably infeasible. However, to the extent that some failure to purchase course materials is due to student economic situations, such as delays in students receiving fi- nancial aid (students also need to eat), it may be possible for universities to arrange support for their students so that they can purchase materials on time. The two-week Cour- tesy Access period was originally designed with this in mind, but does not seem to be sucient.  A related intervention, sometimes termed inclusive ac- cess, is to set up a university-wide program to automati- cally provide all students with courtesy access to the online content at the beginning of class. If they drop the course, the content is not charged. This will help students who tend to procrastinate get access to content and facilitates coordination at the university level between when the stu- dent receives financial aid and when they are charged for the course materials.  Where this type of program is infeasible, other solutions may help students who delay in obtaining or purchasing ac- cess due to reasons such as procrastination. One approach is to work with instructors to emphasize to students the importance of getting access to the course material from the beginning. For example, it may be possible to create infographics that can be shared with instructors showing them the impact of delays in students obtaining access to content. Another potentially useful approach may be to nudge students to buy the product when the courtesy ac- cess lapses. Previous work has shown the benefits obtained from instructors sending email messages to students at risk of poorer performance, explaining why they are at risk [3].  7. DISCUSSIONS AND CONCLUSIONS These findings indicate that it is important for students  to get going quickly and avoid delay. Getting o to a fast start seems to be important for student success. One limita- tion to our findings, however, is that they are correlational rather than causal. Investigating the degree to which these findings are causal, through an experimental study, will be  an important step for future work. What can we do to improve outcomes It may be valuable  to set up inclusive access, where students have free trials that last until they can be expected to receive financial aid checks. Additionally, instructors should emphasize to students that it is important to sign up for access to the course material from the beginning. Finally, students should be nudged to buy the product when the trial period lapses, in order to avoid having a period of time where they dont have access to their learning materials.  Ultimately, taking a college course without access to the learning materials is not a recipe for success. Determining which interventions can feasibly increase student access to course materials may be a valuable step towards improving student outcomes.  8. ACKNOWLEDGMENTS The authors would like to thank Stefan Slater for Python  code for computing Clis Delta in linear time. We would also like to acknowledge Shirin Mojarad and Nick Lewkow for their suggestions for performing analysis.  9. REFERENCES [1] Connect, open learning platform. http://www.  mheducation.com/highered/platforms/connect.html. Accessed: 2016-10-09.  [2] L. Agnihotri and A. Ott. Who are your at-risk students using data mining to target intervention eorts. In Natl Symp on Student Retention, Nov 2013.  [3] K. E. Arnold and M. D. Pistilli. Course signals at purdue: Using learning analytics to increase student success. In Proceedings of the 2Nd Intl Conf on LAK, LAK 12, 267270, NY, USA, 2012. ACM.  [4] R. S. Baker, D. Lindrum, M. J. Lindrum, and D. Perkowski. Analyzing early at-risk factors in higher education elearning courses. In Proceedings of the 8th Intl Conf on Educational Data Mining, 150155 Nov 2015.  [5] N. Cli. Dominance statistics: Ordinal analyses to answer ordinal questions. Psychological Bulletin, 494509, 1993.  [6] A. Hershkovitz and R. Nachmias. Learning about online learning processes and students motivation through web usage mining. IJELL(5), 197-214, Nov 2009.  [7] S. M. Jayaprakash, E. W. Moody, E. J. Lauria, J. R. Regan, and J. D. Baron. Early alert of academically at-risk students: An open source analytics initiative. Journal of Learning Analytics, 1(1):647, 2014.  [8] Y. Levy and M. M. Ramim. A study of online exams procrastination using data analytics techniques. IJELL (8), 41-49, 2012.  [9] M. D. Milliron, L. Malcolm, and D. Kil. Insight and action analytics: Three case studies to consider. Research and Practice in Assessment (9), 7089, 2014.  [10] StudyMode. Eighty-seven percent of high school and college students are self-proclaimed procrastinators. https://goo.gl/C0XdZQ, May 2014.      "}
{"index":{"_id":"4"}}
{"datatype":"inproceedings","key":"Park:2017:DCS:3027385.3027430","author":"Park, Jihyun and Denaro, Kameryn and Rodriguez, Fernando and Smyth, Padhraic and Warschauer, Mark","title":"Detecting Changes in Student Behavior from Clickstream Data","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"21--30","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027430","doi":"10.1145/3027385.3027430","acmid":"3027430","publisher":"ACM","address":"New York, NY, USA","keywords":"change detection, poisson models, regression, student clickstream data","Abstract":"Student clickstream data can provide valuable insights about student activities in an online learning environment and how these activities inform their learning outcomes. However, given the noisy and complex nature of this data, an on-going challenge involves devising statistical techniques that capture clear and meaningful aspects of students' click patterns. In this paper, we utilize statistical change detection techniques to investigate students' online behaviors. Using clickstream data from two large university courses, one face-to-face and one online, we illustrate how this methodology can be used to detect when students change their previewing and reviewing behavior, and how these changes can be related to other aspects of students' activity and performance.","pdf":"Detecting Changes in Student Behavior from Clickstream Data  Jihyun Park Department of Computer Science  University of California, Irvine Irvine, CA 92697  jihyunp@ics.uci.edu  Kameryn Denaro Teaching and Learning  Research Center University of California, Irvine  Irvine, CA 92697 kdenaro@uci.edu  Fernando Rodriguez School of Education  University of California, Irvine Irvine, CA 92697  fernanr1@uci.edu  Padhraic Smyth Department of Computer Science  University of California, Irvine Irvine, CA 92697  smyth@ics.uci.edu  Mark Warschauer School of Education  University of California, Irvine Irvine, CA 92697  markw@uci.edu  ABSTRACT Student clickstream data can provide valuable insights about student activities in an online learning environment and how these activities inform their learning outcomes. However, given the noisy and complex nature of this data, an on- going challenge involves devising statistical techniques that capture clear and meaningful aspects of students click pat- terns. In this paper, we utilize statistical change detection techniques to investigate students online behaviors. Us- ing clickstream data from two large university courses, one face-to-face and one online, we illustrate how this method- ology can be used to detect when students change their pre- viewing and reviewing behavior, and how these changes can be related to other aspects of students activity and perfor- mance.  CCS Concepts Information systems Data mining; Web log anal- ysis; Computing methodologies  Machine learn- ing approaches; Applied computing Learning man- agement systems;  Keywords Student clickstream data; Change detection; Regression; Pois- son models  1. INTRODUCTION One of the major goals in educational data mining (EDM)  is to use student clickstream data to describe and under- stand students behavioral patterns. While past findings  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13-17, 2017, Vancouver, BC, Canada c 2017 ACM. ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027430  have advanced our understanding what we can learn from clickstream data, one of the remaining challenges involves devising statistical techniques that help us identify students who are changing behavior in the middle of a term. There are a number of reasons motivating this problem; one is to identify students who are in need of assistance during the course, another is to identify reasons that students are changing their behavior so that a course could be improved overall. The analysis of clickstream data within a course can also provide invaluable information to course instructors and to education researchers, and there is a need to be able to both summarize and visualize the results in a straightfor- ward manner.  In this paper we will focus on clickstream data from two courses at a large university: one face-to-face course and one online course, both from the 2015-2016 academic year. For each course, clickstream data is obtained through a course management system in the form of {student ID, time stamp, activity}. The types of activities recorded correspond to broad categories of student behavior, such as previewing lecture notes, submitting assignments, or posting and re- sponding to discussion board questions. For instance, one of the courses we examine in this paper had 377 registered students who generated approximately 380,000 click events over a 10-week period. Figure 1 displays each of the indi- vidual student clickstreams over the 85 days of the course, with each row corresponding to a student. While the plot shows some general increases in click activities around quiz and exam dates, it is not easy to see much else, nor to un- derstand how individual student behaviors are related to the overall population due to significant variability in students click patterns. Furthermore, we are unable to determine whether students change their click behaviors in any signif- icant way, or whether or not these behaviors are correlated with course performance.  As discussed in more detail in the next section, student clickstream data has been the subject of a number of prior studies, such as the investigation of potential predictive re- lationships between online student activity and student out- comes (such as course grades). Here we focus instead on detecting changes in individual student activity over time, relative to the activity of the class of a whole. In particular  http://dx.doi.org/10.1145/3027385.3027430   DAYS  S T U D E N T S  0  1  Figure 1: A plot of student clickstream activity in the 10-week face-to-face course over time, where each row represents an individual student and each column represents a day. A black marker in cell i, t indicates clickstream activity for student i on day t.  we investigate the use of statistical change detection tech- niques (e.g., [9]) to automatically detect changes in activity over time for each student. We model the activity of each student relative to the aggregate activity of all students in the class and compare two models on a per student basis; a model where there is no change in student activity versus a model where there is a significant change in activity at some unknown point during the period of the course. Likelihood- based techniques are used to fit both models on a per student basis and model selection criteria is implemented in order to determine whether each student is best modeled under the change or no-change model.  The paper proceeds as follows. In Section 2 we discuss re- lated work. Section 3 outlines the change-detection method- ology that we propose, and Section 4 provides illustrative re- sults on simulated data sets. Section 5 discusses the course data sets that provide an illustration of the methods dis- cussed in Section 3 and Section 6 describes the results of applying our change-detection methodology to these data sets. The paper concludes with discussion and conclusions in Section 7. The primary novel contribution of this work is the development of a systematic quantitative approach for detecting significant changes in a students clickstream over time.  2. RELATED WORK Clickstream data analysis in an educational setting has fo-  cused on what the clickstream can say about the students in terms of learning behavior through a variety of features de- rived from the clickstream. Much of the prior work on click- stream data analysis for understanding student behavior has occurred in the context of Massive Open Online Courses (MOOC) setting. Many of these analyses have focused on using the clickstream data to predict MOOC completion (for example in [5]) and to predict learning outcomes within a MOOC. For example, the relationship between the number of posts and the learning gains of the students [16] has been investigated, as well as how discussion forum views are po- tentially related to learning outcomes [1]. There has also been research focused on improving predictions of learning outcomes by incorporating clickstream events as well as sum-  maries of the clickstream [3]. A secondary research topic has focused on describing stu-  dents with similar clickstreams (e.g., [15]), the activities that the students are engaging in, and in understanding the stu- dents typical online interaction within a class. As an exam- ple, clickstream data analysis was used to better understand whether or not students were following a defined learning path [6]. In other work, students clickstreams were grouped into similar plans of action to better understand learning pathways [14]; how discussion forums and other activities in the MOOC were related to country and culture [12]; and ex- amined whether engagement on discussion forums increased based on the type of video a student watched [2]. All of these clickstream analyses have an underlying goal of de- scribing student behaviors through the clickstream and to draw meaningful conclusions about those students.  MOOCs are typically used by people as a way to learn new skills or keep up-to-date with current ones. Because most MOOCs do not offer formal degrees, there are no se- rious consequences for doing poorly or dropping out. In contrast, college course grades determine whether students succeed or fail (whether they advance to the next course, re- main in their intended major, or graduate). Thus, findings from MOOC clickstream studies cannot offer broad expla- nations about student learning experiences in higher educa- tion settings. So while MOOCs and college courses share some similarities, in terms of course management systems and clickstream data, studying college courses may require a different set of goals and statistical techniques.  For instance, one important area of higher education re- search focuses on student engagement. Studies find that students who are not engaged with the learning process that is, students who do not put in the time and energy into purposeful learningare at greater risk for failing courses and dropping out of college [10]. While this finding is not new, understanding how to quickly identify these students, especially at the course-level, remains a significant challenge.  Clickstream data has the potential to address this since the data is obtained in real time. Researchers can provide instructors with immediate insights how students are engag- ing with the course management system. This is especially important in courses with large enrollments, where problems with student engagement can often go unnoticed [13]. Some recent work has found that student engagement with the course management system, as indexed by number of days students visited the site relative to their peers, was positively related to course outcomes [11]. Our work adds to this area of research by using statistical change detection techniques to further understand course engagement.  More broadly, changepoint detection techniques for event time-series is a widely studied topic and a variety of statis- tical methodologies have been developed (e.g., [7, 9]), with much of this work focused on single (univariate) time-series. Web user behavior has been analyzed to detect changes in an individuals behavior, to reportinterestingsessions, and to detect changes in user activity [8]. There has not been any prior work (to our knowledge) on change detection ap- plied to multiple clickstreams of students in an educational setting.  Thus far, previous work in the analysis of clickstream data in an educational setting has focused on grouping students into similar groups, understanding possible dropout, pre- dicting student success in a course, and defining learning    pathways. Our goal is to add to the current body of research in a meaningful way by using changepoint detection tech- niques as a proxy for understanding student engagement. By detecting whether student behavior changes in a signif- icant manner over the time-period of a particular term, we hope to identify students who increase, decrease, or show no change in their clickstream activities, and whether these changes relate to course performance.  3. METHODOLOGY We discuss below our approach for modeling and change  detection of student activity. We begin by defining some general notation and then introduce two different models: a Bernoulli model for binary data and a Poisson model for count data. The section concludes with a description of changepoint detection for both of these models.  3.1 Notation Let N be the number of individual students in a course  where i is an index that refers to an individual student in the class, i = 1, . . . , N . We will assume below that time is discrete1 with T discrete time-points and t = 1, . . . , T being an index running from the first to the last time-period of clickstream logging for the course. Below we will refer to t on a daily time-scale for convenience but in general other time-periodssuch as days or weekscould be used.  Let X be the observed data for a course, represented as an NT array whose entries are counts xit  {0, 1, 2, ....}. Note that xit represents the number of click events for student i on day t, where 1  i  N and 1  t  T . We will also consider a binarized version of the data xit = I(xit > 0), where I() is an indicator function (as in Figure 1 for example). The number of clicks xit (counts) by student i on a given day t in principle contains more information than the binarized version xit, but could also be quite noisy in the sense that more clicks might not necessarily correlate well with relevant student activity. We explore both options since the choice of looking at a count versus the binarized version in practice will depend on the context of a particular analysis.  3.2 Bernoulli Models for Binary Data For the binary data, xit, let it be the probability that  each student i is active on day t (i.e., the probability that student i generates one or more clicks on day t). The log- odds of it is modeled as:  log it  1 it = t + i (1)  where t, t = 1, . . . , T can be viewed as a time-varying popu- lation mean for the log-odds and i, 1  i  N is a student- dependent offset to account for individual-level variation in student behavior.  The role of i in this model is to modulate the time- varying population mean t in a student-specific manner. A positive value of i for student i will increase the log- odds above the population mean t, which in turn means that student i tends to click more than the mean student as represented by t. A negative value of i has the opposite effect; student i has a lower probability of clicking compared  1A changepoint methodology using a continuous-time model could in principle also be developed in a manner similar to the discrete-time methodology we describe in this paper.  0 10 20 30 40 50 60 70 80  DAYS  0.0  0.2  0.4  0.6  0.8  1.0  F R  A C  T IO  N  O  F  S  T U  D E N  T S  Figure 2: Proportion of students who click each day during a 10-week course.  0 10 20 30 40 50 60 70 80  DAYS  0  5  10  15  20  25  30  35  40  A V  E R  A G  E  N  U M  B E R   O F  C  LI C  K S  EXAM  Figure 3: Average number of click events per stu- dent each day during a 10-week course.  to the average student. t represents time-varying popula- tion behavior on a log-odds scale.  Our approach to change detection relies on modeling each students activity relative to that of the overall student pop- ulation in the class. This population (or background) rate t will typically vary significantly as a function of time t since student behavior is strongly affected by temporal effects such as days of lectures, weekday versus weekend effects, assign- ment deadlines, exams, and so on. As an example, Figure 2 shows the proportion of students who clicked on a file each day, summarizing the data shown earlier in Figure 1.  Modeling the log-odds as a linear function (Equation 1) is a standard technique in generalized linear modeling and ensures that the resulting probability it above lies between 0 and 1, i.e., Equation 1 above can be rewritten as  it = 1  1 + e(t+i) . (2)  3.3 Estimation of Model Parameters The parameters  = {1, . . . , t} and  = {1, . . . , N}  are estimated from the N  T data array X  with entries xit  {0, 1}, 1  i  N, 1  t  T . Since the xits are binary the likelihood for each individual data point xit can be written as:  L(, |xit) =  xit it (1 it)  (1xit), (3)  where it is defined in Equation 2. The likelihood of the full data set X  is then defined as:  L(, |X ) = P (X |, )  =  N i=1  T t=1   xit it (1 it)  (1xit). (4)  Here we make the assumption that the observed data for    each student on each day is conditionally independent of all other observations (for students and for days) given the pa- rameters  and . This is a simplification since it ignores (for example) possible time-varying trends in student be- havior. Nonetheless, as we will see later in the experimental results it provides a useful basis for change detection.  We use a two-stage procedure for parameter estimation as follows2. We first generate an estimate t for the population mean as follows:  t = log qt  1 qt , 1  t  T (5)  where qt = 1 N  N i=1 x   it, which is the proportion of students  (across all students) that generated a click on day t. In the second step, we fit a regression model for each stu-  dent i in Equation 1 with the population mean t set as an offset. i can be thought of as a student-specific intercept term for each student i.  3.4 Poisson Models for Count Data We can also model the counts xit directly, where xit can  have values {0, 1, 2, ...}. A natural model in this context is the Poisson model.  We develop the count model in a manner similar to that for binary case earlier. In particular, we model the loga- rithm of the mean of the Poisson distribution, log it as a linear function of a time-varying population rate t and an individual student effect i:  log it = t + i. (6)  Note that although for convenience we use the same nota- tion,  and , for our two sets of parameters, and they play an analogous role as their namesake parameters in the bi- nary model, these parameters are different from those in the binary model described earlier.  Figure 3 shows the average number of click events for each student per day, reflecting the type of time-varying popula- tion behavior that t is intended to capture. The red dashed lines are the dates for the three midterms and the final, and we can see much more click activities right before the exam dates.  We can write the likelihood function for a single count xit as  P (xit|t, i) = xitit e  it  xit! , (7)  where it is defined in Equation 6. As with the binary case, assuming that the observations xit are conditionally independent given the parameters, the full likelihood can be written as:  L(, |X) = P (X|, )  =  N i=1  T t=1  xitit e it  xit! . (8)  We again make a conditional independence assumption for the Poisson model. A two-stage parameter estimation process is carried out as before. In the first step we estimate t as follows:  t = log mt, 1  t  T (9) 2The estimation could be done in a single-step; we would expect similar results to what we obtain in the two-step approach.  where mt = 1 N  N i=1 xit, representing the average number  of click events across the population that were generated on day t. In the second step we fit a Poisson regression model for each student i as in Equation 6 with an offset t to get an estimate for each i.  3.5 Detecting Changes in Activity To detect changes in activity we allow for the possibility  that each students activity rate changes at some unknown time point during the course. The proposed approach that we describe below works in the same manner for both the Bernoulli binary model and the Poisson count model, the only difference being in how the likelihood is defined and the parameters are estimated for each (as described earlier). For simplicity, the reader can assume below that we are using either the Bernoulli or Poisson model, and the issue is whether to fit a model with a change or with no change.  We fit two different models for each student i. The first model is the one where we assume that the students rate of activity i, defined relative to the background activity t, does not change over time. In the second model, the changepoint model, we assume that a students activity rate switches at some unknown changepoint. We fit both models to the data for each student and use a data-driven model selection technique to select which model is justified given the observed data.  In the changepoint model we assume that there is one ac- tivity rate i1 for student i before changepoint i and a dif- ferent activity rate i2 after the changepoint i. The change- point model for binary data (for example) can be written as follows, where I is an indicator function:  log it  1 it = t + i1I(t < i) + i2I(t > i) (10)  with a similar definition for the Poisson model. We can interpret this model as fitting two regression models with different means on either side of the changepoint.  The value of the changepoint i for each student is un- known. Since time t is discrete the values of i can take one of T  1 possible values, corresponding to the T  1 boundaries between the T observation times.  In effect this changepoint model has 3 parameters (assum- ing t is known): the two activity rates and the changepoint. We generate maximum likelihood estimates of these param- eters by maximizing the log-likelihood defined as follows (for each student i)  li(i1, i2, i, )  =  t<i  logP (xit|i1, t) +  t>i  logP (xit|i2, t) (11)  (with a similar equation for counts xit and the Poisson model). To fit this model, we use a similar two-stage approach as  for the model with no-change described earlier. In the first stage we fit the background rate t using the data across all students, in the same manner as for the no-change model. In the second stage we find the values i1, i2, i, for each student i, that maximize the log-likelihood defined above. Since i is discrete we can reduce the optimization problem to finding the values of i1 and i2 for a fixed i and then iterate over the T  1 possible values of i. For each fixed value of i, the log-likelihood splits into the two parts on the right-hand side of Equation 11 above, a log-likelihood term containing i1 and a second log-likelihood term con-    STUDENT1  0 10 20 30 40 50 60 70 80 90  DAYS  STUDENT2  Figure 4: Simulated activity data for two students.  taining i2. Each can be optimized independently using the same procedure described earlier for estimating i for the no-change model.  For each student i, once the parameters of both the no- change and the changepoint models have been estimated, we select the best model from the two candidate models. The likelihood (or log-likelihood), evaluated at the maxi- mum likelihood values of the parameters, is not useful for model selection since the changepoint model will always have a likelihood value that is at least as high as the no-change model (this is because the changepoint model contains the no-change model as a special case).  There are a variety of model selection techniques in the statistical literature to handle the issue of how to fairly compare models (in the case where models have different numbers of parameters) including techniques such as penal- ized likelihood, Bayesian criteria, and cross-validation [4]. In the results in this paper we use the Bayesian Informa- tion Criterion (BIC) which is a well-established and easily interpretable method for model selection. The BIC score is defined for each student as  BICiM = 2liM + pM log T (12)  where M indicates a particular model (M = 1 corresponds to the no-change model, and M = 2 corresponds to the changepoint model), liM is the log-likelihood for model M for student is data evaluated at the maximum likelihood values of the parameters, pM is the number of parameters in each model (p1 = 1, p2 = 3, for the no-change and change- point models respectively)3, and T is the number of obser- vations per student. The second term in the BIC, pM log T , can be interpreted as a penalty for having additional param- eters in a model.  The BIC method selects the model with the lowest BIC score for each student. In particular, in the context of our changepoint application, we can use BIC to detect if there is evidence that a students rate of activity changed, i.e., if BICi2 < BICi1 then the evidence supports the changepoint model over the no-change model for student i.  4. RESULTS FOR SIMULATED DATA To illustrate how the change-detection methods work, we  simulated daily binary time-series of student click activity for 400 students over 85 days (numbers that are roughly sim- ilar to the larger of the two classes we analyze later in the paper). The true population rate t switched between two different values over time, one with a high rate and one with  3Technically we should also count the background model parameters  here, but since this is the same for both models we can omit it.  0 5 10 15 20 25 30  DAYS  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8   it  POPULATION STUDENT1 i=0.7  STUDENT2 i=-1.52  Figure 5: Estimated activity probabilities (it) for the two simulated students and for the population.  0 10 20 30 40 50 60 70 80 3.5  3.0  2.5  2.0  1.5  1.0  0.5  0.0  0.5  1.0   t +  i  M1, BIC=82.43  M2, BIC=75.74  0 10 20 30 40 50 60 70 80  DAYS  RAW DATA  DETECTED CP  TRUE CP  Figure 6: Log-odds of it, for M1 and M2, and simu- lated data of a student with a changepoint at t = 57.  a low rate. The variability in the simulation roughly corre- sponds to what we observed in the real student data. The offsets, ij , for each student were sampled independently from a normal distribution; ij  Normal(0,  = 1.5). Half of the students were simulated with one i1, i.e. no change in behavior over time. The other half of the students had two different offsets sampled, i1 and i2, on either side of a changepoint i which was sampled independently from a uniform distribution; i  U(15, 70).  Figure 4 is a plot of binary data for two simulated students who did not have changepoints. Student 1 is much more active than Student 2, and therefore Student 1 is going to have a larger estimated value for i. The estimated its for these students over the first 30 days (time t = 1, ..., 30) is shown in Figure 5. The plot illustrates how the estimated activity varies relative to the population probability t (the red solid curve). The more active student (green dashed line) has higher probabilities of clicking over time, while the less active student (blue dotted line) has lower probabilities, and both probabilities rise and fall relative to the behavior of the population. For example, when student activity on average rises on a particular day such as day 10 (e.g., due to an assignment), the click probability for both students rises.  Next we show the results of two different simulated stu- dents, one with a changepoint and the other without a change- point. Figure 6 is a plot from a student with a changepoint, with the raw data in the lower plot and the fitted model (plotted on a log-odds scale) in the upper plot. There is a clear change in student behavior around day 59, and this is visible both in the raw data (lower plot) and the fitted    0 10 20 30 40 50 60 70 80 2.5  2.0  1.5  1.0  0.5  0.0  0.5   t +  i  M1, BIC=86.96  M2, BIC=94.55  0 10 20 30 40 50 60 70 80  DAYS  RAW DATA  DETECTED CP  Figure 7: Log-odds of it, for M1 and M2, and sim- ulated without a changepoint.  changepoint model (top plot). The BIC for the model with the changepoint (M = 2) is significantly smaller than that of a model without the changepoint (M = 1) for this simulated students data, i.e., the BIC method was able to successfully detect that a model with a changepoint is preferred over a model with no changepoint for this data. In contrast, Figure 7 displays the results for a simulated student with no change- point. Both plots show the results of fitting the changepoint model M2. The changepoint model puts a change at day 63, but the BIC method selects the no-changepoint model since the BIC for no change M = 1 is much smaller than that of the changepoint model M = 2.  The BIC method for binary simulated data reliably de- tected changepoints when the magnitude of the change in i1 and i2 (before and after the changepoint i) was rel- atively large, but as the change became smaller it became more conservative. Out of the 400 simulated cases, BIC de- tected a change in 100 cases, with a precision of 91% (91 out of the 100 detected were true changes) and a recall of 46% (91 out of the 200 true changes were detected). The remaining 54% of true changes had much lower magnitude changes (0.96 on average) compared to the detected cases (magnitude 2.61 change on average).  5. CLICKSTREAM DATA SETS The clickstream data that we used in our study was recorded  via the Canvas learning management system (LMS). Can- vas is an open-source LMS that serves as a supplemental instructional technology for students. It has been adopted as the campus-wide LMS system by a number of US universi- ties, including UC Irvine. Students use Canvas to download course content, take online quizzes, watch videos, and sub- mit assignments. The most common data available are click- stream data; every time a student clicks on a URL within the Canvas LMS, the click is recorded and logged with student ID, URL, and time-stamp.  Canvas provides an application programming interface (API) which we used to extract a log of all Canvas clickstream data for a course in addition to other relevant course data such as a list of lecture files, pages, and etc. Our Canvas API crawler for non-clickstream portion of the data is publicly available on github at https://github.com/dkloz/canvas-api-python.  Table 1: Number of students who showed increase, decrease, or no change in their activities for each ac- tivity data type for the 10-week face-to-face course.  Event Type NIncrease NDecrease NNoChange Preview, binary 7 9 361 Preview, count 112 96 169 Review, binary 39 23 315 Review, count 121 159 97  Two types of student behavior are considered for the stu- dents described in this paper. The first is a students pre- viewing behavior. An event is defined as a Preview Event when a student views or downloads a file prior to the event start date. This could indicate how well a student is per- forming in terms of being prepared for the course. The sec- ond type of behavior is related to the students reviewing activities. A Review Event is defined as an event when a student views or downloads a file after the event end date, e.g., a student downloading a lecture file after the class in which the material was covered. We found that focusing on these two types of events allowed us to screen out less rele- vant information in the clickstream data and extract more meaningful information about students activities. While in this paper we will focus on events related to our defini- tions of previewing and reviewing activity, our methodology for change detection is applicable to arbitrary sets of click- stream events.  Extracting each of the previewing and reviewing events results in an activity matrix of size of N  T matrix, where the cell i, t indicates that the number of previewing or re- viewing events by student i on day t. The data can be binarized to create a binary representation for the Bernoulli model described in section 3.2.  We used data sets from two courses at UC Irvine in our study, both offered during the 2015-2016 academic year. The first is a face-to-face 10-week course with 377 enrolled stu- dents. Lectures were held three times a week, and there were 3 midterms and one final exam. Figure 3 shows the average number of click events on each day per student. There is significant variation in students clicking activity over time. For example, students tended to be much more active during days close to the exams (shown as red dashed lines).  The second data set is somewhat different from the first in that it was an online course offered for 5 weeks. There were 176 enrolled students in this course. This data set is significantly smaller than that for the first course both in terms of the number of students N and the number of days T . There were 25 video lectures in total and students were supposed to watch one lecture per day from Monday through Friday. The final exam was held on campus after the 5 lecture weeks.  6. EXPERIMENTAL RESULTS In this section we will discuss the application of our change  detection methodology to the two clickstream data sets de- scribed in the previous section.  6.1 Example 1: 10-Week Face-to-Face Course The clickstream data spanned 85 days, which included 10  weeks of instruction as well as activity before and after the 10 weeks. We applied our change-detection methodology to  https://github.com/dkloz/canvas-api-python   0 10 20 30 40 50 60 70 80 DAYS  NInc  NDec  PREVIEW, COUNTS  0 10 20 30 40 50 60 70 80 DAYS  NInc  NDec  REVIEW, COUNTS  Figure 8: Student preview and review activity data over time, for the students who increased or decreased their behavior in the 10-week face-to-face course. The gray marker at t-th column in each row means that there was click activity on day t for that student, with darker colors reflecting larger counts (more clicks).  4 different versions of the N  T data matrices: for preview and review events, in binary and count form. We restricted changepoints to be in the range from day 10 to day 75, since changepoint detection at the beginning or end of the sequences (i.e., outside of this range) tends to be unreliable due to small sample sizes and not so meaningful in terms of interpreting actual student behavior.  The students that were considered to have changed by the BIC scores were categorized into two groups: students who increased their click activity and students who decreased their click activity. We will refer to these groups as In- creasedandDecreasedrespectively. Note that these terms should be interpreted in a relative sense, since increase and decrease is for the i coefficient for each student relative to the background rate t. Thus, a detected increase for stu- dent i means in effect that the student is ranked higher in the class in terms of activity relative to other students after the changepoint i, compared to their rank before i (and conversely for a decrease).  The numbers of students detected as belonging to each group, for each event type, are shown in Table 1. The Pois- son count model detects significantly more student changes than the Bernoulli binary model, for both preview and re- view event types. This is to be expected since the Poisson model has more information to work with (and thus has bet- ter sensitivity) compared to the Bernoulli model which only sees a binarized version of the daily counts (and thus has less information per day about student activity). In the dis- cussion below we focus primarily on the Poisson results with counts given its better sensitivity.  Figure 8 shows the click data for each of the students for which a change was detected, with one student per row, and one plot per type of event (Preview and Review). The stu- dents are split into two groups within each plot depending on whether their detected changes were increases or decreases, and rows were then ordered within each group based on the chronological location of the changepoint per student. The changepoint locations are marked in red and the plots show a clear distinction between the days with more activity and the days with less activity.  Figure 9 provides a week-by-week summary of the infor- mation in Figure 8, showing the number of detected student  WEEK1 WEEK3 WEEK5 WEEK7 WEEK9 FINAL  0  2  4  6  8  10  12  P E R  C E N  T A  G E  O  F  S  T U  D E N  T S  PREVIEW COUNTS  INCREASED  DECREASED  EXAM  WEEK1 WEEK3 WEEK5 WEEK7 WEEK9 FINAL  WEEKS  0  2  4  6  8  10  12  14  P E R  C E N  T A  G E  O  F  S  T U  D E N  T S  REVIEW COUNTS  INCREASED  DECREASED  EXAM  Figure 9: Percentage of 10-week face-to-face course students who increased or decreased within each week.  changes per week, for each type of event. The vertical lines that are visible in the two count matrices are the exam dates. There are some obvious temporal patterns in this data. For example, the upper plot (preview events) shows that more than a quarter of the students increased their previewing activities in the third week, which is the week before the first midterm. This agrees with the intuition that prior to the first major exam in a class we would expect to see some significant shifts in student activity. The lower plot shows that the most of the changes in reviewing activity happened towards the end of the quarter, particularly during week 10 before the final exam. Again it makes sense that there are significant changes across students in their relative rates of reviewing activity prior to the final exam. We can also see in both plots that the number of detected changes per week, for increases and for decreases, are strongly correlated. As mentioned earlier this is to be expected with this model since increase and decrease for this model is defined relative to overall mean population behavior.  We also investigated how detected changes in preview and review activities were correlated with student outcome in    Table 2: Probability of a student getting a passing grade (A, B, C) depending on which group the stu- dent is in.  P (Pass|Inc) P (Pass|Dec) P (Pass) Probability 0.93 0.76 0.83 Pass (%) 12.1 -7.4 0  p-value 0.0025 0.0458 -  0 10 20 30 40 50 60 70 80 5  4  3  2  1  0  1  2  3   t +  i  M1, BIC=261.02  M2, BIC=228.6  0 10 20 30 40 50 60 70 80  DAYS  0  5  10  15  20  25  30  x it  DETECTED CP  REVIEW COUNTS  Figure 10: Log of it from M1 and M2, and the raw data of a student from the 10-week face-to-face course. The BIC method selected the model with changepoint (M2).  terms of the students final grades in the class. We calculated the probability of a student getting a passing grade given that the student is in the Increased group, P (Pass|Increase), or in the Decreased group, P (Pass|Decrease), and com- pared these numbers with the marginal (unconditional) prob- ability of a student passing P (Pass). For both preview and review count events we used a two-sided binomial test with P (Pass) as the null hypothesis to compute p-values for P (Pass|Increase) and P (Pass|Decrease).  Table 2 shows the results for review count data. At the 0.01 level of significance, P (Pass|Increase) is significant and P (Pass|Decrease) is significant at the 0.05 level. Stu- dents in the Increased group have a higher probability of passing the course, while the students in the Decreased group have a higher probability of failing. This means that stu- dents who increased their reviewing behavior (relative to all of the students in the course), at some point during the quar- ter, ended up getting better grades on average that those that did not. For preview counts, the probabilities were also in the direction of increases in previewing leading to bet- ter outcomes on average (and vice versa), but these changes were not statistically significant. This may suggest, for this particular course, that changes in review activities are better predictors of student outcomes than preview activities.  Finally, for the 10-week course, we analyzed in more de- tail the results for two specific students (using their Review data) to illustrate how the model can be used to interpret clickstream activity at the individual student level. Fig- ure 11 illustrates the results for a student where the lower plot shows the observed daily review clicks, and the upper plot shows the Poisson models for the no-change model and  0 10 20 30 40 50 60 70 80 8  6  4  2  0  2   t +  i  M1, BIC=184.26  M2, BIC=189.3  0 10 20 30 40 50 60 70 80  DAYS  0  1  2  3  4  5  6  7  x it  DETECTED CP  REVIEW COUNTS  Figure 11: Log of it from M1 and M2, and the raw data of a student from the 10-week face-to- face course. The BIC method selected the no- changepoint model (M1).  Table 3: Number of students who showed increase, decrease, or no change in their activities for each activity data type for online course.  Data Type NIncrease NDecrease NNoChange Preview, binary 6 8 162 Preview, binary 41 40 95 Review, binary 11 6 159 Review, counts 47 66 63  the changepoint model (with a detected change at day 70). For this student the BIC method preferred the changepoint model over the no-change model, with BIC2 < BIC1 by a large margin. This is reflected in the observed data in the lower plot where the number of counts for this student increase significantly after the changepoint.  Figure 11 shows the same type of plot for a student where the BIC method selected the model without the change- point. From the raw counts (lower plot) it looks like the students activity level could have changed (increased) after day 68. However, relative to the background activity (par- ticularly around days 76 to 78, leading up to the final exam) this students activity level is not sufficiently different to the mean population behavior to justify the additional parame- ters in the changepoint model, as reflected in the BIC scores (BIC1 < BIC2).  6.2 Example 2: Online 5-Week Course The second course we analyzed was a 5-week online sum-  mer course. The event data set for this course we analyzed was smaller than the first in terms of both the number of students (N = 176) and number of days with clickstream activity (T = 50). The course was offered online and the students were expected to watch a lecture video on every weekday over the 5 weeks, leading to more uniformity and less variability in student clickstream activity over time. In addition, the 10-week class had 3 midterm exams and a final exam, while the 5-week online class only had a single final exam at the end of the course.  The numbers of students detected for each of the Increased    0 10 20 30 40 DAYS  NInc  NDec  PREVIEW, COUNTS  0 10 20 30 40 DAYS  NInc  NDec  REVIEW, COUNTS  Figure 12: Student preview and review activity data over time, for the students who increased or decreased their behavior in the 5-week online course. The gray marker at t-th column in each row means that there was click activity on day t for that student, with darker colors reflecting larger counts (more clicks).  WEEK0 WEEK1 WEEK2 WEEK3 WEEK4 WEEK5 WEEK6  0  5  10  15  20  25  P E R  C E N  T A  G E  O  F  S  T U  D E N  T S  PREVIEW COUNTS  INCREASED  DECREASED  EXAM  WEEK0 WEEK1 WEEK2 WEEK3 WEEK4 WEEK5 WEEK6  WEEKS  0  5  10  15  20  25  P E R  C E N  T A  G E  O  F  S  T U  D E N  T S  REVIEW COUNTS  INCREASED  DECREASED  EXAM  Figure 13: Percentage of students with detected in- crease or decrease in activity for each week in the online 5-week course.  and Decreased groups, for both preview and review events, are shown in Table 3. We see a similar overall pattern to that for the 10-week class, namely that the Poisson model using counts detects considerably more changes than the Bernoulli method using binary data. The overall propor- tions of changes detected are roughly similar across both classes, with about 50% of students having increased or de- creased count activity relative to the population, for each of the two types of events. One difference we found between the two courses was the proportion of students who exhib- ited no change at all, for either preview or review events: 13% of students in the 10-week course and 25% in the 5- week courses. This difference might be due to the interme- diate exams (3 midterms) in the 10-week course, leading to more variability in student behavior compared to the 5-week course which only had a final exam.  The clickstreams for the students with detected changes are shown in Figure 12. We observe very high activities at the end of the course session for students in the Increased group, for both Preview and Review event types. The ma- jority of the changepoints occur just before the darker area of the plot. Figure 13 shows that, among the students who had an increased change that most of them had a changepoint in the fifth week, which is the last week of the course before the  0 10 20 30 40 50 8  6  4  2  0  2  4   t +  i  M1, BIC=154.65  M2, BIC=142.02  0 10 20 30 40 50  DAYS  0  2  4  6  8  10  12  14  x it  DETECTED CP  REVIEW COUNTS  Figure 14: Log of it from M1 and M2, and the raw data of a student from the 5-week online course. The BIC method selected the changepoint model (M2).  final. We did not analyze the relationship of click activity and course outcomes for this course since fewer than 5% of the students received grades of D or F in the class, resulting in a sample size that is too small for reliable inferences.  As with the 10-week class, we examine the results for re- view events for 2 specific students, to illustrate the method- ology at the level of individual students. Figure 14 shows the results for a student where the method detected a change in activity at day 35. Figure 15 shows the results for a student where the no-change model was preferred by BIC. Both stu- dents exhibited increases in their review activities after day 40, but the magnitude of change for the first student is sig- nificantly greater than that for the second student (as can be seen in the lower panels of both plots)relative to the student population as a whole, the second student did not exhibit a significant change in activity.  7. CONCLUSIONS AND FUTURE WORK Student clickstream data is inherently difficult to work  with given its complex and noisy nature. This paper de- scribed a statistical methodology for detecting changepoints in such data and illustrated the potential of the approach by applying the methodology to two large university courses. The proposed approach is relatively simple and allows for    0 10 20 30 40 50  15  10  5  0  5   t +  i  M1, BIC=54.42  M2, BIC=59.3  0 10 20 30 40 50  DAYS  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  x it  DETECTED CP  REVIEW COUNTS  Figure 15: Log of it from M1 and M2, and the raw data of a student from the 5-week online course. The BIC method selected the no-changepoint model (M1).  a number of possible extensions; the development of more flexible changepoint models (such as systematic drifts in stu- dent activity levels), allowing for more than a single change- point, post hoc adjustments for multiple testing, and using robust estimation techniques for parameters and their re- spective standard errors. Bayesian methods could also be potentially useful in this context for both parameter estima- tion and model selection to more fully reflect uncertainty in inferences at the individual student level. A useful extension for educators would be to develop an online detection variant of the offline approach proposed here, potentially allowing for identification of at-risk students, instructor feedback, or interventions while a course is in session.  While the results in this paper are promising and there are interesting methodological avenues to pursue, the most important future direction from an education research per- spective will involve more in-depth investigation of the util- ity of these types of methods in terms of providing actionable insights that are relevant to the practice of education.  Acknowledgments This paper is based upon work supported by the National Science Foundation under Grants Number 1535300 (for all authors) and 1320527 (for PS). The authors would like to thank Sarah Eichhorn, Wenliang He, and Dimitris Kotzias for their assistance in acquiring and preprocessing of the clickstream data used in this paper.  8. REFERENCES [1] Y. Bergner, D. Kerr, and D. E. Pritchard.  Methodological challenges in the analysis of MOOC data for exploring the relationship between discussion forum views and learning outcomes. In Proceedings of the EDM Conference, pages 234241. International Educational Data Mining Society (IEDMS), 2015.  [2] S. Bhat, P. Chinprutthiwong, and M. Perry. Seeing the instructor in two video styles: Preferences and patterns. In Proceedings of the EDM Conference, pages 305312. International Educational Data Mining Society (IEDMS), 2015.  [3] C. G. Brinton and M. Chiang. MOOC performance prediction via clickstream data and social learning networks. In Proceedings of the INFOCOM Conference, pages 22992307. IEEE, 2015.  [4] G. Claeskens. Statistical model choice. Annual Review of Statistics and its Application, 3:233256, 2016.  [5] S. Crossley, L. Paquette, M. Dascalu, D. S. McNamara, and R. S. Baker. Combining click-stream data with NLP tools to better understand MOOC completion. In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, LAK 16, pages 614. ACM, 2016.  [6] D. Davis, G. Chen, C. Hauff, and G.-J. Houben. Gauging MOOC learners adherence to the designed learning path. In Proceedings of EDM Conference, pages 5461. International Educational Data Mining Society (IEDMS), 2016.  [7] I. A. Eckley, P. Fearnhead, and R. Killick. Bayesian Time Series Models, chapter 10 Analysis of changepoint models, pages 205224. Cambridge University Press, Cambridge, 2011.  [8] P. Hofgesang and J. P. Patist. Online change detection in individual web user behaviour. In Proceedings of WWW Conference, pages 11571158. ACM, 2008.  [9] C. Kirch and J. Tajduidje Kamgaing. Detection of change points in discrete valued time series. In R. Davis, S. Holan, R. Lund, and N. Ravishanker, editors, Handbook of Discrete Valued Time Series, chapter 11, pages 219244. Chapman and Hall, 2014.  [10] G. D. Kuh, T. M. Cruce, R. Shoup, J. Kinzie, and R. M. Gonyea. Unmasking the effects of student engagement on first-year college grades and persistence. The Journal of Higher Education, 79(5):540563, 2008.  [11] C. Learning. Community insights: Emerging benchmarks and student success trends from across the civitas. Technical report, December 2016.  [12] Z. Liu, R. Brown, C. Lynch, T. Barnes, R. S. Baker, Y. Bergner, and D. S. McNamara. MOOC learner behaviors by country and culture; an exploratory analysis. In Proceedings of the EDM Conference, pages 127134. International Educational Data Mining Society (IEDMS), 2016.  [13] C. Mulryan-Kyne. Teaching large classes at college and university level: Challenges and opportunities. Teaching in Higher Education, 15(2):175185, 2010.  [14] K. H. R. Ng, K. Hartman, K. Liu, and A. W. H. Khong. Modelling the way: Using action sequence archetypes to differentiate learning pathways from learning outcomes. In Proceedings of the EDM Conference, pages 167174. International Educational Data Mining Society (IEDMS), 2016.  [15] G. Wang, X. Zhang, S. Tang, H. Zheng, and B. Y. Zhao. Unsupervised clickstream clustering for user behavior analysis. In CHI Proceedings, pages 225236. ACM, 2016.  [16] X. Wang, D. Yang, M. Wen, K. R. Koedinger, and C. P. Rose. Investigating how students cognitive behavior in MOOC discussion forum affect learning gains. In Proceedings of the EDM Conference, pages 226233. International Educational Data Mining Society (IEDMS), 2015.    Introduction  Related Work  Methodology  Notation  Bernoulli Models for Binary Data  Estimation of Model Parameters  Poisson Models for Count Data  Detecting Changes in Activity   Results for Simulated Data  Clickstream Data Sets  Experimental Results  Example 1: 10-Week Face-to-Face Course  Example 2: Online 5-Week Course   Conclusions and Future Work  References   "}
{"index":{"_id":"5"}}
{"datatype":"inproceedings","key":"Kaser:2017:MES:3027385.3027422","author":"Kaser, Tanja and Hallinen, Nicole R. and Schwartz, Daniel L.","title":"Modeling Exploration Strategies to Predict Student Performance Within a Learning Environment and Beyond","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"31--40","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027422","doi":"10.1145/3027385.3027422","acmid":"3027422","publisher":"ACM","address":"New York, NY, USA","keywords":"learning, prediction, probabilistic student models, simulations, strategies","Abstract":"Modeling and predicting student learning is an important task in computer-based education. A large body of work has focused on representing and predicting student knowledge accurately. Existing techniques are mostly based on students' performance and on timing features. However, research in education, psychology and educational data mining has demonstrated that students' choices and strategies substantially influence learning. In this paper, we investigate the impact of students' exploration strategies on learning and propose the use of a probabilistic model jointly representing student knowledge and strategies. Our analyses are based on data collected from an interactive computer-based game. Our results show that exploration strategies are a significant predictor of the learning outcome. Furthermore, the joint models of performance and knowledge significantly improve the prediction accuracy within the game as well as on external post-test data, indicating that this combined representation provides a better proxy for learning.","pdf":"Modeling Exploration Strategies to Predict Student Performance within a Learning Environment and Beyond  Tanja Kser AAA Lab  Graduate School of Education Stanford University  tkaeser@stanford.edu  Nicole R. Hallinen Psychology Department and  College of Education Temple University nicole.hallinen @temple.edu  Daniel L. Schwartz AAA Lab  Graduate School of Education Stanford University  daniel.schwartz@stanford.edu  ABSTRACT Modeling and predicting student learning is an important task in computer-based education. A large body of work has focused on representing and predicting student knowl- edge accurately. Existing techniques are mostly based on students performance and on timing features. However, re- search in education, psychology and educational data mining has demonstrated that students choices and strategies sub- stantially influence learning. In this paper, we investigate the impact of students exploration strategies on learning and propose the use of a probabilistic model jointly repre- senting student knowledge and strategies. Our analyses are based on data collected from an interactive computer-based game. Our results show that exploration strategies are a significant predictor of the learning outcome. Furthermore, the joint models of performance and knowledge significantly improve the prediction accuracy within the game as well as on external post-test data, indicating that this combined representation provides a better proxy for learning.  CCS Concepts Applied computing  Computer-assisted instruc- tion; Interactive learning environments; Computing methodologies  Knowledge representation and reason- ing; Bayesian network models;  Keywords probabilistic student models; learning; strategies; predic- tion; simulations  1. INTRODUCTION A major question for the design of computerized learning  environments is whether success within a learning environ- ment translates to success outside of the environment. Many data mining efforts have primarily focused on modeling and  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13-17, 2017, Vancouver, BC, Canada c 2017 ACM. ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027422  predicting performance within the trajectory of the learning environment.  One of the most popular approaches to representing and predicting student knowledge accurately is Bayesian Knowl- edge Tracing (BKT) [13]. Predictive performance of the original BKT model has been improved by applying clus- tering [27] and individualization techniques [26, 39, 40, 42]. Other widely used student modeling approaches include la- tent factors models [7, 8, 29] or dynamic Bayesian networks (DBN) [12, 19, 20, 23]. Most of these models represent student knowledge based on the students past performance within the computerized learning environment, i.e., the stu- dents answers to tasks are assessed and serve as observations for the respective method. When the (predicted) student knowledge within the learning environment does not fully predict success outside of the environment, it may be nec- essary to consider additional features such as engagement, affect or learning behavior for student modeling.  It has been shown that features such as strategies or choices influence the learning outcome. The strategies students ap- plied in an educational game influenced their implicit science learning [32, 15]. Furthermore, students inevitably have to make choices when they learn, such as for example the deci- sion about what and how to learn. Choice-based assessments interpret students choices as an outcome of instruction and use them as a proxy for students future learning [34]. By in- tegrating choice-based assessments in short interactive com- puter games, the influence of critical thinking [10], consul- tation of literature [11], and feedback seeking behavior [14] on the success outside the game was demonstrated.  Furthermore, there has been an increase in the use of open-ended simulations [41] over the last decade. Ideally, students explore different configurations of parameters to infer the underlying principles. Under the best of circum- stances, students learn the principles more deeply through exploration than if they are simply told the principles and asked to practice applying them [35]. Moreover, learning how to explore a simulation or empirical phenomenon is a major goal of science education in its own right. A sig- nificant technical challenge involves evaluating exploration choices to help predict student learning, perhaps with the intent of intervening, characterizing students, or simply to understand the exploratory behaviors worth teaching. If ex- ploratory behaviors are relevant to learning, then we should be able to detect exploration patterns that are associated with learning outcomes and integrate these patterns into our student models.    However, research on integrated models of performance and strategies is sparse. While other additional features influencing the learning outcome such as help-seeking [4, 30, 31] and off-task behavior [2, 3] have been integrated or added to existing student modeling approaches, research on students strategies in learning environments mainly fo- cused on detecting [17, 32] player strategies in an educational game, classifying students problem solving strategies [5, 24] or modeling strategies using interaction networks [15, 16]. FAST [18] is a technique for integrating general features into BKT. Dynamic mixture models [22] and DBNs [33] have been used to trace student engagement and knowledge in parallel. However, none of these existing models combine the representation of performance and learning strategies.  In this paper, we demonstrate that including an analysis of students exploration strategies within the game increases our ability to predict out-of-game performance compared to an analysis that only considers student success within the game. We present a first-of-kind model for integrating ex- ploratory behaviors and problem-solving success to predict both in-game and out-of-game performance. Our work is based on data collected with a short interactive computer- based game assessing students exploration choices. The game is centered around a tug-of-war topic and gives stu- dents the possibility of simulating their own tug-of-war set- ups and testing their knowledge about the (hidden) rules (i.e., the forces) governing the tug-of-war. By extensively analyzing the collected log-file data, we demonstrate that students exploration choices and strategies significantly in- fluence the learning outcome. Furthermore, we build a set of simple probabilistic student models jointly representing student knowledge and strategies and evaluate their predic- tion accuracy within the computer-based game as well as on an external post-test. Our results demonstrate that model- ing the influence of learner strategies on student knowledge significantly improves predictive performance and therefore constitutes a better representation of learning.  2. BACKGROUND Probabilistic graphical models are widely used for repre-  senting and predicting student knowledge and learning. One of the most popular approaches is Bayesian Knowledge Trac- ing (BKT). BKT represents student knowledge by employ- ing one Hidden Markov Model (HMM) per skill. The latent variable of the network represents (binary) student knowl- edge. The observed variable models the binary answers (cor- rect or wrong) of students to questions associated with the respective skill. The model can be specified using five pa- rameters. The transmission probabilities are described by pL, the probability that a student learns a previously un- known skill and pF , the probability of forgetting an already learned skill. In traditional BKT, we assume pF = 0. The emission probabilities of the model are specified using pG, the probability of correctly applying an unknown skill and pS , the probability of incorrectly answering a question as- sociated with an already learned skill. Finally, p0 describes the initial probability of knowing a skill a-priori. Given a sequence of observations O1 = o1, O2 = o2, . . . , OT = oT the learning task amounts to estimating the five parameters by maximizing the likelihood function  L  p(O1, . . . , OT , L1, . . . , LT |p0, pT , pS , pG), (1)  where we marginalize over all the hidden states L. Max- imization of the likelihood is relatively simple and is com- monly performed using expectation maximization [9], brute- force grid search [1] or gradient descent [42].  3. EXPERIMENTAL SETUP All evaluations of this paper were conducted using data  from an interactive computer-game. In the following, we describe the training environment, the associated post-test as well as the collected data.  3.1 Training Environment Learners need to make choices based on their prior knowl-  edge and the (imperfect) information available to them. Stu- dents for example need to decide what and how to learn. Choice-based assessments measure students choices to get a proxy for their future learning. These assessments give stu- dents explicit opportunities to engage in learning behaviors, such as seeking feedback, creating visualizations, or consult- ing references. TugLet is a short, interactive computer-based game assessing students exploration choices. The topic of the game is a tug-of-war, modeled with respect to forces and motion simulation. Each tug-of-war team consists of a maximum of four team members. There are small (weight w = 1), medium (w = 2), and large (w = 3) characters. To determine the winning side, the strength of each party needs to be computed, i.e., the weights need to be summed up. The position of the weights does not matter. The stu- dents are not told the relationships between the different weights, they must be discovered by interacting with the game. In the game, players explore by interacting with a simulation: They can set up opposing tug-of-war teams and see how they fare against each other. The players goal is to figure out how a teams size/weight corresponds with the strength of its pull, so that they will be able to accurately predict which team will win when presented with alternative scenarios.  Students have the choice between two different activities: Explore and Challenge. In the Explore mode (illustrated in Figure 1 (left)), different characters can be set up and the results can be viewed to induce and test hypotheses. The Challenge mode tests the students knowledge about the weights, i.e., the outcome of tug-of-war questions needs to be predicted (see Figure 1 (right)). This mode consists of eight questions ordered by increasing complexity. If a ques- tion gets answered incorrectly, the student is put back into Explore mode. The student is free to choose the Challenge mode at any point in time. The game is over after correctly answering eight Challenge questions in a row.  The interactive computer-game TugLet comes with an as- sociated post-test, which assesses the students knowledge about the rules (i.e., the weights and relationships of the different characters) governing the tug-of-wars. The post- test is a paper-and-pencil test consisting of ten questions. Children are presented a fixed tug-of-war team for the left side as well as ten different tug-of-war teams for the right side. The task is to select all the cases resulting in a tie. A summary sketch of the post-test is provided in Figure 2, where L denotes a large character, M a medium character, and S stands for a small character.    Figure 1: Explore (left) and Challenge (right) activities for TugLet. Students are free to enter Challenge mode at any point in time by clicking on the Challenge button (left).  X  L M  10)  S M S  1)  S M S  2)  S S SM  Figure 2: In the post-test, children have to select the cases resulting in a tie. The second case for example results in a tie, since the weight of three small (S) characters is equal to the weight of one large (L) character.  3.2 Data Set The data set used consists of 127 students (68 male, 59 fe-  male) in the 8-th grade of a middle school. The students had no prior experience with the topic from the science curricu- lum. Students played TugLet for a maximum time of 15 min- utes, followed by a short paper-and-pencil post-test. During game play, all the prompts were recorded in log files. Chil- dren solved on average 44.2 challenge questions ( = 33.3). They spent 42% of their time in the Explore mode. Most of the students (n = 111) successfully completed all challenge questions. The average accuracy in the post-test was 0.76 ( = 0.20), n = 31 students had a perfect post-test.  4. KNOWLEDGE REPRESENTATION We represent the knowledge of the students as a set of  rules describing the relationships between the weights of the different characters. The winning side of a specific tug-of- war configuration is then determined by iteratively applying the available rules. The complete TugLet rule set consists of n = 12 rules R = {Ri} with i  {1, . . . , n} and is listed in Table 1. Remember that a large character has a weight of w = 3, a medium character weighs w = 2 and the weight of a small character is w = 1. The rule set R consists of nine rules describing inequality and equality relationships between the different characters (weights). Furthermore, three meta-rules define basic tug-of-war concepts. Rule R10 states that if the left and right team have the exact same number of characters (and weights), the configuration will result in a tie. In rule R12 the fact that more characters of the same weight are stronger (i.e., three small charac- ters will win against two small characters) is recorded. Rule R11 finally allows for canceling out characters with the same weight on both sides. If the left team for example consists of a large and a medium character and the right side contains a medium and a small character, R11 can be applied to can- cel out the medium characters. The rule set in R contains all the rules necessary to solve all possible configurations in the game as well as in the post-test. Note that already a subset of the rules would (theoretically) be enough to de- rive the relationships between the weights of all characters. The rules R1, ..., R4 and R7, ...R9 can for example be derived from rules R5 and R6. This hierarchy of the rule set is nec- essary, since the students tend to learn in smaller steps, i.e., they test simpler hypotheses first (e.g., Large > Small), and since the questions in Challenge mode are ordered by complexity. The final rule set R therefore is the subset of all possible correct rules necessary to determine the winning side of all tug-of-war set-ups encountered in TugLet and in the associated post-test.  Each tug-of-war configuration is associated with a subset RN  R of rules necessary to determine the winning side. The calculation of RN is performed as follows: each rule    Table 1: Rule set R representing the domain knowl- edge in TugLet.  Rule Description  R1 Large > Small  R2 Large > Medium  R3 Medium > Small  R4 Large > 2Small R5 Large = 3Small R6 Medium = 2Small R7 Large = Medium+Small  R8 2Medium > Large R9 Small+Large = 2Medium R10 Equality  R11 Cancellation  R12 More is better  R6  R3  S S S  SM  M M  Figure 3: Example tug-of-war configuration with two medium (M) and three small (S) characters. The winning side can be determined by applying the rule set RN = {R3, R6}.  Ri  R has a set of conditions attached under which this specific rule can be applied. Rule R6 for example requires the presence of at least one medium character on the left (or right) side, respectively and a minimum of two small characters placed on the right (or left) side, respectively. To build RN , the system iterates through the rules Ri  R and applies them, until no more rule can be applied and hence the winning side is determined. During this process, simpler rules describing basic relationships between characters (e.g., R1 or R2) are prioritized. The resulting rule set RN consists of all the applied rules. Figure 3 shows the rule set RN for an example configuration, where L denotes a large weight, M a medium weight and S stands for a small weight.  During game play, the students are exposed to the rules when testing out tug-of-war configurations in the Explore mode and when answering questions in Challenge mode. We assume that each tug-of-war configuration encountered pro- vides an opportunity for learning. The rules, which can be acquired (or strengthened) from a specific configuration are exactly the rules Ri  RN associated with the given config- uration.  0 5 10 15 20 25 30  Challenge Attempts  0  1  2  3  4  5  6  7  8  Comparison of student trajectories Level  Figure 4: Comparison of student trajectories. Each circle (or cross) denotes exactly one challenge at- tempt of one student, i.e., a circle at (2, 5) means that the student answered five (out of eight) questions correctly at his 2nd attempt to pass the Challenge mode. The size of the circle denotes the number of explored tug-of-war set-ups right before challeng- ing, a cross means that zero set-ups were simulated. Challenge attempts of students with a perfect post- test are colored in red.  5. EXPLORATION STRATEGIES To analyze the influence of students exploration choices  and behavior, we mined the log file data collected with Tu- gLet as well as the external post-test data (see Section 3.2).  While 87% of the students passed the TugLet game, i.e., managed to answer all eight challenge questions at the end of the training, post-test performance is mixed. While the top 24% of the students have a perfect post-test, the bot- tom 20% reach an accuracy (ratio of correct answers) less or equal than 0.5. Therefore, the students training perfor- mance measured by there answers in Challenge mode seems to describe the learning and knowledge of the students in- sufficiently.  Therefore, we investigated students exploration behavior by analyzing their trajectories through the game as well as by examining students specific hypotheses. Figure 4 illus- trates the trajectories of the students within the game. The x-axis denotes the number of attempts so far in passing the Challenge mode, the y-axis denotes the level (number of correctly answered questions) reached: each circle or cross in Figure 4 denotes a challenge attempt of a student. A circle (or cross) at (4, 5) means that the student answered five (out of eight) questions correctly at his 4th attempt to pass the Challenge mode. The size of the circles denotes the    Figure 5: Example student trajectories. The x-axis denotes the number of challenge attempts, the y-axis the level (number of correctly answered questions) reached in the actual attempt. The width of the bar shows the number of exploration set-ups tested right before the actual challenge attempt.  number of tug-of-war set-ups simulated in the Explore mode right before changing to Challenge mode. A cross signifies that no set-ups were simulated, i.e., the student changed right back to Challenge mode. Attempts of students with a perfect post-test are marked in red, the attempts of all other students are colored in blue. Figure 4 shows that while the students with a perfect post-test pass the game soon, other students need a lot of attempts in Challenge mode before passing. Indeed, there is a significant negative correlation ( = 0.28, p = .001) between the number of challenge at- tempts and the achieved post-test accuracy. Figure 4 also demonstrates that the better performing students exhibit a different behavior regarding exploration than those stu- dents with lower post-test accuracies. At the beginning, all the children test many tug-of-war set-ups, this number de- creases over time (as visible from the decreasing circle sizes as well as the many crosses). Therefore, there is no signifi- cant correlation between the post-test performance and the number of tug-of-war set-ups tested before the first chal- lenge attempt (p = .203). However, while the students with perfect post-test tend to test (few) tug-of-war set-ups in- between two challenge attempts, students with lower post- test accuracy stop exploring completely as can be seen from the many blue crosses in Figure 4. Indeed, the average num- ber of tug-of-war set ups tested in Explore mode in-between two attempts to pass the Challenge mode is positively cor- related to post-test accuracy ( = 0.18, p = .048).  Figure 5 illustrates the trajectories of four example stu- dents. The x-axis again shows the number of attempts in  passing the Challenge mode, the y-axis shows the achieved level (number of correctly answered questions). The width of the bar denotes the number of tug-of-war set-ups tested before changing to Challenge mode. Student B and Student C had a perfect post-test, while the post-test accuracy of Student A and Student D was below 0.5. The sample tra- jectories confirm that students with low performance need more challenge attempts to pass the game. It seems that, while all students spend much time in Explore mode in the beginning, students performing badly in the post-test give up exploration much earlier. Student C is an exception: this student does not explore in the beginning, but realizes later that he will not pass without doing so. Student D persists, but does not seem to profit from the investigated tug-of-war set-ups.  We hypothesize that the reason behind these observations might be the fact that the conclusions drawn in the Explore mode are of higher value for the good performers, i.e., that the good performers test more informative tug-of-war set- ups. Figure 6 illustrates this behavior. The initial part of the trajectories of Student B (left) and Student A (right) are displayed. In the initial explore phase, both students exhibit similar exploration strategies. They test hypothe- ses such as equality (tug-of-war set-ups 1 and 4 of Student A), position independence (tug-of-war set-ups 2 and 5 of Student B), and relationships between the weights of the characters (tug-of-war set-up 3 of Student A). Note that during the first exploration phase, the characters available for simulation are limited: only one large and one small character are provided for each team. After the first wrong answer in Challenge mode, the students are put back into Explore mode and have the full set of characters available for hypothesis testing. Now the wheat is separated from the chaff. Student B (see Figure 6 (left)) systematically tests tug-of-war set-ups exploring the relationships between the different characters leading to the (possible) derivation of rules R6, R4, and R5 (see Table 1). Student A on the other hand seems overcharged with the many characters available for testing. As we can see from Figure 5, Student A ex- plores once more before the third challenge attempt, but completely quits exploring later on.  Given these observations, we divide all tug-of-war set-ups tested in the Explore mode into three categories: strong, medium, weak. This categorization is computed automat- ically based on the set of rules RN necessary to determine the winner of the given tug-of-war configuration. We found that a good exploration strategy focuses on isolating one underlying principle at a time. Therefore, a set-up is con- sidered as strong, if |RN | = 1 and Ri  RN is seen for the first time, i.e., the student tests exactly one new rule. If the rule Ri has been tested or seen previously, the set-up is categorized as being medium. If the set-up tests two rules, i.e., |RN | = 2 and R11  RN the tested configura- tion is labeled as a medium hypothesis. We assume that the student could still draw conclusions (i.e., find a new rule Ri) by first applying the cancellation rule R11 (see Section 4 and Table 1) and thus reducing the configuration to a set-up testing exactly one rule. If |RN | = 2  R11 / RN , the tested set-up is put into the weak category. We also categorize tug-of-war set-ups as being weak hypotheses if they require more than two rules to determine the winning    Ex p lo re  Ex p lo re  C h al le n ge  Ex p lo re  Ex p lo re  C h al le n ge  Figure 6: Comparison of initial tug-of-war set-ups for Student B (left) and Student A (right). While they exhibit similar exploration strategies in the be- ginning, strategies start to differ considerably with increasing difficulty.  side, i.e., if |RN | > 2. A set-up testing too many princi- ples at the same time does not allow to draw conclusions on relationships between single characters. An analysis of the training data reveals, that better performers indeed seem to have superior exploration strategies: there is a significant positive correlation between the number of strong tug-of- war set-ups tested and the achieved accuracy in the post-test ( = 0.21, p = .019).  6. PROBABILISTIC MODELS OF STRATEGIES  To investigate the benefits of modeling performance and strategies jointly, we constructed probabilistic graphical mod- els representing student knowledge and exploration behavior in one network and evaluated their predictive performance within the TugLet environment as well as in the post-test.  6.1 Simple Probabilistic Models To model the learning process of the students and to make  predictions about their performance in the game as well as in the post-test, we build probabilistic graphical models based on the representation of domain knowledge as a set of rules (see Section 4).  Pure Challenge Model. The pure challenge model (PCM) is a HMM, employing one model per rule. Figure 7 illustrates  KR ,1i  OR ,1i  KR ,2i  OR ,2i  KR ,Ti  OR ,Ti  p0  p , pG S p , pG S p , pG S  p , pL F p , pL F p , pL F  Figure 7: Structure of the graphical model over T time steps for the PCM, the CHM and the WHM.  the structure of the graphical model. The binary latent vari- able KRi,t represents, whether the student has mastered rule Ri at time t. The observed variable ORi,t is also binary and indicates, whether a student has correctly applied Ri at time t. Correctness is encoded as follows: If a student answers a challenge question at time t correctly, we assume that all rules Ri  RN have been applied correctly, i.e., oRi,t = 1, Ri  RN . If the student gives an incorrect answer, we as- sume the all rules Ri  RN have been applied incorrectly, i.e., oRi,t = 0, Ri  RN . This encoding method also in- fluences prediction: the predicted probability pC,t that the student will correctly determine the winning team of a tug- of-war configuration C at time t depends on the predicted probabilities p(ORi,t = 1) of the rules Ri  RNC :  pC,t =  Ri  p(ORi,t = 1), Ri  RNC . (2)  While this model is based on BKT, we allow a small amount of forgetting (pF > 0). Note that in the PCM, we do not represent actions performed in the Explore mode.  Correct Hypotheses Model. The correct hypotheses model (CHM) is an extension of the PCM. It again employs one HMM per rule (see Figure 7) and the interpretations of the latent and observed variables are accordingly. We encode the answers to the challenge questions in the same way as for the PCM. However, in contrast to the challenge model, the CHM also incorporates the actions performed in Explore mode. For each tug-of-war set-up H tested in the Explore mode, the rule set RNH necessary to find the win- ning side of the simulated set-up are computed. We then assume that all rules in RNH have been applied correctly, i.e., oRi,t = 1, Ri  RNH .  6.2 Modeling Strategies Both the PCM and the CHM are variations of BKT mod-  els and therefore allow for efficient parameter learning and predictions. However, the two models do not (in case of the PCM) or only in a limited way (in case of the CHM) take the exploration behavior of the students into account. Yet, our data analysis has shown that students exploration choices and strategies are significantly correlated to the learning out- come (see Section 5).  Weighted Hypotheses Model. The weighted hypotheses model (WHM) is based on the observation that exploration behavior significantly influences post-test performance. It again employs one HMM per rule and uses the graphical structure illustrated in Figure 7. The binary latent vari-    ables KRi,t again denote, whether the student has mastered rule Ri. The observed variables ORi,t are also binary and denote an application of rule Ri when answering a challenge question C or the testing of a rule Ri in a tug-of-war set-up H in Explore mode. We encode answers in Challenge mode as described in the PCM (see Section 6.1) and rules encoun- tered in Explore mode as explained in the CHM (see Section 6.1). However, the WHM introduces a weighting of the dif- ferent observations. Observations associated with a tested tug-of-war set-up are weighted according to the three cat- egories strong, medium, weak as defined in Section 5. Challenge answers are weighted differently based on their correctness. The sequence of T observations oRi for a rule Ri is therefore given by  oRi = (o w1 Ri,1  , ow2Ri,2 , ..., o wt Ri,T  ), (3)  with weights wj , j  1, ..., T specified as follows:  wj =    whs oRi,j is a strong hypothesis.  whm oRi,j is a medium hypothesis.  whw oRi,j is a weak hypothesis.  wcw oRi,j is a wrong challenge answer.  wcs oRi,j is a correct challenge answer.  (4)  The weights w = (whs, whm, whw, wcw, wcs) are positive integers and can be learned from the collected data using cross validation.  6.3 Experimental Evaluation We evaluated the predictive accuracy of our models within  the TugLet environment as well as on the post-test using the data set described in Section 3.2. We used a train-test setting, i.e., parameters were fit on the training data set and model performance was evaluated on the test set. All the models were fit using a Nelder-Mead (NM) optimiza- tion [25]. The NM algorithm is often used for optimization problems due to its simplicity and fast convergence rate. Predictive performance was evaluated using the root mean squared error (RMSE) as well as the area under the ROC- curve (AUC). The RMSE is widely used for the evaluation of student models, e.g., [26, 39, 40, 42]. The AUC is a useful additional measure to assess the resolution of a model.  Within-Game Prediction. The prediction accuracy of the PCM and the CHM models on the log files collected from TugLet was evaluated using student-stratified (i.e. di- viding the folds by students) 10-fold cross validation. Since the estimation of model performance during parameter tun- ing leads to a potential bias [6, 38], we use a nested 10-fold student-stratified cross validation to estimate the predictive performance of the WHM and to at the same time learn the optimal weights wopt for this model. We used r = 50 random re-starts for the NM algorithm for all models, since the NM algorithm is known for being trapped into local optima and to be sensitive to the initial starting values [25, 28]. We used the same parameter constraints for all models: pi  0.5, if i  {L,F,G, S}. The prior probability p0 remained uncon- strained. Figure 8 displays the RMSE and the AUC for the PCM, CHM, and WHM models.  The WHM demonstrates the highest prediction accuracy within the game (RMSEWHM = 0.3328). The inclusion of  0.32  0.33  0.34  0.35  0.36  0.37  R M  S E  Performance comparison over the different models  PCM CHM WHM  0.77  0.78  0.79  0.8  0.81  0.82  A U  C  Figure 8: Comparison of within-game prediction accuracy of the PCM (modeling knowledge only), CHM (modeling knowledge and exploration) and WHM (modeling knowledge and exploration strate- gies).  Table 2: Mean pair-wise differences  in RMSE be- tween the models along with confidence intervals ci and significance values p for the within-game predic- tion.  Mean  95% ci of  p  dPCM,CHM 0.0092 [-0.0055,0.0239] .309  dPCM,WHM 0.0240 [-0.0093,0.0387] <.001  dCHM,WHM 0.0148 [-0.0001,0.0296] <.05  exploration choices into the model led to an improvement in RMSE by 2.6% (RMSEPCM = 0.3574, RMSECHM = 0.3480), the representation of strategies further reduced the RMSE by 4.4% (RMSECHM = 0.3480, RMSEWHM = 0.3328). A one-way analysis of variance performed on the per-student RMSE of the different models shows that there are indeed significant differences between the mean RMSEs of the different models (F = 7.45, p < .001). The results of multiple comparisons (using a Bonferroni-Holm correction) between the different models are listed in Table 2. There is no significant difference in performance between the PCM and the CHM models. However, the WHM significantly out- performs the PCM and CHM models. All three models are performing well in discriminating challenges from failures (AUCPCM = 0.7985, AUCCHM = 0.7874, AUCWHM = 0.7954), there are no significant differences in AUC between the models.  The optimal weights found for the WHM are wopt = {3, 1, 1, 1, 2}. Tug-of-war set-ups classified as strong hy- potheses have a higher impact than set-ups falling in the medium or weak categories (whs = 3, whm = 1, whw = 1). Strong hypotheses are also assigned more weight than cor- rect answers to challenge questions (whs = 3, wcs = 2).    0.3  0.35  0.4  0.45  R M  S E  Performance comparison over the different models  PCM CHM WHM  0.5  0.55  0.6  0.65  A U  C  Figure 9: Comparison of post-test prediction accu- racy along with standard deviations for the PCM (modeling knowledge only), CHM (modeling knowl- edge and exploration) and WHM (modeling knowl- edge and exploration strategies).  Post-Test Prediction. To evaluate the predictive perfor- mance of the different models on the post-test, we used all within-game observations (i.e., actions performed within the TugLet environment) for training and predicted the outcome of the external post-test. We again used r = 50 random re- starts for the NM algorithm. We constrained the parameters of all models as described for the within-game prediction: pi  0.5, if i  {L,F,G, S}. The prior probability p0 re- mained unconstrained. For the WHM, we can safely use the optimal weights wopt = {3, 1, 1, 1, 2} found in the nested cross validation, since this optimization was performed on within-game data only. Prediction accuracy in terms of the RMSE and the AUC was computed using bootstrap aggrega- tion with re-sampling (b = 100). Figure 9 displays the error measures (with standard deviations) for the PCM, CHM, and WHM models.  The WHM shows the best performance for both error measures. Modeling exploration behavior even in a sim- plistic way leads to an improvement in RMSE of 4.55% (RMSEPCM = 0.4370, RMSECHM = 0.4171), categoriza- tion of the different explored set-ups along with the intro- duction of weighted observations decreases the RMSE by an- other 7.6% (RMSECHM = 0.4171, RMSEWHM = 0.3854).  The low standard deviations in RMSE (PCM = 0.0079, CHM = 0.0094, WHM = 0.0131) indicate significant dif- ferences between the different models. A one-way analysis of variance confirms that there are indeed significant dif- ferences between the mean RMSEs of the different models (F = 633.46, p < .001). Multiple comparisons (using a Bonferroni-Holm correction) between the mean RMSEs of the different models demonstrate that all model means are significantly different from each other. Table 3 illustrates this fact: The 95% confidence intervals for the differences in RMSE between the models do not include zero.  The WHM also exhibits a higher AUC than the PCM  Table 3: Mean pair-wise differences  in RMSE be- tween the models along with confidence intervals ci and significance values p.  Mean  95% ci of  p  dPCM,CHM 0.0199 [0.0165,0.0233] <.001  dPCM,WHM 0.0517 [0.0483,0.0551] <.001  dCHM,WHM 0.0318 [0.0284,0.0352] <.001  Table 4: Mean pair-wise differences  in AUC be- tween the models along with confidence intervals ci and significance values p.  Mean  95% ci of  p  dPCM,CHM -0.0065 [-0.0021,0.0151] 0.184  dPCM,WHM -0.0209 [-0.0295,-0.0123] <.001  dCHM,WHM -0.0273 [-0.0359,-0.0187] <.001  and the CHM (AUCPCM = 0.5956, AUCCHM = 0.5891, AUCWHM = 0.6164). Although the standard deviations (PCM = 0.0246, CHM = 0.0283, WHM = 0.0248) are higher than for the RMSE, a one-way analysis of variance suggests that the mean AUCs of the different models are not the same (F = 30.22, p < .001). The multiple compar- isons (employing a Bonferroni-Holm correction) between the mean AUCs demonstrate that while the differences between the PCM and the CHM are not significant, the WHM significantly outperforms the other two models. Table 4 lists the mean values  for the differences between the models average AUCs along with 95% confidence intervals and sig- nificance values.  7. DISCUSSION AND CONCLUSION The strategies and choices of students in a learning envi-  ronment have a significant influence on their learning out- come. Previous work has shown that strategies used vary considerably across students [36, 37]. Furthermore, stu- dents abilities in critical thinking [10], their literature in- quiries [11], and their feedback seeking behavior [14] have a significant impact on the learning outcome.  Recent research in educational data mining has investi- gated the strategic behavior of children in games. However, most of this work has focused on the data mining part, i.e., measuring implicit science learning based on player moves in an educational game [17, 32] or the classification of problem solving strategies [5, 24]. Research on the modeling part has focused on representing the problem solving behavior only [15].  In contrast to previous work, we represent student knowl- edge and exploration strategies jointly in one model. Our work is comparable to research on engagement modeling, where student knowledge and engagement are simultane- ously traced [33]. FAST [18] also allows for the integration of additional features into a BKT model, however, these ad- ditional features influence prediction of the observed state only. In contrast to this approach, in our joint model of knowledge and strategy, the strategies directly influence the (hidden) knowledge state. This technique allows us to pre- dict performance on an external post-test, where informa-    tion about strategies is not available. Our results demonstrate that even simple probabilistic  models of strategies offer a better representation of learn- ing than a pure performance model. Modeling the strength of student hypotheses leads to a small, but significant im- provement of 6.9% of the RMSE (RMSEPCM = 0.3574, RMSEWHM = 0.3328), when predicting students answers to challenge questions within the learning environment. Im- provements are much larger for the post-test: the joint repre- sentation of performance and strategies improves the RMSE by 11.8% (RMSEPCM = 0.4370, RMSEWHM = 0.3854). Modeling strategies also improves the AUC in the post-test, i.e., the WHM is better at discriminating failures (incor- rectly answered challenge questions) from successes than the PCM. The increased prediction accuracy on the post-test demonstrates that 1) using probabilistic models of strate- gies, we are able to improve the detection of shallow learn- ing [21]: From the 111 students passing the game (measured by an assessment of their performance), 21 students achieved an accuracy less or equal than 0.5 on the post test. The bet- ter predictive performance on the post-test also shows that 2) simple probabilistic models representing performance and knowledge jointly are superior at identifying understanding. The post-test required a higher level of rule understanding and also a transfer, since tasks were asked in a different way than in the game (selecting tug-of-war set-ups resulting in a tie vs. determining the outcome of a given tug-of-war set- up).  The improved predictive performance of our joint repre- sentation of strategies and performance as well as the signifi- cant correlations found between exploration choices, strength of hypotheses and the learning outcome confirm the findings of previous work: Students choices [10, 11, 14] and learning strategies [15, 32] have a significant impact on the learn- ing outcome. These findings give important directions for assessment: not only performance data, but also students strategies and choices need to be measured to reliably pre- dict future learning.  The strategies represented in our models are of course spe- cific to the presented educational game. They can, however, be generalized to the inquiry strategies of simplification and testing one principle at a time. In future work, we would therefore like to model these inquiry strategies for different educational games and simulations in order to analyze and demonstrate the generalizability of our models.  To conclude, we have proposed the use of probabilistic graphical models jointly representing student knowledge and strategies. Our results demonstrate that simple probabilis- tic models of strategies are sufficient to significantly improve prediction accuracy. Furthermore, we have shown that stu- dents strategies significantly influence the learning outcome and therefore, augmented models are a better predictor for learning than pure performance models.  8. REFERENCES [1] R. S. Baker, A. T. Corbett, S. M. Gowda, A. Z.  Wagner, B. A. MacLaren, L. R. Kauffman, A. P. Mitchell, and S. Giguere. Contextual Slip and Prediction of Student Performance after Use of an Intelligent Tutor. In Proc. UMAP, pages 5263, 2010.  [2] R. S. Baker, A. T. Corbett, and K. R. Koedinger. Detecting Student Misuse of Intelligent Tutoring Systems. In Proc. ITS, pages 531540, 2004.  [3] R. S. J. d. Baker, A. T. Corbett, I. Roll, and K. R. Koedinger. Developing a generalizable detector of when students game the system. User Modeling and User-Adapted Interaction, 18(3):287314, 2008.  [4] J. E. Beck, K.-m. Chang, J. Mostow, and A. Corbett. Does Help Help Introducing the Bayesian Evaluation and Assessment Methodology. In Proc. ITS, pages 383394, 2008.  [5] P. Blikstein. Using Learning Analytics to Assess Students Behavior in Open-ended Programming Tasks. In Proc. LAK, pages 110116, 2011.  [6] A.-L. Boulesteix and C. Strobl. Optimal classifier selection and negative bias in error rate estimation: an empirical study on high-dimensional prediction. BMC Medical Research Methodology, 9(1):85+, 2009.  [7] H. Cen, K. R. Koedinger, and B. Junker. Is Over Practice Necessary -Improving Learning Efficiency with the Cognitive Tutor through Educational Data Mining. In Proc. AIED, pages 511518, 2007.  [8] H. Cen, K. R. Koedinger, and B. Junker. Comparing Two IRT Models for Conjunctive Skills. In Proc. ITS, pages 796798, 2008.  [9] K.-M. Chang, J. Beck, J. Mostow, and A. Corbett. A Bayes Net Toolkit for Student Modeling in Intelligent Tutoring Systems. In Proc. ITS, pages 104113, 2006.  [10] M. Chi, D. L. Schwartz, K. P. Blair, and D. B. Chin. Choice-based Assessment: Can Choices Made in Digital Games Predict 6th-Grade Students Math Test Scores In Proc. EDM, pages 3643, 2014.  [11] D. B. Chin, K. P. Blair, and D. L. Schwartz. Got game A choice-based learning assessment of data literacy and visualization skills. Technology, Knowledge, and Learning, 21:195210, 2016.  [12] C. Conati, A. Gertner, and K. VanLehn. Using Bayesian Networks to Manage Uncertainty in Student Modeling. UMUAI, 12(4):371417, 2002.  [13] A. T. Corbett and J. R. Anderson. Knowledge Tracing: Modeling the Acquisition of Procedural Knowledge. UMUAI, 4(4):253278, 1994.  [14] M. Cutumisu, K. P. Blair, D. B. Chin, and D. L. Schwartz. Posterlet: A Game-Based Assessment of Childrens Choices to Seek Feedback and to Revise. Journal of Learning Analytics, 2(1):4971, 2015.  [15] M. Eagle and T. Barnes. Exploring Differences in Problem Solving with Data-Driven Approach Maps. In Proc. EDM, pages 7683, 2014.  [16] M. Eagle, D. Hicks, B. Peddycord, III, and T. Barnes. Exploring Networks of Problem-Solving Interactions. In Proc. LAK, pages 2130, 2015.  [17] M. Eagle, E. Rowe, D. Hicks, R. Brown, T. Barnes, J. Asbell-Clarke, and T. Edwards. Measuring Implicit Science Learning with Networks of Player-Game Interactions. In Proc. CHI in Play, pages 499504, 2015.  [18] J. P. Gonzalez-Brenes, Y. Huang, and P. Brusilovsky. General features in knowledge tracing: Applications to multiple subskills, temporal item response theory, and expert knowledge. In Proc. EDM, pages 8491, 2014.  [19] J. P. Gonzalez-Brenes and J. Mostow. Dynamic Cognitive Tracing: Towards Unified Discovery of Student and Cognitive Models. In Proc. EDM, 2012.    [20] J. P. Gonzalez-Brenes and J. Mostow. Topical Hidden Markov Models for Skill Discovery in Tutorial Data. NIPS - Workshop on Personalizing Education With Machine Learning, 2012.  [21] S. M. Gowda, R. S. Baker, A. T. Corbett, and L. M. Rossi. Towards Automatically Detecting Whether Student Learning is Shallow. IJAIED, 23(1):5070, 2013.  [22] J. Johns and B. Woolf. A dynamic mixture model to detect student motivation and proficiency. In Proc. AAAI, pages 163168, 2006.  [23] T. Kaser, S. Klingler, A. G. Schwing, and M. Gross. Beyond Knowledge Tracing: Modeling Skill Topologies with Bayesian Networks. In Proc. ITS, pages 188198, 2014.  [24] L. Malkievich, R. S. Baker, V. Shute, S. Kai, and L. Paquette. Classifying behavior to elucidate elegant problem solving in an educational game. In Proc. EDM, pages 448453, 2016.  [25] J. A. Nelder and R. Mead. A Simplex Method for Function Minimization. The Computer Journal, 7(4):308313, 1965.  [26] Z. A. Pardos and N. T. Heffernan. Modeling Individualization in a Bayesian Networks Implementation of Knowledge Tracing. In Proc. UMAP, pages 255266, 2010.  [27] Z. A. Pardos, S. Trivedi, N. T. Heffernan, and G. N. Sarkozy. Clustered knowledge tracing. In Proc. ITS, pages 405410, 2012.  [28] J. M. Parkinson and D. Hutchinson. An Investigation into the Efficiency of Variants on the Simplex Method. In Numerical Methods for Non-linear Optimization, pages 115135. Academic Press, 1972.  [29] P. I. Pavlik, H. Cen, and K. R. Koedinger. Performance Factors Analysis - A New Alternative to Knowledge Tracing. In Proc. AIED, pages 531538, 2009.  [30] I. Roll, V. Aleven, B. McLaren, and K. Koedinger. Improving students help-seeking skills using metacognitive feedback in an intelligent tutoring  system. Learning and Instruction, 21:267280, 2011.  [31] I. Roll, R. Baker, V. Aleven, and K. R. Koedinger. On the benefits of seeking (and avoiding) help in online problem solving environment. Journal of the Learning Sciences, 23(4):537560, 2014.  [32] E. Rowe, R. Baker, J. Asbell-Clarke, E. Kasman, and W. Hawkins. Building Automated Detectors of Gameplay Strategies to Measure Implicit Science Learning. In Proc. EDM, pages 337338, 2014.  [33] S. E. Schultz and I. Arroyo. Tracing Knowledge and Engagement in Parallel in an Intelligent Tutoring System. In Proc. EDM, pages 312315, 2014.  [34] D. L. Schwartz and D. Arena. Measuring what matters most: Choice-based assessments for the digital age. The MIT Press, 2013.  [35] D. L. Schwartz, C. C. Chase, M. A. Oppezzo, and D. B. Chin. Practicing versus inventing with contrasting cases: The effects of telling first on learning and transfer. Journal of Educational Psychology, 103(4):759775, 2011.  [36] R. S. Siegler. Strategy choice and strategy discovery. Learning and Instruction, 1(1):89102, 1991.  [37] R. S. Siegler and Z. Chen. Developmental Differences in Rule Learning: A Microgenetic Analysis. Cognitive Psychology, 36:273310, 1998.  [38] S. Varma and R. Simon. Bias in error estimation when using cross-validation for model selection. BMC Bioinformatics, 7(1):91, 2006.  [39] Y. Wang and J. Beck. Class vs. Student in a Bayesian Network Student Model. In Proc. AIED, pages 151160, 2013.  [40] Y. Wang and N. T. Heffernan. The student skill model. In Proc. ITS, pages 399404, 2012.  [41] C. E. Wieman, W. K. Adams, and K. K. Perkins. PhET: Simulations That Enhance Learning. Science, 322(5902):682683, 2008.  [42] M. V. Yudelson, K. R. Koedinger, and G. J. Gordon. Individualized Bayesian Knowledge Tracing Models. In Proc. AIED, pages 171180, 2013.    "}
{"index":{"_id":"6"}}
{"datatype":"inproceedings","key":"Lang:2017:OPM:3027385.3027410","author":"Lang, Charles","title":"Opportunities for Personalization in Modeling Students As Bayesian Learners","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"41--45","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027410","doi":"10.1145/3027385.3027410","acmid":"3027410","publisher":"ACM","address":"New York, NY, USA","keywords":"Bayes, context modelling, individualization, personalization","Abstract":"The following paper is a proof-of-concept demonstration of a novel Bayesian framework for making inferences about individual students and the context in which they are learning. It has implications for both efforts to automate personalized instruction and to probabilistically model educational context. By modelling students as Bayesian learners, individuals who weigh their prior belief against current circumstantial data to reach conclusions, it becomes possible to both generate estimates of performance and the impact of the educational environment in probabilistic terms. This framework is tested through a Bayesian algorithm that can be used to characterize student prior knowledge in course material and predict student performance. This is demonstrated using both simulated data. The algorithm generates estimates that behave qualitatively as expected on simulated data and predict student performance substantially better than chance. A discussion of the results and the conceptual benefits of the framework follow.","pdf":"Opportunities for Personalization in Modeling Students as Bayesian Learners  Charles Lang Teachers College,  Columbia University 525 West 120th St New York, NY, USA  charles.lang@tc.columbia.edu  ABSTRACT The following paper is a proof-of-concept demonstration of a novel Bayesian framework for making inferences about indi- vidual students and the context in which they are learning. It has implications for both efforts to automate personal- ized instruction and to probabilistically model educational context. By modelling students as Bayesian learners, in- dividuals who weigh their prior belief against current cir- cumstantial data to reach conclusions, it becomes possible to both generate estimates of performance and the impact of the educational environment in probabilistic terms. This framework is tested through a Bayesian algorithm that can be used to characterize student prior knowledge in course material and predict student performance. This is demon- strated using both simulated data. The algorithm generates estimates that behave qualitatively as expected on simulated data and predict student performance substantially better than chance. A discussion of the results and the conceptual benefits of the framework follow.  CCS Concepts Applied computing  Computer-assisted instruc- tion; E-learning;  Keywords Personalization; individualization; context modelling; Bayes  1. INTRODUCTION Personalization can be framed as an unsupervised learn-  ing problem in which we try to resolve both the individual structure of a students data and estimate the impact of this structure on their behavior simultaneously. This is dif- ficult for groups of students, but for individuals it has the added difficulty of generating probability statements that make sense at the individual level. What does a statement such as, The probability that Susan is correct is 0.6, actu-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027410  ally mean given that Susan will only ever perform the task in question once [13] This question lies at the heart of what it means to automate the personalization of education, and how we approach it will determine our assessments of stu- dents and the impact of subsequent educational decisions.  The most common solution to deal with the issue of mak- ing meaning from probabilistic statements about individu- als is to make them not about individuals at all. Instead we personalize based on subgroup membership [1]. If Susan is left-handed, her probability of being correct is the prob- ability of left-handed people being correct. The probability statement makes sense and is intuitive in that it corresponds to a proportion of the group, but it is limited in that it re- quires the identification of appropriate subgroups. Subgroup identification is further complicated by the possibility that behavior is mediated not only by membership in one sub- group [9], but by several, and becomes diabolically complex if subgroups are defined as latent psychological constructs which themselves are probabilistic in nature [15].  What follows is one possible way to skirt the theoretical problem of making individual level probability statements by treating probability not only as a frequency but also as a logical property. The most common utilization of logical probability is the use of Coxs Axioms in Bayesian statistics [6] and we can apply this formalization to the problem of per- sonalization by treating students as Bayesian learners (they weigh new data against prior experience to generate a behav- ior) and the probability that they will perform a particular behavior as a logical property belonging to them. Since the property belongs to the student, we can then circumvent the need to rely on group based probability statements to per- sonalize. Rather, we can make probability statements about individuals based purely on probabilistic characterization of their prior knowledge and new data.  1.1 Human Bayesians Modelling people as Bayesian makes some intuitive sense  though it is distinct from the work of Corbett and Anderson on Bayesian Knowledge Tracing that uses Bayes Rule to calculate the parameters of a latent structure [4]. Modelling people as Bayesian is the explicit statement that we weigh new knowledge against our prior beliefs to reach conclusions, and Bayes Rule provides a mathematical formalism for this intuition:  p(H|D)  p(D|H).p(H) (1)  Read as, the probability of a hypothesis (H) given some data (D) is proportional to the likelihood of that data, given the    hypothesis, multiplied by the prior knowledge of the hypoth- esis. The idea that Bayes reflects some internal psycho- logical mechanism has been around at least since 1931 [14] and was applied directly to psychometrics by de Finetti in 1965 [7] and Economics and Decision Theory adopted the idea in the 1960s [16]. Yet, in the 1970s experimental psy- chology largely dismissed Bayes as an illegitimate psycho- logical model [2]. It seems that humans deviate from the model in all kinds of ways: they rely on heuristics [12], their intuitions of the probability of occurrences do not match Bayesian calculations [5], and they do not optimize their economic choices in a way that is coherent with Bayesian logic [17]. In short, people do not behave in a way that is rational in a Bayesian sense. But in the early 2000s interest in Bayes as a psychological model was reignited by Compu- tational Cognitive Psychologists such as Gopnik, Griffiths and Tenenbaum, who successfully re-formulated psychologi- cal problems in Bayesian terms particularly focusing on how young children learn [11, 10, 8] as well as renewed interest within Computational Economics and Decision Theory [3]. This new research pushed against the purely rational model of Bayes, concentrating on Bayes as a model of the algo- rithmic rather than the computational level of explanation. Yet, Learning Analytics may benefit from pushing the model even further from its rational cousins.  1.2 Bayesian Framework The goal in much of the research discussed above is to  demonstrate that, on average, populations perform in ac- cordance with Bayes rule - in other words, they are rational in a Bayesian sense. The following framework will deviate from this goal in some important ways though. The first is that we are not attempting to demonstrate that people are rational and we can relax this assumption by redefining the likelihood function. The second is that we are attempting to model individual students rather than uncover an aver- age trend across students. A rational Bayesian human would take the probabilities observable in the world to calculate a posterior probability of the truth of a hypothesis, given the current data (p(H|D)), from the likelihood of the data in light of the hypothesis (p(D|H)) and their prior belief in the hypothesis from their accumulated experience (p(H)). The rational Bayesian model often assumes that the likelihood is external to the individual - it is an objective observation about the world, but if we instead assume that it is a pro- cess that exists within the individual - it is their subjective interpretation or perception of the conditions, rather than the objective description of them, we can re-frame Bayes Rule as a model for personalization at the individual level. The likelihood is the degree to which the data confirms or dis-confirms the students belief in the hypothesis. The mod- ellers job then becomes to generate estimates of each indi- viduals likelihood and prior, to best predict their individual behavior at a task represented by the posterior probability. Within this framework, if we can characterize probabilis- tically a students prior knowledge and how that student interprets their conditions as a likelihood we should be able to accurately predict their behavior. In other words, mod- elling student behavior becomes a matter of resolving what each individual student brings to the table vs. what the table brings to each student :  Behavior  Context PriorKnowledge (2)  This is not dissimilar to how Snow describes a student tack- ling a test item, behavior is the interface between the affor- dances provided by the environment and the students apti- tude within the task domain [18]. Within this framework we are assuming that all variation for an individuals actions is partitioned into either contextual or personal factors - there is no left over variation. This provides us the guide for inter- preting error in our model and validation more generally. If a prediction based on our conclusions is wrong it is because we have failed to estimate the ratio between prior knowledge and context. As such, we have a framework for building an unsupervised learning algorithm that attempts to simulta- neously impose the structure of context vs prior knowledge and partition the variance in student behavior between these factors.  2. MODEL The following represents one formulation of this personal  Bayesian framework. There are likely more elegant formula- tions but it provides a proof-of-concept to demonstrate the viability of the approach.  There are three parts to the model, the abstract model just discussed as the conception of students as Bayesians who resolve evidence and hypotheses using Bayes Rule, an operational model or how behavior is operationalized (what is being counted) to fit the model, and a validation model, or how the models success at characterizing the students behavior is determined.  2.1 Abstract Model It is easiest to think of each of these models in terms of  a specific example. Here we will use a single student an- swering a test item in an on-line quiz. In this scenario our student is required to confirm or dis-confirm her belief in the content being tested, for example Pythagoras Theorem. In each item on the quiz we observe her posterior probability - if she is correct this indicates a stronger belief in Pythago- ras Theorem, if she is incorrect it indicates a weaker belief. She reaches this posterior probability by weighting her prior knowledge of Pythagoras with the likelihood of the data provided in the quiz items. Prior knowledge is an intuitive concept to some extent, a stronger prior probability repre- sents a stronger belief in the underlying hypothesis, but the likelihood requires some deeper explanation. From the per- spective of the student we can look at the likelihood in two subtly different ways:  1. From a rational point of view, in which the likelihood represents how the student should rationally weigh specific relevant data in the quiz item against their prior belief.  2. From a subjective point of view, in which we acknowl- edge that we dont really have any idea what the stu- dent thinks is relevant in the quiz item in order to an- swer it correctly, and so the likelihood represents how the circumstances of the quiz influence the students behavior. Do they push her away or towards her prior belief For example, the student may find the order of the answers in a multiple choice item to be more relevant than a diagram provided to assist answering the question.    It is this second interpretation that makes Bayes a use- ful model for personalization. It defines the goal for the modeller and provides something to numerically estimate: to parse a students condition (her prior probability) from her context (the likelihood), in other words, separating what she brings to the learning task from how the learning task immediately impacts her.  2.2 Operational Model In the abstract this model makes a lot of sense, though  it is only ever going to be imperfectly operationalized as we cannot directly confirm the mental states of students, or whether the model has any biological or other representa- tion. In the following examples we will treat the student as a Markov Process, where when a student answers a series of questions each answer contains the information of the previ- ous answers. The posterior is represented by a time series of correct/incorrect answers. For this example, the likelihood will be represented by item difficulty (proportion of correct answers). A choice that is convenient but flawed in that it assumes that each student will respond to conditions in the same way. So the operational model, using a normalization factor, is represented as:   = .  (. + (1 ).(1 )) (3)  Where theta is the posterior probability,  is item diffi- culty and  is the prior probability. Our strategy here is to recursively estimate the prior () using the sequential feed of correct/incorrect answers from the student. In a classic recursive Bayes model the prior would be replaced with the posterior, but we want to incorporate feedback from the suc- cess of the estimation into our model through a validation model.  2.3 Validation Model There are many possibilities for a validation model, AUC  or some form of likelihood estimator, but here we will use the conditional probability that the model predicts the an- swer given a particular student. In this way, we will generate an individual probability of accuracy for each student that the model is estimating. The prior probability in the stu- dent model () will then be weighted by this probability of accuracy for the subsequent estimation - higher probability of over-prediction will revise the prior probability down and higher probability of under-prediction will revise the prior up. In this manner, as each item is predicted the proba- bility of accuracy is updated, and used to adjust the prior probability in the operational model. The algorithm can be represented as:  Figure 1: Validation algorithm.  The weighting is calculated as the probability of over pre-  dicting (O) multiplied by the proportion of the prior above the current estimate and the probability of under (U) pre- dicting multiplied by the proportion of the prior below the current estimate:  2 = p(O|student) (1 1) p(U |student) 1 (4)  3. SIMULATION A number of simulations of students with three different  belief values answering 30 items were run to determine how accurately the algorithm captured prior beliefs. The first simulation generated three students individually, with dif- ferent underlying belief values that were unchanging to de- termine if the algorithm could differentiate between their different prior beliefs. Then several changing belief values were simulated: 1. a student whos belief in a single hy- pothesis is linearly increasing, a student whos belief is lin- early decreasing, and students whose belief suddenly jumps up and suddenly jumps down. These belief characteristics were repeated for 1000 students to produce overall accuracy statistics.  To simulate students as they answered items we drew ran- dom values from a Gaussian distribution where the mean represented the students average belief value. The difficulty of the items was modelled as the noise in these random sam- ples. For linearly increasing or decreasing belief the random draws were also weighted by the number of the trials and for a sudden jump/drop in belief they were doubled/halved.  3.1 Simulation Results: Individual Students  Figure 2: Prediction of individual student priors for three simulated students over 30 items.  For three simulated students (Figure 2) simulated by drawing from Gaussian distributions with means of 0.25, 0.5 and 0.75 and predictions made over 30 items with a starting prior of 0.50, the algorithm resolves student priors of 0.29, 0.52 and 0.67. The accuracy for the three is 0.84 and the posterior probability of being correct of a belief value of 0.25, 0.5 and 0.75 is 0.84, 0.77 and 0.83 respectively.  When looking at linear increases over time, over thirty items the individual predictions start to track the distribu- tions after 15-20 items (Figure 3). The average accuracy for these two is 0.86 and the posterior probability of being    Figure 3: Prediction of individual student priors for linear change, increasing and decreasing over 30 items. Dashed lines show mean of the distribution answer was drawn from.  correct for both the increasing and decreasing students is 0.84.  Figure 4: Prediction of individual student priors for sudden increases and sudden decreases over 30 items, change indicated by vertical dashed line.  For sudden changes in learning state (Figure 4) the algo- rithm does a better job, over thirty items the individual pre- dictions start to track the distributions almost immediately though they are less accurate overall. The average accuracy is 0.83 and the posterior probability of being correct is 0.81.  4. DISCUSSION This paper presents a novel algorithm for predicting stu-  dent performance by characterizing students as Bayesian learners. It makes predictions about individual students us- ing only the sequence of correct and incorrect answers. It further attempts to quantify how successful it is at forecast- ing student scores using simulated student data. Forecasting success is important if the parameters from the model are ever to be used to inform automated personalization strate- gies.  Table 1: Accuracy statistics for simulations of 1000 students over 30 items  Type % Accuracy %Over %Under (Av.p(A|S)) (Av.p(O|S)) (Av.p(U|S))  0.25 0.889 0.006 0.104 (0.861) (0.022) (0.116)  0.50 0.506 0.237 0.257 (0.490) (0.237) (0.265)  0.75 0.886 0.106 0.008 (0.858) (0.118) (0.02)  Linear 0.807 0.133 0.060 Increase (0.781) (0.145) (0.074) Linear 0.799 0.077 0.150 Decrease (0.773) (0.077) (0.150) Jump 0.894 0.053 0.053  (0.865) (0.067) (0.068) Drop 0.891 0.055 0.54  (0.862) (0.070) (0.069)  4.1 Accuracy The algorithm estimates the relative impact of student  learning or aptitude factors and contextual factors on stu- dent performance. These aptitude factors are comprised of the skills and information that a student enters a task with, while contextual factors are those things that influ- ence her ability to demonstrate that knowledge. The calcu- lations are made using score sequences but assume an un- derlying continuous spectrum of knowledge that we might think of in terms of belief. The algorithm treats the student as a Bayesian learner; her score is proportional to her prior knowledge and how she interprets her context. The Bayesian algorithm splits student performance into these two factors and then uses that information to make a prediction about the students next score. This process generates estimates of a students knowledge, the impact of context, and prediction accuracy.  The algorithm performs reasonably successfully on the simulated data. It performed adequately over a 30 ques- tion sequence, predicting with substantial accuracy students simulated to have low, medium and high belief in the tested material. It also responded within 10 items to students with gradually decreasing and increasing belief and responded within 1-2 items to students who suddenly and dramati- cally changed their belief. When these tests are repeated over 1000 students we see prediction patterns that we might expect. It is substantially more difficult to predict a student whose belief is middling than those who are very certain or very uncertain. The algorithm also tends to over-predict low values and under-predict high values. This is likely a func- tion of the value that all predictions start from, 0.5. There is a certain burn in period while the algorithm calibrates.  4.2 Interpretation To return to the problem posed at the beginning of this  article, how do we interpret probability statements for an individual student performing an individual task For the Personal Bayesian model this is a more tractable problem as the probability statement refers to a property of the stu- dent, rather than a proportion of students, proportion of times, or proportion of correct/incorrect answers. In this Personal Bayes formulation it is the proportion of the se-    quence of correct and incorrect answers weighted by an esti- mate of the impact of the context on the student. This is a logical probability rather than a Frequentist sense of prob- ability but it also means that we do not fall into the traps of either having to overgeneralize about individual students by assuming they all form a homogeneous group or imag- ining hypothetical long range frequencies of students as is common in psychometrics.  4.3 Conceptual Benefits for Personalization Although the results from this work provide a promis-  ing proof of concept, the most substantial benefits that this Bayesian approach may have for personalization are concep- tual. The abstract model provides a framework for pursuing a quantified approach to personalization. Unlike traditional testing measures that rely on parsing student behavior into signal and noise this approach is optimal - it allows error to be further characterized as either an incorrect estimation about what the student knows, or an incorrect estimation about the context the student is in. This points the way to- ward building an unsupervised learning algorithm that will characterize each students data structure individually. It changes the nature of the problem, from assigning relevant subgroup membership, to estimating the impact of the con- text on the student in a single numerical value or possibly a distribution. This methodology is particularly useful in the personalization enterprise as it does so on an individual student basis and may even be able to generate estimates without reference to other students at all (unlike the solu- tion presented here that uses difficulty as a proxy for the likelihood), using only the variation in the sequence of a students answers. This may find use in situations where only data about learning experiences are available for rea- sons of anonymity. There is currently no method with this characteristic available and it may prove a useful addition to the Learning Analytics methodology as it allows us to make more efficacious statements about individual students, rather than relying on subgroup allocation. The benefits for automated personalization are substantial, but also for con- text modelling as this is an essential part of the method- ology. Since the model requires context to be numerically estimated, context cannot be ignored nor treated as noise.  5. RESOURCES Data and R code can be found at: https://github.com/  charles-lang/LAK17-bayeslearner  6. REFERENCES [1] D. Borsboom. Measuring the Mind : Conceptual  Issues in Contemporary Psychometrics. Cambridge University Press, Cambridge, UK, 2005.  [2] J. S. Bowers and C. J. Davis. Bayesian just-so stories in psychology and neuroscience. Psychological Bulletin, 138(3):389414, 2012.  [3] N. Chater and M. Oaksford. The Probabilistic Mind: Prospects for Bayesian Cognitive Science. Oxford University Press, Oxford, UK, 2008.  [4] A. T. Corbett and J. R. Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge. User Modeling and User-Adapted Interaction, 4(4):253278, Dec. 1994.  [5] L. Cosmides and J. Tooby. Better than rational: Evolutionary psychology and the invisible hand. The American Economic Review, 84(2):327332, 1994.  [6] R. T. Cox. Probability, frequency and reasonable expectation. American Journal of Physics, 14(1):113, 1946.  [7] B. de Finetti. Methods for discriminating levels of partial knowledge concerning a test item. British Journal of Mathematical and Statistical Psychology, 18(1):87123, 1965.  [8] A. Gopnik and J. Tenenbaum. Bayesian networks, Bayesian learning and cognitive development. Developmental Science, 10(3):281287, May 2007.  [9] G. Gray. A Review of psychometric data analysis and applications in modelling of academic achievement in tertiary education. Journal of Learning Analytics, 1(1):75106, May 2014.  [10] T. L. Griffiths, C. Kemp, and J. B. Tenenbaum. Bayesian models of cognition. In R. Sun, editor, The Cambridge Handbook of Computational Psychology, pages 59100. Cambridge University Press, 2008.  [11] T. L. Griffiths and J. B. Tenenbaum. Structure and strength in causal induction. Cognitive Psychology, 51(4):334384, Dec. 2005.  [12] D. Kahneman, P. Slovic, and A. Tversky. Judgment Under Uncertainty: Heuristics and Biases. Cambridge University Press, Cambridge, Massachusetts, 1982.  [13] P. Molenaar, H. Huizenga, and J. Nesselroade. The relationship between the structure of inter-individual and intra-individual variability: A Theoretical and Empirical Vindication of Developmental Systems Theory. In U. M. Staudinger and U. E. R. Lindenberger, editors, Understanding Human Development: Dialogues With Lifespan Psychology, pages 339360. Springer, 2003.  [14] F. P. Ramsey. Truth and probability. The foundations of Mathematics and Other Logical Essays, pages 156198, 1931.  [15] M. A. Sao Pedro, R. S. J. d. Baker, and J. D. Gobert. Improving construct validity yields better models of systematic inquiry, even with less information. In J. Masthoff, B. Mobasher, M. C. Desmarais, and R. Nkambou, editors, User Modeling, Adaptation, and Personalization, number 7379 in Lecture Notes in Computer Science, pages 249260. Springer Berlin Heidelberg, July 2012.  [16] R. Schlaifer and H. Raiffa. Applied Statistical Decision Theory. Studies in Managerial Economics. Division of Research, Graduate School of Business Administration, Harvard University, Boston, MA, 1961.  [17] H. A. Simon. Rationality in psychology and economics. The Journal of Business, 59(4):S209S224, 1986.  [18] G. M. Sinatra, L. J. Cronbach, H. Kupermintz, D. F. Lohman, E. B. Mandinach, A. W. Porteus, J. E. Talbert, and L. Corno. Remaking the Concept of Aptitude: Extending the Legacy of Richard E. Snow. Taylor & Francis, New York, NY, 2001.    "}
{"index":{"_id":"7"}}
{"datatype":"inproceedings","key":"Prinsloo:2017:ELA:3027385.3027406","author":"Prinsloo, Paul and Slade, Sharon","title":"An Elephant in the Learning Analytics Room: The Obligation to Act","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"46--55","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027406","doi":"10.1145/3027385.3027406","acmid":"3027406","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, obligation to act ethics","Abstract":"As higher education increasingly moves to online and digital learning spaces, we have access not only to greater volumes of student data, but also to increasingly fine-grained and nuanced data. A significant body of research and existing practice are used to convince key stakeholders within higher education of the potential of the collection, analysis and use of student data to positively impact on student experiences in these environments. Much of the recent focus in learning analytics is around predictive modeling and uses of artificial intelligence to both identify learners at risk, and to personalize interventions to increase the chance of success. In this paper we explore the moral and legal basis for the obligation to act on our analyses of student data. The obligation to act entails not only the protection of student privacy and the ethical collection, analysis and use of student data, but also, the effective allocation of resources to ensure appropriate and effective interventions to increase effective teaching and learning. The obligation to act is, however tempered by a number of factors, including inter and intra-departmental operational fragmentation and the constraints imposed by changing funding regimes. Increasingly higher education institutions allocate resources in areas that promise the greatest return. Choosing (not) to respond to the needs of specific student populations then raises questions regarding the scope and nature of the moral and legal obligation to act. There is also evidence that students who are at risk of failing often do not respond to institutional interventions to assist them. In this paper we build and expand on recent research by, for example, the LACE and EP4LA workshops to conceptually map the obligation to act which flows from both higher education's mandate to ensure effective and appropriate teaching and learning and its fiduciary duty to provide an ethical and enabling environment for students to achieve success. We examine how the collection and analysis of student data links to both the availability of resources and the will to act and also to the obligation to act. Further, we examine how that obligation unfolds in two open distance education providers from the perspective of a key set of stakeholders - those in immediate contact with students and their learning journeys - the tutors or adjunct faculty","pdf":"An elephant in the learning analytics room  the obligation  to act   Paul Prinsloo  University of South Africa  3-15, Club 1, P O Box 392  Unisa, 0003, South Africa   +27 12 433 4719  prinsp@unisa.ac.za   Sharon Slade  The Open University   Walton Hall  Milton Keynes, UK  +44 1865 486250   sharon.slade@open.ac.uk   ABSTRACT As higher education increasingly moves to online and digital  learning spaces, we have access not only to greater volumes of  student data, but also to increasingly fine-grained and nuanced  data. A significant body of research and existing practice are used  to convince key stakeholders within higher education of the  potential of the collection, analysis and use of student data to  positively impact on student experiences in these environments.  Much of the recent focus in learning analytics is around predictive  modeling and uses of artificial intelligence to both identify  learners at risk, and to personalize interventions to increase the  chance of success.    In this paper we explore the moral and legal basis for the  obligation to act on our analyses of student data. The obligation to  act entails not only the protection of student privacy and the  ethical collection, analysis and use of student data, but also, the  effective allocation of resources to ensure appropriate and  effective interventions to increase effective teaching and learning.    The obligation to act is, however tempered by a number of  factors, including inter and intra-departmental operational  fragmentation and the constraints imposed by changing funding  regimes. Increasingly higher education institutions allocate  resources in areas that promise the greatest return. Choosing (not)  to respond to the needs of specific student populations then raises  questions regarding the scope and nature of the moral and legal  obligation to act. There is also evidence that students who are at  risk of failing often do not respond to institutional interventions to  assist them.    In this paper we build and expand on recent research  by, for  example, the LACE and EP4LA workshops to conceptually map  the obligation to act which flows from both higher educations  mandate to ensure effective and appropriate teaching and learning  and its fiduciary duty to provide an ethical and enabling  environment for students to achieve success. We examine how the  collection and analysis of student data links to both the  availability of resources and the will to act and also to the  obligation to act. Further, we examine how that obligation unfolds   in two open distance education providers from the perspective of a  key set of stakeholders  those in immediate contact with students  and their learning journeys  the tutors or adjunct faculty.   CCS Concepts  Social and professional topicsComputing profession  Applied computingEducation   Keywords Learning analytics; obligation to act ethics   1. INTRODUCTION Recent theoretical, conceptual and empirical research in learning  analytics provides glimpses of the immense promise offered by  the collection, analyses and use of student data in higher  education. As the field continues to evolve, current research and  practice attempt to address a host of ethical, legal and logistical  issues, challenges and concerns [13, 22, 33, 40, 42, 44, 55].    Despite advances in the conceptualisation of the ethical and  privacy challenges in learning analytics as mapped in the  DELICATE framework [13, 22], the potential of learning  analytics remains tempered by the scope and practicalities of  student privacy as a possible show-stopper [22, p. 23].   Against a backdrop of growing research on the ethical  implications of learning analytics, this paper focuses specifically  on the institutional obligation to act. We acknowledge that this  may be curtailed and frustrated by students own  unresponsiveness. Most recently the main emphasis in exploring  the institutional obligation to act has been to focus on the  obligation to safeguard student privacy [13, 22]. The recently  published LACE review of current issues and solutions with  regard to student privacy [22] moots the issue of the role of  knowing and obligation to act as part of the ethical responsibility  of the institution and asks Does the new knowledge gained bring  with it a responsibility to act upon it What the ramifications of  action or inaction (p.7).    We attempt here to respond to the question posed by the LACE  review [22]. Underpinning the obligation to act is the extent to  which higher education institutions have the resources,  understanding and the political will to effectively respond to the  promise of learning analytics. Learning analytics produces  information that can be translated into knowledge and action, but  various factors may impact on this translation of data into  actionable knowledge and understanding [27]. Although we may  know more about our students, choosing appropriate and  effective strategies to respond is entangled in a mess of epistemic,   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13-17, 2017, Vancouver, BC, Canada  2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00 DOI: http://dx.doi.org/10.1145/3027385.3027406    philosophical, political, legal, economic, social, technological and  environmental assumptions and factors. Responding to what we  know and understand is dependent not only on the political and  institutional will to respond, but increasingly also on having the  necessary resources to respond [39]. The increasing costs in  providing an enabling environment to students who are often  underprepared means that higher education institutions  increasingly need to triage and choose how they allocate  resources. As Prinsloo and Slade [39] indicate, this is increasingly  leading to a focus on those students who may provide a return on  investment as a result of an intervention. The combination of  funding constraints with an increasing need to support often  underprepared students places institutions in a moral and legal  double-bind  even if they want to provide the support students  need, they simply cannot.    Examples of higher educations obligation to act include  providing disabled students with equitable access, preventing  discrimination, bullying (and increasingly cyber bullying) and  responding to students who are at risk of committing suicide.   These examples flow from the contractual, moral and fiduciary  duties of a higher education institution towards supporting  students at risk of physical or emotional harm. Compared to case  law which has investigated the institutions duty to act in these  cases, there appears to be no equivalent regarding an institutions  obligation to act in the event of knowing that students are at risk  of failing. With the growth in the scope and detail of learning  analytics, it is a valid question [22] as to whether students would  have recourse to legal or other action if they were identified as  more likely to fail, but not warned or supported.    Acting on what we know about students, especially where this  links to behaviors which might imply higher risk of failure, often  falls on faculty and a range of support staff who may have access  to such information through visualizations provided by learning  analytics dashboards. In reviewing how learning analytics (LA)  has been taken up by various stakeholder groups, Greller &  Drachsler [21] refer to data clients (e.g., teachers) as the  beneficiaries of the LA process who are entitled and meant to act  upon the outcome (emphasis added).   In this paper we question the assumption that we will respond by  firstly mapping the obligation to act from a series of legal and  ethical perspectives. We present two short case studies entailing a  directed content analysis of the tutor contracts in two different  distance learning institutions. The purpose of the case studies is to  determine to what extent tutors are obliged to respond to the  analyses and information provided in learning analytics. The  paper concludes with some tentative pointers as basis for action.    2. PREDICTING AND PREVENTING  STUDENT FAILURE AND DROPOUT  The early theoretical and empirical models developed by Spady  [48, 49] and Tinto [51] emphasized the social conditions and the  role of a lack of consistent, intimate interaction with others [48,  p. 78] in influencing students decision to drop-out. (Also see  [51]). Learning analytics, more than ever before, provides us with  information regarding these social conditions and the various  nuances, instances and frequency of interaction of students with  their peers, resources (whether online or offline) and teaching and  support staff in their learning journey [16, 18]. Given the  contractual nexus between university and student, it follows then  that access to actionable and empirically verifiable data relating to  factors that impact on academic success increases the potential for  liability.     In the context of this papers exploration of the moral and legal  implications that arise from learning analytics, we acknowledge  that student success be understood as the result of multiple, often  inter-dependent and mutually constitutive factors in the nexus  between students and institution in a particular micro and macro  context [50]. While we specifically focus on the moral and legal  obligations of institutions to respond to students at risk, it is  important to note that the responsibility to create an enabling  environment for student success does not exonerate students from  taking responsibility for their learning. Godor [20] however  warns that there is a danger that learning analytics emphasizes  student-related behaviours and student-related characteristics  and fails to examine institution-related behaviours and  institution-related characteristics such as the climate of academia  (p. 2; emphasis added). (Also see [50]). Godor [20] also flags that  whilst early warning systems alert teaching staff, they once again  externally manoeuvre the foundations for student success from the  institution itself back to the student (p. 5).    In the context of this paper, we propose that the knowledge and  actionable information provided by learning analytics raises the  need to reflect on the rationale, scope and constraints inherent in  higher educations moral and legal obligation to act. Given the  relatively recent emergence of learning analytics as one means to  support and potentially save students from drop-out or failure  and a corresponding lack of policy and relevant case law, we  deliberately draw upon parallel arguments relating to student  (self) harm and the obligation of the institution to act to illustrate  comparable points.     3. MAPPING RESPONSIVENESS  Institutional responsiveness entails much more than the protection  of student privacy and the ethical collection, analysis and use of  student data. The latter has received increasing attention and  support in the work of LACE [22] and published research by  individual researchers [13, 40, 44]. This paper specifically focuses  on the obligation to act as the effective allocation of resources to  ensure appropriate and effective interventions to increase effective  teaching and learning.    Given the emphasis in this paper on the institutional obligation to  act, there is a danger in that we disregard the many strategic and  operational initiatives aimed at improving the learning experience  and ultimately the return on investment for both students and the  institution. However, there is some evidence that changes in  funding regimes and increasing competition raise the need for  institutions to (re)consider resource allocation to those students  who are deemed high risk [2, 37, 39]. While it is tempting to focus  only on the impact of the increasing resource constraints on  institutional responsiveness, there are other relevant factors such a  lack of understanding,; the role of political will in shaping  institutional responsiveness; the lack of integration in institutional  sense-making and resource constraints.    3.1 Lack of understanding or the  theory/practice divide  With the advent of Massive Open Online Courses (MOOCs) and  other advances in educational technologies, a number of authors  (e.g., [17, 43]) have pointed to a need for research to take  cognisance of educational theory or past educational research.  However, there is some evidence that management and policy  makers may disregard educational theory and empirical research  [36, 37]. This leads to a potential gulf between higher education  researchers, policy makers and practitioners where much of the  current educational research is deemed irrelevant in the context of     the need for shorter-term investigations and solutions. El-Khawas  [14] highlights management and policy makers who favor  positivist and reductionist research modes rather than qualitative  modes and critical approaches. In responding to problems, there  can be a tendency to seek solutions and just-in-time research that  cut out complexity, turbulence and messiness with an emphasis  on simple and uncomplicated findings and proposals that sponsors  of the research can use and control [37, p. 344]. (See also [26]).    3.2 The role of political will and institutional  responsiveness  While the duty to respond rests on the institution as a whole, it is  the legal duty of management to define and oversee the obligation  to act. Research by El-Khawas [14] suggests that institutional  policy development tends to focus on research findings or  theoretical developments when it is politically convenient to do  so. Similarly Kogan and Henkel [28] posit that policy  development favors research that fits into the political and  technological climate of the time. Institutions respond when a  problem is recognised as serious and requiring a new solution;  when the policy community develops a financially and technically  workable solution; and when political leaders find it  advantageous to approve it [14, p.5] (emphasis added). This may  explain why some initiatives to improve student success and  retention go against the grain of research of even conventional  wisdom.    3.3 Lack of integration in institutional sense- making  While much of the research into improving student success and  retention is focused on the integration (or lack of) of students into  institutional culture (organizational and epistemic) [7, 48, 49, 51],  it is also important to point to the impact of a fragmented systemic  institutional response on an institutions obligation to act [46, 50].  Not only do various stakeholders in the institution work in silos,  responding independently of each other and resulting in overlap  and inconsistencies, institutional sense-making of students at risk  is also fragmented [37]. Institutional sense-making and research  are scattered amongst individual and institutional researchers,  academic and support departments and management and policy  development. Learning analytics may provide insight at a course  level to individual faculty and/or support teams that trigger a  response which may run contrary to institutional policy, strategic  objectives and/or allocation of resources.   There is then a real danger that the obligation to act and the  effectiveness of the institutional response is hampered by non- systematic and non-integrated approaches to respond to students  at risk.    3.4 Resource constraints  As higher education institutions respond to financial constraints  and attempt to re-align initiatives to improve student success,  there is increasing emphasis on allocating resources to move the  murky middle  those students who are currently at medium  risk of non-completion and who with some additional intervention  and support can actually pass [4, 39, 40, 53]. The support and cost  needed to address those considered at higher risk of failure are  increasingly considered to be wasted resource. The phenomenon  of the increasing resource constraints faced by higher education  raises the interesting question on how a lack of resource impacts  on the moral and legal duty to act.    Institutions increasingly find themselves in a double-bind where  they may have the political will and expertise to respond, but not  the resources.  Responding to students deemed at risk, especially  those admitted to higher education but who may need specialized  and personalized support to give them a reasonable chance of  success, is costly. How do we then respond to the moral and legal  necessity to act, when responding in appropriate and effective  ways becomes (resource-wise) impossible   3.5 Student responsibility and autonomy  An ethical approach to learning analytics should acknowledge  both the reciprocal nature of the relationship between the  institution and students as well as the impact of the asymmetrical  power relationship [44, 50]. The institutional obligation to act  does not result in students becoming mere recipients of services  and support. Students have a co-responsibility to fulfil their part in  the learning contract [13]. While students unresponsiveness or  disregard for their own responsibility does not absolve the  institution of its obligation to act, we should acknowledge the  reciprocal nature of the responsibility to ensure effective learning.    4. MAPPING SOME MORAL AND LEGAL  CONSIDERATIONS INFORMING THE  OBLIGATION TO ACT  Having a basic understanding of some of the moral and legal  considerations of the obligation to act is crucial to prevent ill- considered and hasty reactions. As stated earlier, while national  legislation and institutional policies and procedures provide clear  pointers with regard to protecting student privacy, the moral and  legal ramifications of action or inaction in learning analytics [22]  have not yet been fully mapped.    4.1 Saving the Drowning: Mapping Some of  the Moral and Legal Implications on the Duty  to Act  A lawsuit filed by parents of a suicidal student in 2002 [47] and  other similar suits form the basis for research by Massie [31] to  explore the legal responsibilities of college personnel. Central to  this is whether a higher education institution has a duty to rescue  or a contractual obligation to act once there is awareness of  students at risk. Massie [31] states that As every law student  learns in the first year, there is no general duty to rescue a  person in even the gravest danger if the actor has no control over  that person and did not create the danger, regardless of how easy  and risk-free it would be for the actor to do so (p. 637). It is clear  though that, even if there is no legal foundation, there are moral  and ethical issues to be considered. It is not the purpose of this  paper to provide a detailed analysis of the different legal, moral  and ethical implications of the question whether the obligation to  act once student risk is known. Rather, we map some of the  different moral and legal implications before using these to  engage with the consequences for learning analytic research.   4.2 Moral and Ethical Bases for an Obligation  to Act  The duty to rescue has been embedded in philosophical discourses  in various forms such as the trolley problem and the drowning  person (e.g. [1, 32, 45]). Approaches to solving these  philosophical and ethical problems are often complex and, like the  field of law, require specific knowledge and background. Very  broadly speaking, one can distinguish between deontological and  teleological approaches [30]. A deontological approach to ethics  is based on rules and forms the basis for legal and regulatory     frameworks, as well as Terms and Conditions (T&Cs) that clarify  the nature and scope of the rights and responsibilities of parties to  the agreement in a particular context. Deontological approaches  are effective in relatively stable environments. Having consensus  on following a deontological approach necessitates agreeing on  the type and choice of rules (e.g. consent-based or contract- based). A deontological approach is also based on the notion that  decisions to adhere to the rules arise from an autonomous,  objective and impartial agent (Edwards as cited in [6, p. 1072])   An alternative approach is a teleological approach where ethical  norms around the potential for harm, the scope of consent and  recourses in cases of unintended harm are negotiated and agreed  upon.  A teleological approach focuses on fulfilling the needs of  others and to maintain harmonious relations [6, p. 1072] and  considers potential vulnerabilities of those affected by the  intervention or opportunity.   Velasquez, Andre, Shanks and Meyer [54] distinguish between a  number of approaches, such as (1) a utilitarian approach (deciding  on an action that provides the greatest balance of good over  evil); (2) a rights approach (referring to basic, universal rights  such as the right to privacy, not to be injured); (3) a fairness or  justice approach; (4) the common-good approach (where the  welfare of the individual is linked to the welfare of the  community); and (5) the virtue approach (based on the aspiration  towards certain shared ideals). In an attempt to ensure ethical  problem solving, Velasquez et al [54] suggest asking five  questions, namely (1) what are the benefits and harms, to whom  and what are the alternatives (2) what are the rights of those  affected by a course of action and which course of action respects  those rights (3) which course of action treats everyone the same  except where there is a morally justifiable reason not to (4)  how will the common good be served by the action taken and (5)  which possible action develops moral virtues   It is clear that exploring moral and ethical implications may  involve choosing the lesser of two evils (e.g., [1]) and, as  illustrated by [32], deal with messy problems such as deciding to  terminate, allowing to fail or die and withdrawing aid. Contrasted  with the range of ethical approaches, legal frameworks attempt to  make such complexities more palatable by reducing them to a  series of principles or rules. Botes [6] suggests that an ethics of  justice and an ethics of care are often positioned as opposites, but  Gilligan [19] contends that both care and justice have a place in  ethical decision making and that the two aspects are inextricably  linked and in constant interaction [6, p. 1073] (emphasis added).    4.3 An Overview of the some of the Legal  Issues in the Obligation to Act  When does an obligation to act arise and what recourse does an  individual have when an obligation is not fulfilled  whether  intentionally or when the fulfillment of the obligation has become  impossible The duty to act is an established principle in different  legal frameworks. For the sake of this article we consider an  obligation as a legal bond (vinculum iuris) whereby a person, a  group of persons or organization is bound to act or refrain from  acting based on an agreement between said persons. This legal  bond imposes a duty on the obligor to perform or act depending  on the agreement, but also creates a corresponding right by which  the recipient or oblige can demand a performance or action to be  rendered, in fulfillment of the agreement.     Interestingly, in most Anglophone countries there is no general  duty to rescue a person. A duty to rescue arises in two situations   namely: (1) where a person has created a dangerous situation that  causes another person to fall into peril. Under such circumstances  the person who created the context has the duty to rescue the  endangered person; (2) where a special relationship exists  between two persons, e.g., parents and children, and between  employers and employees. The principle of reasonable care  could be questioned in these special relationships where  what  constitutes reasonable care is contextual - the extent and type of  supervision required of young elementary school pupils is  substantially different from reasonable care for college students  [31, p. 639]. However, do universities not specifically take on this  obligation at the point of formal registration     A tort or delict, depending on the legal system in a particular  context may consist of the following elements: (1) Conduct/action  or non-action  where harm may have been caused by the specific  action (commission) or omission or failure to act). It is important  to note that omission only arises when there was a duty to act; (2)  Wrongdoing or wrongfulness  where the act or non-action was in  contravention to the specific legal mores in a particular context;  (3) Intentionality or fault  where the scope of resource to action  or sanction will depend on whether the person acted intentionally  or negligently; (4) Loss/damage - the conduct or inaction must  have resulted in some form of loss or harm to the claimant in  order for them to have a claim such as material loss (e.g. financial  loss) or patrimonial loss (a reduction in a person's financial  position, such as is the case where a claimant incurred medical  expenses) or immaterial loss (e.g. pain and suffering); (5)  Causation  an important element for the complainant to prove is  that the loss or damages (directly or indirectly) arose because of  the action or non-action of the accused - in other words the  damage was a sine qua non of the plaintiffs conduct.    It is important to note that torts (or delict in the US and South  Africa) may result from negligence or criminal actions.  Negligence refers to fault which may be negligent or intentional;  whereas criminal law requires that the act be intentional. Tort  laws have a different burden of proof, such as a balance of  probabilities rather than beyond reasonable doubt as in the case of  criminal action.    A specific concept in tort law is the duty to rescue relating to  circumstances where a party can be held liable for failing to come  to the rescue of another party in peril.  Under common law, the  duty to rescue is rarely penalized through statutes of law, but this  does not erase the implicit moral duty to rescue under a set of  separate ethical arguments.    In general, accountability resulting from the special relationship  between an institution of higher learning and a student is  determined based on a combination of the following factors  existing: foreseeability of harm to the plaintiff (generally  conceded to be the most important consideration); degree of  certainty of harm to the plaintiff; burden upon the defendant to  take reasonable steps to prevent the injury; some kind of mutual  dependence of plaintiff and defendant upon each other, frequently  (as in these cases) involving financial benefit to the defendant  arising from the relationship; moral blameworthiness of  defendant's conduct in failing to act; and social policy  considerations involved in placing the economic burden of the  loss on the defendant [31, p. 639]. Even though most colleges no  longer act in loco parentis we still have a reasonable expectation,  fostered in part by colleges themselves, that reasonable care will  be exercised to protect resident students from foreseeable harm  [31, p. 640]. Massie [31] therefore concludes that while common  law is silent on the positive duty to rescue, both judicial and     legislative policies have developed that help to encourage the  moral impetus to help one in imminent peril (p. 668).   5. (RE)DEFINING RESPONSIVENESS   Institutional responsiveness is most often considered in the  context of resource constraints, and specifically human resource  constraints. We would argue though that this is a somewhat  restricted view. Institutional responsiveness includes not only  policies, systems and human resources but also increasingly  affordances offered by advances in technology such as machine  learning and Artificial Intelligence (AI). AI tools are producing  compelling advances in complex tasks, with dramatic  improvements [9, par. 1]. Without disregarding its potential, we  are also faced with situations where AI systems are already  making problematic judgements that are producing significant  social, cultural, and economic impacts in peoples everyday lives  [9, par. 1].  In the context of learning analytics we have to ask  How can we use algorithmic decision-making in higher  education to ensure, on the one hand, caring, appropriate,  affordable and effective learning experiences, and on the other,  ensure that we do so in a transparent, accountable and ethical  way [38].   Danaher [11] proposes an interesting matrix that maps potential  combinations of human-algorithmic decision-making employing  the basic actions of seeing; processing; acting; and learning. He  proposes that, in each of these four actions, humans can either act  on their own; humans can use algorithmic decision-making in any  single action or combination of actions; algorithmic decision- making can be used in any single action or combination of actions  with human oversight; or algorithms can act independent of  human oversight in any single action or combination of actions.  (See [38] for a discussion on the potential for bias, discrimination  and a range of ethical challenges in algorithmic decision-making).   6. METHODOLOGY  This paper is based on a qualitative interpretative or hermeneutic  study [5, 10] using a dialogical case study methodology as  described by, amongst others, Thomas [52].  In using a dialogical  case study we attempted to apply specific theoretical  understandings of the notion of the obligation to act to two  institutions job descriptions of tutors. As an instrumental case  study [52] we seek to gain insight into the ways that tutor  contracts understand the nature and scope of the obligation to act  in response to learning analytics.    The units of analyses were the tutor contracts of two different  distance learning institutions in respect of online courses. We used  a deductive, directed content analysis approach that entailed  identifying key concepts flowing from the literature review [15,  24]. Case studies do not (and should not) aim to produce  generalizable theories but rather phronesis or practical wisdom  which is about understanding and behaviour in particular  situations [52, p. 214]. The credibility of this research was  ensured by peer debriefing and member checking, and its  dependability and confirmability by being transparent regarding  choices and the limitations of this study as well as keeping a paper  trail of the analysis [15, 29].   The limitations to this study include (1) that neither of the two  researchers is a legal expert and that case law from differing  national contexts may have provided different or more nuanced  insights; (2) the study is only based on two institutions contracts  and job descriptions for tutors; and (3) the field of and  responsibilities resulting from learning analytics have yet to be   incorporated in our understanding of various levels of  responsibility and action. The aim of the research was not to  generalize but rather use the two case studies as a basis to  illustrate the need for further consideration and research.   7. CASE STUDIES   7.1 The Open University (OU)  The Open University (OU) in the UK is a large, open entry,  distance learning institution supporting around 200,000 students  per year. Teaching at the OU is delivered primarily through the  embedded teaching and learning design in module materials,  learning activities and assessment; and in the direct distance  teaching delivered by contracted tutors to their tutor groups  (normally around 20 students) mainly through online forums,  supported by occasional synchronous tutorial events. The distance  learning nature of the university means that learning analytics is  playing an increasing role in delivering proactive student support.  Approaches include the use of simple tracking systems which  identify students matching pre-defined combinations of  demographic and study behavior conditions and then trigger  interventions from support staff, as well as reviews of student  engagement with, for example, assessments, module content and  other online materials in order to better understand teaching and  learning design.   7.1.1 OU tutors and engagement with learning  analytics  The OU has also been developing systems to help identify at-risk  students through the development of two predictive analytics  models. One approach uses logistic regression to calculate the  probabilities of individual students being registered at key  milestones during a module presentation and is being used by  curriculum focused student support teams to guide targeted  intervention at key points [8]. A second approach, known as OU  Analyse [56] also aims to predict at-risk students and is being  piloted in a format which puts information directly into the hands  of students own tutors.    A recent study [23, draft] looked at how OU tutors experienced  this predictive tool. A group of 55 self-selecting tutors from a  range of subject modules were given access to OU Analyse over a  single presentation (around 100 other tutors were told that they  must use the information  no records were kept of their  engagement). At the end of the pilot, a small sample of tutors  were surveyed to establish the extent to which they had engaged  with information from the model and whether it led to actionable  insight. Some also participated in semi-structured interviews.  Weekly tutor engagement with the dashboard varied across the  modules and across time with higher usage generally associated  with key events (such as assignment deadlines) but was typically  25% or less of tutors.    The levels of engagement by tutors with student data are impacted  by a wide range of factors (see for example [40] who refer to the  Technology Acceptance Model developed by Davis et al [12] as  useful in explaining why teachers use educational technology).  However, at the OU at least there seems a possible disconnect  between developers who see the value of learning analytics as a  key tool to provide insight and greater student support and have  greater expectations around both adoption rates and acceptance of  the value of these approaches, and tutors who can opt to employ  such an approach (or not).     Expectations for student support from tutors are set out in several  documents and policies. The OUs Tutor Support Statement [34]  for students states that tutors will Seek to make contact with you  if you appear not to be engaging with the module activities, in  order to discuss ways of supporting you with your studies and/or  options open to you. The formal Terms and Conditions [35] for  OU tutors make no reference to an expectation that a tutor uses  learning analytics to provide student support, save for a broad  statement outlining their duties which includes that tutors should  monitor the progress of students on their course, including  making contact with students who do not submit assignments. A  new contract has been in negotiation for some time, but it is not  apparent that this aspect of the Terms and Conditions will  fundamentally change. However, revised role guidance is likely to  include more reference to the increasing uses of technology.  Specifically the draft guidance includes tasks such as carrying out  an initial identification of student learning support needs using  available student profile data; making proactive contact with  students at critical points; monitoring, recording and supporting  students engagement with learning and their progress on a  module and taking appropriate action as necessary to support  student progress (emphasis added) and acquiring, developing and  updating the necessary skills to work in the OU e-teaching and  learning environment. Although it is encouraging that revisions  attempt to highlight the potential and need for greater tutor  engagement with student data to inform effective support, it seems  a shame that such work is labelled as role guidance rather than a  necessary part of the normal tutor contract. This also potentially  contradicts the Universitys own policy regarding the ethical use  of student data for learning analytics [33] which has, as one of its  main principles, that the University has a responsibility to all  stakeholders to use and extract meaning from student data for the  benefit of students. Although it is fair to say that policy often lags  practice, the reluctance to enforce an obligation to act as part of  the revised tutor contract perhaps appears a missed opportunity  and inconsistent with broader policy.    7.2 University of South Africa (Unisa)  Unisa, with more than 300,000 students, is the largest distance  education provider on the African continent [26] and due to the  cost and sustainability of access to the Internet [25, 57], offers the  majority of their courses in a technology-enabled paradigm.  All  courses have an online presence with a range of resources and  possibilities for interaction, and data shows that the majority of  students access these online resources. Seven of the 8 colleges or  schools at Unisa have a fully online Signature Course (SC) that is  a compulsory requirement towards the completion of a certificate,  diploma or degree. These SCs have a two-pronged objective: to  leverage the interactive potential of the digital technology and to  help students to reflect on the role of their discipline in the  societal transformation of South Africa [26, p. 226].     7.2.1. Unisa tutors and engagement with learning  analytics   In the context of this paper, we focus on the scope and  responsibilities of Unisa e-tutors and on the Teaching Assistants  (TAs) providing digital academic support in these SCs.    In order to reduce the impact of semi-variable costs and to protect  a certain level of economies of scale, each TA must support a  large class (200 students, in four classes of 50) [26, p. 227].  Given that TAs have to mark 10 assignment items per student per  semester, they spend most of their time marking assignments and  dealing with administrative issues, and less time on facilitating   discussions and learning. Hlsmann & Shabalala [26] indicate that  This suggests a self-defeating trait in the design template of the  SCs: In order to increase participation in online discussion the  number of assignments had been increased; as a consequence of  the high marking load, TAs find little time for the facilitation of  online discussions (p. 228). Appointed TAs are subject to a  contract which tasks set out in a formal Task Agreement.  Amongst the tasks specified is an expectation that TAs will:  Monitor student online learning.    Similarly, Unisas e-tutors are bound to 4 Key Performance Areas  (KPAs). KPA 2 which focuses on management of the student  learning experiences online assumes that e-tutors will track  student engagement and requires e-tutors to: Monitor student  participation in online activities; follow up with students who are  not participating in online discussion to assess the reasons for this;  and Monitor and report on the students progress. Although KPA  4 focuses on academic and technical support online, the tasks  included are very much reactive, that is, there is no explicit  reference to the use of a learning analytics approach to make  sense of student behaviors, nor to predict or act upon any insight  gained.    Despite the widespread use of technology as a means to deliver  teaching and learning, little or no use has been made of an active  learning analytics approach. The move toward tracking and alert  systems for staff and students has been flagged as a future  initiative however [3], and it is hoped that such systems may  usefully inform the work of both e-tutors and TAs in the future.   8. SOME POINTERS FOR REALIZING  THE OBLIGATION TO ACT  Earlier in the paper we acknowledge that realizing the potential of  learning analytics to influence strategic decision making in  creating more effective and enabling learning environments, and  increasing students chances of success, depends on a variety of  factors. We alluded to the political will of institutions to respond  in appropriate and effective ways. The appropriateness and  effectiveness of these responses, however, depend on institutions  understanding of the complexities of student retention and success  as well as having the necessary resources and structures in place  to respond accordingly. It is relatively easy to bemoan the  negative impact of issues such as funding constraints and  increased competition, and forget that resources are but a part of  an appropriate and effective response. The scope of an  institutions political will as set out in its policies, appointment  contracts and performance criteria depends on its understanding  and definition of student success and the potential, limitations and  ethical challenges around the collection, analyses and use of  student data.   An often neglected area then is the moral and legal obligation that  arise from having access to more data. More than ever before  institutions have rich, albeit incomplete or holistic pictures of  students learning journeys. Having access to data does not, per  se, translate into complete or necessarily accurate information or  knowledge. But where it does flag a likely outcome which might  result in student harm, it does raise the obligation to act, whether  on moral and/or legal grounds.    In this concluding section we map some tentative pointers for  considering the moral and legal implications of the obligation to  act.     8.1 Co-responsibility in an asymmetrical  power and contractual relationship  It is important to acknowledge that institutions do not bear sole  responsibility for ensuring learning and student success. Effective  learning is found in the nexus between students locus of control  and situated agency and the institutions fiduciary and contractual  duty [44, 50]. While learning analytics has the potential to provide  students with timely feedback and analysis on how to change their  behaviors, we should be careful not to assume this exonerates the  institution from its fiduciary and contractual duty [20]. The  obligation to act is then a co-responsibility of students and  institution, albeit tempered by the asymmetrical power and  contractual relationship in which the institution has very specific  moral and legal duties to respond.   8.2 Data, information and knowledge  As learning analytics matures, the potential for real-time feedback  and personalization of learning becomes an increasing reality. Our  expectations and assumptions regarding data collection processes  (sample size, context, limitations), as well as our understanding of  the collected data itself, should be scrutinized for bias, statistical  error and unintended consequences.  Accepting that data does not  necessarily translate into information and knowledge [27] raises a  number of ethical concerns that should be acknowledged and  addressed [44, 55]. As proposed in this paper, the collection and  analysis of data increases the necessity and scope of the obligation  to act. In responding to the analyses there is a fiduciary and  contractual duty to ensure that processes and analyses are rigorous  and open for peer review and adaptation.    8.3 The scope and implications of the moral  basis for the obligation to act  In approaching the necessity and scope of higher education  institutions obligation to act from a deontological or rule-based  moral position, it is clear that higher education institutions have  an obligation to act (see next point). In the current fluid and  uncertain higher education context we propose that, despite the  stability of a deontological approach, a simple reliance on rules  and legal frameworks is not enough, and perhaps not even fair  [39]. A teleological approach allows for higher education  institutions to create spaces to realize and investigate the potential  of accepting the co-responsibility of students and institution  towards making sense of the responsibilities arising from both  data and analyses and determination of the range and limitations  of available resources to enable more effective and responsive  learning. A teleological approach allows the possibility to explore  the benefits and harms, the responsible actors, and the alternatives  available in a particular context [54]. We suggest then the need to  engage with the potential of both a deontological and teleological  approach in exploring the scope, limitations and reasonableness of  the obligation to act in a particular institutional, disciplinary and  geopolitical context.    8.4 The scope and implications of the legal  basis for the obligation to act  Acknowledging that legal frameworks and case law may differ  depending on the geopolitical context of a higher education  institution, it seems clear that the collection and analyses of data  by higher education is not only part of the mandate of higher  education [39], but also implies a vinculum iuris or legal basis to  act. There is a contractual obligation that arises between student  and institution that stipulates the duties and responsibilities of  both parties to the agreement. We accept that learning analytics   has a case to answer when this leads to information and  knowledge that students are potentially at-risk or that students  behavior at a particular point in time in a course increases the  probability of dropout. While acknowledging that students and  institution are co-responsible, there is ample research that  administrative and operational inefficiencies have a negative  impact on student learning, and as such, increases the legal basis  for students to demand a performance or action in fulfilment of  the agreement.    It falls outside the scope of this paper to explore the various  aspects and implications of intentionality or fault, the proof of  damage or loss, and causality. The obligation to act arising from  the data, information and knowledge harnessed through learning  analytics is also tempered by the notion and definition of  reasonable care. However, we suggest that there should be  account for the responsibilities that arise in knowing more about  our students dispositions, contexts and learning journeys.    8.5 The implications of the obligation to act  for policies and performance agreements  This paper has mapped a number of issues resulting from the  moral and legal bases for the obligation to act. The two cases  provide some insight into the need for review of the contractual  arrangements of tutors as a result of that obligation (whilst  recognizing the differences in maturity in using learning analytics  to make informed pedagogical decisions in these two institutions).   In both cases there is no formal or explicit expectation nor any  contractual requirement that tutors will engage with the  information and knowledge provided by learning analytics. While  the OU case study suggests that tutors are not the only  institutional staff responding to data flagging at-risk students, the  case makes clear that staff at the forefront of the potential of  learning analytics are not (yet) contractually bound to respond.    The two case studies highlight the importance of an institutional,  integrated response to the information and knowledge garnered  from learning analytics. Although some applications do address  issues across the whole student journey, learning analytics is often  focused on effective learning and support at a course level. This  suggests not only a certain granularity of information, but also a  response typically at course level. The ability of staff (whether  tutors or teaching assistants, faculty or support and administrative  staff) to respond is muddied and embedded in broader institutional  policies and funding arrangements, which impact on departmental  and course structures, tutor: student ratios, the growing  outsourcing of academic support to contract, and increasingly,  zero-hour contract staff, etc. And, in addition, the willingness and  availability of resources enabling institutions to respond and fulfil  their obligatory and fiduciary duty to care are further impacted by  external legal, policy, quality assurance and funding regulatory  arrangements and bodies.      Despite this, the recognition of the impact and constraints of the  regulatory and funding contexts does not provide a basis for  institutions to ignore the moral and legal implications of their  obligation to act. We suggest that an integrated and considered  response to the obligation to act arising from learning analytics  will involve responding on the following levels:   8.5.1 Institutional policies and frameworks  Institutions must have the political will to act in response to the  information and knowledge resulting from learning analytics. The  cases presented here illustrate the need for an institutional     response to the provision of appropriate policy and operational  frameworks. One such example is the OUs own policy on the  ethical use of student data for learning analytics [33]. However, as  the OU case illustrates, simply having a policy does not  necessarily result in the will or the resources to respond.   Realizing and optimizing the potential of learning analytics to  enrich institutional and student understanding of the factors that  shape students learning journeys necessitates (re)considering the  different responsibilities flowing from knowing more about our  students. The complexities of responding to this knowledge will  entail appropriately qualified knowledgeable and resourced staff  who not only understand the scope and implications of the  obligation to act, but who are held contractually liable for  responding.    8.5.2 Teaching contracts  Responding appropriately, ethically and effectively to the  information and knowledge resulting from learning analytics  implies that the obligation to act should be embedded in the  contracts and performance criteria of those at the forefront of  teaching, whether as course leaders, faculty or tutors/teaching  assistants. Both cases provide some evidence that contracts and  performance agreements should be more specific than simply  asking teaching staff to monitor students learning. Although  interpretation of monitor may include taking cognizance of the  information and knowledge provided by learning analytics,  contracts and performance agreements should be much more  precise in defining expectations.    8.5.3 Oversight and accountability  In the light of the moral and legal obligations to act on the  information and knowledge resulting from learning analytics,  there is a need to also consider the scope and nature of oversight  and accountability. The need to respond to information  highlighting individual or groups of students risk of failure or lack  of engagement also raises questions around who and how we  ensure that those responses are appropriate, ethical and an  optimum use of available resources. The very nature of learning  analytics as an increasingly fine-grained (yet partial) picture of  students learning assumes and possibly necessitates timely and  detailed responses. While there is increasing consideration for the  ethical dimensions of learning analytics, the exact scope and  nature of accountability and oversight are still uncertain [55].    Also crucial is student agency and their recourse to action. We  propose that students are not only co-responsible for their  learning, but are also impacted by institutional decisions that may  either enable their learning or impact negatively on their learning.  In the event of institutions knowing that students behaviors put  them at risk, we propose that institutions have a moral and legal  obligation to act  to involve, inform and enable students to take  the necessary steps to alleviate risk. Should institutions fail to act,  whether by fault or intentionally, students should have a  contractual right to demand redress.    Case law in different contexts relating to higher education  institutions moral and legal duty to respond to cases of bullying,  discrimination and student suicide provides the rationale for this  papers exploration of the moral and legal obligation to act arising  from learning analytics.    8.6 The potential and perils of new  developments in technology  Advances in educational technology provide interesting and  promising possibilities in considering the obligation of higher   education institutions to act. While we cannot ignore the ethical  challenges posed by the use of algorithms, machine learning and  Artificial Intelligence, we also cannot ignore the potential to make  greater sense of student learning, or to respond to students with  specific needs. (See [38]).     9. LIMITATIONS TO THIS STUDY  This paper arose from the authors involvement in the discourses  emanating from the maturation of learning analytics as discipline  and field of research in two very different contexts. We  acknowledge that though there are differences in pedagogy and  educational delivery between distance education and more  traditional and/or distributed forms of delivery, the issues raised in  this paper, though not conclusive, raise important points for  consideration irrespective of educational delivery mode. We  further acknowledge that not only does the mode of delivery  impact on the maturation and role of learning analytics in our two  institutions, but also that the case studies in this paper cannot be  used to generalize to all distance or distributed learning contexts.  As the evidence from the two case studies presented suggests,  knowing more about our students and having the potential to  surveil students activities and recognize behavior that indicate the  probability of the risk of failure, raises important moral and legal  issues.   While we firmly believe that this paper addresses a real issue in  the maturation and institutionalization of learning analytics, we  acknowledge that the moral and legal issues pertaining to the  obligation to act are complex and require expert opinion. As  educators and researchers we acknowledge that this paper has but  touched the surface of the moral and legal issues in respect of the  obligation to act. Despite this limitation, this paper proposes  tentative pointers to guide engagement with the duties and  responsibilities resulting from knowing more about our students  dispositions, engagement or lack of, and their relative  probabilities of being at risk of failure.    10. (IN)CONCLUSIONS  In the light of the increasing volume, variety, velocity and  veracity of student data available to be collected, analyzed and  used in learning analytics, this paper raised the responsibilities  resulting from knowing more about our students and the resulting  moral and legal foundations in the scope and nature of the  obligation to act. There is ample and increasing evidence that the  insights provided by learning analytics can inform students to  adjust their behaviors, and institutions to provide additional or  more effective and appropriate support.  As the two case studies in  this paper suggests,  knowing more about our students and making  this information and knowledge available to a range of  stakeholders, does not necessarily result in action.    Responding to the analyses and insights provided by learning  analytics is often constrained by a range of factors, such as lack of  political will, gaps in performance contracts and/or a lack of  resources.  While these factors are mitigating and sobering factors  in considering the moral and legal obligation to act, they do not  indemnify or exonerate higher education from considering the  moral and legal implications of knowing more about our students.    This paper proposed that a deontological or rule-based response to  the obligation to act may not be sufficient in the light of the  instability in the field of higher education. Despite the limitations  in a deontological approach, the obligation to act is embedded in a  rich and detailed legal framework of thought that raises important  issues for learning analytics. Complimenting a deontological     approach to considering the scope and nature of the obligation to  act, is a teleological approach that determining the scope,  timeliness and resources needed to enact the obligation to act  should be negotiated between everyone affected.    We have outlined suggestions for realizing the obligation to act  based on an understanding of student and institution co- responsibility despite and amidst the asymmetrical power and  contractual relationship; reconsidering our assumptions regarding  data, information and knowledge; the moral and legal implications  pertaining to an obligation to act and lastly, the implications of the  obligation to act for policies and performance agreements.   Despite their limitations, the two short case studies presented  provide a basis for our assertion that the time has come to explore  the moral and legal obligation to respond to knowing more about  our students dispositions, learning behaviors and risk profiles.   11. ACKNOWLEDGEMENTS  We would like to acknowledge the input and support for this  paper received from key staff at the Open University and Unisa  for input and legal opinions. Any shortcomings in this paper are  the authors own. We appreciate the valuable input received from  the three reviewers on the earlier submission of this paper.   12. REFERENCES  [1] Alexander, L. (2005). Lesser evils: A closer look at the   paradigmatic justification. Law and Philosophy, 24(6), 611- 643.   [2] Altbach, P.G., Reisberg, L., & Rumbley, L.E. (2009). Trends  in global higher education: Tracking an academic revolution.  Retrieved from  http://unesdoc.unesco.org/images/0018/001831/183168e.pdf   [3] Baijnath, N. (2015) Going digital, presented at the Learning  Africa Conference, Ethiopia, 21 May 2015. Retrieved from:  http://www.unisa.ac.za/news/wp- content/uploads/2015/05/eLearning-Africa-2015-Speaking- Notes-N-Baijnath-200515.pdf   [4] Blythe, S., Darabi, R., Kirkwood, B. S., & Baden, W. (2009).  Exploring options for students at the boundaries of the  at- risk  designation. WPA: Writing Program Administration- Journal of the Council of Writing Program Administrators,  33, 9-28.   [5] Bos, W. & Tarnai, C. (1999). Content analysis in empirical  social research, International Journal of Educational  Research, 31, pp. 659-671.    [6] Botes, A. (2000). A comparison between the ethics of justice  and the ethics of care. Journal of Advanced Nursing, 32,  10711075. DOI: 10.1046/j.1365-2648.2000.01576.x   [7] Braxton, J.M. (Ed.). (2000). Reworking the student departure  puzzle. Nashville: Vanderbilt University Press.    [8] Calvert, C. E. (2014). Developing a model and applications  for probabilities of student success: a case study of predictive  analytics. Open Learning: The Journal of Open, Distance and  e-Learning, 29(2), 160-173. doi: 10.1080/02680513.2014.  931805   [9] Crawford, K., & Whittaker, M. (2016, September 12).  Artificial intelligence is hard to see. Why we urgently need  to measure AIs societal impacts. [Web log post]. Medium.  Retrieved from https://medium.com/@katecrawford/artificial  -intelligence-is-hard-to-see-a71e74f386db#.wi7sq5l3a    [10] Creswell, J.W. (2007). Qualitative inquiry & research design.  Choosing among five approaches, 2nd edition, London, UK:  SAGE Publications.    [11] Danaher, J. (2015, June 15). How might algorithms rule our  lives Mapping the logical space of algocracy. [Web log  post]. Retrieved from  http://philosophicaldisquisitions.blogspot  .co.za/2015/06/how-might-algorithms-rule-our-lives.html      [12] Davis, F. D., Bagozzi, R. P., & Warshaw, P. R. (1989). User  acceptance of computer technology: A comparison of two  theoretical models. Management Science, 35(8), 9821002.   [13] Drachsler, H., & Greller, W. (2016). Privacy and learning  analytics  its a DELICATE issue. 6th Learning Analytics  and Knowledge Conference 2016, April 25-29, 2016, pp. 89- 98. Edinburgh, UK. DOI: http://dx.doi.org/10.1145/2883851.  2883893.   [14] El-Khawas, E. (2000). Patterns of communication and  miscommunication between research and policy. In S.  Schwarz & U. Teichler (Eds.), The institutional basis of  higher education research. Experiences and perspectives (pp.  45-55). London, UK: Kluwer Academic Publishers.   [15] Elo, S., & Kyngs, H. (2007). The qualitative content  analysis process, Journal of Advanced Nursing, 62, pp. 107- 115. DOI: 10.1111/j.1365-2648.2007.04569.x.   [16] Ferguson, R. (2012). Learning analytics: drivers,  developments and challenges. International Journal of  Technology Enhanced Learning, 4(5-6), 304-317.   [17] Gasevic, D., Kovanovic, V., Joksimovic, S., & Siemens, G.  (2014). Where is research on massive open online courses  headed A data analysis of the MOOC Research Initiative.  The International Review of Research in Open and  Distributed Learning, 15(5), 134-176   [18] Gaevi, D., Dawson, S., & Siemens, G. (2015). Lets not  forget: Learning analytics are about learning. TechTrends,  59(1), 64-71.   [19] Gilligan, C. (1982). In a different voice: Psychological  theory and womens development. Cambridge, Mass:  Harvard University Press.    [20] Godor, B. P. (2016). Academic fatalism: applying  Durkheims fatalistic suicide typology to student drop-out  and the climate of higher education. Interchange, 1-13. DOI:  10.1007/s10780-016-9292-8   [21] Greller, W. & Drachsler, H. (2012). Translating Learning  into Numbers: Toward a Generic Framework for Learning  Analytics. Educational Technology and Society. 15 (3): 42 57   [22] Griffiths, D., Drachsler, H., Kickmeier-Rust, M., Hoel, T., &  Greller, W. (2016). Is privacy a show-stopper for learning  analytics A review of current issues and solutions. Learning  Analytics Review 6. LACE. Retrieved from  http://www.laceproject.eu/learning-analytics-review/is- privacy-a-show-stopper/   [23] Herodotou, C., Rienties, B, Boroowa, A., Zdrahal, Z., Hlosta,  M., Naydenova, G. (2016) Using Predictive Learning  Analytics to Support Just-in-time Interventions: The  Teachers' Perspective. To be submitted to Teaching and  Teacher Education.     [24] Hsieh, H-F., & Shannon, S.E. (2005). Three approaches to  qualitative content analysis, Qualitative Health Research,  15(9), 1277-1288.  DOI: 10.1177/1049732305276687.   [25] Hlsmann, T. (2016). The impact of ICT on the costs and  economics of distance education: A review of the literature.  Retrieved from http://oasis.col.org/handle/11599/2047    [26] Hlsmann, T., & Shabalala, L. (2016). Workload and  interaction: Unisas signature courses  a design template for  transitioning to online DE. Distance Education, 37(2), 224- 236.     [27] Kitchen, R. (2014). The data revolution. Big data, open data,  data infrastructures and their consequences. London, UK:  Sage.    [28] Kogan, M., & Henkel, M. (2000). Future directions for  higher education policy research. In S. Schwarz & U.  Teichler (Eds.), The institutional basis of higher education  research. Experiences and perspectives (pp. 25-43). London,  UK: Kluwer Academic Publishers.   [29] Lincoln, Y. S., & Guba, E. G. (1990). Judging the quality of  case study reports. International Journal of Qualitative  Studies in Education, 3(1), 53-59. DOI:  10.1080/0951839900030105   [30] Marshall, S., 2014. Exploring the ethical implications of  MOOCs. Distance Education, 35(2), pp.250-262.   [31] Massie, A. M. (2007). Suicide on campus: The appropriate  legal responsibility of college personnel. Marq. L. Rev., 91,  625.   [32] McMahan, J. (1993). Killing, letting die, and withdrawing  aid. Ethics, 103(2), 250-279.   [33] Open University. (2014). Policy on ethical use of student  data for learning analytics. Retrieved from  http://www.open.ac.uk/  students/charter/sites/www.open.ac.uk.students.charter/files/f iles/ecms/web-content/ethical-use-of-student-data-policy.pdf    [34] Open University Tutor (Associate Lecturer) Support  Statement (2012). Retrieved from http://www.open.ac.uk/  students/charter/sites/www.open.ac.uk.students.charter/files/f iles/ecms/web-content/tutor-support-statement.pdf    [35] Open University Terms and Conditions of Service for  Associate Lecturers (2016). Retrieved from  http://intranet6.open.ac.uk/student-services/main/sites/  intranet6.open.ac.uk.student-services.main/files/files/ecms  /web-content/associate-lecturers/al-services/AL-Terms-and- conditions-April-2016.pdf   [36] Patton, MC. 2008. Utilization-focused evaluation. [4th  edition]. London, UK: Sage.   [37] Prinsloo (2016a). Evidence-based decision making as sance:  implications for learning and student support. In Jan Botha &  Nicole Muller (Eds.), Institutional Research in support of  evidence-based decision-making in Higher Education in  Southern Africa (pp. 331-353). Stellenbosch, South Africa:  SUN Media.   [38] Prinsloo, P. (2016b, October 5). The increasing  (im)possibilities of justice and care in open, distance  learning. Presentation at EDEN Research Workshop,  Oldenburg, Germany. Retrieved from  http://www.slideshare.net/ prinsp/the-increasing- impossibilities-of-justice-and-care-in-open-distance-learning    [39] Prinsloo, P., & Slade, S. (2014). Educational triage in higher  online education: walking a moral tightrope. International  Review of Research in Open Distributed Learning  (IRRODL), 14(4), pp. 306-331.  http://www.irrodl.org/index.php/ irrodl/ article/view/1881   [40] Prinsloo, P., & Slade, S. (2016). Here be dragons: Mapping  student responsibility in learning analytics, in Mark  Anderson and Collette Gavan (eds.), Developing Effective  Educational Experiences through Learning Analytics (pp.  174-192). Hershey, Pennsylvania: ICI-Global.     [41] Rienties, B., Giesbers, B., Lygo-Baker, S., Ma, H.W.S &  Rees, R. (2016). Why some teachers easily learn to use a new  virtual learning environment: a technology acceptance  perspective. Interactive Learning Environments, 24(3) pp.   539552. Rule, P., & John, V. (2011). Case study research,  Pretoria: Van Schaik Publishers.   [42] Sclater, N. (2015, March 3). Effective learning analytics. A  taxonomy of ethical, legal and logistical issues in learning  analytics v1.0. JISC. Retrieved from https://  analytics.jiscinvolve.org/wp/2015/03/03/a-taxonomy-of- ethical-legal-and-logistical-issues-of-learning-analytics-v1-0/    [43] Siemens, G., Irvine, V., & Code, J. (2013). Guest editors'  preface to the special issue on MOOCs: an academic  perspective on an emerging technological and social trend.  Journal of Online Learning and Teaching, 9(2), iii-vi.   [44] Slade, S. & Prinsloo, P. (2013). Learning analytics: ethical  issues and dilemmas. American Behavioral Scientist 57(1),  15091528.   [45] Schmidtz, D. (2000). Islands in a sea of obligation: limits of  the duty to rescue. Law and Philosophy, 19(6), 683-705.   [46] Simpson, O. (2013). Supporting students in online open and  distance learning. London, UK: Routledge.   [47] Sontag, D. (2002, April 28). Who was responsible for  Elizabeth Shin The New York Times. Retrieved from  http://www.nytimes.com/2002/04/28/magazine/who-was- responsible-for-elizabeth-shin.html    [48] Spady, W. G. (1970). Dropouts from higher education: An  interdisciplinary review and synthesis. Interchange, 1(1), 64- 85. DOI: 10.1007/BF02214313   [49] Spady, W. G. (1971). Dropouts from higher education:  Toward an empirical model. Interchange, 2(3), 38-62. DOI:  10.1007/BF02282469   [50] Subotzky, G., & Prinsloo, P. (2011). Turning the tide: a  socio-critical model and framework for improving student  success in open distance learning at the University of South  Africa. Distance Education, 32(2): 177-19.    [51] Tinto, V. (1975). Dropout from higher education: a  theoretical synthesis of recent research. Review of  Educational Research, 45(1), 89-125.   [52] Thomas, G. (2011). How to do your case study. A guide for  students and researchers, London, UK: Sage.   [53] Tyson, C.  (2014, September 10). The murky middle. Inside  HigherEd. Retrieved from  https://www.insidehighered.com/  news/2014/09/10/maximize-graduation-rates-colleges- should-focus-middle-range-students-research-shows    [54] Velasquez, M., Andre, C., Shanks, T.S.J., & Meyer, M.J.  (2015, August 1). Thinking ethically. Retrieved from  https://www.scu.edu/ethics/ethics-resources/ethical-decision- making/thinking-ethically/    [55] Willis, J. E., Slade, S., & Prinsloo, P. (2016). Ethical  oversight of student data in learning analytics: a typology  derived from a cross-continental, cross-institutional  perspective. Educational Technology Research and  Development, 1-21.   [56] Wolff, A., Zdrahal, Z., Herrmannova, D., Kuzilek, J., &  Hlosta, M. (2014). Developing predictive models for early  detection of at-risk students on distance learning modules,  Workshop: Machine Learning and Learning. Presented at the  Learning Analytics and Knowledge (2014), Indianapolis,  Indiana, USA. Retrieved from  https://pdfs.semanticscholar.org/3d16/f4008858795b126a351 354101b9e378c9337.pdf   [57] World Bank. (2016). Digital dividends. Washington:  International Bank for Reconstruction and Development /  The World Bank. Retrieved from http://www.worldbank.org  /en/publication/wdr2016.        "}
{"index":{"_id":"8"}}
{"datatype":"inproceedings","key":"Ferguson:2017:ECA:3027385.3027396","author":"Ferguson, Rebecca and Clow, Doug","title":"Where is the Evidence?: A Call to Action for Learning Analytics","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"56--65","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027396","doi":"10.1145/3027385.3027396","acmid":"3027396","publisher":"ACM","address":"New York, NY, USA","keywords":"access, ethics, evidence, evidence hub, generalisability, learning analytics cycle, reliability, validity","Abstract":"Where is the evidence for learning analytics? In particular, where is the evidence that it improves learning in practice? Can we rely on it? Currently, there are vigorous debates about the quality of research evidence in medicine and psychology, with particular issues around statistical good practice, the 'file drawer effect', and ways in which incentives for stakeholders in the research process reward the quantity of research produced rather than the quality. In this paper, we present the Learning Analytics Community Exchange (LACE) project's Evidence Hub, an effort to relate research evidence in learning analytics to four propositions about learning analytics: whether they support learning, support teaching, are deployed widely, and are used ethically. Surprisingly little evidence in this strong, specific sense was found, and very little was negative (7%, N","pdf":"Where is the evidence   A call to action for learning analytics   Rebecca Ferguson  Institute of Educational Technology   The Open University  Walton Hall, Milton Keynes,    MK7 6AA  UK  Rebecca.Ferguson@open.ac.uk                Doug Clow  Institute of Educational Technology   The Open University  Walton Hall, Milton Keynes,    MK7 6AA  UK  Doug.Clow@open.ac.uk        ABSTRACT  Where is the evidence for learning analytics In particular, where  is the evidence that it improves learning in practice Can we rely  on it Currently, there are vigorous debates about the quality of  research evidence in medicine and psychology, with particular  issues around statistical good practice, the file drawer effect, and  ways in which incentives for stakeholders in the research process  reward the quantity of research produced rather than the quality.  In this paper, we present the Learning Analytics Community  Exchange (LACE) projects Evidence Hub, an effort to relate  research evidence in learning analytics to four propositions about  learning analytics: whether they support learning, support  teaching, are deployed widely, and are used ethically. Surprisingly  little evidence in this strong, specific sense was found, and very  little was negative (7%, N=123), suggesting that learning analytics  is not immune from the pressures in other areas. We explore the  evidence in one particular area in detail (whether learning  analytics improve teaching and learners support in the university  sector), and set out some of the weaknesses of the evidence  available. We conclude that there is considerable scope for  improving the evidence base for learning analytics, and set out  some suggestions of ways for various stakeholders to achieve this.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education    K.4.1 [Computers and Society]  Keywords  Access, Ethics, Evidence, Evidence Hub, Generalisability,  Learning Analytics Cycle, Reliability, Validity   1. INTRODUCTION  The first Learning Analytics and Knowledge conference in 2011  explored what the call for papers described as the measurement,  collection, analysis and reporting of data about learners and their   contexts, for purposes of understanding and optimizing learning  and the environments in which it occurs. In contrast to other  areas of technology-enhanced learning (TEL) and quantitative  educational research, there was a concern with closing the loop  [19] to achieve improvements in learning practice. The learning  analytics community and literature have grown steadily since  then. How far have we progressed towards that goal and how can  we evidence this progress   To answer these questions, we will first explore how evidence has  developed in practice in two entirely separate fields (medicine and  psychology). This overview shows that there are problems with  evidence in many scientific fields and that many of the problems  we encounter (for example, publication bias, the Hawthorne  Effect and confusion between causality and correlation) are not  confined to learning analytics. We then move on to an  examination of the use of evidence in education before focusing  on the case of learning analytics and suggesting possible actions.   2. WHAT IS EVIDENCE  2.1 Evidence-based medicine  In the 1970s, Cochrane raised concerns about the evidence base  for medical practice. This led to the establishment of the Cochrane  Collaboration (now simply Cochrane)1 in 1993, with the aim of  improving the evidence base for practice. This dovetailed with the  development of the evidence-based medicine movement, the  conscientious, explicit and judicious use of current best evidence  in making decisions about the care of individual patients [58].  Table 1: Example of a hierarchy of evidence [51]   Systematic reviews and meta-analyses   Randomised controlled trials with definitive results   Randomised controlled trials with non-definitive results   Cohort studies   Case-control studies   Cross sectional surveys   Case reports   Although the evidence-based medicine movement stresses that  evidence-based medicine is not restricted to randomized trials  and meta-analyses [58], these are its main focus, because these  methods rank high in hierarchies of evidence. Like the hierarchy  in Table 1 above, they rank evidence in terms of reliability, with  case reports considered the least reliable and randomised                                                                       1 http://www.cochrane.org/  2 http://www.alltrials.net/   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13 - 17, 2017, Vancouver, BC,  Canada Copyright is held by the owner/author(s). Publication rights  licensed to ACM. ACM 978-1-4503-4870-6/17/03$15.00  DOI:  http://dx.doi.org/10.1145/3027385.3027396     controlled trials (RCTs) the most reliable. This is recognised to be  a simplistic view that can prove problematic in contexts such as  social or public health [51], but the intention is to focus on the  strongest possible evidence to support the evidence-based  medicine that is now almost ubiquitous in Western healthcare.   Although concerns were raised about this approach, there was also  considerable optimism. More recently, there have been worries  that the idea has been diverted from its original goals [33]. In  addition, more fundamental concerns about the quality of the  underpinning research evidence are coming to light.   The ethics of insisting on RCTs of treatments known to be  effective are complex and still under debate. On the one hand,  there are many examples of treatments that were known to be  effective that turned out to be actively harmful; but on the other,  insisting on the highest quality of evidence before taking action  can cause significant avoidable harm. There are at least now  established procedures for ending trials early when sufficiently  strong evidence has been gathered.   One issue is the use of surrogate endpoints [53]. An example is  blood pressure: we know that high blood pressure is a risk factor  for cardiovascular mortality, so it might seem reasonable to assess  a new drug on the basis of whether it lowers blood pressure,  particularly as mortality rates are generally low, so a large trial  would be needed to evaluate the effect on mortality. However, it  may be that the drug lowers blood pressure but does not affect  mortality, or has severe adverse effects that outweigh any benefit.   Another major issue is publication bias, whereby uninteresting or  negative findings are not reported. Positive results are more likely  to be written up and accepted as publications, while negative  results are more likely to languish, unloved, in file drawers,  creating a file drawer effect. This is a particular concern in the  area of clinical research, where it has been the practice of some  pharmaceutical companies not to publish all the research they  have conducted in to the safety and efficacy of new treatments.  There is a movement to address this, with the ambitious AllTrials2  project working to have All trials registered, all results reported.   Ioannidis made the bold claim that most published research  findings are false [37], substantiating this with a model of the  research process that is not restricted to medicine. He later argued  that most of the true research that is published is not useful in  clinical practice [38]. There is a strong statistical rationale  underpinning this concern, along with a complex set of incentives  for researchers and publishers, which are perhaps most vividly  illustrated by recent issues in the broad field of psychology.   2.2 Evidence in psychology  Concerns were raised in psychology about an over-reliance on  samples of people from western, educated, industrialized, rich,  and democratic (WEIRD) societies, and how representative those  samples were of humanity as a whole [35].  This concern about external validity and a deeper concern about  internal validity sparked a replication crisis. After attempts to  replicate famous psychological results failed, a high-profile  Reproducibility Project repeated 100 key correlational studies to  see if the same results could be obtained. The results were  disappointing. A large portion of replications produced weaker  evidence for the original findings, with only 3647% of  replications succeeding, depending on the measure chosen [48].  These efforts have been highly controversial, with critiques of                                                                       2 http://www.alltrials.net/   studies often posted in the grey literature (chiefly blog posts) and  on social media.    An underlying issue is the use of statistics, including researcher  degrees of freedom to make a study reach significance [61]. This  is an issue if researchers carry out multiple comparisons but only  report the significant ones. It also arises where researchers can  perform a reasonable analysis given their assumptions and their  data, but had the data turned out differently, they could have done  other analyses that were just as reasonable [29].  An underlying  problem is that any research carried out with low pre-study odds  is prone to false positives [37].    The problems are deep-seated. A 60-year meta-analysis of  statistical power in the behavioural sciences [shows] that  [statistical] power has not improved despite repeated  demonstrations of the necessity of increasing power [62].  Incentives prompt researchers and journals to produce findings  that are interesting and these drive high false discovery rates, even  if replications are commonplace [62].   2.3 Evidence in education  In the case of education, it is extremely challenging to carry out  RCTs at all. The contexts in which learning occurs are highly  variable and personal, and there is even less consensus about the  ethics of conducting trials than there is in medicine.    Prominent efforts around this form of evidence include the What  Works Clearinghouse3 in the USA, the work of the Gates  Foundation in K12 education4, and a position paper on Building  evidence into education published by the UK Government [31]  written by a prominent advocate of evidence-based medicine.  There is broad consensus even among advocates of RCTs in  education that they are not a panacea [30] and cannot sit in a  research vacuum [2]; others are considerably less supportive of  these approaches.   The transfer of research evidence into practice is also far from  perfect. This problem is perhaps most vividly illustrated by the  example of learning styles. Despite comprehensive evidence  against the concept, all but a tiny minority of practising teachers,  across cultures, believe that individuals learn better when they  receive information in their preferred learning style [36].   One can legitimately criticise many studies in education, and in  technology-enhanced learning (TEL), for insufficient rigour.  However, a narrow focus on methodological purity and RCTs is  unlikely to prove productive. A more important issue is the  framing of the research question and a rigorous consideration of  the goals and underlying model of learning and teaching [42].  Good-quality quantitative research needs to be supported by good- quality qualitative research: we cannot understand the data unless  we understand the context.   The issue of surrogate end points also arises in education. Even if  results can be validly compared, there is often a lack of consensus  that the test measures what people want education to achieve.  Nonetheless, standardised testing has been carried out extensively.   2.4 International data gathering  Countries worldwide collect evidence about education that they  can use to inform and assess social and educational policies, as  well as to judge their performance in relation to other countries.                                                                        3 http://ies.ed.gov/ncee/Wwc/  4 http://k12education.gatesfoundation.org/     The methods they use to do this are usually rooted in  psychometrics, the science of psychological assessment [57].  Well-known examples of systematic evidence gathering include  the Programme for International Student Assessment (PISA) and  the Trends in International Maths and Science Survey (TIMMS).   PISA is an international survey that aims to evaluate education  systems worldwide by testing the skills and knowledge of 15- year-old students. Since 2000, every three years, students from  randomly selected schools worldwide have taken tests in reading,  mathematics and science. These two-hour tests combine open- ended and multiple-choice questions that are based on a real-life  situation. Information collected through questionnaires filled in by  schools and students provides context for the test results.   TIMMS is designed to enable participating countries to make  evidence-based decisions for improving educational policy. Since  1995, the survey has been used to monitor trends in mathematics  and science achievement every four years. Its assessments provide  data relating to student performance in different domains of  mathematics and science as well as problem solving in each of  these areas. The studies also collect data about some of the  contextual factors that affect learning, including school resources,  student attitudes, teaching practices and support at home.   The results of these studies are widely cited and are used to  influence the education policy of many countries. They are also  open to criticism on the grounds of validity, reliability and  generalisability. The US-based National Education Policy Center  [13] reviewed the main critiques that have been made of such  international tests. It noted that students in the samples from  different countries are not directly comparable. When the figures  are adjusted to take this into account, countries that have made  large gains on TIMMS appear to have made little or no gains on  PISA, which calls into question the validity of the results. The  error terms of the test scores are large, so the accuracy of figures  can be questioned. Despite the large sample sizes, the students  tested are not representative samples of a countrys population, so  the results are not necessarily generalizable.   Validity does not depend only on how well tests are designed and  validated, but also on how defensibly the resulting evidence is  employed [17]. As the results of these tests are influential, it is  tempting for countries to undermine the validity of their results by  gaming the system. Although there are formal design  recommendations and sampling protocols, participation of schools  and classrooms tends to be locally determined based on priorities,  politics and resources  especially in developing nations [17].   The results of these tests may also be interpreted in ways that are  not valid. Commentators and politicians confuse correlation and  causation when they claim direct causal connections between test  scores and aspects of schooling. This misuse of evidence is also  seen when measurements of attainment in numeracy or literacy  are taken to provide evidence of the effectiveness of teachers,  schools, states or even the country as a whole [43].   Each of these tests is centrally controlled so that a consistent  research design is employed across countries and across time. The  tests and their results are open to public scrutiny, and the results  of this scrutiny can be fed back into the tests in order to increase  the value of the evidence that they provide.   2.5 Evidence in learning analytics  In the case of an entire research field, such as learning analytics, it  is much more difficult to move consistently towards evidence that  is generalizable, valid and reliable. A major problem for learning   analytics is that the field is now so diverse it is impossible for any  individual or team to keep up with all the literature.   The development of literature reviews helps but these tend to be  aimed at researchers and not practitioners. The LAK Dataset5  makes machine-readable versions of literature available. This is a  rich resource, but is not easily accessible by readers. The SoLAR  website brings resources together in its Info Hub.6 These provide  a useful introduction, but only for those with time to explore a  wide range of resources.  A different way of dealing with the problem of making evidence  accessible was developed in the field of open education. In 2011,  the Open Learning Network (OLNet) project launched the  Evidence Hub for Open Education. The aim was to provide an  environment that could be used to represent the collective  knowledge of the Open Education community. The Evidence Hub  could be used to investigate the people, projects, organisations,  key challenges, issues, solutions, claims and evidence that  scaffold the Open Education movement [25].   3. DEVELOPING A LEARNING  ANALYTICS EVIDENCE HUB    The Learning Analytics Community Exchange (LACE) project7  has used the model developed by the OER Research Hub to  produce an Evidence Hub for the learning analytics community.   3.1 Developing Evidence Hub criteria  In the case of an Evidence Hub, the term evidence refers to the  available body of facts or information that indicates whether a  particular proposition is true or valid. In order for learning  analytics resources to be classified as evidence, they therefore  need to relate to a proposition that may be true or false.    Work to identify the propositions that would underpin the LACE  Evidence Hub began with the structured development of a  framework of quality indicators [59].  An initial version of this  framework included five criteria, each associated with four quality  indicators [59]. LACE consortium members refined initial  propositions based on these. This resulted in four propositions:8   A: Learning analytics improve learning outcomes.   B: Learning analytics improve learning support and teaching,  including retention, completion and progression.   C: Learning analytics are taken up and used widely, including  deployment at scale.    D: Learning analytics are used in an ethical way.    As some evidence is valuable but does not have a positive or  negative polarity in relation to these propositions, the Evidence  Hub allows evidence to be classified as positive, negative or  neutral in relation to a proposition.    These propositions were introduced to the wider learning analytics  community at an event that attracted over 400 participants from  across Europe. Groups discussed the propositions and existing  evidence for or against them. These discussions showed that the                                                                        5 https://solaresearch.org/initiatives/dataset/  6 https://solaresearch.org/core/  7 http://www.laceproject.eu/  8 http://www.laceproject.eu/evidence-hub     propositions could be used effectively to structure evidence in the  field of learning analytics.   4. LACE EVIDENCE HUB   The LACE team has worked to ensure that the Evidence Hub  includes as much relevant evidence as possible. Different work  streams within the project  schools, universities and workplace  learning  each contributed evidence related to its sector. A  focused literature search examined papers from early LAK  conferences. The Hub was publicized at learning analytics events  and, in 2016, those submitting a paper to LAK were invited to link  it to the Evidence Hub criteria. This link to LAK meant that  coverage of 2016 is more extensive than in previous years. The  Evidence Hub does not provide complete coverage  it is  currently skewed towards papers written in English, for LAK,  since 2015. Importantly, its focus does not include most of the  literature around intelligent tutoring systems. Nevertheless, it  represents the most comprehensive and systematic coverage of  evidence that is currently available  and its open nature means  that anyone can contribute additional evidence.   4.1 LACE Evidence Hub findings  One of the early findings was the surprising quantity of published  research papers in the LAK Dataset that did not contain evidence  in this strong sense of being evidence for or against one of the  four broad propositions. Many of the LAK papers are not  empirical research. Of those that are, some are evidence only of  intermediate effects (e.g. reliability of predictions of at-risk  students) rather than evidence for one of the propositions (e.g. that  this can improve their learning, Proposition A). The Evidence  Hub took a fairly broad view of whether a piece of research met  this criterion, as explored in section 4.2 below.   At the time of writing,9 the LACE Evidence Hub contains 123  pieces of evidence, summarised in Table 2.   Table 2: Summary of positive (+), negative (-) and neutral ()  contents of Evidence Hub in relation to four propositions   It is immediately clear that, although this is the seventh annual  LAK conference, there is still very little hard evidence about  learning analytics. What is more, the evidence that we do have is  significantly skewed towards the positive: only 7% of the findings  are negative. This issue that was examined in detail at the 2016  LAK Failathon [21]. As discussed in section 2.1, publication bias  is a well-known problem in medicine and in most other empirical  disciplines. The many accounts of failures given at the Failathon  suggest that the mainly positive evidence presented in the learning  analytics literature does not fully represent the findings of  research work within the discipline.   The papers in the Evidence Hub mainly relate to learning and  teaching in schools and universities. There are particular gaps in  the evidence about informal learning, workplace learning, and  ethical practice.                                                                        9 October 2016   4.2  Evidence problems in one sector  We considered the situation in more detail by focusing on one  area. The Higher Education sector of the Evidence Hub contains  more evidence than any other. Most of that evidence relates to the  proposition, Learning analytics improve learning support and  teaching, including retention, completion and progression.  This analysis therefore focuses on the 28 papers that have been  classified as evidence that learning analytics improve teaching in  universities. The majority of the evidence (22 items) is positive.  Six items are neutral [6; 16; 22; 40; 46; 56] and there is so far no  evidence against the proposition.  While this appears to be good news, it seems unlikely. With  hundreds of researchers working across the world, surely one has  tested a learning analytics innovation and found that it does not  improve learning support and teaching [21]    The Evidence Hub mapping tool suggests that work on supporting  teaching in universities with learning analytics is being developed  and widely disseminated by only a few institutions. Figure 1  shows that almost all the evidence related to this proposition  (represented by green circles) originates in a handful universities  in the south of Australia, the west of Europe and the north-east of  the USA. (The green circle in the Pacific Ocean represents studies  associated with more than one area.) Outside these main areas,  there are single pieces of evidence (represented by orange pins) in  Singapore and Greece. As the LAK conference was receiving  submissions from 31 countries by 2013 [63] and has since grown  considerably, we might have expected to see evidence being  produced and disseminated in many more countries.   The evidence that is available in the Evidence Hub falls into four  main groups: evidence that can support institutions, evidence that  can support the development of learner models, evidence with the  potential to support teaching and evidence that has had impact on  teaching and its effects. In the analysis below, sample sizes are  noted for each study. Unless otherwise stated, the samples were  made up of students from a single institution.  Evidence that can support institutions  Some evidence focuses on student support across the university.  Two pieces of evidence fit into this category. The first relates to  the Learning Analytics Readiness Instrument (LARI), designed to  help institutions gauge their readiness to implement learning  analytics. It is ready for use, but has not yet been deployed  (N=560 respondents, 24 institutions) [49]. The second analyses  the financial benefits for an institution of using an early alert  system. Over three years, the system had significant financial  benefits to the institution (N=16,124). Although the focus is on  financial benefit to the institution, the implication is that   Figure 1: Evidence related to universities and teaching     significant numbers of students benefited by being supported to  remain at the institution. However, no information is supplied  about the early alert system or how it was deployed [34].  Evidence that can support development of learner models   Learner models represent information about a students  characteristics or state, such as their current knowledge,  motivation, metacognition and attitude. Such models can be used  to provide automated support for students. Three pieces of  evidence deal with factors that could be incorporated within a  learner model: time spent on task (N=259) [50], misconceptions  about one-digit multiplication (N not specified) [64] and affect  (N=44) [1]. A fourth piece of work on automated support focuses  on detection and analysis of reflective writing (N=30 pieces of  student work) [8]. While all these studies have the potential to  improve learning support and teaching, there is no evidence as yet  that they have actually done so.  Evidence with potential to support teaching  Some of the evidence that falls into this category has the potential  to support teaching, but there is no clear route from research into  practice. Topic modelling has potential as an analytic tool to help  teachers assess reflective thoughts in written journals (N=80) [18].  An analytics dashboard designed for users of interactive e-books  could potentially be used by teachers [41]. A rule-based indicator  definition tool (RIDT) could support a personalized learning  analytics experience (N=5 staff, 7 students) [47]. Studying eye  fixation patterns could enable educators to understand how their  instructional design using online learning environments can  stimulate higher-order cognitive activities (N=60).   Other evidence has clearer pathways into practice and promises to  have a positive effect on practice in the near future. A study of  possible predictors of student success makes the important point  that predictors are only useful in cases where intervention is  possible (N=1,005 and N=1,006) [65]. Another study supports  teachers build on learning analytics by introducing a conceptual  framework designed to transform learning design into a teacher- led enquiry-based practice (N=12 teachers, 4 universities) [5].  Some data help predict student failure or drop-out. Changes in  user activity in a virtual learning environment can predict failure  when compared with their previous behaviour or that of students  with similar learning behaviour (N=7,701) [66]. Analysis of data  about student movement within and across a learning community  can be used to develop strategic interventions in the learning of at- risk students. For example, teaching staff were found to be more  commonly located in the networks of high-performing students  and analytics made staff more aware of this (N=1,026) [23]. An  investigation of individual student, organizational, and  disciplinary factors that might predict a students classification in  an Early Warning System, as well as factors that predict  improvement and decline in their academic performance, resulted  in tentative recommendations for educators (N=566) [7].   Visualising data can make analytics more accessible to teachers.  Visualising online student engagement/effort provides instructors  with early opportunities for providing additional student learning  assistance and intervention when and where it is required  (N=1,026) [24]. Using visualisations produced by the Student  Activity Meter tool can help awareness and understanding of  student resource use and student time-spending behaviour (two  case studies, N=12 and N=20 evaluators, mainly teachers) [32].  Two large-scale studies have produced robust findings and  recommendations that are currently being put into practice.  Learning design activities seem to have an impact on learning   performance, in particular when modules rely on assimilative  activities (N=19,322) [55]. Development of appropriate  communication tasks that align with the learning objectives of the  course appears likely to enhance academic retention (N=111,236)  [54]. However, this work is too early in the implementation  process to have produced clear evidence that learning support and  teaching have improved. The implications of these studies remain  tentative while a larger ongoing body of work investigates  whether their recommendations work in practice.   The studies considered up to this point could, in future, have a  positive impact on teaching and learner support. With the possible  exception of the analysis of the financial impact of an early alert  system [34], there is no clear evidence that they have  already  achieved that impact. Only two studies in the Evidence Hub  provide evidence that analytics have prompted changes in  teaching and support that have impacted on learners.   Evidence of impact on teaching  The first of these is the highly cited (242 citations in Google  Scholar) work on the use of the Course Signals learning analytics  system at Purdue University. The paper reported that courses that  implemented Course Signals realized a strong increase in  satisfactory grades, and a decrease in unsatisfactory grades and  withdrawals. Students who participated in at least one Course  Signals course were retained at rates significantly higher than  their peers who did not (N=23,000 students, 140 instructors) [3].  This is an important study, with major implications for learning  analytics as a field, and it is considered further in the next section.   The second study in this section used a predictive model for  student drop-out that was very similar [ to] the predictive  model developed at Purdue University [44]. This study  (N=1,379) reported a small but statistically significant difference  in content mastery rates (C grade or above) between intervention  groups and controls. It also found statistically significant  differences in withdrawal rates between intervention groups and  controls. Worryingly, students in the combined treatment group  were more likely to withdraw than those in the control groups  [44]. It seems possible  although this is not explored in the study   that the improvement in grades was due to weaker students  dropping out. The authors note that the increased dropout rate is  consistent with students withdrawing earlier in the course (as  opposed to remaining enrolled and failing). They were not able to  give data about failure rates, however, so their data are also  consistent with the intervention encouraging weak students who  would otherwise have completed successfully to drop out.   So, when considering the positive influence of learning analytics  on teaching and learning support in the higher education sector   the area in which we appear to have the most evidence  our  strongest example remains the 2012 work on Course Signals. It  therefore makes sense to examine this evidence in detail and to  ask whether it is valid and reliable.    4.3 Evidence problems in one study  Valid, reliable evidence is not easy to obtain in the field of TEL  One reason is the Hawthorne Effect, popularly considered to be  the change of behaviour by subjects of a study due to their  awareness of being observed. In the original account of this effect,  in 1925, workers in a study were found to increase productivity.  This increase was not due to the variable under consideration but  was, at least in part, because records were taken more frequently  than usual, which amounted to increased supervision [39]. This  effect may be exacerbated in classrooms, where studies typically  attract extra resource that is removed once the trial is over.     In order to establish the reliability and validity of a study, it is  important to understand both its basis and its context. The Course  Signals tool was developed at Purdue University in Indianapolis.  Work began with an exploratory study by Campbell [9], using  course management system student data from 2005 to determine  undergraduate success. Campbells study, which was written up in  his doctoral thesis and in Educause publications [9-11], suggested  that these data could be used as an appropriate proxy for overall  student effort. The universitys use of data to identify at-risk  students became an early example of what were then known as  academic analytics [10; 12], and the universitys early-warning  system was developed into a tool called Course Signals [52].   Course Signals used empirical data to build a student-success  algorithm. This considered past academic performance but placed  more emphasis on student effort and help-seeking. When students  were classified as at-risk, this classification triggered interventions  set up by instructors [52]. By 2009, more than 7,000 students were  using the system, and the results reported in 2012 appeared very  promising. Courses that implemented Course Signals saw an  increase in A and B grades and a decrease in lower grades. In  addition, students who participated in at least one Course Signals  course appeared to be retained at rates significantly higher than  their peers, and students who took two or more courses with  Course Signals were consistently retained at rates higher than  those who had only one or no courses with Course Signals [3].   The evidence seemed strong. Research based on five years of data  with thousands of students showed that analytics could help to  improve grades and increase retention. These were important  claims that inspired many researchers and institutions to engage  with work on learning analytics. Others took a more detailed look  at the results that had been reported, and their critiques began to  appear online in blogs. In August 2012, Caulfield raised the point  that, between 2007 and 2009, retention at the university had also  risen substantially for courses that did not employ Course Signals.  This suggested that university-wide changes were having a  significant effect on retention figures [14]. Caulfield followed this  with a blog post the following year in which he asked whether the  study had controlled for the number of classes a student took, and  how the first to second year retention had been calculated [15].    Caulfields overarching concern was whether students had been  retained because they had taken more courses that used Course  Signals, or whether they took more of those courses because they  had been retained. Essa built a simulation to explore this issue and  blogged that correlation had indeed been confused with causation  [27]. Clow suggested, again in a blog post, that the Purdue  researchers should urgently re-analyse their data, taking world- class statistical advice [] and publish the outcome in raw,  unreviewed form as fast as possible, and then write it up and  submit it for peer review [20].   So, on the one hand, the learning analytics community has a peer- reviewed conference paper that is frequently cited and that has  inspired many. On the other hand, we have a serious challenge to  the methodology the paper employed. This critique appears in the  grey literature (publications that are not peer reviewed) and is  less commonly cited. The evidence provided by the peer-reviewed  paper is therefore in doubt. It seems probable that Course Signals  does have a positive effect on students, even if this is simply an  effect on their grades on courses that run Course Signals but,  without running another analysis, we cannot be sure.  As Clow suggests, the obvious course would be to reanalyze the  data and to open that analysis to public scrutiny. However, the  authors were university staff rather than faculty members and   were not free to continue the study without university approval.  They are now working on different projects or at different  institutions and do not have the data access or the resources to  carry out another analysis. They are therefore in the unenviable  position of seeing their work called into question without being in  a position to amend, extend or defend their analysis. We would  like to make it quite clear that what we are presenting here is a  critique of the Course Signals study as published. It is in no way a  personal attack on the two authors, whom we know to be talented  and dedicated learning analytics researchers. The statistical issue  here is not trivial and the apparent error is entirely understandable.   One view is that it is not in the interests of Purdue University to  re-examine its data because the university is effectively making  money on the strength of research claims that have now been  called into question [28], and it continues to make those claims  without re-examining them [45].    Another view is that it might not be in the interests of the learning  analytics community to dig too far into the data that underpins one  of its flagship examples. However, that is exactly what we do  need to do, because we need to build our work on firm  foundations.   4.4 Comparison to other areas  It should be emphasised again that this issue is not one unique to  learning analytics. The pattern here  an exciting, significant  finding in a published paper, which appears out to have major  issues that are discussed only in the grey literature  is a very  common one in the replication crisis in psychology discussed  above in section 2. The OER Research Hub, on which the LACE  Evidence Hub was based, found that [w]ith over a decades  investment in OER there remains surprisingly little reliable  empirical research on OER impact [26]. It seems likely that other  areas of TEL research would show the same pattern.   The presence of a large quantity of more qualitative research,  theoretical argument, and policy discussion in the learning  analytics literature is by no means a weakness. However, as a  field founded on the idea of an increase in access to educational  data, it is disappointing that there remains so little top-quality  quantitative research that demonstrably helps us to achieve  improvements in learning and teaching.    The state of the learning analytics literature is in marked contrast  to that of the sister field of educational data mining (EDM).  The  papers in the annual conference and journal of the International  Educational Data Mining Society (IEDMS)10 are overwhelmingly  reports of quantitative, empirical work, and there is a growing  tradition of making datasets and analysis code available for  inspection and re-use, chiefly through the Pittsburgh Science of  Learning Centers DataShop.11 This goes some way to allay many  of the concerns currently live in the psychology community  discussed in section 2.2. Much of this work concerns self- contained interactive learning material, such as intelligent tutors  and simulated lab experiments, rather than the less structured  environments studied by most learning analytics researchers.    Efforts have been made in the past to encourage collaboration  between the EDM community and the learning analytics  community (e.g. [4]). It seems that the two could work together to  consider the best ways of producing high-quality evidence that  benefits learners and teachers. One view might be that EDM                                                                       10 http://www.educationaldatamining.org/  11 http://www.learnlab.org/technologies/datashop/     provides a natural home for rigorous empirical work, and that  learning analytics can employ different standards We disagree: the  focus of learning analytics is distinct, with an emphasis on  practice, which is question that requires its own strong evidence.    5. PROBLEMS WITH THE EVIDENCE  Our analysis of the data in the Evidence Hub highlights important  gaps in the evidence that is readily accessible to the learning  analytics community, which includes people in a range of  different roles, including academics, developers and practitioners.   Lack of geographical spread: Our focus has been on the largest  area of the Evidence Hub, but the Hubs visualisation tool shows  that the majority of widely reported work comes from particular  areas of Europe, North America and Australia, with almost no  evidence yet emerging from South America, Asia or Africa.  Gaps in our knowledge: As noted above, there are particular  gaps in the evidence about informal learning, workplace learning  and ethical practice, as well as a lack of negative evidence.  Little evaluation of commercially available tools: At a time  when most learning management systems incorporate some form  of learning analytics or data visualisation, we lack evidence that  these are having any positive impact on learning and teaching.   Lack of attention to the learning analytics cycle [19]: Learning  analytics involves the measurement, collection, analysis and  reporting of data about learners and their contexts, for purposes of  understanding and optimizing learning and the environments in  which it occurs [60]. Not enough published work is making it  clear how the move will be made from researching the data to  optimising the learning; not enough published work is making a  connection to the next stage of the learning analytics cycle.       Figure 2: The Learning Analytics Cycle, from [19]  Limited attention to validity, reliability and generalizability:  These are not the only criteria for high quality research that can be  used as evidence, but they provide a good starting point. A search  for the stems general-, valid- and ethic- in the 22 papers  considered here found that 14 referred to validity, eight referred to  reliability, seven referred to generalizability and five mentioned  none of these. Two papers included reliability as a key word but  did not deal with it in the body of the paper. Only four papers [1;   8; 23; 66] included consideration of all three. Almost all papers  were based on data from only one institution.   Limited attention to ethics: Despite the high levels of discussion  of ethical issues in the learning analytics community in recent  years, a search for the stem ethic- in the 22 papers considered here  showed that only three had explicitly considered ethics. This does  not imply that the studies were unethical, simply that the authors  did not include any information about how they had dealt with  ethical issues when studying hundreds or thousands of learners.  Sample selection: The 22 papers here are taken to be evidence of  improvement in teaching and learner support. However, relatively  few of them include teachers within their sample and in only two  cases are there more than 20 teachers within the sample.   Access to research findings: The Evidence Hub, which includes  brief summaries of papers and full references, is openly  accessible. However, the research that sits behind it is often  locked away. Of the 22 papers considered here, 18 are LAK  papers, sited behind the ACM pay wall ($15 USD for each PDF),  one is in BJET ($6 USD to rent the paper, $38 USD for the PDF),  and one is a Springer book chapter ($29.95 USD for the chapter).  In many cases, pre-print versions are available free of charge from  institutional repositories, if you know where to look. However, the  default position is to store this research behind pay walls that  make it inaccessible to the practitioners and developers who could  benefit most from the findings.   Over-representation of LAK conference papers: Papers  presented at the annual LAK conference are a very important part  of the evidence base for learning analytics, but they are only one  part of that base. The link between the Evidence Hub and the  EasyChair submission system used by LAK ensures good  coverage of this conference, but it shifts attention from papers  published in a wide variety of journals, from the growing body of  literature related to education mining, and from the reports by  practitioners and developers that do not appear in a conventional  academic format.   6. LIMITATIONS  The coverage of the Evidence Hub is focused largely on the LAK  dataset. This was by deliberate choice, but does mean there are  many other key pieces of research that have not yet been  considered for inclusion. In particular, examples of the extensive  literature on intelligent tutoring systems (ITS) would be a  valuable addition to the Hub. While efforts were made to fully  brief reviewers for the Hub on the criteria, to ensure consistency,  the decisions made were only lightly crosschecked; the resources  available precluded inter-rater reliability checks. The detailed  analysis of papers in section 4 is the collaborative work of the two  authors, again with no inter-rater reliability checks.   7. WHAT IS TO BE DONE  Now that we are aware of these problems, the learning analytics  community can act together to solve them and to establish a firm  and accessible evidence base.   The actions proposed here provide possible ways of addressing  the problems identified in the previous section. However, if these  solutions are to be successful, they need community engagement  and community buy-in. In order to start this process, the authors  are organising a workshop at LAK17 that will bring people  together from different sectors to discuss and develop the  suggestions proposed here. This will be followed by an  opportunity at the LAK17 poster session for the entire LAK  community to engage with these ideas.     Evidence Hub  The LACE special interest group (SIG) of SoLAR now manages  the Evidence Hub. The SIG could work to:    Publicise the Hub and promote engagement   particularly from countries and sectors that have  provided little or no evidence to date; pro-active  measures may help.    Identify gaps in the current evidence on a regular basis  and share these with the community   LAK Conference  The LAK conference committee changes each year, but a set of  guidelines could be developed for use or amendment annually.    Consider how the paper review process could be used to  address the problem with evidence. For example,  reviewers could be asked to check that all papers either  make reference to generalizability, validity, reliability  and ethics, or make it clear why this is not appropriate.    Consider prioritising areas where there are gaps in the  evidence in the call for papers.    Consider how LAK conference papers can be made  more accessible to those without access to academic  libraries. For example, authors could be asked to supply  a separate non-technical summary, and these summaries  could be openly accessible.    Consider measures to strengthen the effectiveness of  statistical scrutiny in the reviewing process, while  simultaneously encouraging the submission of empirical  studies with robust experimental designs.     Consider requiring authors to specify when they submit  a paper how this work fits into the Learning Analytics  Cycle (Figure 2), and how it will be connected with the  next stage in the cycle.    Consider more effective ways of sharing expertise with  the EDM community.     Review best practices from fields such as clinical  research and psychology that are more advanced in their  use of evidence, (e.g. the International Committee of  Medical Journal Editors Recommendations for the  Conduct, Reporting, Editing, and Publication of  Scholarly work in Medical Journals 12).   LAK Doctoral Consortium and PhD Supervisors    Work to develop and share good practice, and to  establish expectations about the quality of evidence    Help doctoral students to develop research questions  and studies that fill significant gaps and fit into the  Learning Analytics Cycle (Figure 2).   Researchers    When submitting grant applications, consider how the  planned research could be used to fill significant gaps in  the existing evidence.    Consider pathways to impact carefully. How can  findings be shared with practitioners who do not read  research papers   Developers    Share work on evaluating tools via the Evidence Hub.                                                                        12 http://www.icmje.org/recommendations/   Journal of Learning Analytics     Consider the steps suggested for the LAK conference.   Where there is a significant body of work available, ask   the team or teams responsible to produce an overview  paper that brings together the main evidence.   Society for Learning Analytics Research (SoLAR)    Consider making pen drives of past LAK proceedings  available to all paid-up SoLAR members, thus  providing an access route for non-academics.    Continue work to engage people from different  countries and different sectors.    Coordinate work across institutions. For example,  evaluation of the learning analytics offered by major  learning management systems could be carried out at  different institutions using the same research design.    Consider the feasibility and desirability of encouraging  pre-registration of empirical studies.   All members of the learning analytics community are encouraged  to submit evidence to the Hub. This will make the Hubs coverage  more comprehensive, making it easier to identify and then fill the  gaps in the evidence.   8. CONCLUSIONS  Learning analytics as a field is not immune from the challenges  facing empirical research in other disciplines, notably medicine  and psychology. These challenges arise from powerful pressures  that are far beyond the scope of individual researchers to address,  no matter how well-intentioned and well-informed statistically.  The nature of the topic area makes it hard to carry out rigorous  quantitative research, and rigorous qualitative research is also  required to yield not only actionable insights, but also action that  improves learning. To validate the field, we must have evidence  about whether learning analytics does improve learning and  teaching in practice. As a field with an abundance of data,  learning analytics should be well placed to produce such evidence.  This papers exploration of the evidence we have to date shows  clearly that there is considerable scope for improving the evidence  base for learning analytics. We believe that doing so is a scientific  and moral imperative. We have set out some suggestions for how  we can move forward as a community, and look forward to being  part of that work.   8.1 Acknowledgement  The European Commission Seventh Framework Programme,  grant number 619424, funded the LACE project, which was  responsible for developing the Evidence Hub.   9. REFERENCES  [1] Allen, L.K., Mills, C., Jacovina, M.E., Crossley, S., D'Mello,   S., and Mcnamara, D.S., 2016. Investigating boredom and  engagement during writing using multiple sources of  information: the essay, the writer, and keystrokes. In LAK16  ACM, 114-123.   [2] Allen, R., 2013. Evidence-based practice: why number- crunching tells only part of the story. Blog post:  https://ioelondonblog.wordpress.com/2013/03/14/evidence- based-practice-why-number-crunching-tells-only-part-of-the- story/. In IOE London Blog.   [3] Arnold, K.E. and Pistilli, M., 2012. Course Signals at  Purdue: Using Learning Analytics To Increase Student  Success. In LAK12 ACM, 267-270.     [4] Baker, R.S., Duval, E., Stamper, J., Wiley, D., and  Buckingham Shum, S., 2012. Educational data mining meets  learning analytics. In LAK12 ACM, 20-21.   [5] Bakharia, A., Corrin, L., De Barba, P., Kennedy, G.,  Gaevi, D., Mulder, R., Williams, D., Dawson, S., and  Lockyer, L., 2016. A conceptual framework linking learning  design with learning analytics. In LAK16 ACM, 329-338.   [6] Bos, N. and Brand-Gruwel, S., 2016. Student differences in  regulation strategies and their use of learning resources:  implications for educational design. In LAK16 ACM, 344- 353.   [7] Brown, M.G., Demonbrun, R.M., Lonn, S., Aguilar, S.J., and  Teasley, S.D., 2016. What and when: the role of course type  and timing in students academic performance. In LAK16  ACM, 459-468.   [8] Buckingham Shum, S., Sndor, ., Goldsmith, R., Wang, X.,  Bass, R., And Mcwilliams, M., 2016. Reflecting on reflective  writing analytics: Assessment challenges and iterative  evaluation of a prototype tool. In LAK16 ACM, 213-222.   [9] Campbell, J.P., 2007. Utilizing Student Data within the  Course Management System To Determine Undergraduate  Student Academic Success: An Exploratory Study, PhD  thesis, Purdue University, available:  http://docs.lib.purdue.edu/dissertations/AAI3287222/.   [10] Campbell, J.P., Deblois, P.B., and Oblinger, D.G., 2007.  Academic analytics: a new tool for a new era. Educause  Review 42, 4 (July/August), 40-57.   [11] CAMPBELL, J.P. and OBLINGER, D.G., 2007. Academic  Analytics. Educause.  http://net.educause.edu/ir/library/pdf/PUB6101.pdf   [12] Campus Technology, 2006. Data mining for academic  success. In Campus Technology (21 May 2006).   [13] Carnoy, M., 2015. International Test Score Comparisons and  Educational Policy: a Review of the Critiques. National  Education Policy Center.   [14] Caulfield, M., 2012. Course Signals and Analytics. Blog  post: http://hapgood.us/2012/08/24/course-signals-and- analytics/. In Hapgood.   [15] Caulfield, M., 2013. A Simple, Less Mathematical Way To  Understand the Course Signals Issue. Blog post:  http://hapgood.us/2013/09/26/a-simple-less-mathematical- way-to-understand-the-course-signals-issue/  In Hapgood.   [16] Charleer, S., Klerkx, J., and DuvaL, E., 2014. Learning  dashboards. Journal of Learning Analytics 1, 3, 199-202.   [17] Chatterji, M., 2013. Global forces and educational  assessment  a foreword on why we need an international  dialogue on validity and test use. In Validity and Test Use,  An International Dialogue on Educational Assessment,  Accountability and Equity, M. Chatterji Ed. Emerald,  Bingley, UK.   [18] Chen, Y., Yu, B., Zhang, X., and Yu, Y., 2016. Topic  modeling for evaluating students' reflective writing: a case  study of pre-service teachers' journals. In LAK16 ACM, 1-5.   [19] Clow, D., 2012. The learning analytics cycle: closing the  loop effectively. In LAK12 ACM, 134-138.   [20] Clow, D., 2013. Looking harder at Course Signals (13  November 2013). Blog post:   https://dougclow.org/2013/11/13/looking-harder-at-course- signals/. In Doug Clow's Imaginatively-Titled Blog.   [21] Clow, D., Ferguson, R., Macfadyen, L., Prinsloo, P., and  Slade, S., 2016. LAK Failathon. In LAK16 ACM, 509-511.   [22] Cooper, M., Ferguson, R., and Wolff, A., 2016. What can  analytics contribute to accessibility in e-learning systems and  to disabled students learning In LAK16 ACM, 99-103.   [23] Dawson, S., 2009. 'Seeing' the learning community: an  exploration of the development of a resource for monitoring  online student networking. British Journal of Educational  Technology 41, 5, 736-752.   [24] Dawson, S., McWilliam, E., and Tan, J.P.-L., 2008. Teaching  smarter: How mining ICT data can inform and improve  learning and teaching practice. In ascilite 2008, Melbourne,  Australia (30 Nov-3 December).   [25] De Liddo, A., Buckingham Shum, S., McAndrew, P., and  Farrow, R., 2012. The Open Education Evidence Hub: a  collective intelligence tool for evidence based policy. In  Proceedings of the Joint OER12 and OpenCourseWare  Consortium Global 2012 Conference (Cambridge, UK, 16-18  April 2012).   [26] De Los Arcos, B., Farrow, R., Perryman, L.-A., Pitt, R., and  Weller, M., 2014. OER Evidence Report 2013-2014 OER  Research Hub. http://oro.open.ac.uk/41866/.   [27] Essa, A., 2013. Can We Improve Retention Rates by Giving  Students Chocolates Blog post:  http://alfredessa.com/2013/10/can-we-improve-retention- rates-by-giving-students-chocolates/. In alfredessa.com.   [28] Feldstein, M., 2013. Purdue University Has an Ethics  Problem (25 November 2013). Blog post:  http://mfeldstein.com/purdue-university-ethics-problem/. In  e-Literate.   [29] Gelman, A. and Loken, E., 2013. The garden of forking  paths: Why multiple comparisons can be a problem, even  when there is no 'fishing expedition' or 'p-hacking' and the  research hypothesis was posited ahead of time.  http://www.stat.columbia.edu/~gelman/research/unpublished/ p_hacking.pdf    [30] Ginsburg, A. and Smith, M.S., 2016. Do Randomized  Controlled Trials Meet the Gold Standard Blog post:  http://www.aei.org/publication/do-randomized-controlled- trials-meet-the-gold-standard/. In American Enterprise  Institute.   [31] Goldacre, B., 2013. Building Evidence into Education.  Department for Education, UK.  https://http://www.gov.uk/government/news/building- evidence-into-education.   [32] Govaerts, S., Verbert, K., and Duval, E., 2011. Evaluating  the student activity meter: two case studies. In International  Conference on Web-Based Learning Springer, 188-197.   [33] Greenhalgh, T., Howick, J., and Maskrey, N., 2014.  Evidence based medicine: a movement in crisis BMJ  2014;348:g3725    [34] Harrison, S., Villano, R., Lynch, G., and Chen, G., 2016.  Measuring financial implications of an early alert system. In  LAK16 ACM, 241-248.     [35] Henrich, J., Heine, S.J., and Norenzayan, A., 2010. The  weirdest people in the world Behavioral and Brain Sciences  33, 2-3, 61-83.   [36] Howard-Jones, P.A., 2014. Neuroscience and education:  myths and messages. Nature Reviews Neuroscience 15, 12,  817-824.   [37] Ioannidis, J.P., 2005. Why most published research findings  are false. PLoS Medicine 2, 8, e124.   [38] Ioannidis, J.P.A., 2016. Why Most Clinical Research Is Not  Useful. PLoS Medicine 13, 6, e1002049.   [39] IZAWA, M.R., FRENCH, M.D., and HEDGE, A. Shining  new light on the Hawthorne illumination experiments.  Human Factors 53, 5, 528-547.   [40] Joksimovi, S., Manataki, A., Gaevi, D., Dawson, S.,  Kovanovi, V., and De Kereki, I.F., 2016. Translating  network position into performance: importance of centrality  in different network configurations. In LAK16 ACM, 314- 323.   [41] Karkalas, S. and Mavrikis, M., 2016. Towards analytics for  educational interactive e-books: the case of the reflective  designer analytics platform (RDAP). In LAK16 ACM, 143- 147.   [42] Kirkwood, A. and Price, L., 2015. Achieving improved  quality and validity: reframing research and evaluation of  learning technologies. European Journal of Open, Distance  and E-learning 18, 1, 102-115.   [43] Klenowski, V., 2015. Questioning the validity of the multiple  uses of NAPLAN data. In National Testing in Schools: An  Australian Assessment, B. Lingard Ed. Routledge, 44-56.   [44] Laura, E.J.M., Moody, E.W., Jayaprakash, S.M.,  Jonnalagadda, N., and Baron, J.D., 2013. Open academic  analytics initiative: initial research findings. In LAK13 ACM,  150-154.   [45] Mathewson, T.G., 2015. Analytics programs show  remarkable results  and its only the beginning. Blog  post: http://www.educationdive.com/news/analytics- programs-show-remarkable-results-and-its-only-the- beginning/404266/. In Education Dive.   [46] Mostafavi, B. and Barnes, T., 2016. Data-driven proficiency  profiling: proof of concept. In LAK16 ACM, 324-328.   [47] Muslim, A., Chatti, M.A., Mahapatra, T., and Schroeder, U.,  2016. A rule-based indicator definition tool for personalized  learning analytics. In LAK16 ACM, 264-273.   [48] Open Science Collaboration, 2015. Estimating the  reproducibility of psychological science. (28 August).  Science 349, 6251.   [49] Oster, M., Lonn, S., Pistilli, M.D., and Brown, M.G., 2016.  The learning analytics readiness instrument. In LAK16 ACM,  173-182.   [50] Papamitsiou, Z., Karapistoli, E., and Economides, A.A.,  2016. Applying classification techniques on temporal trace  data for shaping student behavior models. In LAK16 ACM,  299-303.   [51] Petticrew, M. and Roberts, H., 2003. Evidence, hierarchies,  and typologies: horses for courses. Journal of Epidemiology  and Community Health 57, 7, 527-529.   [52] Pistilli, M.D. and Arnold, K.E., 2010. Purdue Signals:  Mining real-time academic data to enhance student success.  About Campus: Enriching the Student Learning Experience  15, 3, 22-24.   [53] Psaty, B.M., Weiss, N.S., Furberg, C.D., Koepsell, T.D.,  Siscovick, D.S., Rosendaal, F.R., Smith, N.L., Heckbert,  S.R., Kaplan, R.C., Lin, D., and Fleming, T.R., 1999.  Surrogate end points, health outcomes, and the drug-approval  process for the treatment of risk factors for cardiovascular  disease. Journal of the American Medical Association 282, 8,  786-790.   [54] Rienties, B. and Toetenel, L., 2016. The impact of 151  learning designs on student satisfaction and performance:  social learning (analytics) matters. In LAK16 ACM, 339-343.   [55] Rienties, B., Toetenel, L., and Bryan, A., 2015. Scaling up  learning design: impact of learning design activities on LMS  behavior and performance. In LAK15 ACM, 315-319.   [56] Robinson, C., Yeomans, M., Reich, J., Hulleman, C., and  Gehlbach, H., 2016. Forecasting student achievement in  MOOCs with natural language processing. In LAK16 ACM,  383-387.   [57] Rust, J. and Golombok, S., 2009. Modern Psychometrics,  Third Edition. New York, London.   [58] Sackett, D.L., 1997. Evidence-based medicine. Seminars in  Perinatology 21, 1, 3-5.   [59] Scheffel, M., Drachsler, H., Stoyanov, S., and Specht, M.,  2014. Quality indicators for learning analytics. Educational  Technology & Society 17, 4, 117-132.   [60] Siemens, G., Gaevi, D., Haythornthwaite, C., Dawson, S.,  Buckingham Shum, S., Ferguson, R., Duval, E., Verbert, K.,  and Baker, R.S.J.D., 2011. Open Learning Analytics: An  Integrated and Modularized Platform (Concept Paper).  SOLAR.   [61] SIMMONS, J.P., NELSON, L.D., and SIMONSOHN, U.,  2011. False-positive psychology undisclosed flexibility in  data collection and analysis allows presenting anything as  significant. Psychological Science 22, 11, 1359-1366.   [62] Smaldino, P.E. and McElreath, R., 2016. The natural  selection of bad science. Royal Society Open Science arXiv  preprint arXiv:1605.09511.   [63] Suthers, D. and Verbert, K., 2013. Learning Analytics as a  Middle Space. In LAK13 ACM, 1-4.   [64] Taraghi, B., Saranti, A., Legenstein, R., and Ebner, M., 2016.  Bayesian modelling of student misconceptions in the one- digit multiplication with probabilistic programming. In  LAK16 ACM, 449-453.   [65] Tempelaar, D.T., Rienties, B., and Giesbers, B., 2015.  Stability and sensitivity of learning analytics based  prediction models. In 7th International conference on  Computer Supported Education, Lisbon, Portugal, 156-166.   [66] Wolff, A., Zdrahal, Z., Nikolov, A., and Pantucek, M., 2013.  Improving retention: predicting at-risk students by analysing  clicking behaviour in a virtual learning environment. In  LAK13 ACM, 145-149.       "}
{"index":{"_id":"9"}}
{"datatype":"inproceedings","key":"Arnold:2017:SPP:3027385.3027392","author":"Arnold, Kimberly E. and Sclater, Niall","title":"Student Perceptions of Their Privacy in Leaning Analytics Applications","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"66--69","numpages":"4","url":"http://doi.acm.org/10.1145/3027385.3027392","doi":"10.1145/3027385.3027392","acmid":"3027392","publisher":"ACM","address":"New York, NY, USA","keywords":"ethics, higher education, learning analytics, privacy","Abstract":"Over the past five years, ethics and privacy around student data have become major topics of conversation in the learning analytics field. However, the majority of these have been theoretical in nature. The authors of this paper posit that more direct student engagement needs to be undertaken, and initial data from institutions beginning this process is shared. We find that, while the majority of respondents are accepting of the use of their data by their institutions, approval varies depending on the proposed purpose of the analytics. There also appear to be notable variations between students enrolled at United Kingdom and American institutions.","pdf":"Student Perceptions of Their Privacy in Leaning Analytics  Applications  Kimberly E. Arnold  University of Wisconsin-Madison   1305 Linden Drive  Madison, WI 53706   kimberly.arnold@wisc.edu   Niall Sclater  Jisc   One Castlepark, Tower Hill  Bristol, BS2 0JA, UK   niall.sclater@jisc.ac.uk   ABSTRACT  Over the past five years, ethics and privacy around student data  have become major topics of conversation in the learning analytics  field. However, the majority of these have been theoretical in  nature. The authors of this paper posit that more direct student  engagement needs to be undertaken, and initial data from  institutions beginning this process is shared. We find that, while the  majority of respondents are accepting of the use of their data by  their institutions, approval varies depending on the proposed  purpose of the analytics. There also appear to be notable variations  between students enrolled at United Kingdom and American  institutions.   CCS Concepts   Applied computing~Education  Applied  computing~Interactive learning environments  Applied  computing~E-learning    Security and privacy~Social aspects of  security and privacy  Security and privacy~Privacy  protections  Human-centered computing~Information  visualization  Keywords Learning Analytics; Higher Education; Ethics; Privacy   ACM Reference Format:  K.E. Arnold and N. Sclater, 2017. Student Perceptions of Their  Privacy in Learning Analytics Applications. In Proceedings of  Learning Analytics and Knowledge Conference, Vancouver, BC,  Canada, April 2017, (LAK17), 4 pages.  DOI: 10.1145/3027385.3027392   1.  INTRODUCTION  Over the past five years, ethics and privacy around student data  have become major topics of conversation in the learning analytics  (LA) field.  In fact, the annual Learning Analytics & Knowledge  conference has had ethics/privacy as a theme for the past 3 years.  Additionally the Journal of Learning Analytics has published an  entire issue on the concept [1], and American Behavioral Scientist also highlighted the issue [2]. While ethics and privacy have been  a notable theme, these discussions have been mainly theoretical in  nature.  While there is often concern amongst researchers and practitioners  about using student data for learning analytics, there has been very  little documented engagement with students themselves. [3- 5]  At  the date of this writing, the authors can find only three examples.   The first was a survey distributed in undergraduate educational data  mining courses at six Malaysian universities. [6] The second was  an exploration conducted at Manheim University which consisted  of 330 responses from students taking a one credit course for the  purposes of the study. [7] Finally, in April 2016, Jisc commissioned  a study in which 240 students from higher education and 166 from  further education were interviewed. [8]  The purposes of these studies varied, but there was a main theme:  seeking evidence of student acceptance of data about them and their  learning processes being used for the purposes of learning  analytics. The results of all three studies suggest that students  understand that their data has an inherent educational value. Two  of the three studies show direct evidence that students exhibit little  hesitation in sharing data that is clearly learning related. [6, 7]  Ifenthaler and Schumacher claim that students would be willing to  share more extensive data if, in return, the learning analytics system  provided rich and meaningful information.[7] Despite these  findings, privacy work in LA is at an early stage of development.   2.  ETHICS AND PRIVACY IN LEARNING  ANALYTICS   Ethical and privacy issues are inevitable when institutions begin to  plan LA activities. Concerns are expressed in particular by faculty  and staff around the potential misuse of student data. Most of these  issues have now been documented in the growing LA literature.   There are particular concerns that decisions may be taken on the  basis of flawed or inadequate data or that the analytics techniques  themselves may result in invalid predictions. Putting blind faith in  algorithms, particularly when sold as part of black box solutions,  may not be sensible either from an ethical point of view. [9]   Permission to make digital or hard copies of all or part of this  work for personal or classroom use is granted without fee  provided that copies are not made or distributed for profit or  commercial advantage and that copies bear this notice and the  full citation on the first page. Copyrights for components of this  work owned by others than the author(s) must be honored.  Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires  prior specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada  Copyright is held by the owner/author(s). Publication rights  licensed to ACM.  ACM 978-1-4503-4870-6/17/03$15.00  DOI: http://dx.doi.org/10.1145/3027385.3027392    LAK17, April 2017, Vancouver, BC, Canada  K. Arnold and N. Sclater   Another commonly expressed fear is that students autonomy in  decision making may be undermined.  They may even be  infantilized by spoon-feeding them with automated suggestions,  thus failing to facilitate the development of essential research  skills.[10] There are also concerns that some students may be  demotivated by seeing that they are predicted to fail; predictions  that may become self-fulfilling prophecies. However, it can be  argued that not telling a student the reality about where they are,  and thus allowing them to continue on a likely path of academic  failure or withdrawal, is unethical. Early alert systems may enable  an intervention to be taken to prevent a student dropping out; it may  also be in the students interest to withdraw early in order to switch  to another course or minimize wasted time and expenditure. This  so called obligation of knowing on the part of the institution is  indeed one of the strongest arguments that LA should be deployed  (with adequate safeguards) in order to support students. [11]  Other concerns include potential negative impacts of continual  monitoring, and even manipulation of the analytics by students  wishing to game the system to improve their ratings. Prejudicial  categorization, and hence treatment of students, based on their  ratings, is another potential danger. Meanwhile, reduction of the  individual student to a simple metric (e.g. a red, amber, or green  traffic signal) while ignoring their personal circumstances, is  regarded by some as limiting and potentially harmful. A further  ethical issue is that of triage (i.e. deciding which students to devote  an institutions scarce resources to supporting): the ones who are at  greatest academic risk or those who could be helped to move from  a good to a top grade. [11]  Some issues have both a privacy and an ethical dimension. In many  countries there is a legal requirement to be upfront with citizens  about any data collected about them, and the purposes to which it  is being used. Institutions are already subject to a range of legal  requirements in the collection and processing of student data, and  in many cases LA can simply conform to existing policies and  processes. Obtaining consent from students for LA may be  necessary however, and a strong argument is made in universities  that this should be informed consent. However, while this makes  sense in the context of short term research projects, it may not be  appropriate as LA becomes part of the normal business of  institutions. Opting out of data collection and its use to help a  student in their studies may be unwise, may leave gaps in the  dataset which negatively impact other students, and may simply be  impossible. (e.g. LMSs cannot function without accumulating data  about student use).   3.  LEARNING ANALYTICS AT JISC  Jisc is a charitable organization responsible for many aspects of the  information technology infrastructure at universities and colleges  in the United Kingdom (UK).  Since late 2014 Jisc has invested  substantially in building capacity for LA within this sector.   Specific activities include developing a community of practitioners  through online channels and regular meetings at locations across  the UK; discovery and on-boarding processes where  consultants visit institutions to help them develop their LA  capacities; and the development of an open architecture for LA,  combining open source and commercial products as appropriate for  each institution. One of the first areas to be tackled by Jisc was the  lack of guidance in meeting the apparent concerns of students and  other stakeholders around the ethics of using their data for LA. A  taxonomy of ethical, legal, and logistical issues prevalent in the  emerging LA literature was compiled, and this formed the basis for  a Code of Practice for Learning Analytics. [12] While staff and  faculty are often quick to voice their concerns about ethical aspects  of LA, it was realised that little was known about student attitudes.   Jisc therefore employed an external company to interview more  than 400 students across the four nations of the UK.   4.  LEARNING ANALYTICS AT THE  UNIVERSITY OF WISCONSIN SYSTEM  The University of Wisconsin System (UWS) is comprised of 26  campuses: 2 doctoral granting, 11 master granting, and 13 2-year  institutions and is located in the Midwestern region of the United  States.  In addition to the campuses, UWS has a statewide presence  known as UW Extension.  Combined, UWS serves more than  150,000 learners annually.   Given the complexity of the learning  environment, UWS has spent the last four years piloting a variety  of learning analytics tools in diverse contexts.  In that time, 125  courses and over 16,400 students have experienced some form of  the learning analytics initiative. During this pilot, UWS placed a  major focus on technical viability and user experience.    At UWS, like at many other institutions, the University has been  struggling with how to handle student privacy issues. Certainly,  there is no policy (or even best practices) specifically about  learning analytics. Data policy provides little guidance and is  interpreted differently by different offices and individuals. To  address this concern, a student engagement strategy has been  undertaken in which student experience was a major focus.  Students were asked directly about their perceptions of the  institution using their data for learning analytics initiatives.   Questions about student privacy were added to an ongoing end-of- semester survey about user experience. It is important to note that  all students responding to this survey had experienced learning  analytics and had actively used a LA tool (at least 3 times a week)  designed to help them be more reflective learners. This is a  departure from the other studies in that UWS asked about their  actual experience with a LA system rather than their perception of  an unfamiliar tool.  This, we believe, provides a slightly different  perspective.    5.  METHODOLOGY  Basic survey methodology was used for this study with slight  variations between Jisc and UWS. However, three specific  questions were asked of both Jisc and UWS students, requiring a  simple yes/no response.  Therefore, these questions can easily be  compared and contrasted.  The three questions are:   i) Would you be happy for data on your learning activities  to be used if it kept you from dropping out or helped you  get personalized interventions   ii) Would you be happy for your data to be used if it helped  improve your grades   iii) Would you be happy to have your data visualized  through an app where you can look to compare with  your classmates  These student-focused questions were selected because of the  prevalence of ethical and privacy issues among institutions  deploying LA but the lack of data about the attitudes of students  themselves. The questions each highlight a potential benefit to the  learner of LA.   5.1  Methodology at Jisc  Jisc commissioned interviews with students from higher (HE) and  further education (FE) institutions across the UK. 406 students  were interviewed, 59 percent (n=240) from higher education, and  41 percent (n=166) from the college sector. The majority of these  (88 percent; n=357) were in England, with much smaller numbers  in Scotland, Wales, and Northern Ireland. 65 percent of  interviewees were female (n=264), and 35 percent male (n=142).     LAK17, April 2017, Vancouver, BC, Canada  K. Arnold and N. Sclater   17 percent (n=71) were under 18 years old, 29 percent 18 years old  (n=119), with lower numbers at each older age group.   5.2  Methodology at UWS  Online survey methodology was also used at UWS.  All students  enrolled in one of five pilot courses using a particular learning  analytics system were provided a link to complete the survey.  In  all cases, a combination of minimal use of the LA system and  completion of the survey, qualified the student for a small amount  of extra credit.   The courses varied in mode (hybrid and online),  subject, level, and enrollment.  The survey was administered at the  end of both Spring 2016 (S16) and Fall 2016 (F16) semesters.  For S16, 669 students were enrolled in a course using this particular  LA tool.  77 percent returned a survey (n=509).   However, 84  records were removed do to either i) incomplete surveys or ii) not  reaching the threshold of required participation. A total of 425  records were maintained for analyses.  98 percent of the  respondents were traditional college age students, between 18-24  years old. 59 percent of the valid respondents were female (n=249)  and 41 percent (n=176) were male.  Freshman comprised 33  percent (n=142) of the sample with sophomores representing 25  percent (n=105), juniors making up 23 percent (n=96), and seniors  representing 19 percent (n=81).   For F16, 2,687 students were enrolled in a course using the LA  system. Thirty-four percent (n=916) completed a survey and met  the threshold for tool usage during the semester. 96 percent of the  respondents were between 18-24 years old. 65 percent of the valid  respondents were female (n=599) and 34 percent (n=314) were  male.  Freshman comprised 42 percent (n=385) of the sample with  sophomores representing 27 percent (n=249), juniors making up 14  percent (n=129), and seniors representing 13 percent (n=117).    6.  RESULTS Overall, students at UWS seem to be more accepting of their  learning activity data being used. The majority of students enrolled  at both UK and American institutions do not seem to object to the  use of their data for the purposes of helping them improve their  grades (see Table 1; Figure 1).   However, when it comes to  persistence from semester to semester, Americans are still fairly  amenable (76% in S16; 72% in F16) to having their data leveraged  while students enrolled in UK institutions seem less agreeable  (53% in HE; 54% in FE).  On the final item, having data visualized  through an app to facilitate comparison, American students (61%  in S16; 60% in F16) appear to be a bit more hesitant in this regard,   while markedly smaller proportions of the UK students surveyed  are interested in having their data used in that way (21% in HE,  26% in FE). However, it is possible that when the benefits of a  student app for LA are explained to students, or when they  experience it for themselves, they may be more positive about using  such functionality. Many may find it useful to have a better idea of  where they are compared to where they should be, for example.  Written comments from the survey suggest that some students  answered no to having their data used in a comparative fashion  because they were unclear of exactly how that would happen  (anonymously, in aggregate, or identifiably).    Table 1: Percent of students responding yes/no to survey  questions   Jisc HE Jisc FE UW S16 UW F16   Yes No Yes No Yes No Yes No   Dropping  Out 53 23 54 20 76 5 72 5  Improving  Grades 71 12 77 8 94 2 91 2  Compared 25 51 26 47 61 7 60 10   7.  LIMITATIONS The authors wish to be very clear that these results alone are not  sufficient to claim generalizability.  Nor is the methodology  rigorous enough to make any decisions about how learning  analytics is applied in educational settings. Neither of these were  ever the aim of this paper.   Our intent was to expose a seemingly  growing gap between the theoretical and applied perspectives of  learning analytics.  This juxtaposition deserves more thorough, and  rigorous evaluation. Indeed, it is our hope that this short paper  demonstrates the nascent reality of this area of study; and will  encourage hearty discussion, as well as spur additional research.   8.  DISCUSSION While the data reported in this short paper is not complex, it is a  solid position on which to begin engaging students. To date  researchers and ethicists have made strong stands on the use of  student data for learning analytics.  There is an inherent value in  user data, and that is demonstrated regularly in all industries,  especially business, retail, and healthcare.  Further work must be   53  71  25  54  54  26  76  94  61  72  91  91  0 10 20 30 40 50 60 70 80 90 100  Dropping Out  Improving Grades  Comparison  UW F16 UW S16 JISC FE JISC HEFigure 1: Percentage of students answering affirmatively to survey questions.     LAK17, April 2017, Vancouver, BC, Canada  K. Arnold and N. Sclater   done, directly with students and instructors, to get a clearer, more  generalizable idea of how students wish to use their data.  In an  increasingly digital age, expectations of learners may be shifting.   There may be a true generational shift in the acceptance of using  students data to directly benefit them in the optimization of their  learning.  As educators, it is imperative that we are armed with data  from our users, rather than building up walls of theory and  rhetoric.    Our aim is to move the field of learning analytics forward while  still respecting the privacy of those we serve.  However, artificial  barriers need to deconstructed, and a paradigm of cooperation with  students needs to become a central focus. We must work to  demonstrate more generalizable results. Context must be examined.  Ownership of data and the analytics must be considered. Learners  must be consulted.    9.  REFERENCES [1] Society for Learning Analytics Research (2016).  Ethics and  privacy in learning analytics. Journal of Learning Analytics, 3(1).  [2] Haythornthwaite, C., de Laat, M., and Dawson, S. (2013,  October).  Learning Analytics. American Behavioral Scientist, 57,  (10).  [3] Prinsloo, P., & Slade, S. (2015, March). Student privacy self- management: implications for learning analytics. In Proceedings of  the Fifth International Conference on Learning Analytics &  Knowledge (pp. 83-92). ACM.  [4] Drachsler, H., & Greller, W. (2016, April). Privacy and  analytics: it's a DELICATE issue a checklist for trusted learning  analytics. In Proceedings of the Sixth International Conference on  Learning Analytics & Knowledge (pp. 89-98). ACM.   [5] Rubel, A., & Jones, K. M. (2016). Student privacy in learning  analytics: An information ethics perspective. The Information  Society, 32(2), 143-159.  [6] Pardo, A., & Siemens, G. (2014). Ethical and privacy principles  for learning analytics. British Journal of Educational  Technology, 45(3), 438-450.  [6] Wook, M., Yusof, Z.M. & Nazri, M.Z.A. (2016). Educational  data mining acceptance among undergraduate students.  In  Educational and Information Technologies. doi:10.1007/s10639- 016-9485-x  [7] Ifenthaler, D., & Schumacher, C.  (2016). Student perceptions  of privacy principles for learning analytics. Educational  Technology Research and Development, 1-16.  [8] Sclater, N., Peasgood, A., & Mullan, J. (2016). Learning  Analytics in Higher Education.  [9] Sclater, Niall. (2017 - forthcoming). Learning Analytics  Explained. Routledge.  [10] Ellis, C., 2013, Broadening the scope and increasing the  usefulness of learning analytics: The case for assessment analytics,  British Journal of Educational Technology, 44(4), pp. 662-664.  [11] Campbell, J. P., DeBlois, P. B. & Oblinger, D. G., 2007,  Academic Analytics: A New Tool for a New Era, EDUCAUSE  Review, 42(4), pp. 40-57.  [12] Sclater, Niall. (2016). Developing a Code of Practice for  Learning Analytics. Journal of Learning Analytics, 3(1), 16-42.    "}
{"index":{"_id":"10"}}
{"datatype":"inproceedings","key":"Andrade:2017:USL:3027385.3027429","author":"Andrade, Alejandro","title":"Understanding Student Learning Trajectories Using Multimodal Learning Analytics Within an Embodied-interaction Learning Environment","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"70--79","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027429","doi":"10.1145/3027385.3027429","acmid":"3027429","publisher":"ACM","address":"New York, NY, USA","keywords":"embodied cognition, embodiment, learning environments, multimodal learning analytics, science education, sensing technologies","Abstract":"The aim of this paper is to show how multimodal learning analytics (MMLA) can help understand how elementary students explore the concept of feedback loops while controlling an embodied simulation of a predator-prey ecosystem using hand movements as an interface with the computer simulation. We represent student motion patterns from fine-grained logs of hands and gaze data, and then map these observed motion patterns against levels of student performance to make inferences about how embodiment plays a role in the learning process. Results show five distinct motion sequences in students' embodied interactions, and these motion patterns are statistically associated with initial and post-tutorial levels of students' understanding of feedback loops. Analysis of student gaze also shows distinctive patterns as to how low- and high-performing students attended to information presented in the simulation. Using MMLA, we show how students' explanations of feedback loops look differently according to cluster membership, which provides evidence that embodiment interacts with conceptual understanding.","pdf":"Understanding Student Learning Trajectories Using  Multimodal Learning Analytics within an Embodied-  Interaction Learning Environment  Alejandro Andrade   Indiana University  laandrad@indiana.edu     ABSTRACT  The aim of this paper is to show how multimodal learning analytics  (MMLA) can help understand how elementary students explore the  concept of feedback loops while controlling an embodied  simulation of a predator-prey ecosystem using hand movements as  an interface with the computer simulation. We represent student  motion patterns from fine-grained logs of hands and gaze data, and  then map these observed motion patterns against levels of student  performance to make inferences about how embodiment plays a  role in the learning process. Results show five distinct motion  sequences in students embodied interactions, and these motion  patterns are statistically associated with initial and post-tutorial  levels of students understanding of feedback loops. Analysis of  student gaze also shows distinctive patterns as to how low- and  high-performing students attended to information presented in the  simulation. Using MMLA, we show how students explanations of  feedback loops look differently according to cluster membership,  which provides evidence that embodiment interacts with  conceptual understanding.    CCS Concepts   Applied computing ~ Computer-assisted instruction      Applied computing ~ Interactive learning environments   Keywords  Embodied Cognition; Embodiment; Learning Environments;  Multimodal Learning Analytics; Sensing Technologies; Science  Education.   1. INTRODUCTION  Learning Analytics aim to understand and improve learning by  means of analyzing large quantities of student data that has been  automatically captured within technologically-rich learning  environments [21; 22]. To better understand learning and to provide  insights into particular learning processes, researchers use Learning  Analytics to correlate traces of student behaviors with measures of  student academic performance or conceptual, cognitive, or  emotional processes [13]. Furthermore, with the use of emerging  sensing technologies (e.g., Kinect, Leap Motion, Computer Vision)  researchers have been able to develop new learning environments   to support learning via embodied interactions [2; 19] as well as to  capture large amounts of learning-related data through multiple  modalities such as bodily movements, eye gaze, and speech  prosodics [8; 20]. The aim of this paper is to show how we used  multimodal learning analytics (MMLA) to help answer the question  of how elementary students explore the concept of feedback loops  while controlling an embodied simulation of a predator-prey  ecosystem through bodily movementsusing hand movements as  an interface with the computer simulation.  Our goal was twofold: (a) represent student motion patterns from  fine-grained logs of hands and gaze data, and then (b) map these  observed motion patterns against levels of student performance to  make inferences about how embodiment plays a role in the learning  process. This endeavor was technically demanding because motion  tracking is captured by fine-grained temporal data. These time- dependent data had to be analyzed with statistical models that can  account for the lack of independency inherent within body  movement observations. Our methodological approach can be  summarized in the following steps: (1) find motion vectors; (2)  translate the combination of observed motion vectors into latent  motion states using a Hidden Markov Model; (3) compute distance  measures from the sequences of latent motion states using an  Optimal Matching algorithm that accounts for the transition  probabilities; (4) find natural groupings, or clusters, across students  using the distance matrix in a Hierarchical Cluster analysis; (5) find  the association between these clusters and student performance;  and (6) qualitatively show how these clusters represent different  patterns of student reasoning.   Results show that students can benefit from short embodied  tutorials to learn about ecosystem dynamics. Five distinct motion  sequences can represent the variability in the students embodied  interactions with the simulation, and these motion patterns are  statistically associated with initial and post-tutorial levels of  students understanding of feedback loops. Analysis of student  gaze shows distinctive patterns as to how low- and high-performing  students attended to information presented in the simulation. We  show how students explanations of feedback loops look differently  according to cluster membership, which provides evidence that  embodiment interacts with conceptual understanding. Because of  this interaction, and through the use of MMLA techniques, in this  paper we show that embodiment can be used as an indicator of  student current conceptual level, and also as a promising  instructional strategy to foster conceptual developmentas various  theoretical approaches of embodiment have suggested [3; 17].   2. EMBODIED LEARNING AND SENSING  TECHNOLOGIES  Recent theories of learning and cognition, called embodied  cognition and embodied learning, build upon the hypothesis that  body movement and interaction with the physical world has deep   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00  DOI: http://dx.doi.org/10.1145/3027385.3027429     consequences on peoples cognitive processes [3; 12; 17; 19; 24].  It follows that, if cognition is grounded in body movement, then  increasing ones body movement repertoire might spark the  learning of a new concept (or at least the learning of the same  concept in a new light). To elicit movement in such specific ways,  the use of sensing technologies (e.g., Kinect, Leap Motion) in the  design of new learning environments is gaining traction. This is  because these emerging technologies can shape the ways in which  students interact with the computer and help students explore new  ways in which they can move. Therefore, studying the role  technology plays in cueing the body to move in particular ways,  and how this movement facilitates learning and conceptual  development, is of great relevance to furthering our understanding  of how people learn through embodied interactions [9; 18].  However, while most studies have focused on spontaneous  movements (where students freely explore the interaction space),  few studies have looked into eliciting certain kinds of bodily  movements for embodied learning. Because our focus on body  movement is with the students hands in particular, we try to elicit  certain kinds of gestures. In elicited gesturing, the student is cued  to gesture in a particular way [18], congruent with the to-be-learned  concept [6; 16], as illustrated in Figure 1.       Figure 1. Student use bimanual gestures to represent an  inverse relationship between two variables.   In order to study a movement repertoire, new research tools can  shed light on students embodied learning. Specifically, learning  analytics can provide powerful insights into understanding  particular aspects of the learning process, such as embodied  learning. When using sensing technologies to capture students  observable behaviors, these tools are referred to as Multimodal  Learning Analytics [MMLA, 7]. MMLA represents observable  behaviors via multiple modalities such as body movement,  gestures, gaze tracking, speech prosodics, and other discourse  features. Using MMLA techniques, we explore the analysis of  embodied learning in the context of short tutorials with a group of  3rd and 4th graders who interacted with an embodied simulation. We  designed this embodied simulation with the intention to help  students explore complex systems concepts by gesturing with both  hands to represent dynamic relationships within a predator-prey  ecosystem.   3. AN EMBODIED SIMULATION OF  PREDATOR-PREY DYNAMICS    We designed a predator-prey simulation using computer vision to  track color in a video stream using a Python algorithm to track  students hand movements while they interacted with a graphing  simulation, as follows: The student holds two colored balls (see   Figure 2.a) to interact with both the bar chart and line graph  displayed on the computer screen (see Figure 2.b). The red bar (left)  represents the hypothesized fox population size, whereas the blue  bar (right) represents the rabbit population size. The tutorial walks  the student through some tasks of increasing difficulty, from pattern  matching only one bar height to more difficult tasks where the  student replicates the full predator-prey dynamics on her own. We  designed this interaction for congruency between the system (two  population variables) and the interaction afforded by the computer  interface (bimanual gestures).      Figure 2. Our Predator-Prey Dynamics Embodied Simulation.  Our focus is to explore whether and how short tutorials with our  embodied learning environment help elementary students  recognize ecosystem dynamics (e.g., feedback loops and delays) in  the context of predator-prey relationships. Our preliminary results  show that students tend to include more gestures in their  explanations of system dynamics after the tutorial intervention [4;  5]. We argue that students post-tutorial gestures are learned during  the tutorial time, while they interact with the embodied simulation.  Thus, we hypothesize that an analysis of the log data can reveal  patterns in students movements, which then would correlate with  various learning levels. Therefore, our research questions are: Did  students learn about feedback loops after interacting with the  embodied simulation Are there any distinguishable patterns in  students embodied interactions And, do these patterns correlate  with students learning of predator-prey dynamics  In order to answer these questions, we use MMLA to explore fine- grained representations of student bodily movements during their  embodied interaction with the simulation. The goal was to perform  an exploratory analysis in which students hand movements and  gaze direction are regarded as performance indicators. These  performance indicators represent particular ways in which students  interact with the simulation, and are mapped onto various student     conceptual understanding levels. In particular, MMLA analysis is  used to explore how students reveal different patterns to move the  colored balls. We hypothesize that some patterns are more  productive than others. The productive patterns would capture  student understanding of system dynamics, and therefore the  gestures within this pattern would embody a fluid representation of  feedback loops. Less productive patterns, on the other hand, would  embody these feedback loops only partially. Because we gathered  a large number of data points per student, these patterns can be  expressed numerically and, therefore, differences can be compared  statistically.  Additionally, we compare student gaze data to explore whether  there were also differences in information encoding. Because the  students are presented with two types of visual representations, a  bar chart and a line graph, they were actively integrating between  these two sources of visual information. Gaze analysis can help  answering the question of whether students engaged with these  graphs differently, and, hopefully, some possible reasons as to why.  These differences can be indicative of different student readiness  for interacting with these kinds of representations, and even shed  light on possible reasons as to the observed variation in reasoning  levels during the post-tutorial explanations.  In the following sections, we present a detailed description of these  analyses, but first we provide a brief description of the context  where the data was collected.   4. METHODS  Fifteen 3rd and 4th graders (F = 8, M= 7, Avg. Age = 9.13, SD Age  = 0.8) from a mixed class at a school in the Midwest of the US took  part in the study. Students were individually interviewed and they:  (a) answered a pre-tutorial questionnaire, then (b) interacted with  the simulation, and then (c) answered a post-tutorial questionnaire.  Interviews were videotaped and the three interview phases took  thirty minutes in average. The embodied tutorial took about  eighteen minutes and the protocol included nine tasks (each task  lasting for about two minutes). The pre and post-tutorial  questionnaires were adapted from Hokayem, Ma, and Jins [14]  study. We scored students answers with a Feedback Loop  Reasoning coding scheme, also adapted from Hokayem et al. [14]  which consists of seven levels (the higher the level the more in- depth the student understanding is), detailing the reasoning  progression about feedback loops within a predator-prey  ecosystem.   In order to analyze the students hands movements, we focused on  a 2-min task which required students to match both population bars.  The prompt for this task read: As in the previous tasks, the  computer is moving the bars up and down. Now, try and match the  two populations by moving both balls with your hands. In the  following sections, we explain how we built a representation of the  bodily movements with a set of motion sequence typologies, and  how we studied whether these types of motion sequences were  correlated with student understanding of feedback loops.   4.1 Motion vectors  In order to spot distinct patterns in students actions, we used the  simulation log data to create movement vectors of students hands  (see Figure 3). The computer tracks students hand movements and  predicts the direction of the movement of each hand, whether it is  going up, down, or remains static in each time interval. Each time  interval was composed of two consecutive video frames (video  frame rate was seven frames per second). The motion vectors were,  thus, the difference between frame at time t+1 and frame at time t.  In order to account for the idiosyncratic amplitude of each student   movements, the motion vectors were normalized by dividing each  value over the maximum motion vector value per student. Then, to  better distinguish whether the hand was static or actually moving  up or down, a threshold was used to reduce the noise in the values  captured by the tracking system. The threshold was defined as the  semi-interquartile range of the motion vector values per student.     Figure 3. In (a) both hands are moving in opposite directions   simultaneously, whereas in (b) the student moves only one  hand at a time.   4.2 Latent motion states  Direction vectors only indicate the direction of movement between  time t and t+1. If the interest is in understanding the students  underlying intentions with the bimanual movement, a statistical  model can be used to represent the students mental states at each  time point for the duration of the activity. For instance, the student  might be trying to coordinate a motor schema of a simultaneous  movement of one hand going up and the other hand going down.  Or perhaps the student might be trying to coordinate the movement  of one hand after she starts the movement of the other hand. When  using a model-based statistical approach, latent states can be  inferred from the patterns of fine-grained hands movements. This  data reduction would go from two data streams of (categorical)  direction vectors, to a sequence of (cognitive) motion states. A  Hidden Markov Model was fit to the time-series of motion vectors.  An HMM model assumes that the observed combinations of  direction vectors at every point in time are produced by the previous  time point and by a finite set of latent cognitive states. The model  inputs a sequence of observations and predicts a sequence of latent  states of length N, where N is the number of time points in the data  frame. An example of a hypothetical predicted 9-State sequence, fit  to a 150-frame window of a students movement data, is shown in  Figure 4. The top of Figure 4 shows the plotted hands position over  time as blue and red lines. The vertical lines show where there is a  state change in the hands coordination. The periods between  vertical lines correspond to latent states. Figure 4, below, plots  when each state is occurring throughout the time series. The state  sequence is plotted as a sequence of colors where each color  represents a state. This colored graphical representation is preferred  because it does not convey the idea that the latent states have any  meaningful order.  The meaning of the latent states is evaluated by examining the  composition of the mixture of the observed variables. In order to  find the number of latent states responsible for the observed  combinations of direction vectors, several models are fit to the data  and the best model is selected using a fit index, in this case the  negative log likelihood. The HMM also produces a transition  probability matrix between latent states. This matrix contains  information as to which states are more likely to follow after which  states. To run the HMM models, we used the depmixs6 R package  [23]. In summary, our analysis of the students hands movement     requires us to understand the presence of sensorimotor schemas.  Therefore, the HMM helps us translate the hands movement raw  data into a vector of latent states (i.e., a bimanual movement  arrangement) per student.           Figure 4. An example of a sequence of latent states. Top: the  trajectory of hands movement over time (right hand in red   and left hand in blue). The vertical lines show the division of  the hands trajectory by latent state. Bottom: a categorical plot   with the inferred/predicted latent states.   4.3 Dissimilarity within latent motion state  sequences  To characterize the students latent motion states sequences and to  account for time dependencies, a dissimilarity measure was  produced using an Optimal Matching (OM) algorithm. This  quantitative measure is important in that it appraises the degree of  similarity between students level of sensorimotor coordination.  The OM algorithm is a dissimilarity measure, part of the family of  measures known as edit distances, based on the minimal cost of  transforming one sequence into the other [10]. Temporal  information is accounted for by insertion/deletion (indel) costs and  transition values. The OM algorithm inputs a matrix of sequences,  an indel cost, and a matrix of substitution costs based on the matrix  of latent states transition rates. The output is a matrix of pairwise  distances quantifying the number of transformations (insertions,  deletions, or substitutions) required to transform one sequence into  another. To run the OM algorithm, we used the TramineR R  package [10].   4.4 Hierarchical clustering  After the distance matrix is calculated, an agglomerative  hierarchical clustering is used to obtain distinctive sequence  typologies based on the empirical relationships observed in the  data. To run the cluster analysis, we used the cluster R package.   4.5 Categorical statistical analysis  After the sequence typologies were identified and each student  assigned to a particular cluster, various categorical analyses were  conducted to measure the association between this set of clusters  and student performance during the pre- and post-tutorial tests.  Specifically, we used the Cochran-Mantel-Haenszel statistic to  capture the association between ordinal and nominal level  variables, such as is the case for levels of feedback loop reasoning  versus motion sequence clusters. To compute the Cochran-Mantel- Haenszel statistic, we used the vcdExtra R package.   4.6 Eye gaze  To analyze eye gaze, we first extracted gaze coordinates by post- processing each video using the OpenFace software [25]. We chose  to post-process the video data because we did not have in-situ eye                                                                     1 Where r = Z / N   tracking data. The OpenFace software, however, allow one to  extract an approximation of eye gaze and retrieves three  dimensional features to project the location of the gaze in every  frame in the video, provided that the algorithm is able to detect a  face in the video frame (see Figure 5). We explored patterns in  student gaze using heat plots created with the contour function in  the MASS R package. Heat plots show the regions on the screen  that were most gazed at by the student during this task. We made  use of the identified clusters of student motion sequences to create  groups of heat plots. We proceeded to interpret the variability in the  gaze to guide our interpretation of the different way students paid  attention to the graphs displayed by the simulation. We hypothesize  that successful students would focus their attention more on the bar  graphs because it is these bar graphs which elicit their hands  movements.      Figure 5. Postprocessing student gaze using the OpenFace   software.   5. RESULTS  Did students feedback loop reasoning increase after interacting  with our predator-prey embodied simulation The level of feedback  loop reasoning significantly increased 2 points in the feedback loop  reasoning scale from pre to post-tutorial scores (Mdn = 4 and 6,  respectively), Z(15) = 2.779, p-value = .008, with a large effect size,  r = .718.1 Although without a control group our causal claims are  not granted, these results suggest that students might have, at least  partially, benefited from our tutorial with the simulation. However,  were there any distinguishable patterns in students embodied  interactions To find these patterns, we conducted a cluster analysis  in two stages. First, we created latent motion states to reduce the  dimensionality of the log data, and then ran a hierarchical cluster  analysis on the sequences of latent motion states across students.    5.1 Five latent motion states  Several HMM models were fit to the motion vector data, and a 5- state model fit the data better (with a -Log Likelihood = -25,527.7,  compared to -30,030.3 from a 4-state model, and -26,176 from a 6- state model). Latent motion state A represents left hand leveled  while right hand is going up; latent motion state B represents left  hand going up while right hand is leveled; latent motion states C  and D represent left hand going up (or down) while right hand goes  down (or up)although one hand may partially stay still; and latent  motion state E represents both left and right hands staying still (see  Table 1).   5.2 Five clusters of latent motion sequences   Using an Optimal Matching algorithm, we created a matrix of  pairwise distances between motion sequences. This distance matrix  was fed into an agglomerative hierarchical cluster analysis using  Wards method. The dendrogram from the cluster analysis informs     the number of distinct types of motion sequences. The dendrogram  suggests a partition of five (although six is possible too) distinct  sequences (see Figure 6).      Table 1. HMM State Representation   Left Hand Right Hand   State Down Static Up Down Static Up  A 0.00 0.74 0.26 0.00 0.00 1.00  B 0.00 0.00 1.00 0.00 1.00 0.00  C 0.00 0.45 0.55 1.00 0.00 0.00  D 1.00 0.00 0.00 0.15 0.35 0.50  E 0.00 1.00 0.00 0.00 1.00 0.00        Figure 6. Agglomerative dendrogram using Wards method.   The dendrogram suggests a four-cluster partition.  In order to find distinctive cluster characteristics, we explored these  differences visually and numerically. Figure 6 shows the  characteristic progression for each cluster. Clusters 3 and 4 seem to  have longer distinct states, compared to the more rapid state  changes of Clusters 1, 2, and 5. In fact, it is apparent that Clusters  3 and 4 have less successive distinctive states (82 and 84,  respectively), compared to the more successive states of Clusters 1,  2, and 5 (116, 117, and 111, respectively). In addition, Table 2  shows the normalized mean state durations per cluster. Results  show that Cluster 1 is dominated by states A and D; in State A, the  right hand is going up while the left hand remains static; and in  State D, the left hand goes down while the right hand goes up. This  combination makes the movement look unsynchronized, as the  simultaneous movement of both hands is followed by the  movement of only one hand (see Figure 8). Clusters 2 and 4 are  dominated by simultaneous, opposite movements of both hands  (States C and D). The difference between Cluster 2 and 4 lays in  the number of distinct states, as mentioned above, and also in the  transition probabilities among states (not shown here). Cluster 3  has, besides simultaneous, opposite movements of both hands  (States C and D), high frequency of State E, which is when both  hands are still. Cluster 5, is dominated by State E, that is, frequent  pauses between movements. How, then, do we make sense of these  different kinds of movement types In order to find how these  distinct typologies are associated with variation in student  conceptual understanding, we correlated these typologies with  students scores in the pre- and post-tutorial questionnaires.    5.3 Association with student conceptual  understanding  Albeit a small sample size, we were able to spot regularities in the  way that student motion sequences correlate with student  conceptual understanding. In particular, we noted that Clusters 1  through 3 are mostly associated with low understanding, whereas  Clusters 4 and 5 are associated with higher understanding (see  Table 3). Specifically, evidence from the pre-tutorial questionnaire   shows that students in Cluster 1 had the lowest initial performance,  and students in Clusters 2 and 3 performed at a low to moderate  level. On the other hand, students in Clusters 4 and 5 had a high  initial understanding of feedback loops, and 2 students from cluster  4 were already at the ceiling level. This association is statistically  significant at alpha = .05, X2 (4) = 9.69, p = .045, with a large effect  size, Cramers V = 0.688. Furthermore, evidence from the post- tutorial questionnaire shows that not only is this trend very similar  to the pre-tutorial questionnaire, but also that some clusters made  better learning gains than others. Neither did Cluster 1 nor 4 make  good learning gains. On the contrary, Clusters 2, 3, and 5 students  understanding improved greatly. This association is significant at  alpha = .10, X2 (4) = 9.16, p = .057, with a large effect size,  Cramers V = 0.644.        Figure 7. Five Characteristic Motion Sequences.     Table 2. Normalized State Frequency per Cluster    State    A B C D E   Cluster 1 0.48 -1.05 0.19 1.32 -0.94   Cluster 2 -0.68 -1.16 0.98 1.09 -0.23   Cluster 3 -0.81 -1.32 0.47 0.73 0.92   Cluster 4 -0.1 -1.11 1.2 0.81 -0.81   Cluster 5 -0.49 -0.83 -0.27 -0.14 1.73       Figure 8. Cluster 1 characteristic movements   Table 3. Median Feedback Loops Understanding Scores per  Cluster    1 2 3 4 5  Pre-Test 3.0 3 4.0 6.5 6  Post-Test 3.5 5 6.0 6.5 7   Gain 0.5 2 2.5 0.0 1     These associations can be interpreted in the following way. If a  student shifts through latent motion states very frequently (clusters  1 and 2), she would seem to have a hard time understanding how  she is supposed to move to physically represent the ecosystem  feedback loops, unless, as in the case of Cluster 5 where there is  high frequency of State E, she would stop constantly to reflect on  what this movement should look like. In fact, frequent pauses (State  E) seem to be correlated with better initial understanding and better  learning gains, as evidenced by Clusters 3 and 5, in which there is  a high frequency of State E and also high scores in the post-tutorial  questionnaire. On the contrary, not pausing before shifting to other  motion states may represent that the student is having a hard time  improving on her understanding of these feedback loops (Cluster  1). On the other hand, if a student does not shift frequently through  states, but presents long distinct motion states (Clusters 3 and 4),  she would seem to have a good idea about these feedback loops. If  this motion does not show pausesindicated by low incidence of  State E, then she is likely to have a good initial understanding of  feedback loops and perhaps be already at the ceiling of her  understanding for this particular task (Cluster 4). We complement  our interpretation of these motion sequence typologies by referring  to the patterns in student gaze.  Gaze patterns. In this exploratory analysis, we focus on how  student gaze shifted (or not) throughout the task, and leave for  future analyses the study of what students paid attention to. In  particular, we aim to understand differences in how gaze shifted  over time. Specifically, there seems to be regularities in the  amplitude of students gaze corresponding to different motion  clusters (see Figure 9).      Figure 9. Representative gaze patterns by motion sequence  clusters. The other students gaze plots seem very similar to   these within their respective clusters.    Low performing students (Clusters 1 and 2) tend to have a more  limited gaze band on the screen. High performing students  (Clusters 4 and 5), on the contrary, seem to have broader foci on  the screen. Those students who pause frequently (Clusters 3 and 5),  have also broader gaze patterns than other students. These eye gaze  patterns help explain the differences on apparent similarities on  mean frequency motion states between Clusters 2 and 4. Although  both clusters tend to have high frequency for States C and D,  students in Cluster 2 shifts states more frequently and also fix their  gaze on a particular spot on the screen, whereas students in Cluster  4 shift their gaze to various points on the screen. This ampler gaze  pattern for Cluster 4 might indicate that students are able to better   recognize the information coming from the simulation in order to  map it onto their movement efficiently. The even ampler gaze  pattern in Cluster 5 explains the very frequent pauses in students  movement, and perhaps indicate intense efforts to integrating visual  and motion information together, which seems to have paid off at  the end with good learning gains (as reflected by pre- to post- questionnaire scores increases).   6. DISCUSSION  We found that students can learn about feedback loops from a short  tutorial intervention with our predator-prey simulation. Also, we  found five distinct motion sequences in the way students interacted  with the embodied simulation. Cluster 1 presents an asymmetric  movement; Cluster 2 rapid shifts in the type of movement; Cluster  3 slow shifts and frequent pauses; Cluster 4 slow shifts and  symmetric movement; and Cluster 5 symmetric movement but very  frequent pauses. We also found an association between these  distinct motion sequences and student understanding levels. In fact,  motion sequences are associated with initial levels of understanding  as well as with learning gains, as evidenced by changes between  pre- and post-tutorial scores. Cluster 1 has the lowest performance  and low learning gains. Although it is hard for us to disentangle  with the current experimental design whether or not asking students  to move is preventing them from learning (for that we will use a  control group in future replications of the experiment), the apparent  asymmetry in the students movements might at least suggest a  relationship with the low achievements in this group. Also, this  positive association is illustrated in Cluster 4. Students in Cluster 4  display symmetrical and fluid movements (showing distinct states)  and also a good initial understanding of feedback loops. The  association shows that good conceptual understanding correlates  with good understanding on how to physically represent the  concept via movements. In these two cases (i.e., Clusters 1 and 4),  the student is successful (or not) in embodying the concept through  her physical movements and, thus, we argue that initial conceptual  understanding might have some consequence on the students  performance with the simulator. However, the association between  concepts and embodiment can also start and end the other way  around. The great learning gains of students in Clusters 3 and 5,  evidences that pausing to reflect on how to align ones own  movement with that of the simulation might have helped, albeit  indirectly, students conceptual understanding. We illustrate how  this can happen with the following example (see Figure 10).   The student and the interviewer have an interesting discussion  about what is happening with the motion of predator and prey  populations. In order to reconcile the two opposite forces in the  ecosystem (i.e., predators enable a negative feedback loop because  as their population increases prey population decreases; and prey a  positive one because as their population increases predator  population also increases; which creates a dynamic equilibrium  over time), students have to develop, even if tacit, an understanding  of a lag in the way that population sizes affect each other over  time. In this excerpt, Roberto (pseudonym)who stands in cluster  5considers the full sequence of the ecosystem dynamic. We  suggest that he finds there exists a delay in the way these feedback  loops affect the system, and although he does not articulate the idea  of a lag explicitly, we argue that this delay emerges first as a  sensorimotor scheme, and then as a conceptual gain. It is apparent  in his gestures that this delay helps Roberto resolve his conceptual  conflict about positive and negative feedback loops in the system.  In Lines 14-16, Roberto starts his explanation by articulating the  negative feedback loop as foxes eat the rabbitsi.e., more foxes  less rabbits. He accompanies this with a simultaneous movement of     his right hand down (representing less rabbits) and his left hand up  (representing more foxes). Then, in Lines 17-18, Roberto  articulates a positive feedback loop in which less rabbits would  cause there to be less foxes because the foxes would not have  enough food to eat. But note that he accompanies his verbal  articulation with a gesture where he simultaneously moves his right  hand up (representing more rabbits) and his left hand down  (representing less foxes), and then suddenly stops his movement.  Something really interesting has just happened! In Line 19, as  Roberto notices this speech-gesture mismatch [11], he lowers down  his right hand, correcting his mistake. It is a mismatch because he  has not articulated that rabbits would go up because the foxes are  going down, and we believe that he has discovered that he should  move in a different way. This movement would help him represent  positive feedback loops using a sequential movement with a  delayi.e., between the movement of one hand and the other. The  gesture he probably wanted to perform was to just lower his left  hand (representing less foxes) after his right hand is down   (representing less rabbits); not to move both hands simultaneously  in opposite directions, as he did. Then, in Line 20, Roberto  articulates the next negative feedback in which less foxes would  cause an increase in the number of rabbits, and thus he moves his  right hand up (representing more rabbits). In Line 21, Roberto  engages with the next positive feedback loop, where more rabbits  would cause an increase in the number of foxes, but now he has  learned he has to move one hand only after the other has moved.  Then, Roberto offers a view into his understanding of how there is  a lag between these feedback loops (see Line 22). Our interpretation  of this sequence is as follows. When Roberto initiates the  movement, he leaves a lag between the starting point of one hand  before he starts moving the other. Because Roberto is able to create  this delay, he is able to include both positive and negative feedback  loops in an oscillatory equilibrium, as he has seen in the sinusoidal  graphs on the computer screen after interacting with the embodied  simulation.      13 Interviewer: So both [foxes and rabbits] would go up and down at the same time  14 Student: Okay, so [2 sec pause] so the foxes would be eating all the rabbits          [places both hands in the air in front of him]  [foxes represented by the red ball in his left  hand; rabbits represented by the yellow ball  in his right hand]   15  16   Student: and the rabbits would be just going down and the foxes would be going up         [simultaneously moves his right hand  (rabbits/yellow) down and his left hand  (foxes/red) up]   17  18   Student: But then there arent enough rabbits for the foxes to eat so they would go down uhm         [simultaneously moves his right hand  (rabbits/yellow) up and his left hand  (foxes/red) down, but suddenly stops the  movement]   19 Student: Uhm it would go down           [moves his right hand (rabbits/yellow)  down, as in retracting or correcting his  previous movement]   20 Student: Then the rabbits would go up        [moves his right hand (rabbits/yellow) up  while his left hand (foxes/red) remains still]   21 Student: so they [the foxes] would go up        [moves his left hand (foxes/red) up while  his right hand (rabbits/yellow) remains  still]   22 Student: Kind of like they would go like this...           a. Raises right hand  before moving his  left hand   b. Lowers right hand  while still raising  left hand   c. Pauses right hand  at the bottom while  left hand at the top   d. Lowers left hand  before moving right  hand up   23 Interviewer: Uh-huh, kind of like following each other.  24  25   Student: Yeah, like the rabbit is the first for everything. Its like the first to go down and the first to go up.  The foxes would only go up and down according to where the rabbits go.   Figure 10. Qualitative excerpt of bimanual gestures capture the delay as a new motor-scheme    This is one out of many examples we found in our data where we  observed students making use of congruent gestures after  interacting with our simulation. These congruent gestures help  students map meaning via action information in a holistic way not  readily available in speech alone [16]. However, these gestures  appeared after the tutorial, when we had students explain how they  understand the material. We think that this sort of explaining to  others, which is in and of itself a useful pedagogical set up,   encourages students to build their explanations upon both mental  and physical representations constructed and developed throughout  the tutorial experience. If this tutorial experience would have not  had any effect upon the students understanding of feedback loops  at all, students would not have used this particular kind of  conceptually congruent gestures in their explanations. Thus, there  seems to be an interaction in the way that embodiment and  understanding are related to each other. In our study, it seems that     learning to move in a particular way while reflecting upon what this  movement represents (in terms of predator-prey feedback loops)  seems to be beneficial for students to learn about ecosystem  dynamics. These findings are in line with previous findings from  embodied learning research in mathematics [see for instance, 1; 3;  15].  Nonetheless, further replications of this experiment are required to  rule out other possible explanations for the associations found in  this study. In particular, the lack of a control group makes it  difficult to disentangle whether the increase on feedback loop  understanding scores are due to the embodied experience and not  due to other, conflated, sources of variation. With a control group  where, for instance, a video of the graphical representations of the  dynamic equilibrium between predators and preys was shown, it  would be possible to tease whether these are indeed learning gains  from the embodiment. If the video group has lower scores than the  embodiment group, then one can confidently infer that embodiment  does not prevent students to learn. This conclusion, that  embodiment might be preventing students to learn, is possible  because asking students to follow the movement of the graph bars,  instead of simply watching them move, increase the cognitive load  of the task. The experimental-control comparison will help rule out  that this additional cognitive load is not germane to the learning of  feedback loops.   In addition, to better understand how generalizable our results are,  future replications of the study should look into whether other kinds  of physical movements can support the learning of other concepts.  This type of up-down bimanual movements might be applicable to  only so many conceptual relationships. Other kinds of movements,  however, can be tracked by the computer to expand the range of  congruent mappings that could be done. Also, movement speed can  add a new dimension to the kinds of physical representations that  can be simulated. All in all, the up-down bimanual movement and  the relative position of the hands seem like a good mapping space  for conveying numbers of interrelated populations.   7. CONCLUSION  Using MMLA techniques, we were able to spot differences in  students motion sequences while students interacted with our  embodied simulation. Our methodological approach required us to  represent student movement patterns from the low-level, fine- grained logs of hand movements, and then model latent states and  motion sequences statistically. In doing so, we accounted for the  time dependency between observations and were able to create a  meaningful representation of how students engaged with the  simulation. Because these high-level representations of student  movement were found to be correlated with student learning gains,  we are able to make inferences about how embodiment interacts  with the development of students understanding of feedback loops.  By capturing these patterns, we further our own understanding of  how embodied learning happens, and we believe this knowledge  can help orient new, and improve upon old, learning environment  designs. In particular, the use of bimanual gestures and having  students reflect upon what these movements mean in the context of  predator-prey dynamics, contributed to productively reorient their  attention to the interplay between positive and negative feedback  loops in the system. There are other topics on system dynamics that  we believe can be implemented in the embodied simulationfor  instance, representing patterns of accumulation and rate of change  with body movements. Also, we envision a diagnostic system to  help detect student preparedness level for the embodied tasks. This  is because certain clusters of motion sequences indicated low  learning gains, either because the task seemed outside the student   preparedness levelas was the case for Cluster 1 students-, or  because the task was probably too easyas was the case for Cluster  4 students. This diagnostic system can guide the selection of  subsequent tasks to tailor optimal student learning experiences.  There are a couple of limitations we find worth mentioning. The  sample size in this study was small, and our claims should be taken  tentatively. Because of this, we supplemented significance tests  with effect size measures to provide a practical interpretation of the  statistical results. In future studies, not only will we try and increase  our sample sizes, but also, we envision a more authentic interaction  as we plan to move this embodied learning environment into  elementary school classrooms. These more authentic scenarios will  allow for longer interventions than the short 20-min tutorial  experience from this study. In addition, we have plans to integrating  the Kinect and the Leap Motion sensors to track students  movements in a more unobtrusive way and as precise as possible.  Finally, although this study is of an exploratory nature, our results  show promise and value in continuing to design embodied ways to  support student learning and to develop automated, multimodal  ways to capture how students learn while they explore why it is  important to move in different ways.   8. ACKNOWLEDGMENTS  I am grateful to Joshua A. Danish and Adam J. Maltese for their  support in this project. I am also grateful to Dor Abrahamson and  other three anonymous reviewers for their valuable comments  which have greatly helped improving this paper.   9. REFERENCES  [1] Abrahamson, D., Gutirrez, J., Charoenying, T., Negrete, A.,   and Bumbacher, E., 2012. Fostering hooks and shifts:  Tutorial tactics for guided mathematical discovery.  Technology, Knowledge and Learning, 17(2), 61-86. DOI=   https://doi.org/10.1007/s10758-012-9192-7    [2] Abrahamson, D. and Lindgren, R., 2014. Embodiment and  embodied design. In The Cambridge Handbook of the  Learning Sciences, K. Sawyer (Ed.). Cambridge University  Press, Cambridge, UK, 358-376. DOI=  https://doi.org/10.1017/cbo9781139519526.022    [3] Abrahamson, D. and Snchez-Garca, R., 2016. Learning is  moving in new ways: The ecological dynamics of  mathematics education. Journal of the Learning Sciences  25(2), 203-239. DOI=  https://doi.org/10.1080/10508406.2016.1143370    [4] Andrade , A., Danish, J.A., and Maltese, A., 2017. Why are  you gesturing Elicited gestures and learning gains in an  embodied learning environment. In Proceedings of the Anual  Meeting of the American Educational Research Association  AERA 2017, San Antonio, TX, 1-20.   [5] Andrade , A., Maltese, A., and Danish, J.A., 2017. Foxes and  rabbits, fish and dolphins: Learning and representing  ecosystem dynamics with embodied-interaction sensing  technologies. In Proceedings of the National Association of  Research in Science Teaching NARST 2017, San Antonio,  TX, 1-4.   [6] Barsalou, L.W., 2008. Grounded cognition. Annual Review  of Psychology, 59(1), 617-645. DOI=  https://doi.org/10.1146/annurev.psych.59.103006.093639    [7] Blikstein, P., 2013. Multimodal learning analytics. In  Proceedings of the Third International Conference on  Learning Analytics and Knowledge ACM, Leuven, Belgium,  102-106. DOI= https://doi.org/10.1145/2460296.2460316    [8] Blikstein, P. and Worsley, M., 2016. Multimodal learning  analytics and education data mining: using computational   https://doi.org/10.1007/s10758-012-9192-7 https://doi.org/10.1017/cbo9781139519526.022 https://doi.org/10.1080/10508406.2016.1143370 https://doi.org/10.1146/annurev.psych.59.103006.093639 https://doi.org/10.1145/2460296.2460316   technologies to measure complex learning tasks. Journal of  Learning Analytics, 3(2), 220-238. DOI=  https://doi.org/10.18608/jla.2016.32.11    [9] Dourish, P., 2004. Where the action is: The foundations of  embodied interaction. MIT press, Cambridge, Massachusetts.  DOI= https://doi.org/10.1162/leon.2004.37.1.81    [10] Gabadinho, A., Ritschard, G., Mueller, N.S., and Studer, M.,  2011. Analyzing and visualizing state sequences in R with  TraMineR. Journal of Statistical Software, 40(4), 1-37.  DOI= https://doi.org/10.18637/jss.v040.i04    [11] Goldin-Meadow, S., 2004. Gesture's role in the learning  process. Theory into Practice, 43(4), 314-321. DOI=  https://doi.org/10.1353/tip.2004.0045    [12] Goldin-Meadow, S. and Alibali, M.W., 2013. Gestures role  in speaking, learning, and creating language. Annual Review  of Psychology, 64(1), 257-283. DOI=  https://doi.org/10.1146/annurev-psych-113011-143802    [13] Hershkovitz, A., Knight, S., Dawson, S., Jovanovi, J., and  Gaevi, D., 2016. About learning and analytics. Journal  of Learning Analytics, 3(2), 1-5. DOI=  https://doi.org/10.18608/jla.2016.32.1    [14] Hokayem, H., Ma, J., and Jin, H., 2015. A learning  progression for feedback loop reasoning at lower elementary  level. Journal of Biological Education, 49(3), 246-260.  DOI= https://doi.org/10.1080/00219266.2014.943789    [15] Hutto, D.D., Kirchhoff, M.D., and Abrahamson, D., 2015.  The enactive roots of STEM: Gethinking educational design  in mathematics. Educational Psychology Review, 27(3), 371- 389. DOI= https://doi.org/10.1007/s10648-015-9326-2    [16] Kang, S. and Tversky, B., 2016. From hands to minds:  Gestures promote understanding. Cognitive Research:  Principles and Implications, 1(1), 4. DOI=  https://doi.org/10.1186/s41235-016-0004-9    [17] Lee, V., 2014. Learning Technologies and the Body:  Integration and Implementation in Formal and Informal  Learning Environments. Routledge, New York, NY. DOI=  https://doi.org/10.4324/9781315772639       [18] Lindgren, R., 2015. Getting into the cue: Embracing  technology-facilitated body movements as a starting point for  learning. In Learning Technologies and the Body:  Integration and Implementation in Formal and Informal  Learning Environments, V. Lee Ed. Routledge, New York,  39-54.   [19] Lindgren, R. and Johnson-Glenberg, M., 2013. Emboldened  by embodiment six precepts for research on embodied  learning and mixed reality. Educational Researcher, 42(8),  445-452. DOI= https://doi.org/10.3102/0013189x13511661    [20] Ochoa, X. and Worsley, M., 2016. Editorial: Augmenting  Learning Analytics with Multimodal Sensory Data. Journal  of Learning Analytics, 3(2), 213-219. DOI=  https://doi.org/10.18608/jla.2016.32.10    [21] Siemens, G. and Baker, R., 2012. Learning analytics and  educational data mining: towards communication and  collaboration. In Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge ACM,  Vancouver, British Columbia, Canada, 252-254. DOI=  https://doi.org/10.1145/2330601.2330661    [22] Siemens, G. and Long, P., 2011. Penetrating the fog:  Analytics in learning and education. Educause Review, 46(5),  30-32.   [23] Visser, I. and Speekenbrink, M., 2010. depmixS4: An R- package for hidden Markov models. Journal of Statistical  Software, 36(7), 1-21. DOI=  https://doi.org/10.18637/jss.v036.i07    [24] Wilson, M., 2002. Six views of embodied cognition.  Psychonomic Bulletin & Review, 9(4), 625-636. DOI=  https://doi.org/10.3758/bf03196322    [25] Wood, E., Baltruaitis, T., Zhang, X., Sugano, Y., Robinson,  P., and Bulling, A., 2015. Rendering of eyes for eye-shape  registration and gaze estimation. In 2015 IEEE International  Conference on Computer Vision (ICCV) IEEE, Santiago,  Chile, 3756-3764. DOI=  https://doi.org/10.1109/iccv.2015.428                 https://doi.org/10.18608/jla.2016.32.11 https://doi.org/10.1162/leon.2004.37.1.81 https://doi.org/10.18637/jss.v040.i04 https://doi.org/10.1353/tip.2004.0045 https://doi.org/10.1146/annurev-psych-113011-143802 https://doi.org/10.18608/jla.2016.32.1 https://doi.org/10.1080/00219266.2014.943789 https://doi.org/10.1007/s10648-015-9326-2 https://doi.org/10.1186/s41235-016-0004-9 https://doi.org/10.4324/9781315772639 https://doi.org/10.3102/0013189x13511661 https://doi.org/10.18608/jla.2016.32.10 https://doi.org/10.1145/2330601.2330661 https://doi.org/10.18637/jss.v036.i07 https://doi.org/10.3758/bf03196322 https://doi.org/10.1109/iccv.2015.428   1. INTRODUCTION  2. EMBODIED LEARNING AND SENSING TECHNOLOGIES  3. AN EMBODIED SIMULATION OF PREDATOR-PREY DYNAMICS  4. METHODS  4.1 Motion vectors  4.2 Latent motion states  4.3 Dissimilarity within latent motion state sequences  4.4 Hierarchical clustering  4.5 Categorical statistical analysis  4.6 Eye gaze   5. RESULTS  5.1 Five latent motion states  5.2 Five clusters of latent motion sequences  5.3 Association with student conceptual understanding   6. DISCUSSION  7. CONCLUSION  8. ACKNOWLEDGMENTS  9. REFERENCES   "}
{"index":{"_id":"11"}}
{"datatype":"inproceedings","key":"Mills:2017:PYT:3027385.3027431","author":"Mills, Caitlin and Fridman, Igor and Soussou, Walid and Waghray, Disha and Olney, Andrew M. and D'Mello, Sidney K.","title":"Put Your Thinking Cap on: Detecting Cognitive Load Using EEG During Learning","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"80--89","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027431","doi":"10.1145/3027385.3027431","acmid":"3027431","publisher":"ACM","address":"New York, NY, USA","keywords":"EEG, cognitive load, engagement, intelligent tutoring systems","Abstract":"Current learning technologies have no direct way to assess students' mental effort: are they in deep thought, struggling to overcome an impasse, or are they zoned out? To address this challenge, we propose the use of EEG-based cognitive load detectors during learning. Despite its potential, EEG has not yet been utilized as a way to optimize instructional strategies. We take an initial step towards this goal by assessing how experimentally manipulated (easy and difficult) sections of an intelligent tutoring system (ITS) influenced EEG-based estimates of students' cognitive load. We found a main effect of task difficulty on EEG-based cognitive load estimates, which were also correlated with learning performance. Our results show that EEG can be a viable source of data to model learners' mental states across a 90-minute session.","pdf":"Put Your Thinking Cap On: Detecting Cognitive Load  using EEG during Learning     Caitlin Mills1, Igor Fridman2, Walid Soussou2, Disha Waghray3,    Andrew M. Olney4, & Sidney K. DMello5   1 University of British Columbia; 2 Quantum Applied Science and Research (QUASAR) Inc.;    3 Indiana University; 4 University of Memphis; 5University of Notre Dame  12136 West Mall    Vancouver, BC, V6T 1Z4, Canada  caitlin.s.mills@psych.ubc.ca | sdmello@nd.edu       ABSTRACT  Current learning technologies have no direct way to assess  students mental effort: are they in deep thought, struggling to  overcome an impasse, or are they zoned out To address this  challenge, we propose the use of EEG-based cognitive load  detectors during learning. Despite its potential, EEG has not yet  been utilized as a way to optimize instructional strategies. We take  an initial step towards this goal by assessing how experimentally  manipulated (easy and difficult) sections of an intelligent tutoring  system (ITS) influenced EEG-based estimates of students  cognitive load. We found a main effect of task difficulty on EEG- based cognitive load estimates, which were also correlated with  learning performance. Our results show that EEG can be a viable  source of data to model learners mental states across a 90-minute  session.   CCS Concepts    H.5.m. Information interfaces and presentation (e.g., HCI)    Miscellaneous    K.3.1 computers and Education: Computer Uses  in Education   Keywords  EEG; intelligent tutoring systems; engagement; cognitive load   1. INTRODUCTION  What if your learning environment could assess how deeply you  were thinking What if it could anticipate your learning struggles  without you having to click on the help button, idle on the screen,  or before you answered questions incorrectly. This kind of learning  technology would know the perfect time to help, both when you are  having trouble or when you are barely paying attention.   Although such an omniscient learning technology is not yet  available, students cognitive and affective states have been  reliably inferred using a variety of indirect data sources  eye gaze,  mouse movements and keystrokes, click stream data, facial  features, and speech and language, to name a few [3, 4, 8, 9, 20, 31,   43]. In fact, a few intelligent tutoring systems (ITSs) have already  successfully implemented cognitive and affect detection systems to  improve learning [15, 17]. However, access to more direct  neurophysiological measures of students cognitive processes  would likely improve current systems. Electroencephalography  (EEG) is one such measure that is the present focus.   EEG measures the voltage of coordinated neural firing that passes  through the scalp. Different patterns of the neural firing activity can  be indicative of distinct cognitive states, such as attentional focus,  cognitive load, and engagement [1, 7, 14]. EEG is ostensibly the  least invasive and most affordable method of accessing brain  activity, yet it is rarely used in education due to several  complexities involved.  However, we believe that with advances in  technology, there is considerable potential for EEG-based measures  of students cognitive states. As an initial step, we focus on  modeling cognitive load because of its well-established  relationship with both EEG measures and learning outcomes (see  [2, 23]).   Cognitive load theory suggests that working memory capacity is  limited and  cognitive load is essentially a measure of how much  space in working memory is currently being used [49, 54].  Cognitive load can take two different forms: intrinsic and  extraneous (although see [33, 34, 49]  for debate about a third type  of load, called germane load) . Intrinsic load is imposed by the basic  structure of the learning task and is related to the number and  interactivity of informational elements in working memory (e.g.,  three-digit addition imposes more load than single digit addition).  In contrast, extraneous load is imposed by the way information is  presented and is considered ineffective load in that it does not  directly contribute to schema construction [49].  Importantly, the  different types of load are additive forces in memory. Critically, if  cognitive load exceeds memory resources, learning will be stifled.  However, increases in cognitive load are not necessarily negatively  related to learning. In fact, imposing cognitive load that contributes  to schema formation can positively relate to performance [33].   The overarching goal of our project is to build a system that can  automatically measure cognitive load in real-time and optimize its  instructional strategies to promote intrinsic cognitive load, while  avoiding cognitive overload. This requires establishing that we  can reliably model cognitive load during learning  which is the  aim of the present study.   1.1 Related Work  Previous work has explored the utility of EEG in predicting  cognitive load in a variety of real-world tasks [1, 2, 7, 14, 24, 38].   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '17, March     13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00    DOI: http://dx.doi.org/10.1145/3027385.3027431     Kohlmorgen et al. [36] used EEG signals to optimize automobile  drivers cognitive load in a driving simulation study, finding that  lower load was associated with increased responsiveness. Other  studies have shown that differences in cognitive load can be  detected in memorization and vigilance tasks, as well as while  solving arithmetic problems [7, 24, 52]. Cognitive load has also  been shown to predict performance on hand written math problems  and in memory tasks [13, 14].    Only a handful of studies have examined cognitive load during  learning with educational technology [13, 30, 42, 43]. This  omission can be attributed to a lack of scalability of EEG, stemming  from the cost of EEG technologies as well as the (historically)  intrusive nature of the electrodes, which can require dozens of wet  electrodes. However, recent technological developments have  improved EEG in three key ways: the cost is lower, a smaller  number of electrodes can be used, and dry electrodes are just as  effective as wet ones [37, 38, 42]. Thus, as the equipment  limitations subside, it is important to consider what types of  information EEG can provide during learning from technology.    Initial work using EEG during learning with an educational  technology has yielded some promising results [16, 29, 30, 42]. For  example, EEG waves (alpha, beta, gamma, and theta) were shown  to be correlated with motivation during a serious game [16] and  with affect during a true-false question task [12]. In another study,  machine learning classifiers trained on EEG data were used to  predict the correctness of students answers during reading [29],  though these results may be biased since cross validation was not  done at the student level, which can lead to overfitting.   Two particularly relevant studies provide evidence that EEG  measures of cognitive load are related to the difficulty of the  instructional materials [10, 11]. The first study recorded EEG  signals while students engaged in multiple tasks, including basic  tasks known to induce cognitive load (e.g., digit span, logic tasks)  as well as complex problem solving tasks [11]. EEG signals were  used to measure cognitive load, using Gaussian Process Regression  for each participant. Cognitive load was positively correlated with  the difficulty of the task and was negatively related to performance  on the basic tasks, but not the complex problem solving task.   A second study collected EEG data while children and adults read  difficult and easy passages with an ITS, either silently or out aloud  [10]. Easy passages were taken from the K-1 common core  database, whereas difficult passages were from the GRE and GED  exams. The passages ranged from 62 to 83 words in length. The  EEG signal from a single electrode over the frontal lobe was used  to train a reader-specific classifier using leave-one-story-out  validation and a reader-independent classifier using leave-one- participant-out validation. Separately, the adult and children  classifiers were inconsistent in predicting the difficulty of the text.  Accuracies for the adult models ranged from 39% to 58% for the  reader-specific models and from 49% to 60% for the reader- independent models (chance = 50%). None of the models were  above chance for the children (accuracies ranging from 42% to  50%) [10]. When adult and child data were combined, only the  reader-independent models reached above-chance levels (accuracy  = 56%). Moreover, classifiers were also trained to predict whether  comprehension questions would be answered correctly. However,  classifiers were only successful for silent reading (not reading  aloud) when trained on EEG data collected while the question was  being answered. Performance was below chance when the  classifiers were trained on data while students actually read the  texts. Overall, this study provides some evidence that EEG can  index difficulty of the material, but the classification results were   modest and the dataset was limited to reading short passages, which  may not generalize to more interactive learning environments.     Taken together, previous work suggests that EEG might be a  valuable tool for assessing students cognitive processing during  learning. Further, EEG may also be a good candidate for real-time  optimization and feedback [36, 53]. However, it is unclear if the  findings generalize across multiple learning interactions (e.g.,  reading, listening to a lecture) and in a domain-independent  fashion.   1.2 Current Study  Cognitive load theory suggests that learning can be either  facilitated or hindered by the amount and type of cognitive load  introduced by the instructional design [49].  The current project  aims to develop an ITS that can dynamically tailor its instructional  strategies to impose an appropriate amount of cognitive load for  each participant. We begin by developing an EEG-based detector  of cognitive load and testing its sensitivity to an experimental  manipulation of instructional difficulty embedded in an ITS called  Guru [46]. Difficulty was manipulated in an Instruction phase  where material is initially introduced and explained and a  Scaffolding phase where students answer questions about target  concepts and receive immediate feedback.    We used a recently developed QUASAR [37, 38] headset featuring  dry electrodes that fits on the head similar to a hat (see Figure 1).  High and low cognitive load states were trained from data collected  in a separate training phase using an adaptive algorithm that  leverages EEG spectral features using partial least squares  regression. The models were then used to predict cognitive load  during interactions with an ITS. Whereas some of the tasks were  domain-specific (i.e. related to difficulty manipulations in Guru),  others were domain-independent (e.g., one vs. three-digit addition  for easy vs. difficult conditions, respectively). This was done in  order to address the critical research question of how specific the  training tasks need to be to accurately predict cognitive load        Figure 1. Example of QUASAR Headset     We also address the issue of EEG viability over the course of a  learning session. This is an important question when students need  to sustain attention for extended periods of time. Although it is  desirable to collect data to infer cognitive states, it is equally  important that students feel comfortable and unconstrained by the  sensors. Thus, one of the challenges of collecting EEG data is  minimizing interference with the learning session, while     maximizing data validity. We evaluate data viability by quantifying  how much valid data can be collected over the course of two 20- minute back to back tutoring session after setup and training tasks  are completed.   2. Implementation  2.1 Guru  Guru is a dialogue-based ITS in which an animated tutor agent  engages the student in a collaborative conversation that references  a multimedia workspace. Guru is distinct from most dialogue-based  ITSs, such as AutoTutor [27, 56] or Why-Atlas [55] because it is  modeled on 50-hours of observations of expert human tutors that  reveal markedly different pedagogical strategies from novice tutors  [18]. The computational models of expert tutoring embedded in  Guru are multi-scale, ranging from tutorial modes (e.g.,  scaffolding), to collaborative patterns of dialogue moves (e.g.,  information-elicitation), to individual moves (e.g., direct  instruction) [46]. An in-school evaluation study has shown that  Guru is as effective as human tutors in improving learning  outcomes, and is more effective than classroom instruction alone  [47].   Guru covers 120 biology topics aligned with the Tennessee Biology  I Curriculum Standards in 15 to 40-minute tutoring sessions. The  topics are organized around core concepts (e.g. proteins help cells  regulate functions) that Guru attempts to get students to articulate  over the course of the session.    Guru's interface (see Figure 2) consists of a multimedia panel, a 3D  animated agent, and a response box. The agent speaks, gestures,  and points using motion capture driven animation. Throughout the  dialogue, the tutor gestures and points to areas on the multimedia  panel that are most relevant to the current discussion and are slowly  revealed as the dialogue advances. Student typed input is analyzed  using natural language processing techniques to maintain a student  model that is used to tailor instruction to individual students.        Figure 2. Screenshot of typical Guru content.   A typical Guru session is typically ordered in phases: Preview,  Common Ground Building Instruction (CGB Instruction),  Summary, Concept Maps I, Scaffolding I, Concept Maps II,  Scaffolding II, and Cloze Task. We focused on the two dialog- oriented learning phases (CGB Instruction and Scaffolding) in the  current study. CGB Instruction (sometimes called collaborative  lecture) is where basic information and terminology is covered.  This step is essential because biology involves considerable  specialized terminology that needs to be discussed before more  collaborative knowledge building activities can proceed.  Scaffolding in the typical version uses a Prompt  Feedback   Verification Question  Feedback  Elaboration cycle to cover  target concepts. In the adapted version of Scaffolding, the tutor   would ask a question, process the students answer, and provide  feedback/explanation.    We used two topics in the study that were selected in consultation  with the high school where data was collected. The topics were  selected to minimize overlap with previous topics covered in  classroom lecture (based on the syllabus) in order to minimize the  effects of prior knowledge.  The topics were: Maintaining  Temperature (Topic A) and Trophic Levels (Topic B). The  Maintaining Temperature topic focused on how humans and other  animals regulate their internal body temperature in order to stay  alive during both hot and cold environments. The Trophic Levels  topic focused on how energy is transferred across multiple levels of  the food chain.   2.2 Difficulty Manipulation  We manipulated difficulty by creating two versions the CGB  instruction and Scaffolding sections of Guru. Easy and difficult  versions of CGB Instruction were created by manipulating the  complexity of the tutors spoken content based on dimensions that  are known to play an important role in text complexity [25, 28]:  narrativity, syntactic ease, and referential cohesion. Easy versions  consisted of shorter, simpler sentences with higher frequency  words (e.g., replacing the low-frequency word modicum with a  higher-frequency word like small). Difficult versions had more  complex, longer sentences with lower frequency words. As an  example, consider the difficult and easy versions of the following  sentence: (difficult) Once the brain detects increased heat, it  instigates the pumping of blood nearer to the skin. vs. (easy) The  brain will realize that it is too hot. Then it will begin to pump blood  up close to the skin. Importantly, the content and number of words  was kept consistent across the easy and difficult versions (p = .86  for a paired t-test comparing length).   We assessed the linguistic features of the easy and difficult versions  of the CGB Instruction content using Coh-Metrix, a computational  text analyses tool [26]. We focused on the three linguistic features  (narrativity, referential cohesion, and syntactic simplicity) that  have been linked to text difficulty [25] (see Table 1). We conducted  paired samples t-tests to compare the easy and difficult versions of  the CGB Instruction text. There were four easy-difficult pairs, one  for each of the two sections (first or second) within each topic (2 x  2 = 4). The easy and difficult versions were significantly different  (ps < .05), with effect sizes ranging from 1.88 to 6.03 sigma.  In addition, Flesch Kincaid Grade Level (FKGL), a widely used  measure of text difficulty [35], of the easy version was over 3 grade  levels lower than FKGL for the difficult version (4.9 and 8.5,  respectively); the difference was also statistically significantly  different (p < .05).    Table 1. Descriptive statistics for difficulty manipulation  metrics    Easy Difficult   Metric M (SD) M (SD)   Narrativity 58.6 (8.15) 45.5 (5.64)   Syntactic Ease 96.6 (2.63) 84.4 (6.93)   Referential Cohesion 55.2 (7.78) 30.8 (3.68)   FKGL 4.90 (.680) 8.50 (.500)   Note. FKGL = Flesch Kincaid Grade Level.  For Scaffolding, we manipulated difficulty based on evidence that  recall is more effortful than recognition [40, 44]. Thus, we created  two versions of every question. The prompt question was phrased     as a true-false question for the easy version (e.g., True or false:  The second trophic level is composed of primary consumers.),  whereas the difficult version were open-ended questions that  required students to recall the answer in the absence of answer  choices (e.g., What term is used to describe the organism s place  on the food chain) Students answers to the difficult questions  were scored by comparing their typed answers to a list of possible  predefined correct answers (e.g., correct answer: trophic  level[s]).     2.3 EEG Headset  EEG was collected using a prototype QUASAR 24-channel EEG  headset (see Figure 3). The headset uses ultra-high impedance dry- electrode technology, which has been demonstrated to record high- quality EEG without the need for skin preparation of any kind [37,  37, 39]. It was designed to be a light, low cost unit, specifically  developed for ecological data collection. The system is self- contained, including data acquisition, data storage, cable and  wireless data output, and batteries. The headset can be put on the  head similar to wearing a baseball cap, while reliably positioning  the sensors.    Previous work suggests that this system produces reliable data.  First, signal quality recorded from the dry sensors has been shown  to have a 90% correlation to data obtained from clinical wet  electrodes attached by a technician [38]. Another study compared  wet versus dry electrodes by correlating the output from the two  types across three different conditions (eyes closed/open, n-back  task, and an artifact condition where participants were instructed to  move their jaw, head, eyes, and shoulders one at a time). They  reported an average correlation of .85 for the frontal site,  and .39  for the parietal site [21]. The authors were encouraged by the high  correlations at the frontal sites, and suggested that low parietal  correlations may have been due to the close proximity of the  reference location and the parietal electrode site (causing errors in  noise subtraction). Finally, more promising evidence comes from a  recent clinical evaluation of a similar 20-sensor EEG system, which  determined that data quality from a dry electrode system was  suitable for diagnosing status epilepticus and seizure activity within  190 seconds of donning it [51]. The present study is the first test of  the dry EEG sensors during learning in a classroom setting.       Figure 3. Image of EEG and computer setup    (Tetris task shown)   2.4 Training Tasks  EEG recordings of the following tasks were then used to train the  cognitive workload model.                                                                        1 FDS-7 was substituted with FDS-9 for two participants as the   former was too easy for them.   Forward digit span (FDS-x): A sequence of digits is flashed one- by-one on the screen (for one second). The participant is then asked  to recall the sequence in the order presented and enter it using the  keyboard. They are shown a short (2 sec) correctness feedback  message followed by the next sequence of digits. This task taxes  short-term working memory.  Difficulty (x) varies from 4 digits  (easy) to 9 digits (difficult).  Typical adults are 100% accurate on  FDS-4, 75% on FDS-7, and 25% on FDS-9.1    Column addition (CA-x):  The participant is shown three numbers  with x digits, and asked to add them using the column addition  method. They are given 15 seconds to answer each problem.  Participants enter the answer using the keyboard from left to right,  upon which they are given correctness feedback (2-sec display  message). There is a delay of 10 secs for CA-1 and 1 sec for CA-3  before the next problem appears. This task tests numerical  manipulation and short-term working memory.  Typical adults  complete a CA-1 problem within 3 seconds and a CA-3 problem  within 15 sec.    n-Back: Participants are shown a continuing sequence English  letters at 2.5 sec intervals with a .3 sec blank screen between each  letter.  They are asked to press a key on the keyboard when the  current letter matches one that appeared n steps back. This task is  commonly used to assess working memory. Typical adults are  approximately 100% accurate on 1-back and 75% accurate on 3- back.   Tetris-x: Participants play a Tetris-style game.  Difficulty (x) is  varied by changing the level from 1 (easy) to 9 (difficult), which  increases the speed of the pieces traversing the board by a factor of  around 5.  To ensure that participants' prior familiarity with Tetris  does not confound the results, we used a game with non-traditional  pieces made of hexagonal instead of square blocks. Participants  typically were able to play comfortably at level 1, but lost the game  within roughly 45 secs on level 9, after which the game  automatically restarted.    Guru Intro: A short (~100 sec) session where the participant is  first introduced to Guru's interface and the animated tutor.  This  task involves no significant mental activity, but was used to control  for novelty effects.   Guru Train: Whereas the other training tasks, focus on easy and  difficult versions of traditional cognitive load inducing tasks (e.g.  involving digits and speed), the final training task was task- dependent. The task presented students with two short (60 sec)  segments on the (unrelated) topic of Exponential Growth. The  segments were  similar to CGB Instruction and each was  followed  by an on-screen quiz.  Participants first completed the easy version  (Guru Train-E) followed immediately by the difficult version (Guru  Train-D). The idea behind this training task was that the ITS itself  might impose cognitive load demands that are not inherent in the  more traditional tasks. Thus, mimicking easy and difficult versions  of Guru might yield especially informative training data. Data  Collection.  Eyes Train: Closed (EC): The participant holds eyes closed.  Eyes Train: Open (EO): The participant fixes gaze on a static  target cross on the screen.   2.5 Participants  Twelve students were recruited from a high school in the U.S. under  a protocol approved by Aspire IRB.  Students, 5 males and 7     females, were enrolled in 9th grade biology class in Fall 2014.  Participation was voluntary and students received no classroom  credit for their participation.  The students were not informed of the  study protocol and difficulty manipulations prior to the study.   To enroll in the study, parents and students attended an  informational session and signed informed consent / minor assent  forms prior to the study. Communication with students was  coordinated with a local teacher, who facilitated student enrolment  and data collection.   2.6 Design  The difficulty manipulation was implemented using a within- subjects design, where the easy and difficult versions of CGB  Instruction and Scaffolded Dialogue were interleaved at the  midpoint of each activity corresponding to each topic. For example,  the first half of the CGB Instruction was presented using the easy  version of the content followed by the difficult version in the  second half for the first topic and the presentation order (i.e.,  difficult-easy) was reversed for the second topic. Similarly, the first  half of the scaffolding prompts was true-false questions (easy) for  the first topics and the second half open-ended (difficult). We used  an interleaving rather than a blocking strategy (i.e., each for topic 1  and difficult for topic 2; and difficulty for topic 1 and easy for topic  2) to mitigate topic and fatigue effects. Order of difficulty (difficult- easy vs. easy-difficult) was also counterbalanced across topics,  such that each participant received both orderings of difficulty.  Topic order was counterbalanced across participants.      2.7 Procedure  The study lasted approximately 1.5 hours per student.  Four  students were sequentially tested per day.  Data was collected in a  quiet secluded classroom where there was very little interruption or  distraction. The researchers and students were the only people  present in the room during session.  An overview of the procedure is presented in Figure 4. The study  began with basic setup. This included students donning the headset  and making sure the students were comfortable while wearing it.  The sensors were also checked to ensure they were working  properly. Next, students completed all of the traditional training  tasks followed by the Guru Intro and the Guru Train tasks. Students  then completed the two Guru topics with a 10-minute break in  between each topic. They were not required to spend the entire 20  minutes completing each topic, and were not cut off at any point  before completing the topic. After the second topic, participants  completed the EC and EO training tasks. These tasks were  administered at the end because they were unlikely to suffer from  fatigue effects compared to training tasks that required more  attention and action. Students were then debriefed.     Figure 4. Overview of study procedure   Each Guru session began with a pre-test (~10 multiple choice  questions about the topic). Then, students completed an easy and  difficult block of CGB instruction and Scaffolding (see Design). At  the end of each instruction and scaffolding block, students were  prompted to provide subjective assessments of difficulty of the  preceding material on a 6-point Likert scale. For example, during  the lecture, the tutor would prompt the student to respond by asking,  How difficult are you finding this lecture so far on a scale of one  to six Six being very difficult. A cloze task (e.g., fill in the blanks)   followed the Scaffolding phase, but it did not include any difficulty  manipulation and is not analyzed here. Finally, students completed  a post-test on the topic they just completed.     3. MODEL BUILDING  EEG patterns are highly variable between individuals [5] and  across days [42], so the models are not expected to generalize to  new students. Instead, personalized models were constructed for  each student using their respective training data. We tested four  models that varied based on the training tasks used (see Table 2).  Two models used Eyes Open (EO1 and EO2) in order to mimic  instances when there would essentially be minimal cognitive load  incurred. The corresponding high cognitive load models were  either the combination of all other training tasks (EO1) or only the  Guru Train tasks (EO2). The only differences between the two of  the models is that Task2 includes the Guru Training tasks, while  Task1 is complete domain-independent.    These models use an adaptive algorithm, called Qstates, for  cognitive state classification (see [41] for a detailed overview of  Qstates). The models use EEG spectral features to train models on  the low and high cognitive load training tasks via partial least  squares regression [41].  Model output was determined using a  stratified k-fold (k = 6) cross-validation technique. Data epochs  were randomized, with 60 secs used for model training and 30 secs  for classification. This process was repeated six times until all the  data was classified once. Using multivariate normal probability  density functions (MVNPDF), the models produce a real-time  measure of cognitive load by estimating the likelihood that a given  2 second epoch reflects low or high load. The output is normalized  to have values that range from 0 (low) to 1 (high).    Table 2. Description of training tasks used in four models   Model     Low Cognitive Load     High Cognitive Load     EO1 Eyes Open FDS-4, CA-1, 1-back, Tetris-1,  Guru Train-E, FDS-7, CA-3, 3- back, Tetris-8, Guru Train-D,  Guru Intro   EO2  Eyes Open Guru Train-E and Guru Train-D   Task1 FDS-4, CA-1, 1-back,  Tetris-1     FDS-7, CA-3, 3-back, Tetris-8   Task2 FDS-4, CA-1, 1-back,  Tetris-1, Guru Train-E   FDS-7, CA-3, 3-back, Tetris-8,  Guru Train-D      4. RESULTS  The cognitive load models made a prediction once every two  seconds. Predicted levels of cognitive load are expected to change  throughout the course of a Guru session rather than being  exclusively in a low or high state. For example, load might be at  high levels for 10 seconds when the tutor introduces a novel  concept, but then return to a baseline level afterwards, before going  back up again when a vaguely familiar concept is introduced. Thus,  we analyze the data by aggregating the models predictions across  periods corresponding to low or high difficulty predictions. A two- tailed significance criteria of .05 is adopted for all analyses.   4.1 EEG Data Validity  Can EEG yield viable data across a 90-min session All 12  participants were able to wear the headset and reported no major  issues during the learning session (an example of data from one  participant is presented in Figure 5). However, simply wearing the  headset does not automatically equate to having valid data  the     electrodes must remain in contact with the student at all times. This  headset (like many others) was originally designed for adults, so it  was unclear if fit would be a major issue for some students. About  67% of the participants had head circumferences outside the  specified range of the adult size headset used in this study, thereby  some sensors did not make good contact on some locations. The  sensor locations most affected were occipital (O1, O2) and  temporal (T3, T4). Nevertheless, data from the parietal and frontal  locations was largely reliable and of high quality.      Figure 5. Example of EEG data collected from a student with   two eye blinks evident on the top left  Students were required to wear the headset for almost 90 minutes,  so it is possible that data quality might decline over time due to  excessive movement, etc. Although we were able to collect EEG  data from all 12 students, there were some unreliable predictions.  For example, a model could predict 0 for an entire session for a  number of reasons, including poor sensor connection or excessive  movement. To account for this, a session was considered invalid  if a model made no reliable predictions2.    Despite some of the practical challenges, the EEG appears to a be  a viable source of data during learning with an ITS, with validity  rates of 91.7% (EO1); 87.5% (EO2); 79.2% (Task1); & 75%  (Task2). Notably, the invalid sessions were not systematically the  first or second session, so there is no concern of data lost due to  session length.   We also assessed the reliability of model outputs across the two  sessions by correlating average cognitive load across the two  sessions. The correlation across the four models was .660, ranging  from .474 (Task2) to .813 (Task1), suggesting that model output  was relatively stable across sessions.    4.2 Subjective Reports   As a manipulation check, we compared the subjective ratings of  difficulty across the easy and difficult versions of CGB Instruction  and Scaffolded Dialogue (see Table 3).   Paired samples t-tests were conducted to compare participants  subjective difficulty ratings, which were averaged across the two  sessions of Guru. There were no significant differences in difficulty  ratings between the easy and difficult sections of CGB Instruction,  though the trend was in the expected direction t(11) = 1.48, p =  .166, d = .260. It is possible that the study was underpowered to  detect this small effect. In contrast, easy and difficult sections of  the Scaffolded Dialogue were significantly different, consistent  with a medium to large effect, t(11) = 3.63, p = .004, d = .716.   Thus, we can conclude that the manipulation was more effective  for Scaffolded Dialogue than CBB Instruction.                                                                        2 All analyses were re-computed using 100% of the data (including   invalid sessions). The pattern of results remained exactly the  same.   Table 3. Descriptive statistics for perceptions of difficulty    Easy Difficult      M (SD) M (SD) d p   Instruction 2.04 (.722) 2.21 (.582) .260 .166   Scaffolding  2.75 (.754) 3.33 (.861) .716 .004   4.3 Model Comparisons  We evaluated the four cognitive load models by comparing them  across the easy and difficult sections of Guru (separately for the  CGB Instruction and Scaffolding sections). A mixed-effects  modeling approach [50] using the lme4 package in R [6] was  adopted for the analyses. This approach is appropriate because  there are multiple observations per student with occasional missing  data. For all models, participant was the random effect. The fixed  effects were: task difficulty (easy vs. difficult), time in the Guru  session (to account for fatigue effects, or electrode contact  degradation) and session number (to account for differences across  the two sessions). The results are shown in Table 4.   Table 4. Descriptive statistics for average model output    Easy Diff Linear Mixed Model  Model M (SD) M (SD) B p 95% CI  EO1         Instruction  .643 (.237) .646 (.264) .013 .110 -.003,.029    Scaffolding .644 (.198) .717 (.187) .055 .000 .040,.070         EO2          Instruction  .749 (.147) .734 (.134) -.021 .791 -.017,.013    Scaffolding .773 (.217) .795 (.162) .011 .080 -.001,.023         Task1          Instruction  .318 (.176) .335 (.281) -.014 .140 -.034,.005    Scaffolding .371 (.269) .469 (.315) .100 .000 .083,.116         Task2          Instruction  .402 (.220) .469 (.285) .027 .012 .006,.049    Scaffolding .457 (.267) .537 (.313) .078 .000 .061,.095   Notes. Descriptives computed at the participant level.  Despite the fact that students did not reliably perceive difficulty  differences in the CGB phase of Guru, one of the models (Task2)  was able to detect a significant difference. This suggests that  students may not have been consciously aware of their increase in  cognitive load imposed by the more complex language. The Task2  model notably differs from Task1 and both EO models by including  the Easy and Difficult Guru training tasks, which most closely  mimic the Instruction phase of Guru. Thus, inclusion of the  domain-specific training tasks may have been crucial to detect the  subtle difficulty manipulation in CBB Instruction.   Conversely, the main effect of difficulty during the Scaffolding  phase was consistent in all four models (albeit marginally     significant in EO2). Indeed, this corroborates the results with  subjective reports of difficulty. There may have been a more  distinguishable effect of difficulty in the Scaffolding phase, where  true/false questions were juxtaposed with open-ended questions.  Even the domain-independent Task1 model was able to pick up on  these differences.   4.4 Scaffolding Performance  We examined students answers to the tutors questions during the  Scaffolding section of Guru. Cognitive load was higher during the  difficult scaffolding questions, and students rated them as being  more difficult. Thus, we might expect performance to be lower on  the difficult compared to easy questions.    Table 5. Linear mixed effects regressions predicting  scaffolding performance from cognitive load and difficulty   ratings.   Independent Variable B p 95% CI   EO1 -.255 .039 -.489, -.021   EO2 -.184 .245 -.487, .118   Task1 -.049 .721 -.034, .024   Task2 -.035 .804 -.311, .240       Difficulty Rating -.118 .000 -.171, -.061      Easy (true/false) and Difficult (open-ended) questions were scored  for correctness (0 = incorrect; 1 = correct). Proportion of correct  answers was then computed for easy and difficult sections of  Scaffolded Dialogue. After averaging across the two sessions, a  paired-samples t-test revealed that students performed worse on the  difficult questions (M = .589; SD = .121) compared to the easy ones  (M = .771; SD = .152), consistent with a large effect size, t(11) =  4.89, p = .000, d = 1.33).    We conducted linear mixed effects regressions to assess if predicted  cognitive load during Scaffolded Dialogue related to performance  on the questions (see Table 5). Each participant contributed four  data points: one for each easy and difficult section, across two  topics. The averaged model output in each section was used as the  independent variable and the proportion of correct answers in  Scaffolded Dialogue was the dependent variable.       In all four models, predicted cognitive load was negatively related  to performance on the Scaffolded Dialogue questions, but only the  EO1 model was significant. The same mixed-effects approach  revealed that participants subjective difficulty ratings for a  Scaffolded Dialogue section were negatively related to  performance.    4.5 Examples of Model Output  Example output from the Task1 model on a single participants  sessions is shown in Figure 6.  Tutorial modes and their difficulty  are indicated by colored blocks (see legend), and white blocks  indicate times during self-assessment questions.  The order of  difficulty is reversed in these two sessions (i.e. top: DE, bottom:  ED). These two sessions clearly show that the workload model  varies throughout the tutorial and between modes. From  observations, there appears to be non-trivial activity during each of  the tutorial blocks. For example, the workload model shows sharp  changes at the onset and end of tutorial modes, as seen for example  in Figure 5 (top) at the onset of difficult instruction (128 sec) or at  the beginning of the post-test (1366 sec).    5. DISCUSSION  While some advanced learning technologies use generalized  learner models to make inferences about student engagement and  affect, they do so with peripheral rather than central measures of  thought and feeling [4, 19, 32]. We take an initial step towards the  goal using brain-based assessments of mental states to optimize  instruct for individual students by assessing whether EEG-based  estimation of students cognitive load is sensitive to experimentally  manipulated easy and difficult sections of an ITS. Our main  findings are summarized below, followed by a discussion of  applications, limitations, and future work.   5.1 Main Findings  First, we show that it is feasible to collect EEG data while students  learn from an ITS, even for sessions spanning an hour and half.  Further, we used a hat-like headset that sits comfortably on  students heads with 24 dry electrodes. Using dry electrodes is  much more feasible for collecting EEG in ecological contexts  compared to more laborious set up procedures used previously [22,  45, 48]. Additionally, students were relatively unconstrained and  could move freely in their chair. Despite the potential problems that  could occur (e.g., accidental electrode detachment, headset shifts,  hardware issues, etc.), we were able to collect an average of 83%  usable data across the two Guru sessions.    Figure 6. Cognitive load predictions for a single student over the course of two Guru sessions     We also found that manipulated difficulty had an effect on  predicted levels of cognitive load in both the CGB Instruction  (Task2 model) and Scaffolding (all models) phases of Guru.  Moreover, the experimental manipulations mimicked real-life  instructional differences in difficulty levels. For example, ITSs  have the capability to use different levels of complexity in  language, as well as employ a variety of dynamic Scaffolding  techniques. Therefore, this study represents a relatively authentic  manipulation of difficulty during a complex learning session.    We found that our cognitive load detectors were more highly  attuned to the cognitive load differences during the Scaffolding  phase (true/false vs. open-ended questions). Unlike the CGB  Instruction phase, students also perceived that the open-ended  questions were more difficult. Although the domain-independent  Task1 was sensitive to manipulations of difficulty in the  Scaffolding phase, it failed to do so in the CGB Instruction phase.  The only model to successfully pick up on cognitive load  differences (Task2) was trained on the easy and difficult Guru  Training tasks. Thus, task-specific training data may be needed to  detect the subtle differences in cognitive load, especially when  humans are not as metacognitively aware of these differences.   Finally, we have shown that output from the cognitive load models  might be negatively related to performance on the Scaffolding  questions. Although the effect was only significant in one out of  four models (EO1), a similar negative relationship was found  between subjective ratings of difficulty and performance. Taken  together, these findings show that EEG signal holds promise as an  online indicator of students cognitive load and can be used as an  input modality to tailor instruction in a manner that is sensitive to  load.   5.2 Limitations  There were several limitations of this work. We only collected data  from a small number of 9th grade high school biology students (N  = 12). Testing a larger, more diverse sample would allow for a  better evaluation of the cognitive load models. Another limitation  is that despite being conducted in a school, the study setup was  more similar to a laboratory environment than to a typical  classroom setting. Thus, future work should focus on implementing  this type of data collection in a more ecological setting, like in the  classroom. Finally, we only focused on performance during the  Scaffolded Dialogue rather than on performance measured after the  learning session. We did this to provide initial evidence that  predicted cognitive load is sensitive to changes in real-time task  performance, so that instructional strategies may eventually be  individually tailored prior to the completion of the tutoring session.  However, an important next step is to develop and test specific  predictions about how prior knowledge and fluctuations in  cognitive load throughout a learning session relate to learning gains  measured via a posttest.   5.3 Future Work and Potential Applications  The ultimate goal of this work is to develop a system that can detect  and dynamically respond to students cognitive load with  appropriate instructional materials. The present work needs to be  extended in many ways to meet this goal. First, we manipulated two  levels of difficulty in two sections of Guru. However, cognitive  load may have a curvilinear relation with performance [49], where  students learn the most at a moderate level of load. Future work  should investigate additional levels of difficulty so as to more  precisely map out the zone of optimal load.   Our manipulations also increased difficulty in only two ways:  changing the complexity of language or changing the question   format. Future work needs to address whether the detectors are  sensitive to manipulations that induce cognitive load through other  means, such as complexity of the materials (intrinsic load) or  seductive details (extraneous load).   This work might also be extended by considering the temporal  dynamics of cognitive load and how it relates to difficulty. We  adopted a relatively course grained approach in analyzing the data,  by averaging across easy and difficult sections. It would be valuable  to take a more fine-grained approach that focuses on second-by- second changes in detected load. Another extension would be to  employ a multisensor, multimodal approach to modeling students  cognitive states. For example, low cost eye-trackers have been used  to model attention [32] could be combined with EEG-based  cognitive load models. This might be helpful in determining  whether low cognitive load is due to decoupling from the learning  task because the student has zoned out, or due to some other factor.   Finally, future work could also focus on improving EEG  methodology. We used 24 dry electrodes. However, it might be  possible to detect cognitive load from fewer electrodes (seven  should be sufficient [38, 41]), making the head-set even more  scalable in the future. The head-set could also be tailored to fit  heads with a smaller circumference.   6. CONCLUSION  Leveraging advances in intelligent learning environments,  neuroscience, and in wearable EEG sensing, we provide initial  evidence that EEG may be a viable method to track cognitive states  during learning, thereby providing unique possibilities of closing  the loop between the learning technology and the learner.   7. ACKNOWLEDGMENTS  This research was supported by the National Science Foundation  (NSF) (IIP 1416595; DRL 1108845; IIS 1523091). Any opinions,  findings and conclusions, or recommendations expressed in this  paper are those of the authors and do not necessarily reflect the  views of the NSF.   8. REFERENCES   Anderson, E. W., Potter, K. C., Matzen, L. E., Shepherd, J.   F., Preston, G. A., & Silva, C. T. 2011. A user study of  visualization effectiveness using EEG and cognitive load.  Computer Graphics Forum, 30(3), 791800.    Antonenko, P., Paas, F., Grabner, R., & van Gog, T. 2010.  Using electroencephalography to measure cognitive load.  Educational Psychology Review, 22(4) 425438.    Baker, R. S., Corbett, A., & Aleven, V. 2008. More Accurate  Student Modeling through Contextual Estimation of Slip and  Guess Probabilities in Bayesian Knowledge Tracing. In  Proceedings of the 9th International Conference on  Intelligent Tutoring Systems. Berlin, Heidelberg: Springer,  406-415.    Baker, R. S. 2007. Modeling and understanding students  off-task behavior in intelligent tutoring systems. In  Proceedings of the SIGCHI Conference on Human Factors  in Computing Systems. New York, NY: ACM, 10591068.    Basile, L. F. H., Anghinah, R., Ribeiro, P., Ramos, R. T.,  Piedade, R., Ballester, G., & Brunetti, E. P. 2007.  Interindividual variability in EEG correlates of attention and  limits of functional mapping. International Journal of  Psychophysiology, 65(3) 238251.       Bates, D., Maechler, M., & Bolker, B., Walker, S. 2015.  Fitting Linear Mixed-Effects Models using lme4. Joural of  Statistical Software, 67(1), 1-48.    Berka, C., Levendowski, D. J., Lumicao, M. N., Yau, A.,  Davis, G., Zivkovic, V. T.,  Craven, P. L. 2007. EEG  Correlates of Task Engagement and Mental Workload in  Vigilance, Learning, and Memory Tasks. Aviation, Space,  and Environmental Medicine, 78(5) B231B244.    Bixler, R., & DMello, S. 2014. Toward Fully Automated  Person-Independent Detection of Mind Wandering. In  Proceedings of the 22nd International Conference on User  Modeling, Adaptation, and Personalization. Cham,  Switzerland: Springer International, 3748.    Bixler, R., & DMello, S. K. 2013. Detecting Boredom and  Engagement During Writing with Keystroke Analysis, Task  Appraisals, and Stable Traits. In Proceedings of the 2013  International Conference on Intelligent User Interfaces. New  York, NY:ACM, 225234.    Chang, K., Nelson, J., Pant, U., & Mostow, J. 2013. Toward  Exploiting EEG Input in a Reading Tutor. International  Journal of Artificial Intelligence in Education, 22(12), 19 38.    Chaouachi, M., & Frasson, C. 2010. Exploring the  Relationship between Learner EEG Mental Engagement and  Affect. In Proceedings of the 10th International Conference  on Intelligent Tutoring Systems. Berlin, Heidelberg:  Springer, 291293.    Chaouachi, M., & Frasson, C. 2012. Mental Workload,  Engagement and Emotions: An Exploratory Study for  Intelligent Tutoring Systems. In Proceedings of 11th  International Conference on Intelligent Tutoring Systems.  Berlin, Heidelberg: Springer, 6571.    Chaouachi, M., Jraidi, I., & Frasson, C. 2011. Modeling  mental workload using EEG features for intelligent systems.  In Proceedings of the International Conference on User  Modeling, Adaptation, and Personalization. Berlin,  Heidelberg: Springer, 5061.    Cirett Galn, F., & Beal, C. R. 2012. EEG Estimates of  Engagement and Cognitive Workload Predict Math Problem  Solving Outcomes. In Proceedings of the 20th International  Conference on User Modeling, Adaptation, and  Personalization. Berlin, Heidelberg: Springer, 5162.    DeFalco, J., Baker, R. S., & DMello, S. K. 2014.  Addressing Behavioral Disengagement in Online Learning.  In R. Sottilare, A. C. Graesser, X. Hu, & B. Goldberg (Eds.),  Design Recommendations for Intelligent Tutoring Systems  (Vol. 2). Orlando, FL: U.S. Army Research Laboratory, 49 56.    Derbali, L., & Frasson, C. 2010. Players Motivation and  EEG Waves Patterns in a Serious Game Environment. In  Proceeding of the 10th International Conference on  Intelligent Tutoring Systems. Berlin, Heidelberg: Springer,  297299.    DMello, S.K., Blanchard, N., Baker, R. S., Ocumpaugh, J.,  & Brawner, K. 2014. I Feel Your Pain: A Selective Review  of Affect-Sensitive Instructional Strategies. In R. Sottilare,  A. C. Graesser, X. Hu, & B. Goldberg (Eds.), Design  Recommendations for Intelligent Tutoring Systems (Vol. 2).  Orlando, FL: U.S. Army Research Laboratory, 35-48.    DMello, S. K., Chipman, P., & Graesser, A. C. 2007.  Posture as a predictor of learners affective engagement. In  Proceedings of the 29th Annual Meeting of the Cognitive  Science Society. Red Hook, NY: Curran Associates Inc.,  905910.    DMello, S. K., & Graesser, A. C. 2013. AutoTutor and  affective AutoTutor: Learning by talking with cognitively  and emotionally intelligent computers that talk back. ACM  Transactions on Interactive Intelligent Systems (TiiS), 2(4),  139.    DMello, S. K., Olney, A., & Person, N. 2010. Mining  collaborative patterns in tutorial dialogues. Journal of  Educational Data Mining, 2(1), 237.    Estepp, J. R., Christensen, J. C., Monnin, J. W., Davis, I. M.,  & Wilson, G. F. 2009. Validation of a dry electrode system  for EEG. In Proceedings of the Human Factors and  Ergonomics Society Annual Meeting. New York, NY: SAGE  Publications, 11711175.     Ferree, T. C., Luu, P., Russell, G. S., & Tucker, D. M. 2001.  Scalp electrode impedance, infection risk, and EEG data  quality. Clinical Neurophysiology, 112(3), 536544.    Gerjets, P., Walter, C., Rosenstiel, W., Bogdan, M., &  Zander, T. O. 2014. Cognitive state monitoring and the  design of adaptive instruction in digital environments:  lessons learned from cognitive workload assessment using a  passive brain-computer interface approach. Frontiers in  Neuroscience, 8(385), 2041.    Gevins, A., Smith, M. E., Leong, H., McEvoy, L., Whitfield,  S., Du, R., & Rush, G. 1998. Monitoring working memory  load during computer-based tasks with EEG pattern  recognition methods. Human Factors: The Journal of the  Human Factors and Ergonomics Society, 40(1), 7991.    Graesser, A. C., DMello, S. K., Craig, S. D., Witherspoon,  A., Sullins, J., McDaniel, B., & Gholson, B. 2008. The  relationship between affective states and dialog patterns  during interactions with AutoTutor. Journal of Interactive  Learning Research, 19(2), 293312.    Graesser, A. C., & McNamara, D. S. 2011. Computational  Analyses of Multilevel Discourse Comprehension. Topics in  Cognitive Science, 3(2), 371398.     Graesser, A. C., McNamara, D. S., & Kulikowich, J. M.  2011. Coh-Metrix Providing Multilevel Analyses of Text  Characteristics. Educational Researcher, 40(5), 223234.    Graesser, A. C., McNamara, D. S., Louwerse, M. M., & Cai,  Z. 2004. Coh-Metrix: Analysis of text on cohesion and  language. Behavior Research Methods, Instruments, &  Computers, 36(2), 193202.    Heraz, A., & Frasson, C. 2007. Predicting the three major  dimensions of the learners emotions from brainwaves.  International Journal of Computer Science, 2(3), 187193.    Heraz, A., & Frasson, C. 2009. Predicting Learner Answers  Correctness through Brainwaves Assesment and Emotional  Dimensions. In Proceedings of the 14th International  Conference on Artificial Intelligence in Education.  Amsterdam, Netherlands: IOS Press, 4956.    Hussain, M. S., AlZoubi, O., Calvo, R. A., & DMello, S. K.  2011. Affect Detection from Multichannel Physiology during  Learning Sessions with AutoTutor. In Proceedings of the     15th International Conference on Artificial Intelligence in  Education. Berlin, Heidelberg: Springer, 131138.    Hutt, S., Mills, C., White, S., Donnelly, P. J., & DMello, S.  K. 2016. The Eyes Have It: Gaze-based Detection of Mind  Wandering during Learning with an Intelligent Tutoring  System. In Proceedings of the 9th International Conference  on Educational Data Mining, US: International Educational  Data Mining Society, 8693.     Jong, T. de. 2009. Cognitive load theory, educational  research, and instructional design: some food for thought.  Instructional Science, 38(2), 105134.     Kalyuga, S. 2011. Cognitive Load Theory: How Many Types  of Load Does It Really Need Educational Psychology  Review, 23(1), 119.    Kincaid, J. P., Fishburne Jr, R. P., Rogers, R. L., & Chissom,  B. S. 1975. Derivation of new readability formulas  (automated readability index, fog count and flesch reading  ease formula) for navy enlisted personnel (No. RBR-8-75).  Millington, TN: Naval Technical Training Command.    Kohlmorgen, J., Dornhege, G., Braun, M., Blankertz, B.,  Mller, K.-R., Curio, G.,  Kinces, W. 2007. Improving  human performance in a real operating environment through  real-time mental workload detection. In G. Dorhage et al.  (Eds.), Toward Brain-Computer Interfacing, Cambridge,  MA: MIT Press, 409422.    Matthews, R., McDonald, N. J., Anumula, H., Woodward, J.,  Turner, P. J., Steindorf, M. A.,  Pendleton, J. M. 2007.  Novel hybrid bioelectrodes for ambulatory zero-prep EEG  measurements using multi-channel wireless EEG system. In  Proceedings of the 9th International Conference on  Foundations of Augmented Cognition. Berlin, Heidelberg:  Springer, 137146.    Matthews, R., McDonald, N. J., Fridman, I., Hervieux, P., &  Nielsen, T. 2005. The invisible electrodezero prep time,  ultra low capacitive sensing. In Proceedings of the 11th  International Conference on Human-Computer Interaction.  Hillsdale, NJ: Lawrence Erlbaum Associates, 22-27.    Matthews, R., Turner, P. J., McDonald, N. J., Ermolaev, K.,  Mc Manus, T., Shelby, R. A., & Steindorf, M. 2008. Real  time workload classification from an ambulatory wireless  EEG system using hybrid EEG electrodes. In Proceedings of  the 2008 Annual International Conference of the IEEE  Engineering in Medicine and Biology Society. US: IEEE,  58715875.    Mazzoni, G., & Cornoldi, C. 1993. Strategies in study time  allocation: Why is study time sometimes not effective  Journal of Experimental Psychology: General, 122(1), 47.    McDonald, N. J., & Soussou, W. 2011. Quasars qstates  cognitive gauge performance in the cognitive state  assessment competition 2011. In Proceedings of the 2011  Annual International Conference of the IEEE Engineering in  Medicine and Biology Society. US: IEEE, 65426546.    Mostow, J., Chang, K., & Nelson, J. 2011. Toward  Exploiting EEG Input in a Reading Tutor. In Proceedings of  the 15th International Conference on Artificial Intelligence  in Education. Berlin, Heidelberg: Springer, 230237.    Muldner, K., & Burleson, W. 2015. Utilizing sensor data to  model students creativity in a digital environment.  Computers in Human Behavior, 42, 127137.    Nelson, T. O. 1993. Judgments of learning and the allocation  of study time. Journal of Experimental Psychology. General,  122(2), 269273.    Nunez, P. L., Silberstein, R. B., Shi, Z., Carpenter, M. R.,  Srinivasan, R., Tucker, D. M.,  Wijesinghe, R. S. 1999.  EEG coherency II: experimental comparisons of multiple  measures. Clinical Neurophysiology, 110(3), 469486.    Olney, A. M., DMello, S., Person, N., Cade, W., Hays, P.,  Williams, C.,  Graesser, A. 2012. Guru: A Computer Tutor  That Models Expert Human Tutors. In Proceedings of the  11th International Conference Intelligent Tutoring Systems.  Berlin, Heidelberg: Springer, 256261.    Olney, A., Person, N. K., & Graesser, A. C. 2012. Guru:  Designing a conversational expert intelligent tutoring system.  In C. Boonthum-Denecke, P. McCarthy, & T. Lamkin (Eds.),  Cross-Disciplinary Advances in Applied Natural Language  Processing: Issues and Approaches. Hershey, PA, USA: IGI  Global, 156171.    OSullivan, J. A., Power, A. J., Mesgarani, N., Rajaram, S.,  Foxe, J. J., Shinn-Cunningham, B. G.,  Lalor, E. C. 2015.  Attentional selection in a cocktail party environment can be  decoded from single-trial EEG. Cerebral Cortex, 25(7),  16971706.    Paas, F., Renkl, A., & Sweller, J. 2003. Cognitive load  theory and instructional design: Recent developments.  Educational Psychologist, 38(1), 14.    Pinheiro, J. C., & Bates, D. M. (Eds.) 2000. Mixed effects  models in S and S-PLUS. Berlin, Heidelberg: Springer  Verlag.    Slater, J. D., Kalamangalam, G. P., & Hope, O. 2012.  Quality assessment of electroencephalography obtained from  a dry electrode system. Journal of Neuroscience Methods,  208(2), 134137.    Stevens, R., Galloway, T., & Berka, C. 2006. Integrating  EEG models of cognitive load with machine learning models  of scientific problem solving. In D. Schmorrow, K. Stanney,  L. Reeves (Eds.), Augmented Cognition: Past, Present and  Future. (Vol. 2). Arlington, VA: Strategic Analysis, Inc., 55 65.    Sun, J. C.-Y., & Yeh, K. P.-C. 2017. The effects of attention  monitoring with EEG biofeedback on university students  attention and self-efficacy: The case of anti-phishing  instructional materials. Computers & Education, 106, 7382.    Sweller, J., Ayres, P., & Kalyuga, S. (Eds.) 2011. Cognitive  load theory (Vol. 55). New York, NY: Springer, 3776.    VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P.,  Olney, A., & Ros, C. P. 2007. When Are Tutorial Dialogues  More Effective Than Reading Cognitive Science, 31(1), 3 62.     VanLehn, K., Jordan, P. W., Ros, C. P.,  Srivastava, R.  2002. The Architecture of Why2-Atlas: A Coach for  Qualitative Physics Essay Writing. In Proceedings of the 6th  International Conference on Intelligent Tutoring Systems.  Berlin, Heidelberg: Springer, 158167.       "}
{"index":{"_id":"12"}}
{"datatype":"inproceedings","key":"Martinez-Maldonado:2017:AMP:3027385.3027401","author":"Martinez-Maldonado, Roberto and Power, Tamara and Hayes, Carolyn and Abdiprano, Adrian and Vo, Tony and Axisa, Carmen and Buckingham Shum, Simon","title":"Analytics Meet Patient Manikins: Challenges in an Authentic Small-group Healthcare Simulation Classroom","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"90--94","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027401","doi":"10.1145/3027385.3027401","acmid":"3027401","publisher":"ACM","address":"New York, NY, USA","keywords":"awareness, classroom, face-to-face, groupwork, multimodal","Abstract":"Healthcare simulations are hands-on learning experiences aimed at allowing students to practice essential skills that they may need when working with real patients in clinical workplaces. Some clinical classrooms are equipped with patient manikins that can respond to actions or that can be programmed to deteriorate over time. Students can perform assessments and interventions, and enhance their critical thinking and communication skills. There is an opportunity to exploit the students' digital traces that these manikins can pervasively capture to make key aspects of the learning process visible. The setting can be augmented with sensors to capture traces of group interaction. These multimodal data can be used to generate visualisations or feedback for students or teachers. This paper reports on an authentic classroom study using analytics to integrate multimodal data of students' interactions with the manikins and their peers in simulation scenarios. We report on the challenges encountered in deploying such analytics 'in the wild', using an analysis framework that considers the social, epistemic and physical dimensions of collocated group activity.","pdf":"Analytics Meet Patient Manikins: Challenges in an Authentic  Small-Group Healthcare Simulation Classroom   Roberto Martinez-Maldonado1  Roberto@MartinezMaldonado.net   Tamara Power2  Tamara.Power@uts.edu.au   Carolyn Hayes2   Carolyn.Hayes@uts.edu.au   Adrian Abdiprano2  Adrian.Abdiprano@uts.edu.au   Tony Vo2   Huu.Vo@uts.edu.au   Carmen Axisa2  Carmen.Axisa@uts.edu.au   Simon Buckingham Shum1  Simon.BuckinghamShum@uts.edu.au   1Connected Intelligence Centre, 2Faculty of Health, University of Technology Sydney, Australia   ABSTRACT  Healthcare simulations are hands-on learning experiences aimed at  allowing students to practice essential skills that they may need  when working with real patients in clinical workplaces. Some  clinical classrooms are equipped with patient manikins that can  respond to actions or that can be programmed to deteriorate over  time. Students can perform assessments and interventions, and  enhance their critical thinking and communication skills. There is  an opportunity to exploit the students digital traces that these  manikins can pervasively capture to make key aspects of the  learning process visible. The setting can be augmented with sensors  to capture traces of group interaction. These multimodal data can  be used to generate visualisations or feedback for students or  teachers. This paper reports on an authentic classroom study using  analytics to integrate multimodal data of students interactions with  the manikins and their peers in simulation scenarios. We report on  the challenges encountered in deploying such analytics in the  wild, using an analysis framework that considers the social,  epistemic and physical dimensions of collocated group activity.    CCS Concepts    Information systems  Information systems applications    Collaborative and social computing systems and tools    Keywords  Classroom, groupwork, multimodal, awareness, face-to-face   1. INTRODUCTION   Healthcare clinical simulations are practical learning experiences  designed to expose students to a comprehensive range of complex   or typical scenarios that they may encounter in their future  workplaces or professional situations [6]. Some didactic methods,  such as Problem-Based Learning [1], have been effectively applied  in healthcare education in the form of tasks where students engage  in complex medical scenarios, as well as simulations using  manikins or actors as patients. Simulation technology is often used  to assist healthcare training, ranging from very simple (e.g. using  an orange to practice giving injections) to more sophisticated  computer-based systems (e.g. fully simulated high fidelity  manikins or even completely digital patients [16]). Cooper and  Taqueti [5] have outlined different types of healthcare simulation  technologies. For example, high fidelity patient manikins are often  used to simulate medical scenarios in an environment which is safe  for both learners and patients.    Simulations in health care are not associated with specific  technologies. Rather they are learning techniques that substitute or  magnify real experiences with guided experiences that mimic or  reproduce key aspects of real world situations [8]. For example,  students can practice in order to improve: their communication with  the patient and the healthcare team; their expertise in using specific  medical equipment; or their procedural knowledge and technique,  or protocols to be followed under certain circumstances, etc. In this  way, simulations can equip students with essential practical skills  that may be called upon during clinical placements in healthcare  workplaces. Moreover, simulation through scenarios can be an  effective alternative for the achievement of learning outcomes that  are typically met using a traditional lecture format [10].    From the beginning of their degree, healthcare simulations are  integrated into each semester of the Bachelors of Nursing and  Midwifery at the University of Technology Sydney (UTS). During  class, students commonly work in small teams undertaking specific  roles and responding to hypothetical scenarios. Manikins ranging  from newborn to adult (e.g. see Figure 1) give students the  opportunity to practise skills before implementing them in real life  situations. One of the main challenges that teachers and students  face in these scenarios is the very limited awareness of the multiple  co-occurring learning processes in each small group, and the  limited feedback that the teacher can give to the (20 to 30) students  in the classroom. These are common problems that emerge in  several classroom scenarios [15]. As a result, the learning  experience depends on the quality of reflection in which students      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than the author(s) must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to post  on servers or to redistribute to lists, requires prior specific permission and/or  a fee. Request permissions from Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027401                    Figure 1. The healthcare simulation classroom. Left: small group of learners performing a role-based simulation task.    Center: the classroom setting. Right: a depth sensor tracking learners position around a patient manikin in the classroom.        can engage about their own actions and those of others in their  teams. Typically, however, little evidence is gathered that could  support in-class debriefing and reflection, besides video recordings  of the simulation sessions or annotations.    Our research sits in this space, where clinical simulations and  learning analytics can be operationalised together to provide better  feedback to students, and new insights for teachers. There is a  significant opportunity to use multimodal sensors and analytics to  make the normally ephemeral embodied activity in simulations  visible, replayable and referenceable in reflections and analyses by  students and teachers seeking to improve their practice, and (as in  this paper) by researchers. Ultimately, the goal is of course  intensely practical: to make such practices more readily  improvable. This could be accomplished by enriching simulations  with multimodal sensors in order to capture traces of the face-to- face interactions [3]. Possible ways to exploit these multimodal  data can include the provision of visualisations, notifications or  feedback to students or the teachers, during or after the simulation  activity (e.g. [14]). The analysis of students data can also involve  modelling and analytics techniques including the analysis of  behavioural patterns (e.g. interaction of students with the patient);  task performance; students sequential processes (e.g. [12]);  communication among students and roles (e.g. [13]); conversation  patterns (e.g. [2]); space usage (e.g. [16]) and gestures.   This paper presents a synthesis of conclusions drawn from an  authentic classroom study aimed at exploring the mechanisms to  collect and integrate multimodal data about students interactions  with their peers and the manikins to complete simulation scenarios.  The contributions of the paper are firstly, a discussion of the  challenges encountered in this initial attempt to bring learning  analytics into a real health simulation classroom (in contrast to an  experimental laboratory). Secondly, we present illustrative,  preliminary examples of the analytics that such sensors make  possible. We offer these findings to guide other researchers and  developers seeking to provide support for enhanced awareness,  provision of feedback or evidence-based reflection in face-to-face  (f2f) classroom scenarios.    The rest of the paper is structured as follows. The next section  presents an overview of related work in the area of learning  analytics applied to classroom and face-to-face settings. Section 3  provides details about our healthcare simulation classroom study.   Section 4 presents a synthesis of conclusions drawn from the study  and illustrative analytics examples.  The paper concludes in Section  5 with a discussion of lessons learnt and future work.    2. RELATED WORK  The study presented in this paper is relevant for both learning  analytics efforts on tackling multimodal interaction (e.g.  employing video, audio and touch sensors to examine learning in a  realistic, mixed-media learning environment) and those focused on  understanding collocated learning environments (where learning  occurs in a physical environment, such as the classroom, where  students not only interact through learning systems but also face- to-face). Recent reviews of the state of the art of multimodal  analytics and learning analytics in collocated environments can be  found in [3] and [14]  respectively. Both have mostly reported work  conducted in controlled, experimental conditions, thus much work  is still needed to bring learning analytics into authentic classrooms.  Our work addresses this gap by reporting on the challenges  encountered and the potential of utilising mobility, audio and log  data towards providing support in a clinical simulation classroom,  a domain that has not yet been explored by learning analytics   initiatives. Our work is inspired by similar LA works conducted  using interactive tabletops and sensors in the classroom to generate  dashboards for the teacher [14] and produce teaching analytics [15].    In the domain of clinical simulation, tools based on video playback  have been developed to aid in debriefing (e.g. [9]) or self-reflection  (e.g. [4]) after the simulation. Although this can be an effective  means of feedback and reflection [7], lengthy or unrelated video  segments can diminish the discussion of key aspects of the  simulation. As a result, video playback is often not suitable for  being used in a classroom where time is commonly limited. To our  knowledge, this work is the first attempt to integrate data from  simulation experiences, with a number of team activity data  streams. Bringing learning analytics into this design space may  provide alternative ways to support debriefing, generate the means  for providing automated feedback during or after the simulation, or  to find patterns indicative of collaboration profiles or student  strategies.   3. THE STUDY   3.1 The Learning Situation  This study was run in authentic laboratory classes taught in  semester 2, 2016 as part of the undergraduate unit: Integrated  Nursing Practice. This is a final-year subject in the Bachelor of  Nursing at the University of Technology Sydney. The main  objective of this unit is to support students to develop clinical  practice skills and prepare them for entry to the nursing workforce.  There is a strong f2f component for the unit, including 3-hour  weekly laboratory classes where students commonly face simulated  scenarios involving chronic and complex conditions. In total, 580  students attended these classes. Each had from 20 to 27 students.  The study focused on the classes conducted in Week 3 of the  semester. The learning design included a series of tasks to be  completed by small teams of students using the patient manikins.  Students had to collaborate and put into practice their theoretical  knowledge and nursing skills to care for a patient in a clinical  scenario. The tasks included: assessment of chest pain symptoms,  administration of medication, management of adverse drug reaction  and conducting an electrocardiogram (ECG) analysis. Additionally,  each student was asked to play one of 4 possible roles: a Team  leader, the Patient, Nurses, and an Observer. The teacher assumed  the Doctor role.   3.2 Apparatus and Multimodal Data Sources  In this study, we focused on 5 randomly selected laboratory classes.  Only the activity occurring in two of the available five simulated  hospital beds was recorded in each session to allow students to opt  out from the data recording. A total of 56 students and 4 different  teachers were in the observed sessions. The laboratory classrooms  are equipped with 5 medium fidelity manikins in bed spaces (see  Figure 1-centre). These manikins generate physiological data and  can be programmed to improve or deteriorate over time in response  to nursing actions.    See Figure 2 for an overview of the bed spaces enhanced with  sensors, which captured the following data. Some of the physical  actions performed on the manikin were automatically logged (e.g.  checking pulse, blood pressure, etc). The SimPad tablet allowed  students to log the completion of required tasks pre-defined by the  teacher to form a checklist. Students location data was captured  automatically using a Kinect depth sensor (e.g. see the depth image  recording overlapped with a snapshot of students activity in Figure  1-right). Directional audio was captured using the microphone  array (Dev-Audio-Microcone). This captured 6 channels of audio     input around it. Video of the sessions was also captured using  cameras and microphones built into the ceiling. Thus, the types of  data involved in this case were mobility, interaction, self-reported  activity and communication data.    4. THEMATIC ANALYSIS  In order to scaffold the analysis of the data and explore the potential  and challenges in terms of learning analytics, we used the Activity- Centred Analysis & Design (ACAD) framework [13]. This  framework provides a structure for applying different analytical  tools according to four closely related dimensions of user activity:  the social, the setting, the epistemic and the runtime dimensions. In  the following subsections, we present examples of the potential for  learning analytics and the classroom challenges encountered in this  initial attempt. We additionally discuss emerging issues in terms of  data management, ethics and privacy.    4.1 The social dimension   Social formations and mobility. The potential: a feasible source of  students behavioural data is the tracked position of the students  around the manikin, which can provide information about how  group members approach the tasks, the processes they follow  before performing actions on the manikin and behaviour according  to learners roles. For example, Figure 3 shows the mobility  information of two groups in the same session represented as  heatmaps of location data captured by the depth sensor. These show  two very distinct approaches to the task. Group A stayed mostly  away from the patient during the first half of the  task (see red ovals in quarters Q1 and Q2) to then  work near the patient only during the third quarter  of the activity (see blue ovals in Q3). Then, they  finished earlier than other groups (no data in Q4).  By contrast, Group B followed a very different  approach by engaging with the patient from the  beginning of the task and maintaining proximity  throughout (see blue ovals in Q1-4). This is a  preliminary example of how proximity and  mobility data, when visualised in intuitive ways,  could provoke productive reflection on the  different strategies followed by students. It is a  pedagogical decision exactly how and for what  purposes one would deploy such visualisations  with students.   Challenges: a number of challenges emerged in  the classroom. For example, solutions to  identifying people around the bed (see Figure 4-  left, where the teacher is interacting with the manikin but for the  depth sensor is just another student), or their roles, are still required  to provide continued tracking even though students move away  from the depth sensor range. Also, since the depth sensor uses a  computer vision algorithm, problems related with occlusion (e.g.  students occluding each other or being too close to the sensor) and  changes in the setting (e.g. students closing the curtains around the  manikin to perform procedures that required privacy, see Figure 4- right, tracked in blue) may arise. The potential value of these  heatmaps of mobility for assessing students engagement with the  manikin and patient centred care is a current key area of work in  the host university. Still, more work needs to be done to create  robust and unobtrusive solutions to improve the automated  generation of this kind of data.     Audio tracking. The potential: conversation patterns within a group  can be crucial indicators of effective groups strategies, processes  or performance [2]. By using the mic array, we aimed to track when  each group member spoke to detect levels of conversation which  could provide insights at a group level or reveal some of the  collaboration story. For example, Figure 5 illustrates the potential  for learning analytics by integrating audio and mobility data for the  overall activity of Groups A and B. The figure shows the total  effective time of conversation as detected by the 6-channels of the  mic array. It can be observed, for example, that most of the talking  for Group A occurred while students were away from the manikin  (see left and central sectors of the microphone indicating 12, ~13  and ~6 minutes of talking corresponding to the clusters of heatmap  activity further from the patient). By contrast, the talking by Group  B mostly occurred on both sides of the manikin (see how the high  heatmap mobility data corresponds to the sectors of the mic array  that detected more talking: ~17 and ~14 minutes).    Challenges: even though these visualisations can be generated  completely automatically, the data used to generate them may have  included some general classroom noise and voices coming from  other beds. Isolating students voices in the classroom is  challenging, since it is a very unpredictable and dynamic  environment. Students move and talk at varied voice levels and  there are also unexpected noises such as those generated by the  manikins or medical equipment. Although analytics of student  formations, mobility and conversation patterns can effectively  inform collocated collaboration processes, there are still a number  of challenges to be addressed to get clean data from a laboratory  classroom environment.     Figure 3. Heatmaps of mobility data during students activity (1 hour) divided in   quarters for two groups of the same classroom session: A and B. Coloured ovals   mark clusters of activity near (blue) and further (red) from the patient      Figure 2. Simulation manikin enhanced with sensors                  Figure 5. Tracking directional audio (blue areas) and   mobility data (heatmap) for groups A and B   4.2 The setting dimension  The potential: although blended learning models are widespread,  learning analytics have until now focused on the online settings in  which interaction is mediated by computers, since the data is so  easily logged. The f2f setting providing the focus for this paper  seeks to ensure that the co-located elements are no longer invisible,  but could provide automated feedback in the classroom to improve  awareness and reflection in blended-learning settings.    Challenges: unforeseen events can occur in co-located settings.  Commonly, the teacher adapts the task, social aspects and the  setting on-the-fly or in design time to accommodate for these  events. However, these decisions, if not considered in the design,  may affect the quality of the learning analytics outputs or even limit  the types of analytics that can be delivered. In the school hosting  our study, for example, although some manikins are designed for  students to be able to administer injections and intravenous (IV)  fluids (which can generate logged events), other mid-fidelity  manikins do not provide such functionality (thus not generating  logs). External instruments (such as external IV or ECG devices)  are also sometimes used by the teaching team to enhance students  experience or augment some manikins lacking of certain  functionalities. However, they are generally not connected to the  manikin system and/or do not record activity logs. Thus, there can   be a trade-off between the setting needs for classroom orchestration  and learning; and what is feasible to be achieved through learning  analytics innovations.    4.3 Epistemic dimension (the task)   The potential: the possibility of generating visual analytics that  describe the different steps and key milestones of the collaborative  learning process within a group of students can provide an effective  area for discussion, reflection or the provision of more informed  feedback. For example, Figure 6 (see coloured pins on the blue  timeline) shows the logs obtained from two groups in two different  sessions. These correspond to three key actions which are important  for this specific simulation case: 1) when the first set of vital signs  of the patient was obtained (red pins), 2) when a prescription  medication was administered (blue pins) and 3) when an ECG test  was performed after the patient deteriorated (yellow pins). Whilst  other actions can also be visualised, these examples indicate how  the achieved milestones can be helpful for the teacher or the  students to reflect on the process followed by each group or groups  in the same classroom.    Challenges: capturing traces of the task and also making sense of  this information in order to render it actionable can be very  challenging. Another epistemic challenge to overcome is the  impact of the task instructions on the data capture. In our example,  the teacher encouraged students to self-report their actions on a pre- formatted checklist on the tablet, (which although a common  practice for practicing nurses is usually performed on paper). But  only 3 groups completed the checklist, and one of them did it as  explicitly requested by the teacher after the session.   4.4 The runtime enactment of the task  The potential: there is a growing interest in the learning analytics  community in connecting the learning design with evidence-based  data innovations [11] since activity data makes much more sense  when the activity context is known. However, not much has been  done to provide analytics for comparing the enactment of the tasks  in the classroom with the learning design intentions. Figure 6 also  illustrates the timelines that show how two classroom sessions  unfolded. In this case, the manikin logs and the mobility data can  provide evidence about the duration of the simulation session. In     Figure 6. Design analytics for two classroom sessions: showing three key milestones reached by at least 1 group during the   simulation (obtained from the manikin logs and students self-reported events) shown in a timeline that represents the enactment   of the learning design     Figure 4. Tracking challenges. Left: a small group (3 students and the teacher). Right: a very large group formed by latecomers   (tracking affected by occlusion and whenever students closed the medical curtains, e.g. see blue shaded area tagged as 1)     the first class (T1) the teacher took ~45 minutes to provide  instructions and discuss the case and then ~1.5 hours for the  reflection after the simulation. By contrast, the second teacher (T2)  dedicated most of the class to the simulation (almost 2 hours)  allowing students to perform the actions at a different pace (note  the separation between the coloured pins in T1 and T2). This type  of visualisation could be offered to a unit coordinator to fine-tune  the session design, or to be aware of the variation in execution.    Challenges: in our study, each of the four teachers enacted the  learning design quite differently, making an impact on the task and  therefore on the analytics. However, learning analytics can provide  insights about how divergent teachers are in enacting the design.  Overall, unexpected classroom events, besides the ones mentioned  in previous subsections, strongly influenced or limited our first  attempt in collecting students data for analytics. For example, in  one of the classes, students arriving late to the class, one by one,  generated large groups that were quite dysfunctional and also hard  to track using the depth sensor (see Figure 4, right).   4.5 Data management and ethics  Challenges: privacy, consent and data management issues need to  be taken into consideration for deploying learning analytics in  authentic educational settings. This is particularly crucial for  situations where student data cannot be de-identified or which  involve video/audio recording (as in our case). For example, in our  study, all students were informed by the lecturer about the study a  week before the recording sessions. Blanket authorisation was  granted by the ethics committee of the institution. Then, during the  study, only two out of the 5 clinical beds were recorded (there were  always unmonitored beds in each class) allowing students to  voluntarily participate. Students in this Faculty are already used to  the occasional recording session being organised as it is a common  practice in this learning space. As this is more of an exception than  a commonality in other classrooms, work needs to be done to  explore sustainable strategies to request consent and clarify data  management issues without stretching the already very limited  classroom time.   5. CONCLUSION  Learning analytics innovations deployed in authentic physical  classrooms are still rare. We have presented our initial attempt in  bringing learning analytics into a healthcare simulation classroom  aiming to uncover small group learning, collaboration and  enactment processes. We faced several challenges that still need to  be addressed in order to start gaining insights about students processes.  Through our illustrative preliminary examples, we uncovered the  potential that these multimodal analytics can bring to reveal  different aspects of the collaborative process, classroom dynamics  and the enactment of the learning design which are commonly  invisible. Finally, we illustrated the complexity of implementing  learning analytics in the classroom, not only in terms of limitations  in the technology, but also because the classroom is a very complex  and dynamic environment where the epistemic, social and physical  dimensions play a crucial role in making each session unique and  filled with unforeseen events. Although the unpredictable nature of  the classroom may affect learning analytics, it certainly makes a  classroom a rich setting that offers great opportunities for learning.    We envisage that this paper can be useful for other researchers and  developers seeking to provide enhanced support in simulation  laboratories and also in more generic collocated settings. Our future  work is aimed at generating the technical and pedagogical means  for providing automated feedback, and/or supporting awareness   and reflection in f2f classroom scenarios, similarly to what is  currently available in computer-mediated (online) platforms.   6. REFERENCES   [1] Barrows, H. S. 1996. Problembased learning in medicine and   beyond: A brief overview. New directions for teaching and  learning, 1996, 68, 3-12.    [2] Bergstrom, T., and Karahalios, K. 2007. Conversation Clock:  Visualizing audio patterns in co-located groups. In Proc. of  40th Annual Hawaii International Conference on System   Sciences (Big Island, Hawaii, USA, Jan 3-6). IEEE Computer  Society, 78-87.    [3] Blikstein, P., and Worsley, M. 2016. Multimodal Learning  Analytics and Education Data Mining: using computational  technologies to measure complex learning tasks. Journal of  Learning Analytics, 3, 2, 220-238.    [4] Bussard, M. E. 2016. Self-Reflection of Video-Recorded  High-Fidelity Simulations and Development of Clinical  Judgment. Journal of Nursing Education, 55, 9, 522-527.    [5] Cooper, J., and Taqueti, V. 2008. A brief history of the  development of mannequin simulators for clinical education  and training. Postgraduate medical journal, 84, 997, 563-570.    [6] Dieckmann, P. 2009. Using simulations for education, training  and research: Pabst Science Publishers.   [7] Fanning, R. M., and Gaba, D. M. 2007. The role of debriefing  in simulation-based learning. Simulation in healthcare, 2, 2,  115-125.    [8] Gaba, D. M. 2004. The future vision of simulation in health  care. Quality and safety in Health care, 13, suppl 1, i2-i10.    [9] Grant, J. S., Moss, J., Epps, C., and Watts, P. 2010. Using  video-facilitated feedback to improve student performance  following high-fidelity simulation. Clinical Simulation in  Nursing, 6, 5, e177-e184.    [10] LeFlore, J. L., Anderson, M., Zielke, M. A., Nelson, K. A.,  Thomas, P. E., Hardee, G., and John, L. D. 2012. Can a virtual  patient trainer teach student nurses how to save livesteaching  nursing students about pediatric respiratory diseases.  Simulation in Healthcare, 7, 1, 10-17.    [11] Lockyer, L., Heathcote, E., and Dawson, S. 2013. Informing  pedagogical action: Aligning learning analytics with learning  design. American Behavioral Scientist, 57, 10, 1439-1459.    [12] Martinez-Maldonado, R., Dimitriadis, Y., Martinez-Mons,  A., Kay, J., and Yacef, K. 2013. Capturing and analyzing verbal  and physical collaborative learning interactions at an enriched  interactive tabletop. International Journal of Computer- Supported Collaborative Learning, 8, 4 (Nov. 2013), 455-485.    [13] Martinez-Maldonado, R., Goodyear, P., Kay, J., Thompson,  K., and Carvalho, L. 2016. An Actionable Approach to  Understand Group Experience in Complex, Multi-surface  Spaces. In Proc. of CHI' 16 (San Jose, USA, May 7-12). NY:  ACM, 2062-2074.   [14] Martinez-Maldonado, R., Schneider, B., Charleer, S., Shum,  S. B., Klerkx, J., and Duval, E. 2016. Interactive Surfaces and  Learning Analytics: Data, Orchestration Aspects, Pedagogical  Uses and Challenges. In Proc. of LAK' 16 (Edinburgh, UK,  April 25-29). NY: ACM, 124-133    [15] Prieto, L. P., Sharma, K., Dillenbourg, P., and Jess, M.  2016. Teaching analytics: towards automatic extraction of  orchestration graphs using wearable sensors. In Proc. of LAK'  16 (Edinburgh, UK, April 25-29). 148-157.   [16] von Zadow, U., Buron, S., Harms, T., Behringer, F.,  Sostmann, K., and Dachselt, R. 2013. SimMed: combining  simulation and interactive tabletops for medical education. In  Proc. of CHI' 13. NY: ACM, 1469-1478.     "}
{"index":{"_id":"13"}}
{"datatype":"inproceedings","key":"Chiu:2017:ASS:3027385.3027439","author":"Chiu, Ming Ming and Chow, Bonnie Wing-Yin and Joh, Sung Wook","title":"How to Assign Students into Sections to Raise Learning","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"95--104","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027439","doi":"10.1145/3027385.3027439","acmid":"3027439","publisher":"ACM","address":"New York, NY, USA","keywords":"ability grouping, classmates, inequality, international assessment, socioeconomic status","Abstract":"Grouping students with similar past achievement together (tracking) might affect their reading achievement. Multilevel analyses of 208,057 fourth grade students in 40 countries showed that clustering students in schools by past achievement was linked to higher reading achievement, consistent with the benefits of customized, targeted instruction. Meanwhile, students had higher reading achievement with greater differences (variances) among classmates' past achievement, reading attitudes, or family SES; these results are consistent with the view that greater student differences yield more help opportunities (higher achievers help lower achievers, so that both learn), and foster learning from their different resources, attitudes and behaviors. Also, a student had higher reading achievement when classmates had more resources (SES, home educational resources, reading attitude, past achievement), suggesting that classmates shared their resources and helped one another. Modeling of non-linear relations and achievement subsamples of students supported the above interpretations. Principals can use these results and a simpler version of this methodology to re-allocate students and resources into different course sections at little cost to improve students' reading achievement.","pdf":"   How to Assign Students into Sections to Raise Learning  Ming Ming Chiu  Purdue University  5156 Beering Hall   West Lafayette, IN, 47907  +1-765-496-0119   chiu23@purdue.edu   Bonnie Wing-Yin Chow  City University of Hong Kong   Department of Applied Social Sciences  Kowloon, Hong Kong   +852-3442-8954   wychow@cityu.edu.hk   Sung Wook Joh  Seoul National University    Business School  Seoul, Republic of Korea 151-716   +82-2-880-9384   swjoh@snu.ac.kr         ABSTRACT  Grouping students with similar past achievement together   (tracking) might affect their reading achievement. Multilevel   analyses of 208,057 fourth grade students in 40 countries showed   that clustering students in schools by past achievement was linked   to higher reading achievement, consistent with the benefits of   customized, targeted instruction. Meanwhile, students had higher   reading achievement with greater differences (variances) among   classmates past achievement, reading attitudes, or family SES;   these results are consistent with the view that greater student   differences yield more help opportunities (higher achievers help   lower achievers, so that both learn), and foster learning from their   different resources, attitudes and behaviors. Also, a student had   higher reading achievement when classmates had more resources   (SES, home educational resources, reading attitude, past   achievement), suggesting that classmates shared their resources and   helped one another. Modeling of non-linear relations and   achievement subsamples of students supported the above   interpretations. Principals can use these results and a simpler   version of this methodology to re-allocate students and resources   into different course sections at little cost to improve students   reading achievement.   CCS Concepts   Applied Computing  Education   Keywords  Ability grouping; classmates; inequality; socioeconomic status;   international assessment   1. INTRODUCTION  As classmates interactions with each student differ, their effects   differ across students, so some arrangements of students into   classrooms yield superior learning overall compared to others.  For   example, grouping students by ability (tracking) is a common but   controversial education policy. Past studies of tracking has yielded   mixed results (positive effects: e.g., [1]; negative effects: e.g., [2];    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for   components of this work owned by others than ACM must be honored.   Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission   and/or a fee. Request permissions from Permissions@acm.org.   LAK '17, March 13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03 $15.00    DOI: http://dx.doi.org/10.1145/3027385.3027439      and non-significant effects: e.g., [3]). To explain these   contradictory results, we propose that the impact of tracking on   student achievement differs across levels (across schools vs. within   school), differs across types of classmate resources (past   achievement, reading attitude, family socio-economic status [SES])   and depends on a students own academic ability. Armed with this   knowledge and methodology, school principals can analyze their   data and use evidence-based arrangements of students into course   sections for superior learning instead of idiosyncratic systems (e.g.,   gender balance, parent requests, etc.) [4].     Data or analytic limitations might also account for some conflicting   results. While some schools track openly, other schools do so   quietly without a public tracking policy, even though students can   often recognize different achievement patterns across groups [5].   Hence, we examine the distribution of students by their actual past   achievement, instead of deferring to incomplete school declarations   of tracking [6].  Analytic limitations of past studies include omitted   variable bias, multi-collinearity, and failure to model the nested   data structure of students within classes within countries [7, 8].    To disentangle the effects of different types of tracking on reading   achievement, this study considers different levels of factors and   extends past research on tracking in four ways through analyses of   208,057 fourth grade primary school students in 40 countries.   (Many primary schools track their students, some as early as first   grade [9].) First, we examine how tracking students both across   schools (school-level) and across classes within schools (class-  level) affects reading achievement. Second, we examine how both   the amount and variation in different types of classmate resources   (past achievement, reading attitude, family socio-economic status   [SES]) are related to a students reading achievement. Third, we   analyze whether these relations differ across students with different   levels of past reading achievement. Lastly, we overcome the   statistical limitations of past studies through representative   sampling, inclusion of central variables, structured sets of   variables, and multilevel analyses. By understanding how tracking   operates, this study can help explain different tracking effects and   inform school policies on allocation of students into different   course sections to improve student learning.    2. TRACKING MECHANISMS  Under a tracking policy, students with similar past academic   achievement are grouped together for instruction. Whether tracking   raises or reduces student achievement depends on (a) the impact of   student similarities vs. differences and (b) whether classmates   compete or share resources.   2.1 Student Similarities vs. Differences  Education administrators can track students by placing those with   similar academic competences together, separating them from other   mailto:Permissions@acm.org http://dx.doi.org/10.1145/3027385.3027439   students with higher or lower levels of past academic achievement.   Or, they may mix together students with different past   achievements.   2.1.1 Clusters of similar students  Students with similar academic competences can be assigned   together to the same school (academic or vocational streaming) or   to the same classes within a school (course-by-course tracking) [1].   Grouping similar students together can improve their learning by   enabling teachers to customize instruction to similar students,   capitalizing on student preferences to interact with and help similar   peers and enacting self-fulfilling prophecies of their labels and   norms [10].   When facing students with similar academic competences,   educators can customize the curriculum, lessons, teaching   materials, and teaching pace to the needs of each group of similar   students, which can improve their learning (customized instruction)   [11]. In contrast, when the competences of students differ widely,   teachers may focus on the learning needs of a subset of students to   the detriment of other students with much higher or much lower   competences [12]. Such customized instruction is easier for a   streamed school with similar students rather than for a tracked class   with similar students (but different students across classes) [10]. In   a streamed school, teachers design lessons for all students within a   small range of competences [4]. In contrast, tracking within a   school requires much more teacher time and effort to create lessons   for classes of students at different competences [11]. Hence, we   expect instruction customization to occur more often and improve   student achievement more in streamed schools than in other schools   (with or without internal tracking of classes).   As students often prefer to interact with others who are similar to   themselves, those with similar academic competences might be   more likely than others to interact, befriend, and help one another   to learn, compared to dissimilar others (homophily bias) [13]. As a   result, academically similar students in streamed schools might be   more likely to help one another and contribute to a school-wide   community culture of mutual support, compared to students in non-  streamed schools [14]. (While academically similar classmates in   tracked classes within a non-streamed schools might help one   another more, they might be less helpful to academically different   schoolmates in other classes.)    Although students placed in schools or classes labeled as high-  achieving might benefit from self-fulfilling prophecies by enacting   high expectations and norms (assimilation) [15], students labeled   as low-achieving might correspondingly suffer. Teachers and   parents of students in schools labeled as high-achieving typically   have high expectations of them, which students often internalize   [16]. As a result, these students, their teachers and their parents tend   to invest more time, effort and other resources to improve their   learning, compared to those in non-streamed schools [17].    Assimilation effects might be stronger when tracking within a   school rather than streaming across schools. When students are   tracked within a school, teachers can devote more attention, effort   and other resources to students in higher tracks than to students in   lower tracks [18]. This unequal distribution of teacher resources not   only increases the gap between high- and low-achieving students   but its unfairness can demoralize low-achieving students; as a   result, the drop in the academic performance of low-achieving   students might exceed the gain in that of high-achieving students,   thereby yielding lower overall academic performance [14].    In contrast, the negative effects of assimilation and demoralization   might be weaker for students within a streamed school.  As   academic comparisons with other schools are more distant from   students and teachers immediate experiences, such labels might   have less impact on their behaviors, especially as they acclimate to   their streamed school environment [14]. (However, such labels can   still influence parents and attract teachers to reputable schools in   education systems with open markets for hiring teachers [unlike   school systems like South Korea that randomly rotate teachers to   different schools every five years [14].) Furthermore, students   within a school share the same label, so teachers and staff are less   likely to treat students differently [18]. As a result, these students   are more likely to view their teachers as fair, to be less demoralized,   and show higher overall academic performance compared to   students in schools with tracked classes.   Hence, streamed schools might have more benefits from   customized instruction and homophily, and less harm from   assimilation and demoralization, compared to schools with tracked   classes. As many schools systems do not explicitly stream their   schools, we operationalized the degree of streaming across schools   with a school clustering by past achievement measure (ratio of   student past achievement variance across schools over total   variance of past achievement in a country) [18]. Hence, we propose   the following hypothesis:   H-1. In education systems with greater school clustering by   past achievement, students have higher academic   achievement than otherwise.     2.1.2 Mixing different students  While clustering similar ability students together aids instruction   customization and capitalizes on homophily, mixing different   ability students together can facilitate helping behaviors and   learning from schoolmates/classmates with different experiences.   Mixed classes offer more possible pairs of a higher-achieving   student helping a lower-achieving student (help opportunities).   Helping benefits both the recipient who receives additional   information and explanation, and the giver who often learns more   by re-organizing and elaborating his or her knowledge to give a   suitable explanation to the recipient [19]. Unlike clustering which   operates across schools, help opportunities occur primarily in direct   interactions among classmates.    H-2a. When classmates have greater variance in their past   achievements, a student has higher academic   achievement.    Compared to less diverse groups, more diverse groups often have   weaker interpersonal relations, more disagreements and less early   learning, but their greater range of experiences and resolution of   disagreements can increase their later learning [20]. Due to   homophily bias within a group, members often categorize   themselves into subgroups based on similarity (similar to ingroup   members and different from outgroup members, social   categorization) [21]. People trust ingroup members more,   cooperate with them more often, and have better relationships with   them, compared to outgroup members. Moreover, diverse   groupmates often have unfamiliar ideas, attitudes, and experiences   [22] that can conflict, thereby igniting disagreements that hinder   interpersonal relations initially [21]). Thus, less diverse groups   often initially function better than more diverse groups do [20].    However, diverse groups different ideas and disagreements can   legitimize different opinions, thereby stimulating group members   to pay attention, share more ideas, and reduce premature consensus   [23]. Group members' diverse views also help them recognize flaws   and correct them to yield better ideas [24]. If diverse groupmates   can reconcile their different views, resolve their disagreements, and     integrate their information, they can create and learn new ideas   [25]. Lastly, divergent views can stimulate a group to reflect and   improve on its own functioning [26]. Thus, over longer time   periods, diverse groups produce ideas that are more diverse and   learn more compared to homogeneous groups [27]. As these   students have shared a classroom for several months, we focus on   the long-term effects of diversity. Also, many countries do not have   much racial diversity across classmates, so we examine diversity of   classmates family SES and propose the following hypothesis.     H-2c. When classmates have greater variance in their family   SES, a student has higher academic achievement.    Greater differences among classmates increase the extremes of   diametric opposites, which can draw attention to them and aid   learning. Consider two classes whose students reading attitudes   have the same mean but greater variance in the second class than   the first. As reading attitude extremes are likely greater in the   second class than the first [8], the second class likely has both the   student with the best reading attitude (let us call her Heidi) and the   one with the worst (Lola). Heidis concrete behaviors embody her   reading attitude (e.g., reads many books; tells their stories by acting   them) and yield beneficial consequences (high reading quiz scores;   smiles at her graded quizzes; teacher praises her, reading awards,   etc.). In contrast, Lola rarely reads (I never read books theyre   boring), has low reading quiz scores, frowns at her graded quizzes   and so on.    As extremes, Heidi and Lola mutually highlight their differences   (contrasting cases), thereby attracting and focusing more attention   on the two of them than on either one alone or on classmates with   smaller reading attitude differences (focusing function, Schwartz &   Bransford, 1998). This focusing function helps classmates include   important differences and omit unnecessary information from their   working memories (cognitive load theory) [28], which are smaller   in young children than in adults [29]. As contrasting cases of people   rather than abstract ideas, Heidi and Lola serve as detailed reference   points for inferences about students in the continuum between them   (cognitive reference point reasoning) [30]. Rather than observing   other students or either Heidi or Lola alone, a student who focuses   on both of them can create a network of their contrasting reading   attitudes and related information in his or her short-term working   memory (hippocampus) and then encode it into his or her long-term   memory (cerebral cortex) [29], thereby learning effectively and   efficiently about reading attitude, remembering it reliably, and   acting on it accordingly to learn more than otherwise [31]. Hence,   we propose this hypothesis.     H-2b. When classmates have greater variance in their reading   attitudes, a student has higher academic achievement.    2.2 Classmates Compete vs. Share Resources  The impact of tracking on student achievement also depends on the   extent to which classmates compete or share resources. When   competing with classmates with greater cognitive, social and   material resources in a zero-sum game, a student could have lower   academic achievement. Classmates can serve as a collective ruler   against which to judge a students relative competence   (comparative reference-group view). When surrounded by lower-  achieving classmates, a student often has greater confidence in his   or her competence (self-concept), expectation of future success,   motivation, and academic achievement (social comparison) [32].   Conversely, higher-achieving classmates can demoralize a student,   reduce his or her self-concept, lower future expectations, and yield   lower academic achievement.     H-3. When classmates have a higher mean past academic   achievement, a student has lower academic   achievement.    In this social comparison view, tracking benefits lower-achieving   students at the expense of higher achieving students.   On the other hand, classmates, especially high-achieving ones, can   help a student learn directly or indirectly [33]. Classmates can   directly help a student by sharing information. For example, a   higher-achieving classmate can help a student correctly spell an   unfamiliar word.    A classmate can also help students learn indirectly through   motivation or norms. For example, a classmate can dramatically   enact a scene from a storybook, which can entice and motivate   other students to discuss it and learn about it [33]. Over time,   students greater motivation helps them exert more effort and   persevere when facing difficulties [34].   Classmates, especially higher-achieving ones with higher status,   can help create and maintain norms of positive attitudes toward   reading, regular learning behaviors and high reading achievement   [35]. Classmates can articulate and model positive academic   attitudes, such as sharing their enjoyment of specific stories.   Furthermore, they can discuss their readings daily to promote peer   pressure toward regular reading of new books. Together,   classmates can cultivate a culture of positive reading attitudes and   behaviors in which to immerse a student, which typically yields   higher reading achievement [36]. Note that hypothesis H-4a   competes with H-3 above.   H-4a. When classmates have a higher mean past academic   achievement, a student has higher academic   achievement.    H-4b. When classmates have a higher mean attitude toward   reading, a student has higher academic achievement.    A student can benefit not only from family resources but also from   classmates family resources.  Families can use their financial,   human, cultural and social capital to give their children learning   opportunities. Specifically, families with more money (financial   capital) can buy more educational resources (books, calculator,   etc.) to create a richer learning environment [18]. Furthermore, high   SES students often spend more time with their parents (due to less   parent time on housework and multi-tasking parents), so they can   benefit more from their parents human, social and cultural capital.   Families with more education, knowledge or skills (human capital)   often create better learning environments for their children, foster   better attitudes toward reading and teach them more skills   compared to other families [18]. Likewise, high SES families often   have cultural possessions or experiences (cultural capital) that can   help their children learn societys cultural knowledge, skills and   values to adapt to their school culture [34]. High SES families also   often have large social networks of relatives, friends and   acquaintances with skills or resources (social capital) that can help   their children learn [35]. Using their greater financial, human,   social and cultural capital, higher SES students can better   understand others expectations, behave properly at school, have   closer relationships with teachers and classmates, and learn more in   school than lower SES students do.    Similarly, a student can benefit from a classmates family resources   directly or indirectly [35]. In the most direct case, a student visits a   classmates home and uses the latters educational resources. A   student may work with a classmate on the classmates computer   (classmate family financial capital), discuss a book with the   classmates mom (classmate family human capital), discuss a     painting in the living room (classmate family cultural capital) or   chat with a family friend over dinner (classmate family social   capital). Less directly, a student can learn from a classmate   learning experiences at home [35], for example, when the classmate   talks about it (my mom was telling me about the presidential   election).    H-4c. When classmates have a higher mean family SES, a   student has higher academic achievement.    Tracking provides higher-achieving students with higher-achieving   classmates who often have more intellectual resources. However,   tracking provides low-achieving students with low-achieving   classmates who often have more intellectual resources. Hence,   tracking might benefit high achieving students more from low-  achieving students.   2.3 The Present Study   This study examines the effects of tracking on students reading   achievement through analyses of 208,057 fourth grade primary   school students in 40 countries. We focus on three research   questions. First, does tracking at school- or class-levels linked to   better reading achievement Second, do these links operate through   the mechanisms of customized instruction, homophily, giving help,   receiving help, diversity, competition/social comparison, sharing   resources or superior benefits for high-achieving students Lastly,   do these relations differ across subsamples of the lowest-achieving   10%, 20% and 50% of students and the highest-achieving 50%,   20% and 10% of students   As past studies have shown that reading achievement is related to   the following variables, we included them in our regression model   to reduce omitted variable bias [8]: country economic growth   (gross domestic product per capita) [18], family income inequality   in a country (Gini index) [18], family SES [35], home educational   resources [18], student gender [36], reading self-concept [38],   attitude toward reading [35], parent attitude toward reading [35],   school climate [39], and teacher gender [40].   3. METHODS   3.1 Data   In 40 countries, the International Association for the Evaluation of   Education Achievement Progress in International Reading Literacy   Study (IEA-PIRLS) assessed 208,057 fourth-grade students, and   their parents, teachers and school principals completed   questionnaires [41]. IEA chose at least 150 representative schools   in each country, based on neighborhood SES and student intake.   From each school, IEA selected one or two 4th grade classes,   yielding a sample size of at least 4,000 students per country   (stratified sampling) [41]. Participating students completed an 80-  minute assessment booklet and then a 1530-minute questionnaire.   The World Bank collected economic data for each country (annual   income and income inequality) [42].    3.2 Variables  Table 1. Summary Statistics (N = 208,044 Students)   Variable Mean  SD Min Max   Reading achievement 496 113 5 813   Past reading achievement before school   (Recognize alphabet; Read words;   Read sentences; Write alphabet; Write   words.) Reliability = 0.95.   0.00 1.00 -2.49 1.82   Log (income per person) Log Gross   Domestic Product per capita   9.73 0.65 7.97 10.5   Variable Mean  SD Min Max   Inequality of Family income  (Gini index) 35.44 7.90 25 58   School inequality based on students past   reading achievement; Ratio of variance   across schools  / country variance   0.12 0.09 0.05 0.54   Family socio-economic status (parents   educations, jobs, incomes);     Reliability = 0.94.   0.00 1.00 -2.95 2.84   Parents attitude towards reading;   Reliability = 0.82.   0.00 1.00 -2.94 1.69   Home education resources (books;   children books; computer; child   desk/table); Reliability = 0.76.   0.01 1.00 -2.54 1.98   Class mean SES 0.00 0.61 -2.72 2.18   Class mean home education resources 0.01 0.68 -2.54 1.98   Class mean parents' attitude towards   reading   0.00 0.37 -2.67 1.41   School violence (bullying by me and of   me; injury by me and of me)    Reliability = 0.93   0.00 1.00 -1.37 2.09   Female teacher (vs. male) 0.84      HW time mismatch (in teacher and   student reported times)   0.89 0.61 0.00 8.19   Girl (vs. boy) 0.49      Students reading attitude              Reliability = 0.79   0.00 1.00 -3.00 1.59   Students reading self-concept    Reliability = 0.80   0.00 1.00 -2.77 1.54   Class mean past reading achievement 0.00 0.48 -2.49 1.66   Class mean attitude towards reading 0.00 0.41 -1.66 1.59   Past reading achievement -class variance 0.80 0.35 0.00 3.89   SES -class variance 0.66 0.31 0.00 3.35   Students' attitude towards reading                   -class variance   0.87 0.35 0.00 3.54   Note: Indices were standardized (mean = 0; SD = 1). All   reliabilities refer to the composite score reliability coefficient.    3.3 Analysis  The outcome variable Readingijk of student i in school j in country   k has a grand mean intercept 000, with unexplained components   (residuals) at the student-, school-, and country-levels (eijk, fjk, gk).   First, we enter past student achievement (Past_Literacy_Skills).   Readingijk = 000 +eijk + f0jk + g00k + rjkPast_Literacy_Skillsijk         + 00sCountry00k + tjkFamilyijk + ujkClassmates_Familiesijk   + vjkSchool&Teacherijk + wjkStudentijk + xjkClassmatesijk     + zjkClassmates_Varianceijk          (1)    Then, we entered a vector of s variables at the country level: GDP   per capita, Gini index, school clustering of students by past reading   achievement before school (Country, see table 1) to test whether   more school clustering is linked to higher reading achievement   (hypothesis H-1). To test for non-linear effects, squared terms (e.g.,   Gini2) were added. We tested whether sets of predictors were   significant with a nested hypothesis test (2 log likelihood) [8].   Non-significant variables were removed. Next, we added family   variables: family SES, parents' attitude towards reading and home     education resources (Family). Then, we added class means of   family SES, home education resources, and parents attitudes   toward reading (Classmates_Families) to test whether higher   reading achievement is linked to higher classmate SES (H-4c).   Next, we added school and teacher variables: school violence,   female teacher, and homework mismatch (School&Teacher),   followed by student variables: gender, students attitude towards   reading, students reading self-concept (Student). Then, we added   the class mean of past reading achievement and class mean of   attitude towards reading (Classmates). This tests the competing   hypotheses of whether higher classmates past achievement is   linked to a students lower reading achievement (H-3) or higher   reading achievement (H-4a). It also tests whether classmates with   superior reading attitudes are linked to a students higher reading   achievement (H-4b).   Lastly, we added the class variances of past student achievement,   SES, and students' attitude towards reading   (Classmates_Variance) to test whether higher academic   achievement is linked to higher variance in classmate past   achievement (H-2a), to higher variance in classmate reading   attitude (H-2b) or to higher variance in classmate SES (H-2c).     To determine whether high-achieving vs. low-achieving students   have greater access to or benefits from available resources, we   tested if the above links differed across student sub-samples. We   created six sub-samples of students by reading achievement   (bottom 10%, bottom 20%, bottom 50%, top 50%, top 20%, and   top 10% from each country).    4. RESULTS  4.1 Summary Statistics  This samples countries ranged from poor, very unequal nations   (e.g., Iran) to rich, relatively equal ones (e.g., Luxembourg). See   Table 1 for overall summary statistics.   4.2 Explanatory Model  Past reading achievement, country, family, school, teacher, student,   classmates and class variance variables accounted for differences   in students reading achievement (see Table 2). The differences in   reading achievement at the country-, school-, and student-levels   were 58%, 13%, and 29%. All results discussed below describe first   entry into the regression, controlling for all previously included   variables.    4.2.1 Past Reading Achievement   Students whose past reading achievement exceeded the mean by   10% averaged 5 more points in reading achievement (Table 2,   model 1). Past reading achievement accounted for 2% of the   differences in students reading achievement.   4.2.2 Country  Meanwhile, students in richer or more equal countries scored   higher (see Table 2). If a richer country's GDP per capita exceeded   the country mean by 10%, its students averaged 4 more points in   reading achievement. (Regressions with linear GDP per capita did   not fit the data as well, explaining less of the variance in students'   reading achievement.) Meanwhile, when a country's GINI   exceeded the mean by 10%, its students averaged 20 points lower   in reading.    If a countrys clustering of students by past reading achievement   exceeded the mean by 10%, students averaged 45 more points in   reading achievement, supporting hypothesis H-1 (instruction   customization). Furthermore, school clustering showed a non-  linear effect (see Figure 1). The effect size increases up to a    Table 2. Multilevel Regression of Students Reading   Achievement with Unstandardized Coefficients (and Standard   Errors in Parentheses)    Reading achievement   Explanatory variable Beta     SE p   Past reading achievement before school 9.05 (0.12) ***   Log GDP per capita 25.03 (3.34) ***   GINI -7.03 (0.13) ***   Clustering students by past reading   achievement  -1913.59 (118.78) ***   Clustering students by past reading   achievement  1100.19 (62.99) ***   Family SES 8.07 (0.15) ***   Home education resources 8.87 (0.15) ***   Parents' attitude towards reading 5.29 (0.12) ***   Class mean SES -5.47 (0.61) ***   Class mean SES 20.95 (0.73) ***   Class mean home education resources 1.58 (0.57) **   Class mean home education resources 22.37 (0.83) ***   Class mean past reading achievement2 14.97 (0.67) ***   Class mean past reading achievement 10.36 (0.73) ***   School violence -3.43 (0.11) ***   Female teacher 4.02 (0.61) ***   HW mismatch -5.29 (0.37) ***   Girl 9.60 (0.23) ***   Students' attitude towards reading 8.25 (0.12) ***   Students' reading self-concept 21.57 (0.11) ***   Class mean students' reading attitude 9.88 (0.69) ***   SES -class variance 2.50 (0.77) **   Past reading achievement -class variance -5.64 (0.94) ***   Past reading achievement -class variance 13.46 (2.09) ***   Students' reading attitude -class variance  -8.00 (1.13) ***   Students' reading attitude -class variance 22.47 (2.37) ***   Variance at each level Variance explained   Country (58%) 0.64     School (13%) 0.43     Student (29%) 0.29       Total variance explained 0.51      Note. A constant term was added. *p < .05, **p < .01, ***p < .001.   maximum when school clustering is 0.30 (~95th percentile of a   normal curve or 2.0 SDs above the mean; 2.0 = [0.30  0.12] / 0.09   = ([X  mean] / SD). Extreme school clustering beyond two   standard deviations (> 0.30) tends to reduce student reading   achievement. Together, country variables accounted for about 29%   the total differences in reading achievement and for 51% of its   variance across countries.     4.2.3 Family  Family variables were linked to students reading achievement   (see Table 2). If parents have 10% better reading attitude, their         Figure 1. School Clustering by Past Achievement. All   figures graphs have the same reading achievement y-axis   scale of -100 to 200. Each blue dot indicates an explanatory   variables mean value, and the solid blue curve indicates   values within two standard deviations of this mean. The   dashed curves indicate extreme values outside two standard   deviations of this mean. Some curves have a turning point   (minimum or maximum), indicated by a red X.    children averaged 2 points higher in reading. Meanwhile, students   with 10% more home education resources averaged 4 points   higher in reading. Family variables accounted for an extra 7% of   the variance in students reading achievement.   4.2.4 Classmates Families  Classmate family variables were also linked to students' reading   achievement (see Table 2). If classmate family SES exceeded the   mean by 10%, students averaged 4 points higher in reading   achievement, supporting the classmate sharing hypothesis H-4c,   and this non-linear effect decreases for higher classmate family   SES (diminishing marginal returns; see Figure 2) [18].       Figure 2. Class Mean SES   Students with classmates who had 10% more education resources   at home averaged 5 points higher reading achievement. (This   technically non-linear effect was nearly linear, see Figure 3.)      Figure 3. Class Mean Home Education Resources   4.2.5 School and teacher  School and teacher variables were linked to students' reading   achievement (see Table 2). Students in schools with 10% less   school violence averaged 2 points higher in reading, and students   with a female teacher averaged 5 points higher than those with a   male teacher. Also, if teachers' misjudgment of students' homework   time exceeded the mean by 10%, their students averaged 1 point   lower in reading. School and teacher variables accounted for an   extra 1% of the variance in students reading achievement.   4.2.6 Student  Girls outscored boys by 13 points on average (Table 2, model 6).   Moreover, students with 10% better reading attitude or 10% higher   reading self-concept averaged 3 or 6 points higher in reading,   respectively (see Table 2). Student variables accounted for an extra   4% of the variance in students' reading achievement.   4.2.7 Classmates  When classmates past reading achievement exceeded the mean by   10%, students averaged 2 points higher reading achievement,   supporting hypothesis H-4a (classmates share) and rejecting   hypothesis H-3 (classmates compete). See Table 2. This nonlinear   effect has a minimum when classmate past achievement is -0.30   (~10th  percentile at -0.63 SD (below the mean); -0.63 = [-0.30  0]   / 0.48); at lower values beyond this turning point, student reading   achievement scores tend to be higher (see Figure 4). Together,   classmates' family variables accounted for an extra 6% of the   variance in students' reading achievement.      Figure 4. Class Mean Past Reading Achievement   When classmates reading attitudes exceeded the mean by 10%,   students averaged 1 point higher in reading (see Table 2),   supporting hypothesis H-4b; the link is slightly stronger for higher   classmate reading attitudes (see Figure 5).  This classmate variable   accounted for an extra 0.1% of the variance in students' reading   achievement.      Figure 5. Class Mean Reading Attitude   4.2.8 Classmate variance  Classmate variances were also linked to student reading   achievement (see Table 2). When variance of classmates past   acheivement exceeded the mean by 10%, students averaged 1 point   higher in reading achievement, supporting hypothesis H-2a (giving   and receiving help). The strength of this link increases up to a   maximum at a variance of 1.19 (~86th percentile for a normal curve   or 1.11 standard deviations above the mean; 1.11 = [1.19  0.80]   / .35 = ([X  mean] / SD). See Figure 6. Extreme values at greater    -100  -50  0  50  100  150  200  0.0 0.2 0.4 0.6  -100  -50  0  50  100  150  200  -4 -2 0 2 4  -100  -50  0  50  100  150  200  -3 -2 -1 0 1 2  -100  -50  0  50  100  150  200  -2.5 -1.5 -0.5 0.5 1.5  -100  -50  0  50  100  150  200  -2 -1 0 1 2       Figure 6. Past Achievement  Class Variance   variances tend to reduce students reading achievement. Also,   students in classes with 10% more variance in classmates' reading   attitudes averaged 2 points higher in reading, supporting hypothesis   H-2b (attitude diversity). The strength of this link increases up to a   maximum at a variance of 1.40 (~87th percentile for a normal curve   or 1.51 standard deviations above the mean; 1.51 = [1.40  0.87]   / .35 = ([X  mean] / SD)). See Figure 7. Lastly, students in classes   with 10% more SES variance averaged 0.2 points higher in reading   achievement, supporting H-2c (SES diversity). Class variances   accounted for an extra 1% of the variance in students' reading   achievement. Other variables were not significant.      Figure 6. Reading Attitude  Class Variance   4.3 Achievement Sub-samples  Many variables showed similar, significant results in all   achievement sub-samples (see Table 3): past reading achievement,   log GDP per capita, GINI, clustering of students by past reading   achievement, parents' attitude towards reading, home education   resources, classmates mean SES, classmates home education   resources, school violence, students' gender, students' reading self-  concept, and class variance in students' attitude towards reading.    (The consistency of non-linear results can be seen by graphing the   quadratic functions. Due to space considerations, these graphs are   not included but are available upon request.)   However, the achievement subsample results varied for the   following attributes: teacher gender, homework mismatch, and   many classmate-related variables (SES variance; mean and   variance of their parents reading attitudes; mean and variance of   past achievement; and reading attitude). Teacher gender and   homework mismatch were not significant for the top 10% of   students in each country, suggesting that these factors do not affect   the highest-achieving students (perhaps because higher ability   students have successful, resilient ways of learning even in less   hospitable environments). Furthermore, variance in classmates   SES was significant for only the top 50% and top 20% of students   by achievement, suggesting that these students primarily benefit   from other students diverse resources and experiences; the results   are not significant for the other subsamples.   Also, the classmate reading attitudes results for the lowest-  achieving 10%, 20% and 50% of students were similar to the   overall result. However, the results were only significant for the   lowest-achieving 10%, 20% and 50% of students. This result   suggests that only the lower achieving students benefit from more   classmates with better attitudes toward reading.     For many subsamples of students, the links between mean and   variance of classmate past achievement and student reading   achievement were similar to the overall result. However, lower   mean classmate past achievement was linked to higher student   reading achievement for the lowest-achieving 10% and highest-  achieving 20% and 10% of students, which is consistent with the   classmate help hypothesis. Meanwhile, classmates past   achievement variance is positively related to reading achievement   for the lowest-achieving 10%, 20% and 50% of students but   negatively related to reading achievement for the highest-achieving   20% and 10% of students and not significant for the highest-  achieving 50% of students. We discuss these results below.   None of the above results show substantial homophily or labeling   effects. Analyses using standardized scores within each country   yielded similar results. Analyses of residuals showed no influential   outliers.   5. DISCUSSION  We examined the effects of tracking on students reading   achievement through analyses of 208,057 fourth grade primary   school students in 40 countries. There are five major findings. First,   tracking across schools (streaming) was linked to higher reading   achievement. Specifically, school clustering of students by past   reading achievement yielded higher reading achievement,   supporting the instruction customization hypothesis. Second, a   student generally benefited from classmates resources (SES, home   educational resources, reading attitude, past achievement),   suggesting that classmates shared their resources and this sharing   of resources yielded higher academic achievement. Third, tracking   at the class level was linked to lower reading achievement. Greater   variance of classmate family SES, reading attitude or past   achievement were all linked to higher reading achievement,   supporting the helping and diversity hypotheses. Fourth, several   relations were non-linear with turning points (minimums or   maximums): school clustering by past reading achievement, mean   and variance of classmate past achievement, and variance of   classmate reading attitude. Fifth, some relations differed across   achievement subsamples, notably classmate reading attitude,   classmate past achievement, variance of classmate SES, and   variance of classmate past achievement.   5.1 School Clustering  The school clustering by past reading achievement results likely   reflect instruction customization rather than homophily. As   homophily was not evident in the classroom-level results, it likely   does not account for the broader, school clustering result. Instead,   the school clustering and classmate results are consistent with the   view that instruction customization is likely more efficient and   effective at the school level (streaming) than at the class level. They   suggest that greater school clustering of students by past reading   achievement up to about two standard deviations above the mean   might help educators target curricula and instruction to specific   schools of students with similar academic competence to help them   learn more. The lower reading achievement of students in education   systems with extremely high clustering (more than two standard   deviations above the mean) suggest that classmates with extremely   similar past reading achievement can be harmful; instead, moderate   classmate variation is useful, consistent with the classmate-level   results.   -100  -50  0  50  100  150  200  0 1 2 3 4  -100  -50  0  50  100  150  200  0 1 2 3    Table 3. Multilevel Regressions of Reading Achievement with Six Subsamples by Achievement (Bottom 10%, 20%, and 50%; and   Top 10%, 20%, and 50%).     Regressions of Reading Achievement with Six Achievement Subsamples    Bottom  Top   Explanatory variable 10% 20% 50% 50% 20% 10%    Past reading achievement 2.70 *** 3.18 *** 4.72 *** 3.83 *** 2.22 *** 1.93 ***   Log GDP per capita 19.89 *** 17.43 *** 12.17 *** 41.22 *** 40.11 *** 34.20 ***   GINI -2.53 *** -3.52 *** -3.61 *** -6.29 *** -4.94 *** -3.15 ***   Clustering of students by past reading achievement -1443.74 *** -1493.31 *** -1419.10 *** -1878.35 *** -1620.42 *** -1425.54 ***   Clustering of students by past reading achievement 480.53 *** 524.28 *** 574.52 *** 1122.71 *** 900.99 *** 786.00 ***   Family SES 2.20 *** 2.46 *** 3.64 *** 4.26 *** 2.54 *** 1.77 ***   Home education resources 2.11 *** 3.21 *** 7.08 *** 4.09 *** 3.17 *** 3.08 ***   Parents' attitude towards reading 0.61 ** 1.00 *** 1.73 *** 1.67 *** 0.43 ** 0.45 *   Class mean SES 0.37   0.21   -1.75 *** 0.87 * 3.13 *** 2.45 ***   Class mean SES 7.75 *** 7.69 *** 6.98 *** 10.83 *** 4.71 *** 2.18 **   Class mean home education resources -0.06   0.56   1.21 * -1.74 *** -3.71 *** -3.59 ***   Class mean home education resources 8.57 *** 11.15 *** 14.21 *** 10.07 *** 8.35 *** 7.50 ***   Class mean past reading achievement2 2.25 ** 3.17 *** 4.59 *** 5.20 *** -0.87   -1.74 **   Class mean past reading achievement -4.21 *** -1.79 * 2.78 *** 1.15 * -2.02 *** -0.89     School violence -0.55 * -0.69 *** -1.90 *** -1.93 *** -1.15 *** -1.29 ***   Female teacher 1.97 ** 1.90 *** 3.58 *** 2.13 *** 1.71 *** -0.71     HW mismatch -3.85 *** -4.59 *** -4.77 *** -1.53 *** -0.82 * -0.24     Girl 1.15 ** 2.63 *** 4.67 *** 1.90 *** 1.25 *** 1.21 ***   Students' attitude towards Reading 1.00 *** 1.50 *** 2.44 *** 3.92 *** 1.83 *** 0.93 ***   Students' reading self-concept reading 2.79 *** 5.35 *** 9.24 *** 9.22 *** 5.83 *** 4.15 ***   Class mean students' attitude towards reading 0.29   0.03   1.49   0.05   0.57   -1.49     Class mean students' attitude towards reading 6.70 *** 7.25 *** 8.63 *** 0.22   -0.03   -0.12     SES -class variance 0.08   0.64   0.62   1.44 * 2.62 *** 1.19     Past reading achievement -class variance -7.57 *** -7.31 *** -9.95 *** 1.51   1.33   1.37     Past reading achievement -class variance 16.56 *** 14.61 *** 20.04 *** -2.65   -3.90 * -5.61 **   Variance of classmates attitude towards reading2 -9.91 *** -8.76 *** -7.52 *** -2.64 ** -2.17 * -1.45     Variance of classmates attitude towards reading 27.03 *** 21.89 *** 20.78 *** 8.06 *** 6.37 ** 5.60 *   Variance at each level Variance Explained   Country 0.62  0.63  0.63  0.63  0.64  0.64    School 0.37  0.39  0.39  0.42  0.42  0.43    Student 0.28  0.28  0.29  0.37  0.37  0.37    Total variance explained 0.49   0.50   0.50   0.53   0.53   0.53    Note. Each regression included a constant term.  *p < .05, **p < .01, ***p < .001.   5.2 Classmate Resources  The classmate mean results show that a student generally benefited   from classmates sharing of resources (social capital [35]).  A   student whose classmates had higher family SES, home educational   resources, reading attitudes, or past achievements often had higher   reading achievement than other students. Classmate family SES   and home educational resources both had positive relations with   student reading achievement, consistent with the view that   classmates shared educational materials, ideas and experiences     with a student to aid their reading achievement [35]. Classmate   family SESs diminishing marginal returns show that it benefits   lower SES students more than higher SES students; hence, mixing   high SES students with low SES students maximizes the classmate   SESs benefits [14]. If SES is highly correlated with student past   achievement in a country however, we cannot both track students   into schools by achievement and ensure that all students have some   classmates with high SES.   Meanwhile, classmates with better reading attitudes primarily   benefited low-achieving students, showing no significant effect on   high-achieving students. This result suggests that classmate reading   attitude has a ceiling effect, helpful for low-achieving students who   might have poorer reading attitudes but not for high-achieving   students who may already have sufficiently positive attitudes   toward reading (reading attitude has a weak positive correlation   with reading achievement in these data, r = .12). Hence, assigning   some students with positive reading attitudes into the same classes   as low-achieving students might improve the latters reading   achievement without harming the former.   5.3 Tracking across Classes  Very high or very low classmate past achievement was linked to   higher student reading scores, supporting the hypothesis that   students learn more by receiving help from higher-achieving   classmates or by giving help to lower-achieving classmates [19].   The achievement subsamples further showed that lower mean   classmate past achievement was linked to higher student reading   achievement for the highest-achieving 10% and 20% of students   and for the lowest-achieving 10% of students.  As a high-achieving   student is less likely than other students to ask for help from their   classmates, she can learn more with more low-achieving   classmates, to whom she can give help (more help opportunities).   On the other hand, a very low-achieving student benefits from   classmates with lower reading achievement, perhaps because they   are more likely to understand his or her learning difficulties and   help appropriately (Vygotskys zone of proximal development   [43]). Or, a low-achieving student might have more help   opportunities with still lower-achieving classmates. Together, these   results suggest that a variety of low- and high-achieving students in   a classroom creates more helping opportunities that can aid the   learning of both givers and receivers of help.   The results involving classmate variances (reading attitude, family   SES, or past achievement) do not support tracking across classes   (within school). Greater variance in classmate reading attitude was   linked to greater reading achievement, consistent with the view that   students learn from observing the consequences of classmates with   different reading attitudes and appreciating the importance of   desirable ones. Variance in classmate reading attitude also showed   diminishing marginal returns, indicating that some variation in   classmate reading attitude is sufficient to realize much of the   benefits and that maximizing variation is not necessary.   While higher classmate family SES generally benefits all students,   greater variance in classmate family SES is linked to greater   reading achievement only for the highest-achieving 50% and 20%   of students, with no significant effects on other students.  Hence,   mixing students with different family SES together might benefit   these high-achieving students without harming other students.   Lastly, the link between variance in classmate past achievement   and reading achievement differs across achievement subsamples.   For the lowest-achieving 10%, 20% and 50% of students, greater   variance in classmate past achievement is linked to higher reading   achievement. This result supports the view that a greater variety of   classmates across achievement levels offers more opportunities for   a low-achieving student to give and receive help to learn more.   Meanwhile, the highest-achieving 20% and 10% of students have   higher reading scores when their classmates have similar, low past   achievement levels. Perhaps, a high achieving student can elaborate   his or her knowledge with a helpful explanation to a lower   achieving student, but has difficulty creating several explanations   for students at different, lower-ability levels which might be   frustrating, time-consuming or harmful to his or her learning.    Meanwhile, variance in classmate past achievement is not   significantly related to reading achievement for the highest-  achieving 50% of students, possibly because they generally face   sufficient variation in classmate ability to both give and receive   help.  Together, these results suggest placing high-achieving   students with a limited range of low-achieving classmates.   This study not only provides general advice for assigning fourth   grade students to classrooms but also showcases a methodology for   principals to use on their own school data to customize assignment   of their own students. While this analysis uses data from many   countries and schools, a principal can analyze his or her own school   data across time for assigning his or her students.   6. CONCLUSION  This study investigated the links between grouping students by   ability and their reading achievement, suggesting possible   mechanisms and showing how it might influence the learning of   students at different levels of reading ability. Greater clustering of   students by past reading achievement in schools is linked to greater   reading achievement, suggesting that streaming across schools can   be effective. However, tracking at the class level was not linked to   greater reading achievement. Instead, greater variances in   classmates family SES, in classmates attitudes toward reading   and in classmates past reading achievement were linked to greater   reading achievement. Moreover, when classmates had greater   family SES, home education resources, attitudes toward reading or   past reading achievement, a student had higher reading   achievement. This result suggests that classmates shared   educational resources, attitudes, ideas and experiences to help a   student learn. The nonlinear links between several factors   (especially school clustering and classmate past achievement) and   reading achievement, and the results of the achievement subsample   analyses further support these interpretations. These results suggest   low-cost education policies, namely re-allocation of students and   resources, for improving students reading achievement.   7. ACKNOWLEDGMENTS  We appreciate the research assistance of Yik Ting Choi.    8. REFERENCES  [1] Chmielewski, A.K., 2014. An international comparison of   achievement inequality in within-and between-school   tracking systems. American J. of Education, 1203, 293-324.    [2] Hanushek, E. A., and Wbmann, L. 2006. Does educational  tracking affect performance and inequality The Economic   J., 116, 510, C63-C76.   [3] Opdenakker, M.C. and Damme, J., 2001. Relationship  between school composition and characteristics of school   process and their effect on mathematics achievement. British   Educational Research J., 27, 4, pp.407-432.    [4] Chiu, M. M. and Walker, A., 2007. Leadership for social  justice in Hong Kong schools Addressing mechanisms of   inequality. J. of Educational Administration, 45, 6, 724-739.      [5] Boaler, J., 2013. Ability and mathematics. Forum, 55, 1,  143-152.    [6] Hanushek, E.A., Kain, J.F., Markman, J.M. and Rivkin, S.G.,  2003. Does peer ability affect student achievement J. of   applied econometrics, 185, 527-544.   [7] Goldstein, H., 2011. Multilevel statistical models. John  Wiley & Sons, Sydney.   [8] Kennedy, P., 2003. A guide to econometrics. Blackwell,  Cambridge, MA.   [9] Lleras, C. and Rangel, C., 2009. Ability grouping practices in  elementary school and African American/Hispanic   achievement. American J. of Education, 115, 2, 279-304.    [10] Watanabe, M. 2008. Tracking in the era of high stakes state  accountability reform. Teach. Coll. Rec., 110, 3, 489-534.    [11] Smith, B. J. 2013. Dropouts and differentiation. In  Differentiated instruction, E. F. Sparapani, Ed. University   Press of America, Lanham, MD, 221-230.   [12] Westwood, P. 2013. Inclusive and adaptive teaching.  Routledge, New York, NY.   [13] Brechwald, W. A. and Prinstein, M. J. 2011. Beyond  homophily. J. of Research on Adolescence, 21, 1, 166-179.    [14] Chiu, M. M. 2008a. Inequality mechanisms that hurt both  privileged and disadvantaged students' learning. In I. H.   Wadells Ed. Income Distribution. Nova Science Publishers,   Hauppauge, NY, 79-98.   [15] Jansen, M., Schroeders, U., Ldtke, O. and Marsh, H. W.  2015. Contrast and assimilation effects of dimensional   comparisons in five subjects: An extension of the I/E model.   J. of Educational Psychology, 107, 4, 1086.   [16] Pintrich, P. R. 2003. A motivational science perspective on  the role of student motivation in learning and teaching   contexts. J. of Educational Psychology, 95, 667-686.   [17] Kaplan, S. N., Guzman, I. and Tomlinson, C. A. 2009. Using  the parallel curriculum model in urban settings, grades K-8.   Corwin, Thousand Oaks, CA.   [18] Chiu, M. M. and Khoo, L. 2005. Effects of resources,  inequality, and privilege bias on achievement. American   Educational Research J., 42, 575-603.   [19] Blatchford, P., Pellegrini, A. D. and Baines, E. 2016. The  Child at School. Routledge, New York, NY.   [20] Watson, W. E., Johnson, L. and Zgourides, G. D. 2002. The  influence of ethnic diversity on leadership, group process,   and performance. Int. J. of Intercult. Rel., 26, 1, 1-16.    [21] Van Knippenberg, D. and Schippers, M. C. 2007. Work  group diversity. Annual Review of Psychology, 58, 515-541.    [22] Sharan, Y. 2010. Cooperative learning for academic and  social gains. European J. of Education, 45, 2, 300-313.   [23] De Dreu, C. K. W. & West, M. A. 2001. Minority dissent  and team innovation. J. of Appl. Psychology, 86, 1191-1201.    [24] Chiu, M. M. 2008. Flowing toward correct contributions  during groups' mathematics problem solving. J. of the   Learning Sciences, 17, 3, 415-463.   [25] Paulus, P. B. and Brown, V. 2003. Ideational creativity in  groups. In Group creativity, P. B. Paulus and B. A. Nijstad   Eds. Oxford University Press, New York, NY, 110-136.   [26] Schippers, M. C., Den Hartog, D. N. and Koopman, P. L.  2007. Reflexivity in teams A measure and correlates. Applied   psychology, 56, 2, 189-211.   [27] Van Offenbeek, M. 2001. Processes and outcomes of team  learning. European J. of Work and Organizational   Psychology, 10, 303-317.   [28]  Sweller, J. 2010. Cognitive load theory. In Cognitive load  theory, J. L. Plass, R. Moreno and R. Brunken, Eds.   Cambridge University Press, New York, NY, 2947   [29] Baddeley, A. 2003. Working memory. Nature reviews  neuroscience, 4, 10, 829-839.    [30] Tribushinina, E. 2008. Cognitive reference points.  Netherlands Graduate School of Linguistics, Utrecht,   Netherlands.    [31] Glogger, I., Holzapfel, L., Kappich, J., Schwonke, R.,  Nuckles, M. and Renkl, A. 2013. Development and   evaluation of a computer-based learning environment for   teachers. Education Research International.    [32] Liu, W. C., Wang, C. K. J. and Parkins, E. J. 2005. A  longitudinal study of students' academic self-concept in a   streamed setting: The Singapore context. British J. of   Educational Psychology, 75, 4, 567-586.   [33] Skibbe, L. E., Phillips, B. M., Day, S. L., Brophy-Herb, H. E.  and Connor, C. M. 2012. Childrens early literacy growth in   relation to classmates self-regulation. J. of Educational   Psychology, 104, 541-553.   [34] Chiu, M. M. and Chow, B. W.-Y. 2010. Culture, motivation,  and reading achievement: High school students in 41   countries. Learning and Individual Differences, 20, 579-592.   [35] Chiu, M. M. and Chow, B. W. Y. 2015. Classmate  characteristics and student achievement in 33 countries. J. of   Educational Psychology, 107, 1, 152-169.   [36] Chiu, M. M. and McBride-Chang, C. 2006. Gender, context,  and reading. Scientific Studies of Reading, 10, 331362.   [37] Baker, D. P., Goesling, B. and Letendre, G. K. 2002.  Socioeconomic status, school quality, and national economic   development. Comparative Education Review, 46, 291-312.   [38] Chiu, M. M. and Klassen, R. M. 2009. Calibration of reading  self-concept and reading achievement among 15-year-olds.   Learning and Individual Differences, 19, 372-386.   [39] Thapa, A., Cohen, J., Guffey, S. and Higgins-DAlessandro,  A. 2013. A review of school climate research. Review of   Educational Research, 83, 3, 357-385.   [40] Winters, M. A., Haight, R. C., Swaim, T. T. and Pickering,  K. A. 2013. The effect of same-gender teacher assignment on   student achievement in the elementary and secondary   grades. Economics of Education Review, 34, 69-75.   [41] Martin, M. O., Mullis, I. V. S. and Kennedy, A. M. 2003.  PIRLS 2001 technical report. TIMSS & PIRLS International   Center, Boston College, Chestnut Hill, MA.   [42] World Bank. 2007. The world development report 2006.  Oxford University Press, New York, NY.   [43] Vygotsky, L. S. 2014. Genesis of the higher mental  functions. In P. Light, S. Sheldon, M. Woodheads (Eds.)   Learning to think (pp. 35-45). Routledge, New York, NY.      "}
{"index":{"_id":"14"}}
{"datatype":"inproceedings","key":"Gadiraju:2017:ILT:3027385.3027402","author":"Gadiraju, Ujwal and Dietze, Stefan","title":"Improving Learning Through Achievement Priming in Crowdsourced Information Finding Microtasks","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"105--114","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027402","doi":"10.1145/3027385.3027402","acmid":"3027402","publisher":"ACM","address":"New York, NY, USA","keywords":"achievement priming, crowd workers, crowdsourcing, information finding, learning, microtasks, retention","Abstract":"Crowdsourcing has become an increasingly popular means to acquire human input on demand. Microtask crowdsourcing market-places facilitate the access to millions of people (called workers) who are willing to participate in tasks in return for monetary rewards or other forms of compensation. This paradigm presents a unique learning context where workers have to learn to complete tasks on-the-fly by applying their learning immediately through the course of tasks. However, most workers typically dropout early in large batches of tasks, depriving themselves of the opportunity to learn on-the-fly through the course of batch completion. By doing so workers squander a potential chance at improving their performance and completing tasks effectively. In this paper, we propose a novel method to engage and retain workers, to improve their learning in crowdsourced information finding tasks by using achievement priming. Through rigorous experimental findings, we show that it is possible to retain workers in long batches of tasks by triggering their inherent motivation to achieve and excel. As a consequence of increased worker retention, we find that workers learn to perform more effectively, depicting relatively more stable accuracy and lower task completion times in comparison to workers who drop out early.","pdf":"Improving Learning through Achievement Priming in Crowdsourced Information Finding Microtasks  Ujwal Gadiraju L3S Research Center  Leibniz Universitat Hannover Appelstr. 9a, Hannover, Germany  gadiraju@L3S.de  Stefan Dietze L3S Research Center  Leibniz Universitat Hannover Appelstr. 9a, Hannover, Germany  dietze@L3S.de  ABSTRACT Crowdsourcing has become an increasingly popular means to ac- quire human input on demand. Microtask crowdsourcing market- places facilitate the access to millions of people (called workers) who are willing to participate in tasks in return for monetary re- wards or other forms of compensation. is paradigm presents a unique learning context where workers have to learn to complete tasks on-the-y by applying their learning immediately through the course of tasks. However, most workers typically dropout early in large batches of tasks, depriving themselves of the opportunity to learn on-the-y through the course of batch completion. By doing so workers squander a potential chance at improving their performance and completing tasks eectively. In this paper, we propose a novel method to engage and retain workers, to improve their learning in crowdsourced information nding tasks by using achievement priming. rough rigorous experimental ndings, we show that it is possible to retain workers in long batches of tasks by triggering their inherent motivation to achieve and excel. As a consequence of increased worker retention, we nd that workers learn to perform more eectively, depicting relatively more sta- ble accuracy and lower task completion times in comparison to workers who drop out early.  CCS CONCEPTS Applied computing Education; Information systems World Wide Web; Human-centered computing Human com- puter interaction (HCI);  KEYWORDS Crowdsourcing, Microtasks, Learning, Retention, Crowd Workers, Information Finding, Achievement Priming  ACM Reference format: Ujwal Gadiraju and Stefan Dietze. 2016. Improving Learning through Achievement Priming in Crowdsourced Information Finding Microtasks. In Proceedings of ACM LAK Conference, Vancouver, BC, Canada, March 13-17, 2017 (LAK17), 10 pages. DOI: hp://dx.doi.org/10.1145/3027385.3027402  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. LAK17, Vancouver, BC, Canada  2017 ACM. 978-1-4503-4870-6/17/03. . .$$15.00 DOI: hp://dx.doi.org/10.1145/3027385.3027402  Figure 1: Typical task consumption in long batches of tasks, depicting low worker retention rate (amount of work on the y-axis, distinct workers on the x-axis). Very few workers complete all the available tasks in a batch, thereby maximizing the opportunity to learn through the course of the batch. Most workers drop out aer completing only a few tasks in a long batch.  1 INTRODUCTION In a technologically advanced world today, there are still several problems that cannot be solved by machines alone and require human intervention. In the last decade crowdsourcing has emerged as an eective paradigm that enables access to human intelligence at scale, thereby playing a pivotal role in hybrid man-machine systems. Paid crowdsourcing platforms provide a means to reach millions of people around the world (called crowd workers), creating endless opportunities to acquire human input at scale in return for monetary rewards. Amazons Mechanical Turk (AMT1) and CrowdFlower2 are popular examples of microtask crowdsourcing platforms, where requesters3 can easily access labor on demand and leverage the prevalent wisdom and skills to satisfy varying requirements. e principle categories of crowdsourced work on such platforms are diverse; information nding, verication and validation, interpretation and analysis, content creation, content access and surveys [15].  Understanding various aspects of crowd workers such as their motivations, behavior, and capabilities has been pivotal in building crowd-powered systems that are ecient (in terms of costs incurred) and eective (in terms of quality of the results produced) [16, 24]. Crowd workers have been shown to participate in crowdsourcing 1hps://www.mturk.com/ 2hps://www.crowdower.com/ 3Requesters are people who deploy tasks on crowdsourcing platforms to gather re- sponses from crowd workers.  https://www.mturk.com/ https://www.crowdflower.com/   LAK17, March 13-17, 2017, Vancouver, BC, Canada Ujwal Gadiraju and Stefan Dietze  due to various intrinsic and extrinsic motivations; to earn their primary livelihood or as a secondary source of income [22], for community or enjoyment related motivations [2, 3], or due to the meaningfulness of their contributions [6]. Inherent human factors play an important role in the dynamics of crowd work on microtask platforms [25, 34].  Previous work in technology enhanced learning has brought to fore the unique learning environment that characterizes the crowd- sourcing paradigm [14]. Authors reected on the short-lived learn- ing phase and the immediate application of learned concepts in crowdsourcing microtasks. While it is a general notion that crowd- sourcing microtasks are short and can be completed quickly, a recent analysis of tasks on AMT over a ve year period (from 2009- 14) has shown that a majority of tasks are deployed in large batches [9]. e authors showed that such large batches of repetitive tasks are more aractive to crowd workers compared to shorter batches. However, it was found that long batches tend to starve towards the end; with a fewer number of available tasks to complete in such batches, fewer workers choose to contribute. Retaining workers in tasks for as long as possible is useful since workers who gain experience through the course of task completion become more eective. Empirical evidence supports the notion that workers who complete large batches of tasks exhibit a tendency to learn through the course of batch completion [8]. is suggests that by retaining workers longer in microtasks of a given type, it is possible to induce learning, resulting in improved worker performance over time. In this paper, we tackle the problem of retaining crowd workers in large batches of microtasks to facilitate learning among workers.  We aim to improve worker retention in the real-world crowd- sourcing microtask category of information nding. Information nding tasks were found to be on the rise on AMT in a recent data-driven study [9]. Some of such typical tasks that were de- ployed on AMT in the past include nding contact details on a list of websites, email addresses of a series of people, TV-shows listed on dierent channels, and so forth. Oen such tasks involve large batches where workers need to adopt and repeat the same workow to accomplish the objectives. We explore the potential of triggering the motivation to achieve among crowd workers, with an aim to improve their retention in crowdsourcing tasks and thereby facilitate learning. To this end, we investigate the applicability of achievement priming to retain workers in information nding tasks. We measure the eectiveness of our approach in terms of (i) the worker retention rate, dened by the average number of tasks that workers complete in a given batch of available tasks, (ii) worker per- formance, i.e., the accuracy with which workers complete the tasks, and (iii) the worker learning rate, that describes the average change in worker performance through the course of batch completion.  2 RESEARCH QUESTIONS Several studies have armed that the primary motivation of most workers to participate in crowd work is to earn monetary rewards that can contribute to their income and livelihood [15, 22]. We also know that crowd workers aim to maintain a high level of accuracy in their work and a good reputation to access more tasks, thereby maximizing their earnings [23]. We build on this understanding of the crowd workers general aitude to accomplish good work, and  investigate the potential of stimulating their motivation to elicit good quality and sustained work.  Achievement Motivation. Achievement motivation can be dened as the need for success or aainment of excellence [32]. A number of prior works in psychology have studied achievement motivation. McClelland rst proposed that achievement motivation may be understood as a motivational process that involves the regulation of dierent social goals [28]. is was supported by more recent research corresponding to goal pursuits [35]. In this paper, we adopt the understanding of achievement motivation proposed by Hart and Albarracin [20]; the goal to achieve is an alternative to the goal to have fun or indulge in leisurely activity. e authors show that people choose to pursue excellence (at the expense of having fun) or pursue fun (at the expense of achieving) depending on their level of chronic achievement motivation (the amount of pleasure in achieving goals). People with high chronic achievement motivation exhibit goal-seeking behavior when they encounter motivational triggers, while those with low chronic achievement motivation exhibit a fun-seeking behavior in such cases.  We aim to address the following research questions with respect to information nding (IF ) tasks. RQ1 How can achievement priming be used to increase worker  retention and facilitate learning in IF tasks RQ2 How is the learning process of workers eected when they  are retained in long batches of IF tasks Based on prior works discussed earlier and crowd worker moti-  vations, we construct the following hypotheses. Hyp-I : Worker Retention On presenting crowd workers with  motivational stimuli in the form of achievement primes, we can improve the worker retention rate and thereby facilitate learning.  Hyp-II : Worker Learning When workers are retained in long batches of information nding tasks, they learn more about the tasks and perform more eectively.  3 RELATED LITERATURE e unconscious manipulation of participants behavior (in the sense that subjects are not aware of being manipulated [21]) by means of semantic or pictorial cues is known as priming to be- havior [1]. A large number of studies have presented empirical evidence that it is possible to positively aect participants achieve- ments in dierent types of tasks by means of verbal ([12, 20]) or pictorial ([36]) cues.  We position and compare our work in this paper to the following two distinct realms of related work.  3.1 Priming in Crowdsourcing Environments Researchers have also studied the impact of aective priming on creativity of people. Lewis et al. presented a method for manip- ulating aect by using pictures [26]. rough their experiments with crowd workers on Amazons Mechanical Turk (AMT), the authors showed that positive aective priming helped to improve the quality of ideas generated in two tasks testing the creativity of workers. Similarly, Morris et al. explored the use of aective priming and aective pre-screening to improve the creativity of crowd workers [29, 30]. In these works, the authors used musical excerpts as postive and negative primes, and found that positively    Improving Learning through Achievement Priming in Crowdsourced Microtasks LAK17, March 13-17, 2017, Vancouver, BC, Canada  (a) Diculty-Level I (level-I) (b) Diculty-Level II (level-II) (c) Diculty-Level III (level-III)  Figure 2: Progressive diculty-levels in the information nding task of nding the middle-names of famous persons.  valenced music can signicantly enhance the creative performance of workers on AMT. Following research that suggested the inuence of individual personality dierences in performance with visual- izations [18], Harrison et al. showed that aective priming can be used to inuence user performance in classic graphical perception tasks [19].  Although these prior works use crowd workers and crowdsourc- ing platforms to show that aective priming can help in improving the performance of workers, the tasks that were used to gauge the impact of aective priming were largely cognitive tests requiring insight and creative problem-solving, that do not suciently corre- spond to the landscape of real-world microtasks. To our knowledge this is the rst work that investigates the applicability of priming techniques in real-world microtask crowdsourcing.  3.2 Learning and Retention in Microtasks In previous work, authors analyzed the task performance and learn- ing outcomes in a real-world classroom setup, showing that ap- propriate learning conditions resulted in an improvement in task performance [10]. Other work proposed the use of crowdsourcing in classrooms to improve the learning process of students by receiv- ing feedback on learning material [11]. In [14], authors introduced crowd workers as learners in a unique learning environment and showed that implicit and explicit training can help to improve the performance of workers in crowdsourced microtasks.  It is known that a majority of crowdsourced microtasks are repet- itive in nature and consist of batches of similar tasks [9]. Working for long periods on such tasks can lead to boredom and fatigue, re- sulting in a potential drop in worker performance and productivity. Previous works have addressed this issue and proposed dierent means to retain and engage workers. Rzeszotarski et al. suggested introducing micro-breaks into workows to refresh workers, and showed that under certain conditions micro-breaks help to retain workers and improve their accuracy slightly [33]. Similarly, Dai et al. proposed to intersperse diversions (small periods of entertainment) to improve worker experience in lengthy, monotonous microtasks and found that such micro-diversions can signicantly improve worker retention rate while maintaining worker performance [7]. Other works proposed the use of gamication to increase worker retention and throughput [13]. Mao et al. studied worker engage- ment, characterized how workers perceive tasks and proposed to predict when workers would stop performing tasks [27]. Difallah et al. introduced pricing schemes to improve worker retention, and showed that paying periodic bonuses according to pre-dened mile- stones has the biggest impact on retention rate of workers [8]. A side eect of workers dropping out early in long batches is the lack of opportunity to facilitate learning among participating workers.  In contrast to these prior works, we aim to improve worker re- tention and learning rate by relying on inherent characteristics of workers (i.e., their level of chronic achievement motivation),  thereby triggering goal-seeking behavior. Moreover, channeling the workers achievement motivation to improve worker engage- ment and retention is a relatively less intrusive approach due to no tangible change in the workow, from a workers standpoint.  4 METHODOLOGY AND SETUP 4.1 Task Design - Information Finding Since there has been a steep rise in information nding tasks on the most popular microtask crowdsourcing platform, Amazons Mechanical Turk (AMT), we consider this type of tasks [9]. We adopt the task of nding the middle-names of famous people, to emulate the workow of real-world information nding microtasks where workers are typically asked to nd contact details, addresses, or names of particular people, organizations or companies. De- pending on the information that is to be searched for on the web, these tasks may comprise of varying diculty. Recent work has shown how task complexity plays an important role in worker per- formance [37]. To account for varying levels of the inherent task diculty in our information nding tasks and to study the impact of task diculty on worker learning rate, we model task diculty objectively into 3 levels, wherein workers need to consider an ad- ditional aspect in each progressively dicult level as shown in Figure 2. In level-I, workers are presented with unique names of famous persons, such that the middle-names can be found using a simple search on Google4 or Wikipedia5. In level-II workers are additionally provided with the profession of the given person. We manually selected the names such that there are at least two dierent individuals with the given names in level-II, and the distinguishing factor that the workers need to rely upon is their profession. In level-III workers are presented names of persons, their profession, and a year during which the persons were active in the given profession. ere are multiple distinct individuals with the given names, associated with the same profession in level-III. e workers are required to identify the accurate middle-name by relying on the year in which the person was active in the given profession.  4.2 Inspiringotes as Achievement Primes In their studies of achievement behavior, Hart and Albarracin used words that emulated achievement-related meanings as achievement primes (such as win, master, achieve, excel and so forth) [20]. In this work, we hypothesize that inspirational quotes about achievement by famous and successful gures can instill a similar priming eect. We manually collected 100 quotes from hp://www. brainyquote.com/ by searching for quotes related to achievement. To pick the quotes which can be considered to emulate inspiration, we deployed a crowdsourcing task on CrowdFlower, gathering 10 4hp://google.com/ 5hp://en.wikipedia.org/  http://www.brainyquote.com/ http://www.brainyquote.com/ http://google.com/ http://en.wikipedia.org/   LAK17, March 13-17, 2017, Vancouver, BC, Canada Ujwal Gadiraju and Stefan Dietze  judgments from distinct workers on a 5-point Likert scale (as shown in Figure 3) on each of the 100 quotes. We awarded workers with 4 USD cents for every 10 quotes that they rated, and controlled for quality by using aention check questions [16]. Based on the average aggregated rating corresponding to each quote, we consider the top 25 quotes as our achievement primes (all with an average rating >= 4.5).  Figure 3: Task to standardize inspiring quotes.  4.3 Measuring Achievement Motivation We measure the level of chronic achievement motivation of workers using the excellence motivation subscale introduced by Cassidy and Lynn to capture ones motivation to pursue standards of excellence [5]. e scale had good internal reliability by means of Cronbachs reliability coecient,  = .71. Workers rate seven questions6 on a 5-point Likert scale ranging from 1:Not at all like me to 5:Ex- tremely like me. We computed the level of chronic achievement motivation for each worker by adding their responses to each of the seven questions aer appropriate reverse coding. Workers with an aggregated score greater than the scales midpoint (21) were consid- ered to be more achievement-oriented, and the remaining workers were considered to be more fun-oriented. We refer to the two groups of workers as the ACHIEVE and FUN groups respectively.  4.4 Study Design We consider the following variations in our studies. (i). Passive Achievement Priming (AP-Passive)  In this seing, crowd workers are presented with the quotes amidst the actual information nding units of the task. Workers do not necessarily have to interact with quotes beyond reading them (see Figure 4). e quotes act as passive achievement primes, and are randomly interspersed among units of the actual task. e order in which the quotes appear is also randomized to prevent ordering bias eects.  Figure 4: A passive achievement prime embedded between two units of the information nding task.  We deployed an initial task consisting of the excellence motiva- tion scale to measure the achievement motivation of workers on CrowdFlower. e task consisted of a few background questions, an aention check question, and seven questions corresponding to measuring their level of achievement motivation. On completion of 6hps://sites.google.com/site/lak2017learning/  Figure 5: An active achievement prime embedded between two units of the information nding task.  this task, workers were provided a link to participate in a follow-up information nding task if they wished (described earlier). In this seing, we gathered responses from a total of 240 distinct workers from the top-level on CrowdFlower. We gathered responses from these workers to additionally analyze and carry forward learnings to the other task setups about (i) the distribution of workers with high and low achievement motivation, (ii) the fraction of workers that would choose to participate in the follow-up task. Of these 240, 106 trustworthy workers7 participated in the follow-up information nding task with embedded passive achievement primes.  (ii). Active Achievement Priming (AP-Active)  In this seing, crowd workers are presented with quotes and are asked to nd the author of the quotes. By modeling the active achievement primes as information nding units, the workers treat these primes as a part of the actual task (the workow in both cases is to search the web to nd either a name or a middlename). ese quotes act as active achievement primes since there is a direct interaction in the workow with these primes. us, the active primes, masked as additional information nding units are randomly interspersed between the units of the actual task where workers are asked to nd the middle-names of people (see Figure 5).  We followed the same setup as described in the previous case of AP-Passive. Here we collected responses from 100 distinct top-level workers in the intial task consisting of the excellence mo- tivation scale deployed on CrowdFlower. ese workers were then presented with an opportunity to proceed onto a follow-up infor- mation nding task (with embedded active achievement primes). 56 trustworthy workers went on to participate in the follow-up task.  (iii). No Priming (NP-Baseline)  To adequately gauge the im- pact of passive and active achievement priming using inspirational quotes in information nding tasks with varying levels of diculty, we also consider the basic seing without any primes. All other task parameters remain the same as in case of passive and active achievement priming.  In this seing, since there were no primes embedded in the actual task, we directly deployed the information nding task on CrowdFlower and acquired responses from a total of 150 distinct top-level workers. Of these, 138 workers were trustworthy and we consider their responses alone in our analysis.  (iv) Randomotes for Achievement Priming (RQ-Passive and RQ-Active)  To verify that any impact on worker retention is due to the inspiring nature of quotes which in turn triggers the chronic achievement motivation of workers, we run two additional experi- ments with exactly the same seings as in case of AP-Passive and  7Trustworthy workers are those who responded correctly to a very simple aention check question.  https://sites.google.com/site/lak2017learning/   Improving Learning through Achievement Priming in Crowdsourced Microtasks LAK17, March 13-17, 2017, Vancouver, BC, Canada  AP-Active primes but using randomly selected quotes unrelated to achievement or inspiration. To this end, we randomly select 100 quotes from hp://www.brainyquote.com/. We deployed a similar task on CrowdFlower as shown in Figure 3 to collect 10 indepen- dent judgments for each quote on a 5-point Likert scale, based on how inspiring workers found these random quotes. Once again, workers were paid 4 USD cents for every 10 quotes that they rated. Here, we chose the top 25 quotes which corresponded to the least aggregated average rating as our primes. From earlier experiments in (i) and (ii), we note that most crowd workers have a high level of chronic achievement motivation. us, we do not dierentiate between high and low level achievement motivation in this setup.  We replicate the task designs of AP-Passive and AP-Active conditions, with the only exception of introducing random quotes as primes. In the RQ-Passive seing, we gathered responses from 100 distinct trustworthy workers. In the RQ-Active seing we gathered judgments from 60 distinct workers, who were also al- lowed to complete as much work as they wished. Of these workers, 58 were found to be trustworthy and are considered for our further analysis.  4.5 Important Design Considerations We completely randomized the order of dierent units as well as the embedded primes (in case of AP-Passive, AP-Active) within the information nding tasks. e dierent variations of tasks were deployed on dierent days and we ensured that there was no overlap in the set of workers that participated across the three variations. To minimize timezone driven worker particpation eects, we deployed the tasks at the same time on the dierent days. Since the primary goal of our work is to improve worker retention and learning rate in information nding tasks, we chose to restrict participation to only the top-level workers on CrowdFlower to ensure reliability of our ndings. By considering three dierent levels of objective diculty of the information nding tasks, we can additionally measure the impact of task diculty on worker dropout rates. It is important to note that in all cases (AP-Passive, AP-Active, NP-Baseline) a worker who entered the information nding task was allowed to complete as much work as she wished to (up to a maximum of 120 units), without any experimentally induced constraints. Due to this reason, we can reliably measure worker retention (or dropout) rates. We compensated workers with 5 USD cents for each set of 10 units that they completed in all the variations. To satisfy minimum wage requisites we paid additional bonuses aer workers either completed the work, or dropped out.  4.6 Measuring Learning Rate As crowd workers proceed to complete the tasks in a given batch, and then move on to complete more batches of the same type, it has been shown that workers learn from their experience and begin to perform beer [8]. We aim to measure the overall worker learning rate across the dierent batches of tasks they complete. Let us consider, B = {b1,b2, ...,bi ,bj , ...,bn } to be the set of available batches of tasks of a given type. In the given task setup, we can assume that the dierence in worker performance from a given batch and that in the preceeding batch of tasks can be aributed to the learning (or a lack of learning) that has occurred. However,  this alone does not dene the rate at which the workers learn. For example, a worker can perform with a 100% accuracy across a sequence of batches while still learning from one batch to the next. is implies that the overall performance of the worker should also feature in measuring a workers learning rate.  Hence, we dene the worker learning rate as a linear combina- tion of the average dierence in worker performance from one batch to the next (called the learning constant, l ), and the overall performance of the worker across the dierent batches. us, we measure the learning rate of workers (denoted by Lr ) by using the following formula:  Lr = 1 n      1<i< (n1) i+1<j<n  (acc j  acci ) + n  k=1 (acck )     (1)  where, n is the number of batches in B that the given worker has completed, i = 1...(n  1), j = (i + 1)...n, k = 1...n, and acci represents the accuracy of a worker in the ith batch of tasks.  Note that the worker learning rate, Lr , can be either positive or negative. A positive learning rate indicates that a worker learns through the course of batch completion and depicts an improvement in performance, while a negative learning rate indicates that a worker does not learn through the course of batch completion and thereby depicts no improvement in performance.  5 RESULTS AND ANALYSIS 5.1 Achievement Motivation in the Crowd Of the 240 workers that responded to the motivation excellence measurement task before entering the follow-up information nd- ing tasks in the AP-Passive condition, 211 workers corresponded to being achievement-oriented (ACHIEVE group) as opposed to fun- oriented (FUN group). Similarly, of the 100 workers that responded to the motivation excellence measurement task before entering the follow-up information nding tasks in the AP-Active condition 90 workers were found to belong to the ACHIEVE group. We believe that these distributions, indicating a high achievement motivation among crowd workers, can be explained by the primary motivation of most workers to participate in crowdsourcing microtasks; to earn and maximize monetary rewards [16], despite typically facing ob- stacles such as unfair pay or rejection of work, power asymmetries [23], and sub-optimal worker environments [17].  5.2 Results in Dierent Priming Conditions (i). Passive Achievement Priming We note that of the 106 trust- worthy workers that participated in the AP-Passive information nding tasks, 99 were found to be achievement-oriented and formed the ACHIEVE group, while 7 workers were found to be fun-oriented and constitute the FUN group.  Workers constituting the ACHIEVE group performed with a greater individual accuracy, depicted higher retention rates and produced a higher overall quality of work in comparison to the workers constituting the FUN group (see Table 1). However, the two groups are unbalanced and the dierences are not statistically signicant. e overall task accuracy is computed by considering  http://www.brainyquote.com/   LAK17, March 13-17, 2017, Vancouver, BC, Canada Ujwal Gadiraju and Stefan Dietze  Table 1: Worker performance, retention, and task comple- tion time (TCT) in AP-Passive information nding tasks.  Achievement Avg. Acc. Worker Overall Avg. TCT Motivation Per Worker (%) Retention Rate (%) Task Acc. (%) (in mins)  ACHIEVE 79.60 36.78 80.96 10.74 FUN 75.85 26.19 74.25 9.60  the accuracy and the amount of work completed by each worker (i.e., the weighted average accuracy of all workers in the task).  (ii). Active Achievement Priming Of the 56 workers that partici- pated in the AP-Active information nding tasks, 53 workers were found to be achievement-oriented and formed the ACHIEVE group, while 3 workers were found to be fun-oriented and constitute the FUN group. Similar to our ndings in AP-Passive information nding tasks, workers in the ACHIEVE group performed with a greater individual accuracy, depicted higher retention rates and produced a higher overall quality of work (see Table 2). ese dierences however, were not found to be statistically signicant.  Table 2: Worker performance, retention, and task comple- tion time (TCT) in AP-Active information nding tasks.  Achievement Avg. Acc. Worker Overall Avg. TCT Motivation Per Worker (%) Retention Rate (%) Task Acc. (%) (in mins)  ACHIEVE 76.98 45.60 80.45 11.41 FUN 65.17 27.78 67 12.02  (iii). No Priming Table 3 presents our ndings with respect to worker performance, retention rate, and task completion time in the case where no primes are embedded in the information nding tasks, in comparison to the AP-Passive and AP-Active variations.  Figure 6 draws a comparison between the worker retention curves in each of the dierent priming conditions, where workers can submit a minimum of 10 units and a maximum of 120 units, i.e, the entire batch. rough multiple t-tests and Bonferroni correction, we note that there is a signicant improvement in the worker reten- tion rate (over 8%) in case of the AP-Active (M=44.64, SD=35.97 ) seing when compared to the NP-Baseline (M=36.23, SD=32.09) seing; t (192) = 1.735,p < .05, and Hedges g=.3. In addition, AP- Active corresponds to a higher retention rate when compared to AP-Passive (M=36.08, SD=31.53); t (160) = 1.698,p < .05. We did not observe a signicant dierence in the worker retention rate be- tween AP-Passive and NP-Baseline. e average task completion time of workers is signicantly lesser in case of the NP-Baseline (M=9.11, SD=4.28) seing in comparison to AP-Passive (M=10.67, SD=5.19); (t (242) = 2.551, p<.05), and also in comparison to AP- Active (M=11.44, SD=4.8); t (192) = 3.309,p < .05.  5.3 Do inspiring quotes matter We analyzed the performance of workers in the presence of ran- dom quotes as primes, and our ndings are presented in the Ta- ble 3. rough multiple t-tests and Bonferroni correction, we found that in the RQ-Passive seing, the worker retention rate (M=28.58, SD=24.22) is signicantly lesser than in AP-Active (M=44.64, SD=35.97); [t(154)= 3.321, p < .01], AP-Passive (M=36.08, SD=31.53); [t(204)= 1.907, p < .05], and NP-Baseline (M=36.23, SD=32.09); [t(236)= 2.01, p < .05]. In the RQ-Active seing,   0   10   20   30   40   50   60   70   80   90   100  10 20 30 40 50 60 70 80 90 100 110 120  Pe rc  en ta  ge  o  f W or  ke rs  No. of Units Completed Before Dropping Out  AP-Passive AP-Active  NP-Baseline  Figure 6: A comparison of the worker retention curves in each of the dierent priming conditions.  Table 3: Worker performance, retention, and average task completion time (TCT) in the information nding tasks with dierent priming conditions.  Priming Avg. Acc. Worker Overall Avg. TCT Variation Per Worker (%) Retention Rate (%) Task Acc. (%) (in mins)  NP-Baseline 76.71 36.23 78.88 9.11* AP-Passive 77.72 36.08 77.60 10.67 AP-Active 76.35 44.64* 79.96 11.44 RQ-Passive 78.74 28.58 77.58 9.72 RQ-Active 74.81 32.61 75.41 9.87  the worker retention rate is 32.61%. is was found to be sig- nicantly lesser than the retention rate in the AP-Active condi- tion; t(112)=1.892, p < .05. We did not nd signicant dierences between the worker retention rate in RQ-Active and either AP- Passive or NP-Baseline. We also did not nd signicant dier- ences in the average worker accuracy and the overall task accuracy between each of the three priming conditions with respect to RQ- Passive and RQ-Active.  Based on our ndings, we can conrm that the improvement in the worker retention rate observed in the AP-Active seing can be aributed to the inspiring nature of quotes which act as achievement primes and trigger the intrinsic achievement motiva- tion of workers. Randomly selected quotes which are not inspiring in nature fail to have the same eect.  5.4 Eects of Task Diculty Recent work has shown the impact of intertask eects on the quality of responses from crowd workers [31]. e order of microtasks, especially with respect to their complexity, was shown to have a signicant impact on the quality of work produced [4]. us, it was important to control for intertask eects by randomizing the order of units, and doing so further with respect to their level of diculty. In this section, we describe our investigation of the impact of task diculty on worker accuracy and on worker retention rate.  To understand the inuence of task diculty on worker dropout, we illustrate the average fraction of information nding tasks that workers completed with respect to each of the three diculty levels (before dropping out) in Figure 7. Note that every worker that com- pleted all the 120 units in each of the seings, completed an equal fraction of 40 units corresponding to each level of diculty. We refer to such workers as nishers, and the fraction of participating workers who complete all the 120 units as the nisher rate. We    Improving Learning through Achievement Priming in Crowdsourced Microtasks LAK17, March 13-17, 2017, Vancouver, BC, Canada  (a) NP-Baseline (b) AP-Active (c) AP-Passive  Figure 8: Average accuracy of workers in each of the short,medium, and long groups through the course of batch completion in the dierent priming conditions. Note that each batch (from B1 through B12) consisted of 10 tasks.  0  25  50  75  100  SHORT MEDIUM LONG Worker Groups  Ac cu  ra cy LONG  MEDIUM  SHORT  (a) NP-Baseline  25  50  75  100  SHORT MEDIUM LONG Worker Groups  Ac cu  ra cy LONG  MEDIUM  SHORT  (b) AP-Active  0  25  50  75  100  SHORT MEDIUM LONG Worker Groups  Ac cu  ra cy LONG  MEDIUM  SHORT  (c) AP-Passive  Figure 9: Overall accuracy per worker and each of the short,medium, and long groups across the dierent priming conditions. e y-axis presents the accuracy per worker in percentage.   0   10   20   30   40   50   60   70  NP-Baseline AP-Active AP-PassiveA vg  . F ra  ct io  n  of   U ni  ts  P  er  W  or ke  r( in   % )  Task Difficulty Levels  Level-I Difficulty Level-II Difficulty Level-III Difficulty  Dropout Rate (in %) Finisher Rate (in %)  Figure 7: e average fraction of units with a given di- culty that workers encountered in dierent settings until they dropped out, corresponding dropout and nisher rates.  observe the highest nisher rate (14.29%) and the least dropout rate (55.36%) in the AP-Active seing.  Table 4: Average accuracy of workers on units with varying levels of diculty in dierent priming conditions. * indi- cates statistical signicance at p < .05, ** at p < .01.  Priming Diculty Diculty Diculty Condition Level-I Level-II Level-III  NP-Baseline 84.79* 79.28 66.46 AP-Passive 88.86** 88.77** 65.55 AP-Active 77.82 83.18 69.86 Overall 83.82 83.75 67.29  Table 4 presents the average accuracy of workers on units with a given diculty level across the dierent priming conditions. To  assess the relationship between the level of diculty of the infor- mation nding tasks (modeled as an objective quantitative variable) and the average accuracy of workers across the dierent conditions, we computed Pearson r. We found a moderate negative correlation between the diculty level and the average accuracy of workers, r(894) = .30, R2 = .09, p < .001.  To assess the interaction eects between the priming condition and the task diculty on worker accuracy and retention rate, we computed a two-way MANOVA. is revealed a statistically sig- nicant interaction eect between the task diculty and priming condition on the dependent variables (worker accuracy and reten- tion rate); F(4,887)=2.14, p<.05; Wilks  = 0.98, 2p = .01.  Given the signicance of the overall test, the univariate main eects were examined for the worker accuracy and retention rate. We found a signicant dierence in the worker accuracies across the diculty levels; F(2,887)=58.51, p<.001, but not across the dier- ent priming conditions; F(2,887)=2.82, p=.06. In contrast, we found a signicant dierence in worker retention rates across the dierent priming conditions; F(2,887)=3.48, p<.05, but not across the di- culty levels; F(2,887)=1.85, p=.15. is was conrmed by post-hoc comparisons using the Tukey HSD test. Our ndings reveal the signicant dierences in impact of the dierent priming conditions on worker retention rates.  5.5 Learning rough the Batches To analyze the learning that workers exhibit through the course of task completion, we investigated their performance. We divided workers into three groups based on the number of units completed (similar to [8]); the short group consists of workers who completed 25% or less units, the medium group consists of workers who com- pleted more than 25% and less than or equal to 75% of the units, and    LAK17, March 13-17, 2017, Vancouver, BC, Canada Ujwal Gadiraju and Stefan Dietze  the long group consists of workers who completed more than 75% of the units. Figure 10 presents the distribution of workers across the groups in each of the priming conditions. We note that the distributions are similar in case of AP-Passive and NP-Baseline. In case of AP-Active, we found that more workers belonged to the long group than medium, indicating the retention induced by the active achievement priming.   0   10   20   30   40   50   60   70  NP-Baseline AP-Active AP-Passive  Nu m  be r o  f W or  ke rs   in  th  e  Gr  ou p   (in  %  )  Different Priming Conditions  Short Medium Long  Figure 10: Distribution of workers into short, medium, and long groups as per the amount of work completed.  Since workers completed tasks in batches of 10 units, we ana- lyzed their accuracy across the batches (from B1 through a max- imum of B12, as there were 120 available units in total). Figure 8 presents the average accuracy of workers belonging to the short, medium and long groups across the batches of tasks in each of the priming conditions. We observe that in case of the NP-Baseline condtion, the average accuracy of workers tends to drop through the course of the batches across all three groups. In contrast, we note that there is a steady increase in the average accuracy of work- ers in the long group in case of the AP-Passive condition. We analyzed the overall accuracy of each worker in the groups, and our ndings are presented in Figure 9. We note that the average ac- curacy of workers does not vary across each of the three groups or the dierent priming conditions. However, in case of AP-Active, as workers complete more tasks the standard deviation in their accuracy becomes lower.  is is consistent with prior work, where authors found that workers who went on to complete more tasks in the best pricing scheme depicted an improvement in accuracy through the course of the tasks [8]. is suggests the impact of the active and passive priming conditions in retaining and engaging workers, enabling them to learn and apply learned concepts eciently through the course of long batches.  5.6 Worker Learning Rate We computed the learning rate of workers in the dierent priming conditions using the formula in Equation 1. Figure 11 presents our ndings with respect to the learning rate of workers in each of the short, medium and long groups across the priming conditions.  We found that the learning rate of workers is maximum in case of workers constituting the short group, and this gradually decreases in case of the medium group, and further in case of the long group (see Table 5). is can be explained intuitively by the fact that when workers begin a rst batch of new tasks of a given type, there is more to learn through the course of batch completion. Hence, those workers who complete only 25% of the tasks available, exhibit the  0.0  2.5  5.0  7.5  10.0  Short Medium Long Worker Groups  Le ar  nin g   Ra te   o f W  or ke  rs  NP-Baseline AP-Active AP-Passive  Figure 11: Learning Rate of workers in short, medium and long groups in the dierent priming conditions (stacked).  Table 5: Learning Rate of workers in the dierent groups across the priming conditions (* indicates signicance).  NP-Baseline AP-Active AP-Passive  Short 2.96 3.43* 3.15 Medium 1.28 1.30 1.55* Long 0.69 0.64 0.73 Overall Avg. 1.64 1.79 1.81  most learning rate on average. As workers proceed through towards the completion of all batches available (i.e., workers who constitute the medium and long groups), due to their learning progress, the learning rate gradually decreases.  0  5  10  15  Short Medium Long Worker Groups  Av g.   T as  k C om  ple tio  n  Ti  m  (i  n  m  ins )  NP-Baseline AP-Active AP-Passive  Figure 12: Task Completion Time (TCT) of workers in short, medium and long groups in dierent priming conditions.  e gradual decrease in learning rate through the course of batches coincides the eectiveness with which workers complete tasks within the batches. Figure 12 presents the average task com- pletion time (TCT) of the three groups of workers in the dierent priming conditions. is supports our hypothesis (Hyp-II) that with an increase in worker retention, workers learn to perform more eectively in information nding microtasks, i.e., worker accuracy stabilizes as seen in Figure 9 and the TCT of workers decreases.    Improving Learning through Achievement Priming in Crowdsourced Microtasks LAK17, March 13-17, 2017, Vancouver, BC, Canada  6 DISCUSSION In our experiments, by using the excellence motivation scale we ob- served that a vast majority of crowd workers belong to theACHIEVE group, i.e., they exhibit a prioritization of achievement over hav- ing fun, in contrast to the FUN group. is observation can be explained from two complementary standpoints. It is a well under- stood notion that crowd work can be tedious, monotonous and is oen rewarded with meager pay. It is in these circumstances that crowd workers seek to earn monetary rewards, indicating an inher- ent motivation and a will to excel. We enforced the CrowdFlower lter of recruiting the highest quality crowd workers. For these workers to consistently complete several tasks while maintaining a high accuracy (leading to their highest quality level qualica- tion on CrowdFlower8), we argue that it takes more than skill and competence, indicating further motivation. Although we found that workers belonging to the ACHIEVE group depicted beer per- formance (accuracy and retention rate) in comparison to the FUN group, due to the highly skewed sample sizes the dierences were not statistically signicant. is however, means that a majority of crowd workers can be positively motivated and retained in long batches of information nding tasks, enabling them to learn and improve their performance.  Based on our experimental ndings, we observe a signicant impact of active achievement priming on worker retention in infor- mation nding tasks. e AP-Active priming condition leads to an improvement of over 8% in worker retention when compared to the NP-Baseline and AP-Passive conditions. is supports our hypothesis that achievement priming can improve worker retention in information nding tasks (Hyp-I). However, we note that the laer conditions correspond to a faster task completion time. is can be explained by the fact that (i) in the AP-Passive condition, workers dont necessarily have a direct interaction with the primes and can proceed in the tasks by ignoring them, and (ii) in the NP- Baseline condition workers do not encounter primes; they neither have to read quotes nor nd author names that otherwise constitute additional units of work. We also found that the improvement in worker retention in the AP-Active priming condition was due to the inspiring quotes stimulating the inherent achievement moti- vation among workers. Worker retention rate deteriorates when using random primes that are not inspiring, thereby serving only as a distraction to the workers.  e AP-Active priming condition led to an improvement in the average learning rate of workers by nearly 8.5% compared to the NP-Baseline. Similarly, the AP-Passive priming condition improved the worker learning rate by nearly 10.5%.  By acknowledging the fact that information nding tasks that are typically crowdsourced have varying levels of diculty, we in- vestigated and found signicant eects of task diculty on worker accuracy. However, we found task diculty did not eect the worker retention rate. Worker retention rate was eected by the priming conditions signicantly, notably by the AP-Active con- dition. e improvement in worker retention rate due to active achievement priming using inspirational quotes and the impact on  8hp://crowdowercommunity.tumblr.com/post/80598014542/ introducing-contributor-performance-levels  work quality (wherein workers who complete more work improve their accuracy) support Hyp-II.  6.1 Caveats and Limitations We investigated the average number of primes that workers en- counter in each batch of 10 units. We found that in both conditions there were 2 primes on average across each batch of 10 units, with SD=.55 in case of AP-Passive and SD=.44 in case of AP-Active, indicating no bias due to the frequency of primes.  In our experiments, we considered a pool of 25 (active or passive) achievement primes that were randomly distributed over a batch of 120 units. We found that 2 primes in each set of 10 units on average triggered workers suciently (in AP-Active condition) to complete more work in the batch. However, further experiments are required to draw conclusions regarding the optimal frequency of achievement primes across the batch of units to maximize worker retention. We took care to ensure that workers were unaware of the achievement primes that are embedded into the actual task. We aempted to achieve this by integrating the primes into the design and not disrupting the workow in case of AP-Passive, and by creating the same interaction with the active prime as with other units of work in the AP-Active condition.  Our measure for worker learning rate is simplistic, in that we do not consider external factors such as worker fatigue and boredom, previous experiences with similar tasks and so forth, that may eect the learning rate of workers.  6.2 Ethical Considerations We must consider the ethical implications of using achievement priming in crowdsourcing microtasks. Achievement primes should not just be used to increase worker retention rates, but also with an aim to help improve the workers performance and widen the corresponding monetary opportunities and returns. If achievement primes are used to retain workers for relatively longer periods of time, workers should be adequately compensated. Paying heed to the ethical aspects of design and compensation with respect to achievement priming, we believe that in the digitally immersive current age, we are inadvertently but constantly primed by several aspects around us. It is the duty of task requesters to ensure that workers are not adversely aected due to irresponsible task design. However, due to the short-lived eects of priming we believe that if achievement priming is used responsibly it can be a useful means to improve crowd work and positively eect task consumption in crowdsourcing marketplaces.  7 CONCLUSIONS AND FUTUREWORK In this paper, we investigated the use of achievement priming tech- niques to improve worker retention and learning rate in crowd- sourced information nding tasks. We found that a vast majority of workers who participated in our tasks from the highest quality level on CrowdFlower are driven by achievement as opposed to fun. us, we proposed the use of inspirational quotes from famous peo- ple as achievement primes, and showed that active interaction with these primes in an inadvertent manner within information nding tasks led to a signicant improvement in the worker retention rate (over 8% on comparison to the baseline method devoid of primes).  http://crowdflowercommunity.tumblr.com/post/80598014542/introducing-contributor-performance-levels http://crowdflowercommunity.tumblr.com/post/80598014542/introducing-contributor-performance-levels   LAK17, March 13-17, 2017, Vancouver, BC, Canada Ujwal Gadiraju and Stefan Dietze  Further investigation of work quality between groups of workers who completed varying amounts of work in dierent priming con- ditions, revealed that workers who encountered active achievement primes show an improvement in their accuracies during the course of the tasks and exhibit relatively more stable performance (low standard deviation). We proposed worker learning rate as a metric to measure the learning that occurs when workers complete tasks or batches of tasks of a given type. We found that in addition to improving worker retention, achievement priming improves the learning rate of workers (thereby answering RQ1, RQ2).  Our ndings and the novel method for improving learning in in- formation nding tasks enrich the current understanding of crowd work and structuring workow. In the imminent future, we will investigate the use of achievement priming in other domains of crowd work. We also aim to investigate how task framing can inuence goal prioritization of crowd workers.  Acknowledgments is research has been supported in part by the European Commis- sion within the H2020-ICT-2015 Programme (AFEL project, Grant Agreement No. 687916), and the Erasmus+ Agreement 2015-1-LI01- KA203-000041 (Learning Analytics and Learning Process Manage- ment for Small Size Organisations in Higher Education). We also thank Peter Holtz for his valuable advise and feedback from the Psychology perspective.  REFERENCES [1] John A Bargh, Peter M Gollwitzer, Annee Lee-Chai, Kimberly Barndollar, and  Roman Trotschel. 2001. e automated will: nonconscious activation and pursuit of behavioral goals. Journal of personality and social psychology 81, 6 (2001), 1014.  [2] Daren C Brabham. 2008. Moving the crowd at iStockphoto: e composition of the crowd and motivations for participation in a crowdsourcing application. First monday 13, 6 (2008).  [3] Daren C Brabham. 2010. Moving the crowd at readless: Motivations for participation in a crowdsourcing application. Information, Communication & Society 13, 8 (2010), 11221145.  [4] Carrie J Cai, Shamsi T Iqbal, and Jaime Teevan. 2016. Chain reactions: e impact of order on microtask chains. In Proceedings of the 34th Annual ACM Conference on Human Factors in Computing Systems (CHI16). ACM, Vol. 6.  [5] Tony Cassidy and Richard Lynn. 1989. A multifactorial approach to achieve- ment motivation: e development of a comprehensive measure. Journal of Occupational Psychology (1989).  [6] Dana Chandler and Adam Kapelner. 2013. Breaking monotony with meaning: Motivation in crowdsourcing markets. Journal of Economic Behavior & Organi- zation 90 (2013), 123133.  [7] Peng Dai, Jerey M Rzeszotarski, Praveen Paritosh, and Ed H Chi. 2015. And now for something completely dierent: Improving crowdsourcing workows with micro-diversions. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. ACM, 628638.  [8] Djellel Eddine Difallah, Michele Catasta, Gianluca Demartini, and Philippe Cudre- Mauroux. 2014. Scaling-up the crowd: Micro-task pricing schemes for worker retention and latency improvement. In Second AAAI Conference on Human Com- putation and Crowdsourcing.  [9] Djellel Eddine Difallah, Michele Catasta, Gianluca Demartini, Panagiotis G Ipeiro- tis, and Philippe Cudre-Mauroux. 2015. e dynamics of micro-task crowdsourc- ing: e case of amazon mturk. In Proceedings of the 24th International Conference on World Wide Web. ACM, 238247.  [10] Son Do-Lenh, Patrick Jermann, Sebastien Cuendet, Guillaume Zuerey, and Pierre Dillenbourg. 2010. Task performance vs. learning outcomes: a study of a tangible user interface in the classroom. In European Conference on Technology Enhanced Learning. Springer, 7892.  [11] Steven Dow, Elizabeth Gerber, and Audris Wong. 2013. A pilot study of using crowds in the classroom. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 227236.  [12] Stefan Engeser. 2009. Nonconscious activation of achievement goals: moderated by word class and the explicit achievement motive Swiss Journal of Psychology  68, 4 (2009), 193200. [13] Oluwaseyi Feyisetan, Elena Simperl, Max Van Kleek, and Nigel Shadbolt. 2015.  Improving paid microtasks through gamication and adaptive furtherance in- centives. In Proceedings of the 24th International Conference on World Wide Web. ACM, 333343.  [14] Ujwal Gadiraju, Besnik Fetahu, and Ricardo Kawase. 2015. Training Workers for Improving Performance in Crowdsourcing Microtasks. In Design for Teaching and Learning in a Networked World. Springer, 100114.  [15] Ujwal Gadiraju, Ricardo Kawase, and Stefan Dietze. 2014. A taxonomy of mi- crotasks on the web. In Proceedings of the 25th ACM conference on Hypertext and social media. ACM, 218223.  [16] Ujwal Gadiraju, Ricardo Kawase, Stefan Dietze, and Gianluca Demartini. 2015. Understanding Malicious Behavior in Crowdsourcing Platforms: e Case of Online Surveys. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI 2015, Seoul, Republic of Korea, April 18-23, 2015. 16311640.  [17] Neha Gupta, David Martin, Benjamin V Hanrahan, and Jacki ONeill. 2014. Turk- life in India. In Proceedings of the 18th International Conference on Supporting Group Work. ACM.  [18] Lane Harrison, Ronald Chang, and Aidong Lu. 2012. Exploring the impact of emotion on visual judgement. In Visual Analytics Science and Technology (VAST), 2012 IEEE Conference on. IEEE, 227228.  [19] Lane Harrison, Drew Skau, Steven Franconeri, Aidong Lu, and Remco Chang. 2013. Inuencing visual judgment through aective priming. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 29492958.  [20] William Hart and Dolores Albarracn. 2009. e eects of chronic achievement motivation and achievement primes on the activation of achievement and fun goals. Journal of Personality and Social Psychology 97, 6 (2009), 1129.  [21] E Tory Higgins and John A Bargh. 1987. Social cognition and social perception. Annual review of psychology 38, 1 (1987).  [22] Panagiotis G Ipeirotis, Foster Provost, and Jing Wang. 2010. ality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD workshop on human computation. ACM.  [23] Lilly C Irani and M Silberman. 2013. Turkopticon: interrupting worker invisibility in amazon mechanical turk. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 611620.  [24] Nicolas Kaufmann, imo Schulze, and Daniel Veit. 2011. More than fun and money. Worker Motivation in Crowdsourcing-A Study on Mechanical Turk.. In AMCIS, Vol. 11. 111.  [25] Walter S Lasecki, Samuel C White, Kyle I Murray, and Jerey P Bigham. 2012. Crowd memory: Learning in the collective. arXiv preprint arXiv:1204.3678 (2012).  [26] Sheena Lewis, Mira Dontcheva, and Elizabeth Gerber. 2011. Aective computa- tional priming and creativity. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 735744.  [27] Andrew Mao, Ece Kamar, and Eric Horvitz. 2013. Why stop now predicting worker engagement in online crowdsourcing. In First AAAI Conference on Human Computation and Crowdsourcing.  [28] David C McClelland. 1965. Toward a theory of motive acquisition. American psychologist 20, 5 (1965), 321.  [29] Robert R Morris, Mira Dontcheva, Adam Finkelstein, and Elizabeth Gerber. 2013. Aect and creative performance on crowdsourcing platforms. In Aective computing and intelligent interaction (ACII), 2013 humaine association conference on. IEEE, 6772.  [30] Robert R Morris, Mira Dontcheva, and Elizabeth M Gerber. 2012. Priming for beer performance in microtask crowdsourcing environments. Internet Computing, IEEE 16, 5 (2012).  [31] Edward Newell and Derek Ruths. 2016. How One Microtask Aects Another. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM.  [32] Sco T Rabideau. 2005. Eects of achievement motiva- tion on behavior. Retrieved from Personality Research: hp://www.personalityresearch.org/papers/rabideau.htm (2005).  [33] Jerey M Rzeszotarski, Ed Chi, Praveen Paritosh, and Peng Dai. 2013. Inserting micro-breaks into crowdsourcing workows. In First AAAI Conference on Human Computation and Crowdsourcing.  [34] Harini Alagarai Sampath, Rajeev Rajeshuni, Bipin Indurkhya, Saraschandra Karanam, and Koustuv Dasgupta. 2013. Eect of Task Presentation on the Performance of Crowd WorkersA Cognitive Study. In First AAAI Conference on Human Computation and Crowdsourcing.  [35] James Y Shah and Arie W Kruglanski. 2008. e challenge of change in goal systems. e Handbook of Motivation Science (2008), 217229.  [36] Amanda Shantz and Gary Latham. 2011. e eect of primed goals on employee performance: Implications for human resource management. Human Resource Management 50, 2 (2011), 289299.  [37] Jie Yang, Judith Redi, Gianluca Demartini, and Alessandro Bozzon. 2016. Mod- eling Task Complexity in Crowdsourcing. In Proceedings of e Fourth AAAI Conference on Human Computation and Crowdsourcing. AAAI.    Abstract  1 Introduction  2 Research Questions  3 Related Literature  3.1 Priming in Crowdsourcing Environments  3.2 Learning and Retention in Microtasks   4 Methodology and Setup  4.1 Task Design - Information Finding  4.2 Inspiring Quotes as Achievement Primes  4.3 Measuring Achievement Motivation  4.4 Study Design  4.5 Important Design Considerations  4.6 Measuring Learning Rate   5 Results and Analysis  5.1 Achievement Motivation in the Crowd  5.2 Results in Different Priming Conditions  5.3 Do `inspiring' quotes matter  5.4 Effects of Task Difficulty  5.5 Learning Through the Batches  5.6 Worker Learning Rate   6 Discussion  6.1 Caveats and Limitations  6.2 Ethical Considerations   7 Conclusions and Future Work  References   "}
{"index":{"_id":"15"}}
{"datatype":"inproceedings","key":"Aghababyan:2017:EAM:3027385.3027388","author":"Aghababyan, Ani and Lewkow, Nicholas and Baker, Ryan","title":"Exploring the Asymmetry of Metacognition","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"115--119","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027388","doi":"10.1145/3027385.3027388","acmid":"3027388","publisher":"ACM","address":"New York, NY, USA","keywords":"achievement, big data, confidence, discipline difference, learnign analytics, metacognition, performance","Abstract":"People in general and students in particular have a tendency to misinterpret their own abilities. Some tend to underestimate their skills, while others tend to overestimate them. This paper investigates the degree to which metacognition is asymmetric in real-world learning and examines the change of a students' confidence over the course of a semester and its impact on the students' academic performance. Our findings, conducted using 129,644 students learning in eight courses within the LearnSmart platform, indicate that poor or unrealistic metacognition is asymmetric. These students are biased in one direction: they are more likely to be overconfident than underconfident. Additionally, while the examination of the temporal aspects of confidence reveals no significant change throughout the semester, changes are more apparent in the first and the last few weeks of the course. More specifically, there is a sharp increase in underconfidence and a simultaneous decrease in realistic evaluation toward the end of the semester. Finally, both overconfidence and underconfidence seem to be correlated with students' overall course performance. An increase in overconfidence is related to higher overall performance, while an increase in underconfidence is associated with lower overall performance","pdf":"Exploring the Asymmetry of Metacognition  Ani Aghababyan McGraw-Hill Education  281 Summer Street, 7th Floor Boston, Massachusetts  USA ani.aghababyan  @mheducation.com  Nicholas Lewkow McGraw-Hill Education  281 Summer Street, 7th Floor Boston, Massachusetts,  USA nicholas.lewkow  @mheducation.com  Ryan Baker University of Pennsylvania  3700 Walnut St. Philadelphia, Pennsylvania  USA rybaker@upenn.edu  ABSTRACT People in general and students in particular have a tendency to misinterpret their own abilities. Some tend to underes- timate their skills, while others tend to overestimate them. This paper investigates the degree to which metacognition is asymmetric in real-world learning and examines the change of a students confidence over the course of a semester and its impact on the students academic performance.  Our findings, conducted using 129,644 students learning in eight courses within the LearnSmart platform, indicate that poor or unrealistic metacognition is asymmetric. These stu- dents are biased in one direction: they are more likely to be overconfident than underconfident. Additionally, while the examination of the temporal aspects of confidence reveals no significant change throughout the semester, changes are more apparent in the first and the last few weeks of the course. More specifically, there is a sharp increase in un- derconfidence and a simultaneous decrease in realistic eval- uation toward the end of the semester. Finally, both over- confidence and underconfidence seem to be correlated with students overall course performance. An increase in over- confidence is related to higher overall performance, while an increase in underconfidence is associated with lower overall performance.  CCS Concepts General and reference  General conference pro- ceedings; Information systemsData mining; Applied computing  Interactive learning environments;  Keywords confidence, metacognition, achievement, performance, disci- pline difference, learnign analytics, big data  1. INTRODUCTION  1.1 Background  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13-17, 2017, Vancouver, BC, Canada c 2017 ACM. ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027388  Currently many educational digital environments customize a students path to completion and mastery by allowing them to focus on content that they do not know and skip over parts that they believe they have already mastered. However, what if these assertions of knowledge are simply demonstrations of overconfidence What if students judg- ment of their abilities, their feeling of knowing [2, 9], is in- accurate What if this overestimation of abilities creates an unrealistic expectation for the course, thus, discouraging the students and damaging their attitude toward their teachers or the pedagogy [15] If we want to customize based on stu- dents perception of their abilities, we need to understand how their perception correlates to their actual knowledge and performance.  There is an extensive body of work that investigates and discusses students metacognitive experiences, particularly student confidence. Confidence has been defined as the ability to believe in oneself where this belief is consid- ered to be learned [6]. Some researchers suggest a con- nection between students confidence levels and their mo- tivation, where initial level of confidence and subsequent changes may affect ones motivation, performance, and pos- sibly knowledge retention [4]. However, many view confi- dence as-task specific metacognitive experience [7].  According to some studies, student academic success is dependent on many factors one of which is their confidence in being capable to succeed [3, 8, 10, 12, 13, 19]. In fact, most of these studies suggest that confidence is a reliable predictor of performance and success [18, 19, 20]. In his extensive meta-analyses, John Hattie identified student self- reported grades as being the factor most correlated with student achievement [11]. Hattie suggests that if we man- age to help the students outperform their own expectations, it could lead to higher grades. As confidence also reflects an expression of self-evaluation, the same could apply to confi- dence.  While research shows that confidence is a continuous con- cept and may range from low to high levels [17], two con- structs in particular have emerged within this body of re- search as being important: overconfidence and underconfi- dence. These are the two ways that a students estimation of their abilities can fail to be realistic. Current theories of student motivation suggest that if a student is overconfi- dent, they may study less than if they possessed more accu- rate perceptions [16]. Perhaps overconfidence develops due to students past positive grade experiences, which leads to their assumption that they will perform equally well in a new topic. As a result, they remain unaware of their need to ad-    just or develop their study skills [5]. By contrast, undercon- fidence may stem from a students lack of self-assurance and belief in their own abilities. Findings suggest that encourag- ing realistic expectations and boosting academic confidence may benefit these students, leading to better performance [14]. Hence, both overconfidence and underconfidence need to be monitored.  1.2 Study Goals The purpose of this paper is to examine student confidence  across eight courses. We aim to explore how symmetric con- fidence is (how balanced it is between overconfidence and underconfidence) in real-world learners. We will also study how these two manifestations of inaccurate confidence relate to student performance in the course, and how confidence evolves throughout the semester. In addition, we will exam- ine whether student metacognition is differentially successful in different subject domains.  2. METHODS  2.1 Materials This study was conducted using data collected from one of  McGraw-Hill Educations learning platforms, LearnSmart. LearnSmart is an adaptive learning program that personal- izes learning and provides study paths for students. Within this environment students access their course materials, learn and practice the content, and complete assignments. Over 5.9 billion questions have been answered since 2009.  Figure 1: LearnSmart Interface  For this study, we selected data from eight different courses that were taught in the Spring 2015 semester using the plat- form. Our selection included four courses from humani- ties/social sciences and four from the physical/life sciences. To ensure that these courses were comparable in the number of total questions answered throughout the semester, we se- lected eight of the courses that had relatively equivalent us- age in 2015: Spanish, Psychology, Introduction to Business, Management, Practical Introduction of Medical Assisting, Anatomy & Physiology, Biology, and General Chemistry.  2.2 Participants Due to the regulations regarding student data collection  and usage, our platform does not collect gender, ethnicity, or other demographic information from our participants. Every student who submitted at least 1 assignment in their course were included in this study, for a total of N=130,791 students and 102,082,551 item responses by students.  2.3 Measures of Confidence  In its efforts to improve educational outcomes, the field of education faces obstacles such as the abundance of multiple- choice tests that reinforce students guesswork behavior and in the meantime fail to measure the degree of confidence that students exhibit towards their knowledge [1]. Relatively few learning systems measure the confidence that students ex- hibit towards their knowledge. In an attempt to implement confidence measurement, LearnSmart asks for students per- ception of their confidence with each content question, avoid- ing delay in response or recall bias. (see Figure 1).  For each question within each assignment, students are asked to provide an answer. Using the confidence scale em- bedded within the interface, before submitting each answer students were prompted to report their confidence level on a four-item scale: I know it (64.7% of the data), Think so (27.7% of the data), Unsure (5.6% of the data), No Idea (5.5% of the data).  2.4 Measures of Accuracy In addition to the confidence metric, the interface also  recorded the correctness (otherwise known as the score) of the students answer. Each student response was automati- cally graded according to the following categories: incorrect, partially correct (this is only for items with dual questions, which comprise approximately 5% of total questions), and correct. These responses were given three possible scores; 0 (incorrect, which was 32.8% of the data), 1 (partially cor- rect, which was 5.3% of the data), and 2 (correct, which was 65.3% of the data).  Due to lack of a final score metric within the database, we calculated our own total grade for each student. We calcu- lated each students accuracy score by dividing the number of correct answers by the total number of questions answered by the student. The result showed a mean score of 69%.  3. ANALYSES & RESULTS  3.1 Data Exploration Prior to exploring the data for confidence profiles, we  removed responses where the student had received partial credit (about 5% of all questions), as these responses were ambiguous for the analyses we will present below. Addi- tionally, we excluded rows with think so and unsure re- ports of confidence, as these responses were not indicative of overconfidence or underconfidence. As a result, our total number of items was reduced to 68,363,910 with a total of N=129,644 unique users, and a total of 51,657 unique ques- tions answered. The average number of questions answered per user was around 424.  3.2 Confidence Profiles To begin our analyses on confidence, we operationalized  students confidence profiles as seen in Figure 2. From this diagram, we calculated overconfidence and underconfidence respectively by calculating the conditional probabilities of student being confident (confidence = 3) when their answer was incorrect (score = 0) and of student being not confi- dent (confidence = 0), when their answer was correct (score = 2). As discussed below, we also analyze this separately for the different categories of courses: humanities/social or physical/life science.  To identify the proportions of overconfidence, underconfi- dence, realistic, and knowledgeable beliefs, we used the gen-    Figure 2: Confidence Profiles  eral conditional probability formula as seen below:  Realistic= P(c = 0 & s = 0)/n Underconfidence= P(c = 0 & s = 2)/n Overconfidence= P(c = 3 & s = 0)/n Knowledgeable= P(c = 3 & s = 2)/n  where c is confidence, s is score, and n is the number of questions in our sample.  We then create three tables to see the overall prevalence of each category: confidence profiles for all courses combined, confidence profiles for courses in physical sciences, and con- fidence profiles for courses in humanities/social sciences (see Tables 1, 2, and 3).  Table 1: Confidence profile for all courses  Table 2: Confidence profile for physical/life science courses  Table 1 shows that 22.71% of the time students are over- confident in their abilities; by contrast, they are only un- derconfident 0.24% of the time. This suggests that students  Table 3: Confidence profile for humanities/social science courses  are more likely to overestimate their abilities than to under- estimate their abilities. The same pattern repeats itself in both groups of courses (25.73% vs 0.16% in physical sciences and 19.89% vs. 0.31% in humanities/social sciences), which suggests that despite the difference in discipline, students metacognition is indeed asymmetric within LearnSmart and it leans toward overestimation of abilities. However, there is a difference in how much overconfidence is seen by discipline. 25.73% of the time students are overconfident in their abil- ities in physical/life sciences vs. 19.89% of the time shown in humanities/social sciences. This difference in proportions was very large; students in the physical/life sciences were overconfident 29.3% more often (we do not present a sta- tistical test due to the massive data set size; virtually any difference would be statistically significant).  3.3 Temporal Representation of Confidence We can understand how confidence changes over the course  of the semester, by visualizing the proportion of each com- bination of accuracy and confidence, shown in Figure 3. In this Figure, the x axis is the weeks of the semester starting from January 1st and ending on June 15th, while the y axis is the log scale of the percentage of questions answered which had each category of reported confidence in that week.  Figure 3: Temporal Representation of Confidence for Physical/LIfe and Humanities/Social Sciences  In this figure we can notice several interesting changes. The underconfidence for both discipline types is low through- out the semester. However, at the beginning of the semester, between weeks 2 and 3, when underconfidence rate rises in humanities/social sciences, the underconfidence in physical sciences drops. A similar shift happens at the end of the semester, toward week 21, when the underconfidence rate in physical sciences rises considerably while the rate drops for the humanities/social sciences. Additionally, in the same week (week 21), when underconfidence rate in physical sci-    ences rises dramatically, the rate of students being realistic drops both for physical/life and humanities/social sciences. It is possible this indicates that at this point students are more worried about their final grade and their preparedness.  3.4 Correlations: Accuracy score vs. Confi- dence Profiles  Finally, we explore the relationship between students over- all course performance and their reported confidence levels. For this purpose, we calculated the proportion of correctly answered questions that were underconfident for each stu- dent. Similarly, the overconfidence ratio was calculated as the proportion of incorrectly answered questions that were overconfident per student. Then we correlated these two new variables with the students overall course performance. Pearson correlation coefficient revealed that higher overcon- fidence seems to be correlated to higher scores, while higher underconfidence is negatively correlated with success. In addition, this correlation is larger in magnitude for human- ities/social sciences (see Tables 5 and 2). We also calcu- lated the Spearman correlation coefficients, which were lower across all categories. This emphasizes a larger linear corre- lation as opposed to a rank correlation. It can be explained by the skewed nature of our data and the influential obser- vations in the tails of the distribution.  Table 4: Accuracy vs. Confidence Correlation Re- sults for All Courses  Table 5: Accuracy vs. Confidence Correlation Re- sults for Physical Science Courses  Table 6: Accuracy vs. Confidence Correlation Re- sults for Humanities/Social Science Courses  We chose scatter plots to display the correlation results for all courses using a color legend to visualize the course cat- egories within each plot. Figure 4 demonstrates a medium positive correlation between students overconfidence ratio and their scores. The opposite is visible in Figure 5 where we see a small negative correlation between students overconfi- dence ratio and their scores. For underconfidence ratio plot, we retained only the data from students that had at least  one question where they demonstrated underconfidence. In addition to correlation pattern, these plots also reveal a pat- tern where students in physical/life science courses are con- sistently more accurate in their estimations of their ability than students in humanities/social science courses).  Figure 4: Correlation of Accuracy vs. Over- confidence Ratio For Physical/LIfe and Humani- ties/Social Science Courses  Figure 5: Correlation of Accuracy vs. Under- confidence Ratio For Physical/LIfe and Humani- ties/Social Science Courses  4. DISCUSSION & FUTURE WORK A large number of research studies have already asserted  the importance of students metacognition and confidence. Hence, learning how students confidence interacts with their performance, how it evolves throughout the course, and how it varies by discipline can bring important insights to moni- toring and helping students learn to regulate it.    In this paper we have explored students academic con- fidence. We created four confidence profiles and discov- ered that students perception of their abilities in real-world learning is asymmetric; students are much more likely to be overconfident than underconfident. This pattern is even more pronounced for physical/life sciences than for other courses. We also explored the change of confidence over the course of the semester, noting increased variability in the levels of confidence at the beginning and at the end of the semester. Finally, the results from this work support previ- ous findings that students perception of their performance is in fact correlated with their actual performance. However, we find that overconfident students perform relatively well. This finding suggests that some of this seeming overconfi- dence may actually represent slips; the student may really have known the skill despite getting the answer wrong. Al- ternatively, estimations of skill may be general rather than pertaining to the current situation. By contrast, students who were underconfident generally did worse. Whether this implies that underconfident students should become more confident, or that they need more help, is a relevant area for future research. We also found that both forms of inaccurate confidence are more prominent in humanities/social science courses than in physical/life science courses. It is possible that this is because it is easier to estimate ones proficiency on procedural skills than on factual matters; this is also a relevant area for future work.  There are other directions that will also be valuable for ex- panding scientific understanding of these phenomena. First, it may be worth incorporating measures of item difficulty to see how it influences over/underconfidence. In addition, it would be valuable to increase the number of courses within each discipline category for more rigorous investigations into how discipline impacts over/underconfidence; similarly, break- ing down different types of material will help us to explore whether discipline-level effects are due to disciplnary culture or due to the types of material being studied. Finally, these results suggest that it may be worth developing interven- tions to help students be more realistic about what they do not know, within platforms such as LearnSmart.  5. ACKNOWLEDGMENTS This paper is based on work supported by McGraw-Hill  Education. We would like to extend our appreciation to our CDO Stephen Laster and the analytics VP Alfred Essa. Any opinions, findings, or recommendations expressed in this pa- per are those of the authors and do not necessarily reflect positions or policies of the company.  6. REFERENCES [1] T. M. Adams and G. W. Ewen. The importance of  confidence in improving educational outcomes. In 25th Annual Conference on Distance Teaching & Learning, Madison, WI., 2009.  [2] R. Azevedo and J. Cromley. 2004. Does training on self-regulated learning facilitate students learning with hypermedia Journal of educational psychology., 96(3):523.  [3] A. Bandura. 1977. Self-efficacy: toward a unifying theory of behavioral change. Psychological Review., 84(2):191.  [4] M. Besterfield-Sacre, N. Y. Amaya, L. J. Shuman, C. J. Atman, and R. L. Porter. Understanding student confidence as it relates to first year achievement. In Frontiers in Education Conference, 1998. FIE98. 28th Annual, volume 1, pages 258263. IEEE, 1998.  [5] D. E. Clayson. 2005. Performance overconfidence: metacognitive effects or misplaced student expectations Journal of Marketing Education., 27(2):122129.  [6] L. Copeland. 1990. Developing student confidence: the post clinical conference. Nurse Educator., 15(1):7.  [7] A. Efklides. 2011. Interactions of metacognition with motivation and affect in self-regulated learning: The masrl model. Educational psychologist., 46(1):625.  [8] K. A. Ericsson, R. T. Krampe, and C. Tesch-Romer. 1993. The role of deliberate practice in the acquisition of expert performance. Psychological review.., 100(3):363.  [9] J. Hart. 1965. Memory and the feeling-of-knowing experience. Journal of educational psychology., 56(4):208.  [10] S. Harter. 1978. Effectance motivation reconsidered. toward a developmental model. Human development., 21(1):3464.  [11] J. Hattie. Visible learning: A synthesis of over 800 meta-analyses relating to achievement. Routledge, 2008.  [12] J. Kuhl. 1984. Volitional aspects of achievement motivation and learned helplessness: Toward a comprehensive theory of action control. Progress in experimental personality research., 13:99171.  [13] J. Nicholls. 1984 Achievement motivation: Conceptions of ability, subjective experience, task choice, and performance. Psychological review., 91(3):328.  [14] L. Nicholson, D. Putwain, L. Connors, and P. Hornby-Atkinson. 2013. The key to successful achievement as an undergraduate student: confidence and realistic expectations Studies in Higher Education., 38(2):285298.  [15] M. Nicolaidou and G. Philippou. 2003. Attitudes towards mathematics, self-efficacy and achievement in problem solving. European Research in Mathematics Education III. Pisa: University of Pisa..  [16] C. Nowell and R. M. Alston. 2007. I thought i got an a! overconfidence across the economics curriculum. The Journal of Economic Education., 38(2):131142.  [17] B. D. Pulford. Overconfidence in human judgement. University of Leicester, 1986.  [18] L. Stankov, J. Lee, W. Luo, and D. J. Hogan. 2012. Confidence: A better predictor of academic achievement than self-efficacy, self-concept and anxiety Learning and Individual Differences., 22(6):747758.  [19] R. Stiggins. 1999. Assessment, student confidence, and school success. The Phi Delta Kappan., 81(3):191198.  [20] A. Zajacova, S. M. Lynch, and T. J. Espenshade. 2005. Self-efficacy, stress, and academic success in college. Research in higher education., 46(6):677706.    "}
{"index":{"_id":"16"}}
{"datatype":"inproceedings","key":"Lee:2017:TAD:3027385.3027386","author":"Lee, Alwyn Vwen Yen and Tan, Seng Chee","title":"Temporal Analytics with Discourse Analysis: Tracing Ideas and Impact on Communal Discourse","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"120--127","numpages":"8","url":"http://doi.acm.org/10.1145/3027385.3027386","doi":"10.1145/3027385.3027386","acmid":"3027386","publisher":"ACM","address":"New York, NY, USA","keywords":"discourse analysis, idea measurement, learning analytics, social network analysis, temporality","Abstract":"This paper presents a study of temporal analytics and discourse analysis of an online discussion, through investigation of a group of 13 in-service teachers and 2 instructors. A discussion forum consisting of 281 posts on an online collaborative learning environment was investigated. A text-mining tool was used to discover keywords from the discourse, and through social network analysis based on these keywords, a significant presence of relevant and promising ideas within discourse was revealed. However, uncovering the key ideas alone is insufficient to clearly explain students' level of understanding regarding the discussed topics. A more thorough analysis was thus performed by using temporal analytics with step-wise discourse analysis to trace the ideas and determine their impact on communal discourse. The results indicated that most ideas within the discourse could be traced to the origin of a set of improvable ideas, which impacted and also increased the community's level of interest in sharing and discussing ideas through discourse.","pdf":"Temporal Analytics with Discourse Analysis:   Tracing Ideas and Impact on Communal Discourse   Alwyn Vwen Yen Lee  Nanyang Technological University, Singapore   50 Nanyang Avenue  Singapore 639798   alwynlee@ntu.edu.sg   Seng Chee Tan  Nanyang Technological University, Singapore   50 Nanyang Avenue  Singapore 639798   sengchee.tan@ntu.edu.sg        ABSTRACT  This paper presents a study of temporal analytics and discourse  analysis of an online discussion, through investigation of a group  of 13 in-service teachers and 2 instructors. A discussion forum  consisting of 281 posts on an online collaborative learning  environment was investigated. A text-mining tool was used to  discover keywords from the discourse, and through social network  analysis based on these keywords, a significant presence of relevant  and promising ideas within discourse was revealed. However,  uncovering the key ideas alone is insufficient to clearly explain  students level of understanding regarding the discussed topics. A  more thorough analysis was thus performed by using temporal  analytics with step-wise discourse analysis to trace the ideas and  determine their impact on communal discourse. The results  indicated that most ideas within the discourse could be traced to the  origin of a set of improvable ideas, which impacted and also  increased the communitys level of interest in sharing and  discussing ideas through discourse.    CCS Concepts   Applied computing  Education  Computer Assisted  Instruction.    Keywords  Learning Analytics; Discourse Analysis; Temporality; Social  Network Analysis; Idea Measurement   1. INTRODUCTION  Learning analytics as a nascent field has been gathering broad  interests in educational research and practice [13], and recent  research has contributed significantly to further our understanding  of learning processes [30]. Discourse analysis has long been  conducted to analyze language beyond literal language use, and  detailed analysis of online discussions can inform the design and  improve the productivity of knowledge creation [6]. The analysis  of discourse over time can provide additional insights of agent  interactions and specifically students level of understanding in  discussions. However, this form of temporal analysis is often an  undervalued process that is not studied in greater detail [18],  resulting in broad and ambiguous conclusions to be drawn from  studies. By combining the usage of discourse analysis with   temporal analytics, we propose to integrate data sources and  analysis from interactions and discourse among students during  learning activities. The results obtained from such analysis could  provide clearer indications on how temporal analysis of discourse  could provide feedback that is useful for learning.    To conduct an analysis of temporal features within group  interactions, emergent methods and measures are required for  characterizing temporal dynamics [24]. Usage of networks,  especially social networks can provide substantial insights into the  quantity and types of relationships that exist between participants  of the network. The problem of analyzing and understanding  complex networks is understandably a challenging one, but it could  also be seen as an area of research that contains utmost potential.  In this regard, social network analysis (SNA) [29, 32] has become  one of the most commonly applied methods in learning analytics  research [4].  With the prevalent usage of online databases and ease  of access to an abundance of data, little temporal data are being  used by analysts to provide a deeper understanding of the content  [24]. Even with the abundance of platforms and tools that are used  with SNA [17], few studies can make full use of network analysis,  and there has yet to be significant breakthroughs that assist  researchers in furthering the understanding of learning.    By using novel methods in conjunction with SNA [17], students  can learn deeply to understand the meaning behind written text and  spoken words [1] through feedback and assistive tools. The  standard social network measures (eg., degree, closeness,  betweeness, centrality) can represent learning processes and  interactions within the group to a certain extent using network  graphs, but they are restricted in the ability to provide the  descriptive and quantitative feel of individual students or a  communitys level of engagement. There is a limit to what SNA  can achieve by inference, as most network variables and  relationships are heavily dependent on user or analysts  interpretations and can be tedious and challenging especially with  large scaled-up networks. Also, research findings have so far been  unable to robustly conclude or determine which types or  combination of network measures can predictably determine levels  of understanding within the group [32].    Therefore, by making use of the current prevalence of learning  analytics deployed alongside discourse platforms, SNA can be  combined with temporal analysis to determine sections of discourse  where feedback and interventions could prove to be most helpful in  helping to stimulate and increase students engagement in the  discourse. By seeking to automate the procedure of discourse  analysis with temporal analytics, larger social networks can be  tackled and similarly analyzed without requiring educators to sift  through all the contents within the discourse manually. The  following research question is to be addressed: How could temporal   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components for this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org,  LAK17, March 1317, 2017, Vancouver, BC, Canada.   2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00.  DOI: http://dx.doi.org/10.1145/3027385.3027386      analytics assist discourse analysis in tracing ideas and impact on  discourse to gauge students and communitys level of  understanding   2. BACKGROUND  2.1 Social Network Analysis  Social Network Analysis (SNA) is the process of investigating  social structures using network and graph theories, and has been  adopted since the 1930s [29], with the focus mainly on three  standard social network measures: degree, closeness, and centrality  [10]. The degree of a graph indicates the number of edges incident  to a vertex or referred in this paper as a node. Metrics such as  closeness centrality is often used to denote average lengths of the  shortest path between all nodes in the graph. Studies have shown  that it is often unclear how these network measures can be  associated with learning outcomes, for example, whether a higher  social centrality could lead to higher academic performance [10,  32]. In addition, it can be difficult to generalize models and try to  apply them to scaled-up experiments or deployments within  schools.   Advancement in technology has provided an array of newer  technology and tools that can be used with SNA [17], which were  previously technologically unfeasible but now integrated into  social network analysis. By analyzing the interactions as key  measurements within the social network, educators who have been  using online platforms for teaching (e.g., online discussion forums,  learning management systems) could then also focus on the content  that students generate as part of their learning process. As the  majority of online content are mainly text, it is only natural that  content analysis programs such as text mining and image  comparison are integrated to assist in deeper analysis. Further, there  are works that made use of neural networks [3], which are computer  systems modeled on the human brains, and fuzzy clustering  algorithm [25] that allow data to belong to two or more clusters.  These are possible content analysis methods, indicating the large  range of technology that could provide assistance in the analysis of  educational data using more efficient manners. However, the  results from these analyses tend to be inconclusive in helping us to  answer our research question of how ideas can provide the meaning  of context and impact discourse within a community. As most  current tools and analysis platforms perform specific roles in  summarization work [17], they are therefore not suitable to be used  for analysis and scaffolding for deeper learning.   2.2 Online discourse analysis  Discourse analysis has long been a framework and means of  exploring the imbrications between language and social- institutional practices [9]. Pioneers Gilbert and Muckay [11] started  by examining scientific dispute and analyzed how claims were  made and positions were defended. Discourse analysis has since  evolved with different interpretations, but the focus has always  been explicitly on language as a social action, with discourse and  argumentation as tools which participants use to compare thinking,  shaping of agreements, exploring and identifying of ideas [7]. With  technological advancements, written discourse has been moved  onto online platforms and expressed within online learning  environments, with accompanying analysis of online discourse  becoming much more prevalent and ubiquitous.    2.3 Temporal analysis  The emergence of temporal analytics provides a different  dimension in which data could be analyzed and also allows  researchers to pay attention to details that transpire in parallel to  other main events within the timeline. A common understanding of   how time entails for different parties involved in a social network  is, therefore, crucial for navigating through series of events that  unfold over time [19], and procedures often require segmentation  of the timeline to be analyzed. There is also a need to bridge the  gap between temporal details which are mostly collected at the  micro level [18] and underlying theory which operates on a macro  level. Temporal patterns [5] are also able to provide insights of  students actions from clickstream data, showcasing the potential  for determining predictive actions and potential interventions.    The process of analyzing ideas is, however, complicated, could  consist of events and content that overlap with each other and  transpire over short periods of time. The commonly used code and  count methods that aggregate results over time [31] tend to ignore  fine-grained details and would, therefore, mask minute yet possibly  significant details that could indicate the level of understanding  within an individual or community at a particular time juncture  within the discourse. Moreover, understanding of content within  any discourse is heavily dependent on the context and is often not  generalizable [12], especially when the textual content is framed  within a certain structure that requires deeper analysis so as to  understand the intentions and motive of the writer truly. Hence  there needs to be an integrated and scalable method where ideas can  be automatically identified, be verified to be part of learning  outcomes as pre-determined by the teachers, while not losing  contextual information during the process of determining ideas  within the discourse.   2.4 Analyzing Ideas and Relevant Tools   2.4.1 Definition and Focus on Ideas  John Locke used the word idea to represent the most basic unit of  human thought, and these immediate objects of perception are  interesting to him as they point beyond themselves [16]. The  general concept of an idea is more than just a unit of thought, but  one which allows room for improvement. The crucial feature in  ideas is not so much about the content of ideas, but rather what the  ideas are capable of doing, such as providing the epistemic function  to represent something else, and the ability to improve beyond itself  in order to provide a different higher level of understanding.   We propose to understand the effects of ideas on discourse, as  knowledge is mediated by ideas and the search for improvable ideas  subsequently leads to the creation of new knowledge. Within a  traditional instruction-based classroom, most ideas within the  community are provided by teachers and are also mostly limited in  improvability, due to the factual nature and pre-assigned sources of  content, such as structured syllabus designated by authorities and  institutions. Communications within classrooms are also often  constrained by a one-way instruction from teacher to students, with  little room for feedback or inquiries from students, due to limited  time for cramming of syllabus into the academic year. Therefore,  by identifying and searching for improvable ideas, we are striving  to discover communally interesting ideas present in the community  discourse that can be improved through sharing of knowledge and  may prove elusive to the human eye but can be detected using  temporal analytics with social network analysis in a time-efficient  manner.    2.4.2 Engaging Ideas through Knowledge Building  Ideas are complex entities, representation of opinions and  knowledge, especially difficult to interpret when represented by  textual format within online discussion communities. Problems that  have to be addressed are also difficult to solve, and the use of either  quantitative or qualitative approaches alone is inadequate to  address this complexity. Small talk (30%) and logistic information     are major sources of information overload on discussion forums  which do not enhance learning experience [2], and hence there is a  need to identify and filter out main ideas from discourse, which are  essential for maintaining interest within the community.   In the present study, we choose to focus on using knowledge  building [27] as our model of knowledge creation, among other  models such as the transformation of activity system and the  creation of new products at organizational level [8, 20]. Knowledge  building is a pedagogical approach that engages students in  continual production and improvement of ideas useful to the class  community through the collaborative efforts of its members [27].  Using knowledge building, the learners capability of natural idea  generation can be leveraged for continual improvement of ideas,  and for teachers to maintain student engagement in idea  improvement. The identification of ideas within discourse is a  natural approach towards efficient learning, and discourse itself  often plays a creative role in encouraging improvements on ideas  [14]. We acknowledge the concept of idea improvement [26] as  being crucial to students who are often required to have motivation  in their search for solutions to handle significant challenges.  Students have to be aware of emergent themes of inquiry from  multiple sources of inputs, acknowledge knowledge gaps, and  participate in collaborative idea improvement [34].    Therefore, to ensure that students continue to stay engaged in the  learning process throughout their learning journey, they are  encouraged to participate in knowledge building discourse to share  and continuously improve their ideas. This paper investigates and  analyzes content from knowledge building discourse, which allows  learners to engage in collaborative inquiry to enable creation,  contribution, and advancement of community knowledge. By  engaging students in knowledge building discourse, students could  collaboratively build on each other ideas so as to meaning-make  and also improve the communitys level of understanding through  deeper learning.    2.4.3 Analyzing Ideas using SNA and Keywords  To analyze temporal aspects of knowledge building discourse, we  used an analysis platform, Knowledge Building Discourse Explorer  (KBDeX) [22]. KBDeX was developed with technological  affordances for visualizing student interaction networks, discourse  unit networks, and keyword networks. As KBDeX was created with  SNA in mind and the backend analytical processes are catered for  this specific purpose; standard social network measures are offered  and measurements such as density and centrality are often chosen  for determining the level of interaction between students [33].  KBDeX is a unique analysis platform that allows users to conduct  a step-by-step analysis of social networks through visual displays  of networks and the respective social measures. This unique step- wise functionality of assessing discourse allows us to pause the  discourse at any time junctures of the discourse, conduct the  necessary analysis before moving onto the next timeframe of  discourse. Other than visual indicators and prompts to assist users,  standard social network measures can be retrieved for a more  detailed form of analysis that provides insights into the interactivity  between different nodes and related networks. Overall, the role of  KBDeX is to provide a discourse analysis platform that fulfills the  role of idea identification and also provide step-wise analysis of a  continuous discourse through social network analysis using  keywords.   As the commonly used type of SNA is insufficient in examining  community knowledge advancement through students  collaboration and interaction networks [21], text analysis processes  such as identification of fundamental or advanced usage of   keywords becomes increasingly and specifically important in  determining ideas in discourse. This is so since keywords can be  representative of knowledge that is recognized and understood by  the students. At times, the usage of keywords within text might not  be wholly representative of a students thoughts and ideas, or could  also be misrepresented by students due to incorrect conceptual  understanding. However, when these individual opinions are  situated within discourse as part of the communitys sequence of  thoughts, the usage of keywords and content can be monitored to  understand further the overall usage flow of keywords and how  they are used within the discussion threads in response to their  peers ideas and thoughts.      Therefore, it is important to seek out tools that can determine  keywords from a text. SOBEK is a tool that can identify relevant  keywords and concepts within the text, through the frequency  analysis of textual material and selection of important related  keywords [23]. Text mining is often used for extracting relevant  information from unstructured data, which in this case, from  sources such as the knowledge building discourse. Graphs are  constructed with connections between related nodes, for users to  better understand relevant terms, with frequency and sizes of nodes  indicating frequency of usage. Similar concepts are also run  through an inbuilt thesaurus to consolidate common terms and  ideas into a single node. The primary purpose of implementing  SOBEK within this present study is not only for displaying graphs  visually to users but also mainly for determining keywords from a  statistical perspective, to be provided as inputs to social network  analysis using KBDeX.        3. METHODS  3.1 Dataset and Setting  The dataset involved in this study was obtained from the discourse  of a single graduate-level course, delivered by two instructors over  a period of 13 weeks.  The graduate class consists of 13 in-service  teachers who were undergoing further training and professional  development. Participants were encouraged to actively share their  thoughts and opinions using a computer-supported collaborative  learning environment called Knowledge Forum [28]. A total of 281  Knowledge Forum notes, referred as discourse units (DU) in this  paper, were written with each of the 13 students providing at least  a note of a reasonable amount of content. The role of the two  teachers was to facilitate the lesson and co-create knowledge. Over  the span of 13 weeks, students learned and applied the basic  principles of knowledge building, and shared with the community  on how knowledge building could be applied to affect, improve and  apply to future learning opportunities for themselves and classes  that they would instruct.    Each weekly lesson of three hours would consist of discussions and  sharing sessions regarding readings that would be prepared for  lessons. Instructors are available to provide face-to-face instruction  during lessons and can provide consultation if help is required in  understanding topics. The participants were able to use scaffoldings  and analytical tools to help each other improve ideas and to also  reach a greater level of communal understanding. The participants  were provided with credentials to use Knowledge Forum  throughout the duration of the course, as this was the principal way  in which teaching staff interacted with them during the weekly  instructional sessions and outside of classrooms. This  asynchronous mode of learning is a means of learning that allows  the participants to engage in discussions transcending time and  space, also ensuring knowledge building as a continuous process is     not restricted within lessons but can also be conducted outside of  classrooms with inputs and considerations from authentic learning  situations.   3.2 Procedures and Measures   3.2.1 Using SOBEK for finding keywords  Our previous design made use of expert opinion for determining  keywords [15]. For the present study, we used text-mined keywords  to represent ideas which are not obtained from domain experts, that  is, the instructors for the course. There are a few reasons for  following this course of action: the possibility that there could be  inherent bias from experts regarding the field of knowledge when  deciding keywords; the scope of knowledge regarding the topic  could instinctively be narrowed down to only a specific range of  keywords which the instructors would want the participants to  learn; and keywords determined by experts often originate from  their own perspectives and experience, often neglecting keywords  that are representative of students views and opinions within  discourse. The text-mined keywords would provide an unbiased  source of ideas obtained from the participant-generated discourse.  SOBEK allows us to filter out common words such as a, the,  with, related prepositions and adverbs to ensure important  keywords and concepts are captured during the analysis. Similar  concepts represented by different words were also consolidated  under a common keyword so that there would be less repetitive  synonyms, while relationships between different keywords are also  linked together to indicate inter-keyword relationships.   3.2.2 Using KBDeX for Social Network Analysis  Text-mining of discourse data using SOBEK would provide a list  of keywords that could be provided as input for KBDeX to conduct  SNA. KBDeX provides social network analysis on three different  kinds of discourse data: (a) the participating students and teachers,  (b) discourse units, and (c) the identified keywords within  discourse. These three categories of data are considered as nodes  within their respective networks, such as student network,  discourse unit network, and keywords network. As time is a  continuous variable within discourse analysis, it is inevitable that it  has to be broken down into discrete segments to compare and  contrast different portions of the discourse. KBDeX assists with  this segmentation process by designating discourse from different  agents into turns, such that all participants are included in a turn- based discussion. Analysts are therefore able to determine the total  number of discussion units within the discourse, the initiating time  of specific discourse units and the related keywords which are  highlighted in color, together with the actual contents of the unit,  all within a single view (see Figure 1).         As this study analyzes the relationships between sets of keywords  within discourse units, we use the discourse unit network within  KBDeX for analysis, with the help of underlying bipartite graphs  that determine relationships between discourse units and keywords.  For example, discourse units A, B and C in the discourse unit  network contain texts of What is learning, Learning is a  process. and I understand knowledge building.. If learning is  an identified keyword, discourse units A and B will be connected  through the keyword. This sequence is then repeated for all  discourse units and keywords in the respective discourse unit and  keyword networks, to form a bipartite graph.    3.2.3 Using Betweenness Centrality Measure for  Analyzing Temporality  The present study uses a conventional network measure,  betweenness centrality (BC) coefficient to detect and identify  important connections between words inside discourse units. We  interpret the BC coefficient as the degree of importance of  discourse units within discourse and consider the ideas within these  units as relevant to the context and forum discussions at the point  in time. Through graphical displays, we can identify and determine  time junctures where the ideas are initiated, shared or fade over the  entire discourse. By viewing the BC measure of a specific discourse  unit that spans over multiple time junctures, the observed trend of  BC values is representative of the units significance and the  subsequent impact on community discourse after its introduction  into the discourse space.      4. RESULTS AND DISCUSSIONS  There are two major aspects of analysis in this study:1) the use of  auto-mining tools to help determine keywords and relationships  within discourse, and 2) using keywords to find promising ideas,  and making use of temporal analytics to assist and provide  additional insights pf the promising ideas in discourse analysis.     4.1 Keywords and Relationships  The discourse was collected and anonymized at the end of the  course before being parsed through SOBEK for determination of  the keywords. We used a frequency threshold setting of 50, which  sets the criterion to display keywords that appear more than 50  times within the discourse. This setting was chosen, because it  provides an average of 10 to 15 relevant keywords for the whole  discourse, and this number is comparable to the number of  keywords that experts provided to us in previous studies [15]. A  total of 11 keywords were found, as shown in Table 1 with their  respective frequency counts throughout the discourse.      Table 1. Keywords with frequency from SOBEK   Keyword Frequency Keyword Frequency   knowledge 325 kb 273   learning 172 students 151   knowledge  building   151 understanding 114   discourse 82 community 79   idea 67 based 63   information 55      Figure 1. Segmented Discourse View in KBDeX.     These keywords were used as indicators to determine ideas that  students are generating and discussing regarding the topic of  knowledge building. The usage of keywords in discourse often  represents inquiry to understand the concepts behind keywords  further, and could reflect learners possession of knowledge.  However, it could also be construed as misconceptions. On the one  hand, keywords could be unique words which are unlikely to be  repeated without sufficient evidence of understanding. On the other  hand, misconceptions could complicate processes of  understanding, especially if it is propagated within a community  without verification. Through discourse, misinterpretations can be  corrected through knowledge sharing and critique by other learners,  or through corrective guidance from the teacher.    The relationship between text-mined keywords are displayed in the  graph (Figure 2) and the graphical interpretation shows that ideas  related to knowledge building as an approach to learning are  naturally linked to the following keywords knowledge, learning  and community.' A closer qualitative look at the content in  discourse units show that ideas regarding the understanding of  knowledge building are present within the discourse, and this is  representative of the participants understanding regarding this  field of knowledge. From the keyword graph, we can determine that  participants perspectives are varied and the graph encapsulates a  rudimentary level of understanding on the assigned topic.   We made further adjustments to SOBEK so as to determine if more  advanced concepts and principles, such as knowledge building  principles (e.g., improvable ideas and rise above) are present  within the discourse. These are some of the principles explored by  researchers and were not present or linked within the initial graph  in Figure 2. As there are interests in discovering the presence of  such principles within discourse, we subsequently relaxed the  frequency criterion in SOBEK to a frequency threshold setting of  15, that is, any keywords with at least 15 appearances within the  discourse will appear in the graph. The keywords idea  improvement and rise above were eventually found in our  analysis, and idea improvement was illustrated with linkage to the  main keyword kb with a frequency count of 16, which explains  the relatively smaller size of the node (see Figure 3). Among a large  amount of keywords in discourse, the presence of the idea  improvement principle was largely obscure and hidden in Figure  2, but with an expanded search, the principle can be found as shown  in Figure 3. Hence we question if it is possible to detect lesser  discussed but nevertheless important ideas using a more optimized  method such as text mining for keywords, as compared to casting  the net wide and capturing less important keywords that might  deviate discussions. By using temporal analysis, previously  unnoticed ideas can be found in an alternatively more visible  manner, such as through analysis of BC trends over time.           In addition to the process of relaxing this criterion during analysis,  we also noticed that the number of identified keywords increased  significantly and there are not many conclusions that could be made   about the larger keyword network, apart from indicating presence  or absence of keywords and possible ideas. We recognize that even  though the analysis of social networks based on keywords provided  relationships and detects presence of ideas within discourse, the  process of identifying keyword relationships was, however, unable  to adequately show the process of improving ideas and deeper  understanding of students within discourse. Therefore, there is a  need to conduct a more detailed analysis such as using temporal  analytics to provide greater insights and also track the improvement  of ideas throughout the entire discourse.      4.2 Analyzing Temporality of Discourse  By focusing on the temporal dimension of discourse, we were able  to observe how the content, usage of language and connectedness  of discourse units within the discussion space change over time.  Although the frequency of keyword usage can be tracked, we  focused on the usage of BC trends of individual discourse units.  Prior work such as the I2A methodology [15] has shown that ideas  within discourse can be identified and classified to give an  indication of the communal understanding and an approximate  number of ideas within discourse.  In essence, I2A methodology  involves visualization and step-wise analysis of discourse network  measures using SNA and measurement coefficients of chosen  keywords to trace the evolvement of ideas within a discourse space.  This methodology was validated with a qualitative analysis but  previously only contains a brief temporal analysis. In this paper, we  seek to provide a more detailed analysis using the present study, to  show that identified ideas with potential to improve can be further  worked on through tracking of ideas and understanding of their BC  trends. With the understanding of BC trends, we are able to show  how a certain idea was influencing or was influenced by other  ideas, and subsequently how its introduction into the discourse  impacted overall community discourse.   4.2.1 Finding promising ideas with I2A methodology  Firstly, we applied the I2A methodology with the help of KBDeX,  in searching through the whole discourse to identify groups of  ideas. The methodology then classified the ideas according to their  level of promisingness, a combined measure of the following  attributes, namely 1) idea relevancy to context, 2) underlying  motive of idea initiation, and 3) impact of idea on the community.  These attributes were measured from the perspectives of  participants within the community, be it students who are  consumers of content or teachers who help co-create knowledge in  the community. This methodology was similarly applied to the   Figure 2. Graph of relationships between keywords text- mined from discourse.   Figure 3. Idea improvement (highlighted rectangle  linked to kb) within large network of keywords.     current study to obtain indications of promising ideas. Discourse  units with high relevancy and significantly high community interest  displayed significant peaks in their respective BC trends with  consistent fluctuations, and are classified as containing promising  ideas (see Figure 4). The y-axis in Figure 4 denotes the normalized  BC values, while the x-axis indicates the number of discourse turns  in the whole discourse.    Discourse units in this paper are named with DU followed by a  number that represents the turn in which the discourse unit was  introduced into the discourse. We identified discourse units that  contain promising ideas using I2A, and choose to discuss a few such  as DU8, DU18 and DU66. These discourse units are frequently  engaged by the community and consistently garnered community  interests during the discourse. The ideas were traced to understand  how and when were they initiated, how did the ideas change over  time, and whether the ideas created impact or influenced other  discourse units within the discourse.      4.2.2 Tracing ideas and impact on discourse   Tracing ideas within DUs allow us to determine if the ideas stem  from a curious inquiry, factual statement or belongs to part of an  outlandish claim. We can also gauge the reception of these DUs by  the community, whether they are cursorily accepted or ignored,  sufficiently thought-invoking that invite queries and discussion, or  are disruptive content that sparks debate from the time of initiation  within the discourse. There are two phases of the BC trends where  ideas within the DUs can be analyzed. The first phase occurs when  the DU is introduced into the discourse, and the second phase is the  period after the DUs point of introduction till the point when ideas  in the DU eventually fade out or when the discourse ends,  whichever comes first. By focusing on these two phases, we can  differentiate and temporally classify the different BC trends.    Using data from the present study, we analyzed DU8 and DU66,  because both discourse units exhibited differing types of BC trends  during the early phases of discourse (see Figure 5). The I2A  methodology has previously classified the two discourse units as  containing promising ideas, and therefore the second phases are  expectedly similar, where the BC trends are consistent with high  fluctuating values due to constant engagement and interest from the  community.    On the one hand, with regards to the first phase, where the idea was  initially introduced and the BC values peaked at a relatively high  value, we know that there is an immediate interest by the  community in the contents of the mentioned DU that could lead to  further debates. The ideas within this kind of DU tend to be  thought-invoking, novel or disruptive, which in any case naturally  leads to more discussions among a larger part of the community.  An example that exhibits this type of BC trend is DU66, which was   introduced in the early-middle part of discourse at turn 66 but was  able to invoke interest from the community rapidly.    On the other hand, if the DU does not exhibit any significantly  drastic BC changes when the discourse unit was initially introduced  into the discourse, it is safe to then label the contained ideas within  the DU as the typical type of assertion that does not require too  much attention by the community at that particular juncture in the  discourse. However, subsequent increasing BC values and trends  that arise in later parts of the discourse mean that there was a  delayed pick-up in communal interests, signaling a possible return  of interests to a previously discussed topic or concept. Both  discourse units DU8 and DU18 exhibit such similar behavior, and  are therefore classified as having low initial community interests  upon introduction but contain significantly promising ideas that  impact later stages of the discourse.    We show DU8 in Figure 5 with low initial community interests  when initiated at Turn 8, and significant communal interest picked  up later during the discourse at Turn 24. DU66, however, initiated  at Turn 66 and immediately registered a spike in communal  interests when the discourse unit was introduced.           The two different types of BC trends belonging to DU8 and DU66  suggest that the temporal pattern of communal interests in ideas  could be predicted using social network measures such as BC. We  verify the observations in this study by scrutinizing the qualitative  contents of the DUs to determine if the nature and contents of the  DUs are consistent with how the community perceives the  discourse units. As both DU8 and DU66 are lengthy and descriptive  in nature, the important parts of the discourse units are mentioned  in the following quotations.    Excerpt of DU8 from Student S7: knowledge is a very complex  entity. philosophically, knowledge refers to justified true belief. in  other words, knowledge is a form of beliefs which one may possess.  what differentiates knowledge from a belief is the process one takes  to reason to oneself why s/he thinks that this belief is true.  is  also called propositional knowledge. there exist other forms of  knowledge,  learning refers to a process where one acquires  new knowledge with respect to the learner     knowledge building (kb) can be seen as a way of learning, where  learners gain new knowledge in the process. this happens in a  community setting, where students come to gather to generate their  inquiry, contribute their different views (questions, ideas, theories,  etc.) about the topic of interest. it is an interactive process where  students may perform individual research before coming back to  revisit some of these ideas that were originally proposed. the  purpose is to improve the ideas, or perhaps if i may call it  knowledge to the group of learners.    Figure 4. BC graphs of the three chosen DUs containing  promising ideas.   Figure 5. Different BC trends exhibited by DU8 (low  initial interest) and DU66 (immediate interest).     DU8 was written by student S7 in response to generic questions  that were posed to the learning community, regarding knowledge,  learning and knowledge building. The general concepts were  explained with details using examples and DU8 was considered a  well-written note, despite being initially uninteresting to the  community due to its lengthy response and the communitys lack  of familiarity and metaknowledge about the topic in the early stages  of discourse. However, the community started paying more  attention to DU8 after subsequent DUs (DU 21 to DU26) consisting  of inquiries from the community required a clear explanation, and  students started seeking for answers and explanations from DUs  that transpired earlier in the discourse. We detected students  retrospectively referring to DU8 after Turn 26, resulting in a higher  BC trend for DU8, even though there were other similar DUs that  were present in later stages of the discourse. Students realized that  other students DUs were unable to adequately address their  inquiries and concerns as succinctly and clearly as S7s note (DU8).  Another discourse unit that we analyzed is from Student S6 and the  following excerpt is quoted summarily from DU66.   Excerpt of DU66 from Student S6: in the first lesson of the course,  I posted the commonly asked question of  how different or similar  are knowledge sharing, knowledge construction and knowledge  creation Subsequently, we were being tasked to read the two  articles  further questions evolved after the reading:  (1) how different or similar is justification of true beliefs  (knowledge) from validation of knowledge  (2) what are the  differences and similarities of authentic knowledge building and  creation of professional knowledge Eventually, T1 mentioned in  the second lesson that kb and kc are equivalent. however, how are  they equivalent Why are they equivalent In the pursuit of deeper  understanding, I read an article- distinguishing knowledge  sharing, knowledge construction, and knowledge creation  discourses that explains the differences between the  terminologies   personally, I think this question and line of  thought would have implications on the implementation of kb in  school settings where teachers have once explored the usage of  discussion forums/bulletin boards, blogs etc. for instructional  practice. How is kb distinctive from other pedagogies How is KF  different from other technological tools Is the tool an essential or  supplemental these are practical questions on the  how   (implementation of kb). what do the rest of you think    There was significant community interest in the discourse unit right  from the moment in time when it was introduced at discourse turn  66. This level of interest was maintained throughout the discourse,  leading us to believe that DU66 contained ideas and content that  were sufficiently intriguing to the community while providing food  for thought for the community. S6 pointed out differences in similar  processes of knowledge sharing and knowledge building while  encouraging the community with thought-invoking questions to  further discuss and debate on Knowledge Forum.    Upon further scrutiny, it was determined that the ideas and content  within DU66 consist of deeper analysis built onto an earlier  discourse unit DU25 by the same student S6. The student S6 had  previously provided his/her perspectives in addition to thought  processes, and have additionally conducted follow-up research to  understand the topic in his/her own capacity further. The analyzed  DU66 was written one week after the first discourse unit DU25,  therefore serving as a report of S6s findings over the week to the  community, and also forms part of the documentation chain that S6  used for reflecting on his/her learning. The community  acknowledged the effort put into DU66 in the following DUs by  answering and discussing mentioned topics within DU66.  Participants in the community were also challenged to improve on   this group of ideas, by building onto S6s perspectives regarding  the topic, so as to further discussion regarding the practical  implementation of knowledge building within the classroom.   Overall, the three analyzed discourse units (DU8, DU18 and DU66)  attracted significant community interests in their content over the  entire period of discourse within the study. Most ideas in the three  discourse units either acted as sources which students continuously  referred to, or served as inspiration for improving ideas within  discourse. Even though DU8 and DU18 attracted delayed  communal interest sometime after introduction in discourse, the  ideas within the two mentioned discourse units were eventually  broadly shared and built upon within the community. DU66  managed to invoke immediate interests from the community and  sustained community engagement throughout the period of  discourse, and this was sufficiently backed up by qualitative  analysis of the discourse unit.    Temporal analysis of BC trends provided us with deeper insights of  how ideas can be traced within discourse units and an alternative  method of analyzing discourse. The usage of temporal analytics  with discourse analysis aids us in further understanding of how  ideas can influence and impact overall community discourse.      5. CONCLUSIONS AND FUTURE WORK  We examined the use of temporal analytics with a step-wise  discourse analysis of an online discussion, which provided insights  into the nature and evolution of ideas that were proposed by  students. We used text mining techniques through SOBEK to  obtain an unbiased set of keywords, which were used with social  network analysis and I2A methodology to identify and classify  ideas in the study of student discourse. We traced the ideas and  determined the impact of these ideas on subsequent discourse using  the BC measure and temporal analysis of BC trends. Overall  findings show that identified ideas within discourse can be  improved through collaborative discussions on forums, and a more  thorough understanding of ideas can be achieved through tracking  and analyzing the impact of promising ideas on discourse.    We are currently building on this study with similar  implementations of analytical tools and methodology presented  within this paper on archived data. In addition, we intend to use  data obtained from ongoing lessons and make use of outcomes  obtained through analysis to provide educators with actionable  feedback, so as to conduct interventions to improve online teaching  and learning.      6. ACKNOWLEDGMENTS  The research reported here is supported by the Centre for Research  and Development in Learning, Nanyang Technological University  (CRADLE@NTU). The research team would also like to thank the  teaching staff and students who participated in this study.      7. REFERENCES  [1] Biggs, J. 1999. What the student does: teaching for enhanced   learning. Higher Education Research & Development, 18, 1,  57-75.   [2] Brinton, C. G., Chiang, M., Jain, S., Lam, H. K., Liu, Z., and  Wong, F. M. F. 2014. Learning about social learning in  MOOCs: From statistical analysis to generative model. In  IEEE Transactions on Learning Technologies, 7, 4, 346-359.     [3] Bnz, D. and Gtschow, K. 1985. CATPAC - An interactive  software package for control system design. Automatica, 21,  2, 209-213.   [4] Cela, K., Sicilia, M., and Snchez., S. 2015. Social Network  Analysis in E-Learning Environments: A Preliminary  Systematic Review. Educational Psychology Review, 27, 1,  219246.   [5] Chen, B., Haklev, S., Harrison, L., Najafi, H. and Rolheiser,  C., 2015. How do MOOC Learners Intentions Relate to  Their Behaviors and Overall Outcomes. Statistics, 53, 5.    [6] Chiu, M. M., and Fujita, N. 2014. Statistical Discourse  Analysis of Online Discussions: Informal Cognition, Social  Metacognition, and Knowledge Creation. In Knowledge  Creation in Education. Springer Singapore, 97-112.   [7] De Liddo, A., Shum, S.B., Quinto, I., Bachler, M. and  Cannavacciuolo, L., 2011. Discourse-centric learning  analytics. In Proceedings of the 1st International Conference  on Learning Analytics and Knowledge. ACM, 23-33.   [8] Engestrm, Y. 1999. Activity theory and individual and  social transformation. In Perspectives on activity theory, Y.  Engestrm, R. Miettinen and R.-L. Punamki, Eds.  Cambridge University Press, Cambridge, England, 19-38.   [9] Fairclough, N., 2013. Critical discourse analysis: The  critical study of language. Routledge.   [10] Freeman, L. C. 1978. Centrality in social networks  conceptual clarification. Social networks, 1, 3, 215239.   [11] Gilbert, G.N., Mulkay, M. 1984. Opening pandora's box: A  sociological analysis of scientists' discourse. Cambridge  University Press, Cambridge, England.   [12] Holsti, O.R. 1969. Content analysis for the social sciences  and humanities, Addison-Wesley Pub. Co.   [13] Johnson, L., Smith, R., Willis, H., Levine, A., and Haywood,  K. 2011. The 2011 horizon report. The New Media  Consortium, Austin, Texas.   [14] Lakatos, I. 1970.The methodology of scientific research  programmes. In Criticism and the Growth of Knowledge, I.  Lakatos and A. Musgrave, Eds. Cambridge University Press,  Cambridge, England, 91-195.   [15] Lee, V. Y. A., Tan, S. C., & Chee, J. K. K. 2016. Idea  identification and analysis (I2A): A search for sustainable  promising ideas within knowledge-building discourse. In  Transforming learning, empowering learners: The  International Conference of the Learning Sciences (ICLS)  2016, Singapore. International Society of the Learning  Sciences, 90-97.   [16] Locke, J. 1841. An essay concerning human understanding.   [17] Lowe, W. 2002. Software for content analysisA Review.  Cambridge: Weatherhead Center for International Affairs  and the Harvard Identity Project.   [18] Mercer, N. 2008. The seeds of time: Why classroom dialogue  needs a temporal analysis. The Journal of the Learning  Sciences, 17, 1, 3359.   [19] Molenaar, I., 2014. Advances in temporal analysis in  learning and instruction. Frontline Learning Research, 2, 4,  15-24.   [20] Nonaka, I., & Takeuchi, H. 1995. The knowledge-creating  company: How Japanese companies create the dynamics of  innovation. Oxford University Press.   [21] Oshima, J., Oshima, R., and Knowledge Forum Japan  Research Group. 2007. Complex network theory approach to  the assessment on collective knowledge advancement  through scientific discourse in CSCL. In Proceedings of  CSCL 2007, Mahwah, NJ. Lawrence Erlbaum, 563-565.   [22] Oshima, J., Oshima, R., and Matsuzawa, Y. 2012.  Knowledge Building Discourse Explorer: a social network  analysis application for knowledge building discourse.  Educational technology research and development, 60, 5,  903-921.   [23] Reategui, E., Epstein, D., Lorenzatti, A., and Klemann, M.  2011. Sobek: a text mining tool for educational applications.  In International Conference on Data Mining, 59-64.   [24] Reimann, P. 2009. Time is precious: Variable-and event- centred approaches to process analysis in CSCL research.  International Journal of Computer-Supported Collaborative  Learning, 4, 3, 239257.   [25] Rodrigues, M.M. and Sacks, L., 2004, December. A scalable  hierarchical fuzzy clustering algorithm for text mining.  In Proceedings of the 5th International conference on recent  advances in soft computing, 269-274.   [26] Scardamalia, M. 2002. Collective Cognitive Responsibility  for the Advancement of Knowledge. In Liberal Education in  a Knowledge Society, B. Smith Ed. Open Court, Chicago, 67- 98.   [27] Scardamalia, M., Bereiter, C. 2003. Knowledge building.  Encyclopedia of Education. Macmillan Reference, New  York, NY, 1370-1373.   [28] Scardamalia, M. 2004. CSILE/Knowledge Forum. In  Education and technology: An encyclopedia, Santa Barbara.  ABC-CLIO, 183-192.    [29] Scott, J. 2012. Social Network Analysis. SAGE Publications.   [30] Siemens, G. 2013. Learning Analytics: The Emergence of a  Discipline. American Behavioral Scientist. 57, 10, 1380- 1400.   [31] Suthers, D. 2006. Technology affordances for intersubjective  meaning making: A research agenda for CSCL. International  Journal of Computer-Supported Collaborative Learning, 1,  3, 315-337.   [32] Wasserman, S., and Faust, K. 1994. Social network analysis:  Methods and applications. Cambridge University Press,  Cambridge, England.   [33] Wortham, D.W. 1999. Nodal and Matrix Analyses of  Communication Patterns in Small Groups. In Proceedings of  the Computer Support for Collaborative Learning (CSCL)  1999 Conference, Palo Alto, CA. Stanford University, 681- 686.   [34] Zhang, J., Scardamalia, M., Reeve, R., and Messina, R. 2009.  Designs for collective cognitive responsibility in knowledge  building communities. Journal of the Learning Sciences, 18,  1, 744.       "}
{"index":{"_id":"17"}}
{"datatype":"inproceedings","key":"Boroujeni:2017:DMD:3027385.3027391","author":"Boroujeni, Mina Shirvani and Hecking, Tobias and Hoppe, H. Ulrich and Dillenbourg, Pierre","title":"Dynamics of MOOC Discussion Forums","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"128--137","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027391","doi":"10.1145/3027385.3027391","acmid":"3027391","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, content analysis, discussion forum, massive open online courses, social network, temporal analysis","Abstract":"In this integrated study of dynamics in MOOCs discussion forums, we analyze the interplay of temporal patterns, discussion content, and the social structure emerging from the communication using mixed methods. A special focus is on the yet under-explored aspect of time dynamics and influence of the course structure on forum participation. Our analyses show dependencies between the course structure (video opening time and assignment deadlines) and the over-all forum activity whereas such a clear link could only be partially observed considering the discussion content. For analyzing the social dimension we apply role modeling techniques from social network analysis. While the types of user roles based on connection patterns are relatively stable over time, the high fluctuation of active contributors lead to frequent changes from active to passive roles during the course. However, while most users do not create many social connections they can play an important role in the content dimension triggering discussions on the course subject. Finally, we show that forum activity level can be predicted one week in advance based on the course structure, forum activity history and attributes of the communication network which enables identification of periods when increased tutor supports in the forum is necessary.","pdf":"Dynamics of MOOC Discussion Forums  Mina Shirvani Boroujeni CHILI Group, EPFL, RLCD, Station 20  1015, Lausanne, Switzerland mina.shirvaniboroujeni@epfl.ch  Tobias Hecking COLLIDE group,University of Duisburg-Essen  47048 Duisburg, Germany hecking@collide.info  H. Ulrich Hoppe COLLIDE group,University of Duisburg-Essen  47048 Duisburg, Germany hoppe@collide.info  Pierre Dillenbourg CHILI Group, EPFL, RLCD, Station 20  1015, Lausanne, Switzerland pierre.dillenbourg@epfl.ch  ABSTRACT In this integrated study of dynamics in MOOCs discussion forums, we analyze the interplay of temporal patterns, dis- cussion content, and the social structure emerging from the communication using mixed methods. A special focus is on the yet under-explored aspect of time dynamics and influ- ence of the course structure on forum participation. Our analyses show dependencies between the course structure (video opening time and assignment deadlines) and the over- all forum activity whereas such a clear link could only be partially observed considering the discussion content. For analyzing the social dimension we apply role modeling tech- niques from social network analysis. While the types of user roles based on connection patterns are relatively stable over time, the high fluctuation of active contributors lead to fre- quent changes from active to passive roles during the course. However, while most users do not create many social connec- tions they can play an important role in the content dimen- sion triggering discussions on the course subject. Finally, we show that forum activity level can be predicted one week in advance based on the course structure, forum activity history and attributes of the communication network which enables identification of periods when increased tutor sup- ports in the forum is necessary.  Categories and Subject Descriptors K.3.1 [Computer Uses in Education]: collaborative learning; H.1.2 [User/Machine Systems]: Human Fac- tors  Keywords Massive open online courses, MOOCs, Discussion forum, So- cial network, Temporal analysis, Content analysis  1. INTRODUCTION  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13-17, 2017, Vancouver, BC, Canada c 2017 ACM. ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027391  Massive Open Online Courses (MOOCs) offer high qual- ity education provided by domain experts of various sub- jects to a massive number of participants with very different backgrounds and constraints. Thereby, flexibility in plan- ning and organizing learning activities is often considered as one of the main benefits offered by MOOCs [17]. Re- cently, importance of time factor in MOOCs analysis has been highlighted in several studies [7, 4, 3]. The timing of learning activities is a key issue in any educational situa- tion, and it is even more critical in MOOCs, where different time structures have affordances and constraints [7]. In [3] timing of learners activities over different weekdays and dif- ferent times of the day and its relation to external factors such as employment status has been explored. Moreover, in a recent work shirvani et al. [4] proposed quantitative meth- ods for analyzing the timing patterns of learners activities and measuring the regularity level of students in terms of following a particular weekly study plan.  Furthermore, social learning is considered an important element of scalable education in MOOCs [5] and the po- tential to establish collaboration on massive scale has been argued. However, in most MOOCs, with their lack of indi- vidual support for students by tutors, discussion forums are the only channel for support and for information exchange between peers. Many studies point out limitations of discus- sion forums such as low overall participation [15, 19], and sometimes a lack of responsiveness [30]. Consequently, there is a discrepancy between the goal of establishing a learning community and the actual implementation of collaboration mechanisms. Given this, it has been argued that collab- oration in MOOCs under consideration of the limitations of asynchronous communication and heterogeneous popula- tion of participants has to be much better supported [22, 33]. This includes personalization, support in finding peers for information exchange, and formation of learning groups [22].  In order to foster the development of sophisticated sup- port mechanisms for peer exchange, a deep understanding of the current situation and learners interactions within the discussion forums is inevitable. For this reason, analyses of MOOCs discussion forums have received much attention in recent years. Previous studies in the literature have inves- tigated discussion forums from different perspectives such as learners engagement and activities [15, 19, 1], discussion themes and topics or linguistic properties of written mes- sages [28, 23, 16], structure of the communication network,    group formation and social interactions among forum par- ticipants [9, 10, 18]. A detailed overview of previous works on forum analysis is presented in section 2.  The goal of this work is to extend this body of research by providing an integrated study on all the mentioned aspects combining different analysis methods. Moreover, consider- ing the importance of time in online discussions, a particular focus in this paper is on dynamics and temporal patterns. Time dynamics are often neglected by existing works which consider aggregated variables over time to describe forum communications. In particular this work covers three main dimensions of discussion forums: Time, content and social. In time dimension we consider daily timeline of course duration with the main course related events, namely video openning time and assignment deadlines, and track the evo- lution of forum activity with respect to the course timeline. In content dimension, we investigate the topics of forum discussions and in social dimension, we study the un- derlying social structure of discussion forums (global level) and learners roles in the communication network (individ- ual level). By contrasting the results of these analyses in- sights on the interrelation between forum activity, discussion content, and social communication structure can be shown. Concerning the aforementioned aspects we aim to answer the following research questions:   RQ1. How does the overall activity in discussion fo- rums evolve over time and is it influenced by course structure [time dimension]   RQ2. How do the discussion topics evolve over time and is it related to the course structure [content + time dimension]   RQ3. Does the course structure influence the struc- ture of information exchange network [social + time dimension]   RQ4. How do the students structural roles in discus- sion forum evolve over time [social + time dimension]   RQ5. How are the students structural roles in the communication network related to discussion content [content + social dimension]   RQ6. Is the overall forum activity predictable  The rest of the paper is organized as follows. Section 2 reviews related work and section 3 present the dataset. In Section 4 overall forum activity across course timeline is investigated (RQ1 ) Section 5 explores the dynamics of dis- cussion topics (RQ2 ). In section 6 dynamics of social com- munication structure at global (RQ3 ) and individual level (RQ4 ) is being explored. Section 7 investigates the relation between social and content aspects (RQ5 ). In section 8, extracted features from several of previous sections are inte- grated into a machine learning model to predict the forum activity level (RQ6 ). Section 9 provides a comprehensive discussion of results and concludes the paper.  2. RELATED WORK Existing studies on discussion forum analysis focus on dif-  ferent aspects such as users activity, produced content, and social structure. The question how engaged different MOOC users are in discussion forums was addressed in various stud- ies. The fact that the discussion forums are commonly used only by a small fraction of the course participants [15] is  meanwhile commonly known. Furthermore, the fraction of users who use the forums intensively is even smaller [19] and discussion volume often represents a continuous declines over the duration of the course [5]. These findings contra- dict the intention of discussion forums to foster collaborative knowledge building between course participants from vari- ous knowledge backgrounds [24]. On the other hand there is evidence for a relation between engagement in discussion fo- rums and different styles of engagement with respect to other course activities [1] and that forum activity goes along with completion rates [1, 9]. Forum participation features have also been employed to predict performance and engagement in the course [20, 31].  Apart from questions about the activity of course partic- ipants in the discussion forum the actual content of the dis- cussions is of interest as well. This typically requires natural language processing to analyse the textual contributions of forum users. The types and themes of discussion forums can be very diverse and are not necessarily related to the actual course subject [19], for example, non-course subject related discussions like search for learning groups or personal intro- ductions, technical and organisational support. Since col- laborative knowledge building and information exchange is of great interest Wise and Cui [28] proposed content-based indicators for subject related discussions. Similarly, Rossi et al. [23] build supervised classifiers to predict the type of discussion of forum threads. Another strand in content- based analysis is concerned with the nature of forum posts. Classification of speech-acts in MOOC discussion forums [2, 16], such as questions, answers or issue resolution, provides insights into the composition of discussion forum from the perspective of contribution types. Apart from speech acts, contributions can also be classified according to constructs of conceptual and operational learning levels according to the Anderson and Krathwohls taxonomy [29].  In the aspect of the social and communication structures emerging from interaction in course forums, social network analysis is applied to networks of interconnected forum users to investigate structural patterns and the underlying rela- tional organisation of a course community. Gillani et al. [9, 10] analyzed networks of forum users connected by co- contribution to the same discussion threads. They argue that the coherence of the social structure mainly depends on a small set of central users and the forum users can be rather be considered as a loosely connected crowd rather than a strongly connected learning community. These difference between regular forum users and occasional posters was ex- plicitly taken into account by Poquet and Dawson [18] show- ing that regular users shape a denser and more centralised communication network since they have more opportunities to establish connections. In the context of structural analy- sis of forum communication networks, different studies used exponential random graph models (ERGMs) [21] or related statistical network analysis models to identify factors that influence the emergence of the observed network character- istics [18, 13, 32, 14]. In general these results reveal an effect of reciprocated ties and a lack of centralization of the net- works to few influential users. On the level of individuals, social network analysis is further applied to identify different roles of users based on their social connections and thematic affiliations [12, 11]. This will be explicitly taken up in this work later on in Section 6.    Figure 1: Number of message per day in Scala.  3. DATASET The dataset used in this study comprises of two engineer-  ing MOOCs offered by Coursera entitled: Functional Pro- gramming Principles in Scala and Principles of Reactive Programming. Hereafter referred to as Scala and Reactive. Both courses were eight weeks long and videos were released in a weekly basis. The final grade was based on a weighted average of six assignments corresponding to different weeks and passing grade was 60 out of 100. Discussion forums in both courses were structured several into sub-forum such as general discussions, search for learning group, questions and clarifications about course lectures and assignments. We re- stricted our analysis to lectures and assignments sub-forums as our focus is in tracking the evolution of discussions related to the course content. This resulted in 7,699 messages (posts or comments) by 1,175 different participants in 939 threads for Scala course and 12,283 messages by 1,902 participants in 1,702 threads in Reactive course.  4. FORUM ACTIVITY OVER TIME To investigate the time dynamics of forum activity and  its relation with the course structure (RQ1), we extracted number of messages (posts or comments), number of forum contributors (participants who wrote a message) and num- ber of threads added to the content forums on each day. As an example, Figure 1 represents daily count of messages in the discussion forum in Scala MOOCs with respect to main course events: video release (dashed blue lines) and assign- ment deadlines (solid red lines). Contributors and threads charts follow a similar trend. As it can be perceived from the charts, despite the decline of forum activity over time, at several points close to the video release or assignment deadlines there is an increment of messages in the discus- sion forum. This is better perceived from Figure 2 reflecting that the highest level of forum activity is associated with two to three days after the video release and activity level declines afterwards. Considering the proximity assignment deadlines, as represented in Figure 2b, the forum activity level increases as the deadline approaches. These observa- tions further confirm the dependency between course struc- ture and forum activity level suggesting that forum activity is a function of the weekly course structure.  5. DISCUSSION CONTENT OVER TIME With respect to our research question on how the content  of discussion forum evolves during the outline of the course (RQ2), in the following we present an in-depth analysis con- sidering the posts content over time.  5.1 Method  (a) (b)  Figure 2: Average number of new messages depend- ing on the proximity to video release (a) and assign- ment deadline (b) in Scala.  To investigate the discussion content over time [RQ2], we compute the distributions of certain indicator phrases over time to investigate potential relations with the course events. Table 1 gives an overview of the used indicator phrases. The set of subject related keywords was created specifically for each course. First, the most frequent concepts in the discussion threads were determined using the Open Calais API1. This initial set of keywords was then manually refined based on the course outlines and detailed knowledge about the course topics (e.g. common concepts and tools in func- tional and reactive programming). Different spellings and synonyms were explicitly taken into account, for example, lambda function and anonymous function were mapped to the same concept. This resulted in a set of 25 subject related keywords for Scala and 19 for Reactive. In addition to subject related keywords that can directly be mapped to specific course topics, we can also identify terms that are of general nature but indicate content related discourse. These general content related keywords can appear without mentioning specific domain concepts (e.g. I have no idea how to approach this problem. Can someone clarify ), or they can be used in combination with domain terms (e.g. Is there a difference between a lambda and an anonymous func- tion). Such keywords as difference between have been characterised by Daems et al. [6] as signal concepts. Our overall distinction between types of indicators for certain categories of contributions has also been inspired by Wise and Cuis findings on the identification of forum threads related to the course content using indicator phrases [28]. Additionally, we also identify resource related keywords mentioning course material, i.e. videos and assignments as well as posts containing hyperlinks to other resources. Fi- nally, since both of the courses analysed in this study are concerned with programming, technical posts which con- tain source code or error messages are identified based on <code> or <error> tags in the post markup.  5.2 Results  5.2.1 Course related posts over time The ratio of posts containing general content related indi-  cator phrases and mentions of course resources for each day in Scala can be seen in Figure 3. For the Reactive course there is a similar pattern so we do not report these diagrams for space reasons. In general, one can see that content re-  1http://www.opencalais.com/opencalais-api/    Table 1: Examples of indicator phrases used to track discussion topics over time  Subject related keywords Keywords related to the main course topics.  General content related keywords dont understand, difference, no idea, solution, feed- back, clarify, grade, question, answer, example  Resource related keywords video/lecture, assignment/quiz, submission, post con- tains a hyperlink  Technical content Post contains source code, Post contains an error mes- sage  Figure 3: Genereal content, technical, and resource related keywords over time (days) in Scala  lated discussions are not strongly influenced by the course structure. Moreover, content is discussed throughout the en- tire period of the course. However, regarding the mentions of lecture videos and assignments (resource related keywords) there is often an increase short after video releases indicating that a new video is directly discussed after release.  5.2.2 Subject related discussions over time As reported above general course content related  keyphrases do not show a strong pattern that can be re- lated to the course structure. Apart from the the occur- rence of general content related keywords (c.f. Table 1), Figure 4 gives concrete examples for the occurrence terms in forum posts that specifically relate to the course subject of Reactive course (Same patterns could be identified for Scala). Certain terms cannot be definitely related to course events. This suggest that some participants of the MOOC have a certain background knowledge when they join the course, and thus, are able to discuss important concepts independently from the conveyed knowledge in the lecture videos. On the contrary there are terms that are discussed much more extensively after a video release. In given exam- ple of the Reactive course, terms like promise or mentions of the akka reactive programming framework are clearly introduced by the corresponding lecture videos. Interest- ingly the lecture introduced concepts remain in the discus- sion until the end of the course indicating that the discussion  Figure 4: Examples for subject related keyphrases in Reactive  forums are to some extent useful for further discussion of lec- ture introduced knowledge that can be connected with the following course sections.  6. SOCIAL COMMUNICATION STRUC- TURE  In this section we explore the social aspect of discussion forum and investigate the network of information exchange among forum contributors. In particular we study the infor- mation exchange network at two levels: global and individ- ual. At the global level we explore the evolution of network over time (section 6.2.1), whereas at the individual level we focus on students roles in the network (section 6.2.2).  6.1 Methods  6.1.1 Network extraction In related work there are several approaches to model so-  cial networks from forum communication. The most simple approach is to build an undirected network by linking all fo- rum users who contribute to the same discussion threads as in [10]. Another approach is to build reply networks where users who write in a discussion thread are linked to the thread initiator by directed outgoing relations [14]. However, since we are interested in the concrete informa- tion exchange relations between course participants a more complex network extraction method introduced in [12] was applied. This method incorporates three steps: (1) Classifi- cation of forum posts into three classes information giving, information seeking, and others using supervised classi- fication models (bagged random forests). The models were trained on a set of 200 hand classified posts, where all posts that request information, for example, concrete questions on course topics or asking for advice are coded as informa- tion seeking. Posts that provide any kind of information to information seekers are subsumed as information giving. Posts classified as others cannot be associated to any of the other classes. As reported in [12] the accuracy of the classification model is considerable (F1=0.77). (2) Extrac- tion of relations between posts. After deletion of all other posts, discussion thread (or a sub-thread comprising of com- ments to a parent post) can be decomposed into sequences    of information seeking posts followed by sequences of in- formation giving posts. In this sense a thread is considered as a sequence of alternating information seeking sequences and information giving sequences. In the most usual case, posts of information giving sequence refer to the most re- cent information seeking sequence. This allows to extract a network of posts. Since the time when forum posts was made is available, an edge between two post nodes carry a timestamp indicating when it was created. (3) The fi- nal information exchange network is derived by collapsing all nodes of the post network from step (2) with the same author into a single node. In this network there exist a di- rected edge between two nodes (representing forum users) if the first user provided some information to the second user. For more details about the network extraction process we refer to [12].  Based on the timestamps of the edges, the resulting net- work can be divided into a sequence of time slices corre- sponding to certain time window. Each time slice contains all the nodes (forum users) but only the edges that where present in the corresponding time window. This allows to study the dynamics of the social communication structure in detail, as it will be explained in the following.  6.1.2 Network structure over time In order to study the temporal dynamics of network, we  consider network slices over one week periods using a sliding window approach. This results in one network slice for each day of the course (d > 6) corresponding to the forum activity during the past seven days ([d  6 : d]). For each network slice a set of classic structural attributes were then com- puted, including number of nodes and edges, average node degree2, network density3, average path length4, diameter5  and global clustering coefficient6.  6.1.3 Role modeling Role modeling in social network analysis is often referred  to as positional analysis [26], where the position of a node is determined by its connection patterns to other nodes. Blockmodeling [8] is a technique to decompose the set of nodes of a network into clusters of nodes with (almost) equivalent connection patterns. In network science those clusters are interpreted as users who have similar roles or positions in the community, hereby referred to as structural roles. Various notions of equivalence can be used for clus- tering the nodes. The most common are structural equiva- lence and regular equivalence. While structural equivalence between two nodes requires that the nodes have exactly the same neighbours, regular equivalence relaxes this strict cri- terion such that equivalent nodes should have connections to nodes that are equivalent themselves. This recursive defini- tion of regular equivalence can easily be understood as color- ing the nodes of a network such that nodes that are equiva-  2average number of connections that a node has to other nodes 3ratio of the number of edges and the number of possible edges 4average number of steps along the shortest paths for all possible pairs of connected nodes 5length of the longest shortest path (geodesic distance) be- tween each two nodes 6Fraction of closed triangles (cliques of three) and possible triangles  Figure 5: Core-periphery role structure  lent have the same color, and nodes of the same color receive edges from the same set of colors and point to nodes of ex- actly the same set of colors. We refer the interested reader to [8] for mathematical details. Clustering of nodes according to structural and regular equivalences implies some interest- ing characteristics regarding the possible relations between the clusters. In a perfect structural equivalence clustering all relations between each pair of clusters c1 and c2 are ei- ther complete (all nodes in c1 point to all nodes in c2), or non-existent (there are no relations between nodes in c1 and c2). In the less strict notion of regular equivalence all nodes in c1 point to at least one node in c2 and all nodes in c2 receive arcs from at least on node in c1. In the case of com- munication networks such as the ones extracted from the discussion forums regular equivalence clustering are more reasonable because the requirement of structural equivalence is too strict for sparse networks and would result in many very small clusters. Furthermore, instead of perfect regular equivalence an approximate notion of regular similarity is used in conjunction with hierarchical clustering.  A blockmodel depicts clusters of nodes and the relational patterns between these clusters, and thus, reduces the possi- bly very complex social network into an interpretable macro structure. This allows for uncovering the inherent organ- isation of a network, such as hierarchical communication structures having nodes assigned to different levels, or core- periphery structures. In this study the core-periphery pat- tern based on regular similarity of nodes is of particular importance. While related works report that the overall forum communication network in MOOCs resemble a core- periphery structure [12, 14], we discovered that this is also the case for each time slice of the evolving forum communi- cation network of the investigated courses.  Figure 5 depicts a typical core-periphery role structure that can be found in communication networks. There are four roles (clusters) represented as nodes and connections between them. The core users form a cohesive subgroup in the sense that they have many communication relations within their cluster. Furthermore, there are two peripheral roles that are not cohesive but are connected to other clus- ters. These two peripheral roles can be characterized as help givers (HG) or help seekers (HS) respectively since they have mainly outgoing or ingoing relations. There is also a fourth role inactive/isolated, which comprises all users who do not have any connections to others in a particular time slice. This can have two reasons, either they were not active during the time span for which the model was created (inactive) or their posts could not be linked to other posts (isolated), for example, a help-seeking post without replies or posts not related to information exchange (Section 6.1.1).  Later on in Section 6.2.2 a core-periphery blockmodel is    derived from each time slice of the evolving information ex- change network representing the role structure in different periods of the course and used to investigate role changes of course participants over time.  6.2 Results  6.2.1 Evolution of network structure over time Considering our research question on the evolution of net-  works structure (RQ3), based on the trends observed in section 4, we hypothesize that course schedule influences discussion forum network structure. For instance with the increment of contributors and messages in the forum close to the video release or assignment deadlines, new nodes or edges could appear in the network influencing the network size, degree, density or other attributes.  Figure 6 for Scala is representative for the evolution of network attributes over weekly network slices (extracted us- ing sliding window as described in section 6.1.2) for both courses. The overall decrease of forum activity towards the end of the course is also reflected by network size metrics (nodes and edges count). The networks are very sparse since the low average degree in relation to the network size results in a low density (< 0.02). Despite the compara- tively larger network size in Reactive, in both courses aver- age path length are relatively small (< 3 in Scala and < 6 in Reactive) throughout the course. Short path length though sparsity of the connections are a typical property of small- world networks [27] but in contrast to classical small-world network models the clustering coefficient is low. This indi- cates that the communication structure does not evolve into densely connected communities but rather into sparse parts interconnected via a few highly connected nodes.  On contrary to our hypothesis, course events do not show any direct influence on the network structure. One plau- sible explanation could be the structural limitations of the communication network such as the absence of persistent discussion groups throughout the course duration, which is also pointed out in previous studies such as [18] and [9]. Moreover, the increase of messages count in the discussion forum in a particular period of time, could be resulting from a sequence of information exchange messages between few students, which would not add new edges or nodes to the network. Since an edge in the networks aggregates possi- bly multiple communication events between a pair of users, such message sequences would not be reflected in the net- work structure.  6.2.2 Structural roles over time To track the evolution of individuals structural role in the  discussion forum (RQ4), we consider successive bi-weekly slices of the network described in section 6.1.1 and extract the macro structure of each network slice as described in section 6.1.3. The resulting role models for all four time slices in both courses follow the same structure as in Figure 5. Based on the resulting role models, we then construct se- quence of roles for each individual over the four time slices (phases). Additionally in each phase we differentiate late- comers or drop-outs from students who follow the course but have no contribution in the information exchange net- work. This results in a fifth role in sequences which we refer to as course inactive. Figure 7 show role sequences of fo- rum participants in both courses. In the sequence charts,  Figure 6: Network attributes over time (Scala)  (a) Scala (b) Reactive  Figure 7: Role sequences of forum participants  each horizontal line corresponds to role sequence of a par- ticular participant over the four biweekly phases. According to Figure 7, in both courses students are often active only in one or two phases and instances of active forum partici- pation throughout the course are quite rare. Furthermore, a considerable portion of active students in each phase are new users (i.e. for the first time have a role different from inactive/isolated), which could also imply that persistent discussion groups in the forum are not very common.  Next, to identify common patterns in structural role se- quences, we clustered role sequences using hierarchical clus- tering method with optimal matching as distance metric and substitution costs determined based on transition probabili- ties between states [25]. Number of clusters was determined based on the resulting dendograms.  Figure 8 and 9 represent the resulting clusters of role se- quences for both courses. As it can be inferred from the figures, clusters in both courses can be characterized as one time help seekers (cluster 1, N = 273 in Scala and N = 752 in Reactive), one time help givers (cluster 2, N = 383 in Scala and N = 276 in Reactive), active forum participants    Figure 8: Clusters of role sequences (Scala).  (cluster 3, N = 123 in Scala and N = 160 in Reactive) or dropouts (cluster 4, N = 124 in Scala and N = 303 in Reactive). The first cluster in Reactive course is slightly different from Scala, as it includes one time help seekers and also some help givers.  Comparison of average grade obtained by each cluster of participants (Figure 10) reveals that in in both courses, one time help seekers have significantly lower grades compared to one time help givers (77 vs 86, F [1, 1] = 23.29, p < 0.001 in Scala, 86 vs. 92, F [1, 1] = 0.2, p = 0.002 in Reactive). Additionally, despite the fact that one time help givers have lower forum participation compared to active participants, both groups achieve comparably high scores (86 and 88 in Scala, 92 in Reactive). One possible interpretation could be that active forum participants in this course, take advantage of discussion forum to advance their knowledge and resolve difficulties with respect to the course materials, whereas one time help givers are students with higher expertise level who only occasionally participate in the discussion forum, and when they do so, they provide answers to questions asked by other participants.  7. SOCIAL STRUCTURE AND DISCUS- SION CONTENT  For the sake of investigating the relation between social structure and content (RQ5), in the following results of the previous section 6.2.2 are combined with the content analy- sis reported in Section 5. Tables 2 and 3 give statistics on the content of the posts made per user in the four different role sequence clusters (c.f. Section 6.2.2). Note that one post can contain keywords of different types. There is a sig- nificant relation between the different role sequence clusters and the distributions of different post types (chi2 = 19.29, p = 0.02 in Scala, chi2 = 27.71, p = 0.001 in Reactive). Especially for the Scala course, it is interesting to note that one time help seekers, in comparison to the other groups, have relatively more posts mentioning course subject key- words or posts containing general content related phrases.  Figure 9: Clusters of role sequences (Reactive)  (a) Scala (b) Reactive  Figure 10: Average grades for clusters role se- quences  Furthermore, in both courses these types of user have more technical posts and mention course resources more often. This finding is not obvious and gives interesting insights into the characteristics of forum users who are engaged in discus- sions in a limited time span. While related works suggest that the structural coherence of the forum communication mainly depends on the small set of very active users [10, 19], the content analysis of the posts shows that the other users can also have an important impact on the discourse by triggering focused discussions on specific subject areas and mentioning concrete problems.  8. PREDICTING FORUM ACTIVITY LEVEL  In order to predict the overall forum activity (RQ6) we trained three predictive models for number of new threads, messages and forum contributors on each day of the course.  8.1 Method Table 4 provides an overview of the features considered in  the predictive models. In particular three categories of fea- tures were extracted for each course day (d): previous forum activity, structural features and network features. Previous    Table 2: Number of keywords in posts per user by structural role clusters for Scala.  Cluster Subject General content  TechnicalResource #Posts  One time help seekers  0.74 0.26 0.15 0.25 1148  One time help givers  0.59 0.23 0.12 0.22 1284  Active par- ticipants  0.65 0.21 0.16 0.23 2978  Dropouts 0.67 0.22 0.14 0.19 504  Table 3: Number of keywords in posts per user by structural role clusters for Reactive.  Cluster Subject General content  TechnicalResource #Posts  One time help seekers  0.59 0.21 0.11 0.23 3405  One time help givers  0.59 0.2 0.08 0.22 1601  Active par- ticipants  0.56 0.21 0.08 0.22 4534  Dropouts 0.56 0.18 0.08 0.22 1947  forum activity features encode the volume and intensity of forum activity on each day and on the previous days. For instance T0 represents count of threads created on the cur- rent day (d) and Tk>0 reflects the number of new threads on k days before. Structural features describe the properties of a day, related to the course structure such as time after video release or time before assignment deadlines. Network features describe the attributes of the network slice on k days before the current day (See section 6.1.2 for details on network partitioning and features description).  Additionally we consider a forth category of features de- scribing the initial forum activity level, during the first week of the course. Such features could act as a normaliza- tion factor to compensate the difference in intrinsic popu- larity of discussion forums in different MOOCs.  Using the described features, we built three regression models for estimating number of forum threads (T0), mes- sages (M0) and contributors (C0) on a day. Several machine learning methods such as support vector regression models (SVR) with linear and RBF7 kernels, random forests and neural networks were applied for training the models. Data from Scala and Reactive courses was randomly partitioned into train (70%) and test (30%) sets and 10-fold cross val- idation was used to tune the models parameters. Highly correlated (r > 0.7) and linearly dependant features were removed from the features set prior to model training.  8.2 Results In all cases SVR with linear kernels resulted in smallest  prediction error, reported in Table 5. Predictive models cap- ture 81% to 83% variance of the dependant variable and pro- vide quite accurate predictions as reflected by low values of normalized root-mean-square errors (NRMSE) on the test data. Based on the analysis of the variable importance in the obtained models the first six features include count and  7radial basis function  Table 4: Description of features for each day Previous forum activity  Tk number of new threads initiated on day d k. Mk number of new messages created on day d k. Ck number of forum contributors on day d k. TMk mean time between successive forum writing events TTk mean time between successive thread initiating  events Structural features  Dv number of days after the latest video release. Da number of days after the latest assignment release. Dd number of days left to the next assignment dead-  line. Dr ratio of the current day (d) to the course length  encoding what percentage of the course is passed. Na number of assignments open for submission.  Network features Netk network features on day dk, including nodes and  edges count, diameter, average degree, path length and clustering coefficient  Initial forum activity (first week) W1T mean and standard deviation of threads count per  day, mean time between new threads (3 features) W1M mean and standard deviation of messages count  per day, mean time between messages (3 features) W1C mean and standard deviation of forum contributors  per day (2 features)  average time between previous forum activity, ratio of the passed course length (Dr), number of days after latest video release (Dv), average network degree on previous days and number of open assignments (Na). Proposed models are capable of predicting the forum activity level one week in advance as forum activity history and network features in- cluded in the models correspond to seven days before the prediction day (k = 7). This information provided to the teaching team, could enable them to prepare the logistics to efficiently support students in discussion forums, mainly during the high activity periods.  Table 5: Overview of predictive models and results DV Features R2  (train) NRMSE  (train) NRMSE (test)  T0 Prev, Struct, W1T , Net7 0.81 0.40 0.28 M0 Prev, Struct, W1M , Net7 0.80 0.27 0.23 C0 Prev, Struct, W1C, Net7 0.83 0.27 0.22 Normalized RMSE by mean of observed values  9. DISCUSSION AND CONCLUSION In this work we applied mixed methods to investigate the  forum communication in two MOOCs in the time, content, and social dimension leading to insights regarding the re- search questions formulated in the beginning (Section 1).  In the time dimension, in Section 4 we investigated how the outline of the courses (video release and assignment deadlines) influences the overall forum activity (RQ1). We could observe an increase of the number of posts before dead- lines and after video release days, and thus conclude that course events have an impact on the forum communication.    Surprisingly, based on the content analysis of the posts over time (RQ2), there was no clear coupling between the course structure and quantity of general content related, re- source related, and technical posts as reported in section 5. However, mentions of some specific terms regarding the course subject tend to increase after video releases indicat- ing that some discussion topics are introduced by the course while others are brought into discussion by the participants themselves.  The temporal dynamic of the social structure emerging from the forum communication with respect to course events (RQ3) was analyzed in Section 6.2.1. Here we could show that the global organisation (network characteristics) of the communication network is independent of the course struc- ture. One reason can be the absence of a sustainable forum community and the high fluctuation of the active contribu- tors. Consequently, there is no inherent self-organisation of the network, which would require coordination and mainte- nance of social relations. This further supports the claim of Gillani and Eynon [9] that MOOC forums resemble decen- tralized crowd behaviour rather than a social community.  A more user centric perspective on the integrated analy- sis of the time and the social dimension was taken in sec- tion 6.2.2 to answer research question on how roles of forum users evolve (RQ4). Interestingly, the role structure of the information exchange network comprising of a small cohe- sive core of active contributors, and peripheral help-giving or help-seeking users that has also been reported for static snapshots of the network in [12] and [14], persist over sev- eral time slices. While the overall structural organisation of the networks is stable it could be shown that the associa- tion of users to roles changes drastically over time. Only a small subset of the most active (core) users retain an active role over time, and majority of learners are active in only very few (mostly one) time slice. It could be seen that the fluctuation of active users is so pervasive such that in each time slice even the majority of users are newcomers in the sense that they form connections to other users for the first time. This can be considered as a major obstacle for the emergence of a sustainable community and further explains the irregularities in the overall network structure mentioned above.  Regarding research question on the relation between stu- dents roles in the communication network and discussion topics (RQ5), in section 7 the combination of structural role models and discussion content was investigated. Re- sults showed that even if peripheral users (one time help- givers and seekers) are genrerally not as important for the structural cohesion of the communication network as the core users, they make fewer but important contributions in- dicated by their high rates of general content related and course subject related posts. Especially the participants in the group of occasional (or one-time) help-seekers post similar or even more information requests related to course content, technical issues including source code, or mentions of concrete course materials. Consequently those users can often be notable as initiators of discussions even if their ac- tivity is limited.  The question about the predictability of forum activity (RQ6) was answered in section 8. It could be shown that the forum activity level in terms of the number of discussion threads, messages and participants is predictable one week in advance given the course structure (video and assignment  dates), history of forum activity and the described network features. Further, the predictive models can be considered as a building block for teaching support tools to forecast periods when increased tutor support for forum discussions is needed.  In summary, while the majority of research works focus- ing on single aspects of on MOOC discussion forums point to the conclusion that the current implementation of discus- sion forums are only used intensively by a small amount of course participants, and further, only a subset of the discus- sions are relevant for the information exchange, the outcome of our study suggests that discussion forums are much more complex. Activity, content, and structural related analyses highlight different aspects of forum communication, while there are several interdependencies between the progress of the course, the contributed content and structural roles of participants that have to be taken into account to get a clearer picture and to foster the development of future col- laboration support. Recommendation mechanisms to find the right information and adequate discussion partners [22, 30] can be one initial step, but in order to transform the loosely connected crowd of forum users into a sustainable community in the sense of social learning requires also sup- port for maintaining social contacts. Furthermore, combina- tion of predictive models proposed in section 8 with content analysis of the forum contributions can potentially support instructors to turn their attention to upcoming important discussions and enable interventions and community man- agement.  10. ACKNOWLEDGEMENTS This study is part of the leading house DUAL-T research  project funded by the Swiss State Secretariat for Education, Research and Innovation (SERI).  11. REFERENCES [1] A. Anderson, D. Huttenlocher, J. Kleinberg, and  J. Leskovec. Engaging with massive online courses. In Proceedings of the 23rd International Conference on World Wide Web, WWW14, pages 687698. ACM, 2014.  [2] J. Arguello and K. Shaffer. Predicting speech acts in mooc forum posts. In Proceedings of the 9th International AAAI Conference on Web and Social Media, ICWSM15, 2015.  [3] M. S. Boroujeni, L. Kidzinski, and P. Dillenbourg. How employment constrains participation in moocs. In Proceedings of the 9th International Conference on Educational Data Mining, pages 376377, 2016.  [4] M. S. Boroujeni, K. Sharma,  L. Kidzinski, L. Lucignano, and P. Dillenbourg. How to quantify students regularity In European Conference on Technology Enhanced Learning, pages 277291. Springer, 2016.  [5] C. G. Brinton, M. Chiang, S. Jain, H. Lam, Z. Liu, and F. M. F. Wong. Learning about social learning in moocs: From statistical analysis to generative model. IEEE transactions on Learning Technologies, 7(4):346359, 2014.  [6] O. Daems, M. Erkens, N. Malzahn, and H. U. Hoppe. Using content analysis and domain ontologies to check    learners understanding of science concepts. Journal of Computers in Education, 1(2):113131, 2014.  [7] P. Dillenbourg, N. Li, and  L. Kidzinski. From Books to MOOCs Emerging Models of Learning and Teaching in Higher Education, chapter The complications of the orchestration clock. Portland Press, 2016.  [8] P. Doreian, V. Batagelj, A. Ferligoj, and M. Granovetter. Generalized Blockmodeling (Structural Analysis in the Social Sciences). Cambridge University Press, 2004.  [9] N. Gillani and R. Eynon. Communication patterns in massively open online courses. The Internet and Higher Education, 23:18  26, 2014.  [10] N. Gillani, T. Yasseri, R. Eynon, and I. Hjorth. Structural limitations of learning in a crowd: communication vulnerability and information diffusion in moocs. Scientific Reports, 4:6447, 2014.  [11] T. Hecking, I. A. Chounta, and H. U. Hoppe. Analysis of user roles and the emergence of themes in discussion forums. In Proceedings of the 2nd European Network Intelligence Conference, ENIC15, pages 114121. IEEE, 2015.  [12] T. Hecking, I.-A. Chounta, and H. U. Hoppe. Investigating social and semantic user roles in mooc discussion forums. In Proceedings of the 6th International Conference on Learning Analytics & Knowledge, LAK16, pages 198207. ACM, 2016.  [13] S. Joksimovic, A. Manataki, D. Gasevic, S. Dawson, V. Kovanovic, and I. F. de Kereki. Translating network position into performance: Importance of centrality in different network configurations. In Proceedings of the 6th International Conference on Learning Analytics & Knowledge, LAK16, pages 314323. ACM, 2016.  [14] S. Kellogg, S. Booth, and K. Oliver. A social network perspective on peer supported learning in moocs for educators. The International Review of Research in Open and Distributed Learning, 15(5), 2014.  [15] R. F. Kizilcec, E. Schneider, G. L. Cohen, and D. A. McFarland. Encouraging forum participation in online courses with collectivist, individualist and neutral motivational framings. Experiences and best practices in and around MOOCs, pages 1726, 2014.  [16] W. Liu,  L. Kidzinski, and P. Dillenbourg. Semiautomatic annotation of mooc forum posts. In State-of-the-Art and Future Directions of Smart Learning, pages 399408. Springer, 2016.  [17] A. Loya, A. Gopal, I. Shukla, P. Jermann, and R. Tormey. Conscientious behaviour, flexibility and learning in massive open on-line courses. Procedia-Social and Behavioral Sciences, 191:519525, 2015.  [18] P. Oleksandra and D. Shane. Untangling mooc learner networks. In Proceedings of the 6th International Conference on Learning Analytics & Knowledge, LAK16, pages 208212. ACM, 2016.  [19] D. F. Onah, J. Sinclair, R. Boyatt, and J. Foss. Massive open online courses: learner participation. In Proceeding of the 7th International Conference of Education, Research and Innovation, pages 23482356, 2014.  [20] A. Ramesh, D. Goldwasser, B. Huang, H. Daume III,  and L. Getoor. Modeling learner engagement in moocs using probabilistic soft logic. In NIPS Workshop on Data Driven Education, volume 21, page 62, 2013.  [21] G. Robins, P. Pattison, Y. Kalish, and D. Lusher. An introduction to exponential random graph (p*) models for social networks. Social Networks, 29(2):173  191, 2007.  [22] C. P. Rose and O. Ferschke. Technology support for discussion based learning: From computer supported collaborative learning to the future of massive open online courses. International Journal of Artificial Intelligence in Education, 26(2):660678, 2016.  [23] L. A. Rossi and O. Gnawali. Language independent analysis and classification of discussion threads in coursera mooc forums. In Proceedings of the 15th International IEEE Conference on Information Reuse and Integration, IRI14, pages 654661. IEEE, 2014.  [24] A. Sharif and B. Magrill. Discussion forums in moocs. International Journal of Learning, Teaching and Educational Research, 12(1), 2015.  [25] M. Studer and G. Ritschard. What matters in differences between life trajectories: a comparative review of sequence dissimilarity measures. Journal of the Royal Statistical Society: Series A (Statistics in Society), 179(2):481511, 2016.  [26] S. Wasserman and K. Faust. Social Network Analysis: Methods and Applications, volume 1 of Structural analysis in the social sciences. Cambridge University Press, 1994.  [27] D. J. Watts and S. H. Strogatz. Collective dynamics of small-worldnetworks. Nature, 393(6684):440442, 1998.  [28] A. F. Wise, Y. Cui, and J. Vytasek. Bringing order to chaos in mooc discussion forums with content-related thread identification. In Proceedings of the 6th International Conference on Learning Analytics & Knowledge, pages 188197. ACM, 2016.  [29] J.-S. Wong, B. Pursel, A. Divinsky, and B. J. Jansen. Analyzing mooc discussion forum messages to identify cognitive learning information exchanges. In Proceedings of the 78th ASIS&T Annual Meeting: Information Science with Impact: Research in and for the Community, ASIST15, pages 23:123:10. American Society for Information Science, 2015.  [30] D. Yang, D. Adamson, and C. P. Rose. Question recommendation with constraints for massive open online courses. In Proceedings of the 8th ACM Conference on Recommender systems, pages 4956. ACM, 2014.  [31] D. Yang, T. Sinha, D. Adamson, and C. P. Rose. Turn on, tune in, drop out: Anticipating student dropouts in massive open online courses. In Proceedings of the 2013 NIPS Data-driven education workshop, volume 11, page 14, 2013.  [32] J. Zhang, M. Skryabin, and X. Song. Understanding the dynamics of mooc discussion forums with simulation investigation for empirical network analysis (siena). Distance Education, 37(3):270286, 2016.  [33] S. Zheng, M. B. Rosson, P. C. Shih, and J. M. Carroll. Designing moocs as interactive places for collaborative learning. In Proceedings of the 2nd ACM Conference on Learning @ Scale, L@S15, pages 343346. ACM, 2015.    "}
{"index":{"_id":"18"}}
{"datatype":"inproceedings","key":"Peffer:2017:ALA:3027385.3027425","author":"Peffer, Melanie E. and Kyle, Kristopher","title":"Assessment of Language in Authentic Science Inquiry Reveals Putative Differences in Epistemology","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"138--142","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027425","doi":"10.1145/3027385.3027425","acmid":"3027425","publisher":"ACM","address":"New York, NY, USA","keywords":"TAALES, assessment, authentic science inquiry, lexical sophistication, science classroom inquiry simulations, science epistemology, science practices","Abstract":"Science epistemology, or beliefs about what it means to do science and how science knowledge is generated, is an integral part of authentic science inquiry. Although the development of a sophisticated science epistemology is critical for attaining science literacy, epistemology remains an elusive construct to precisely and quantitatively evaluate. Previous work has suggested that analysis of student practices in science inquiry, such as their use of language, may be reflective of their underlying epistemologies. Here we describe the usage of a learning analytics tool, TAALES, and keyness analysis to analyze the concluding statements made by students at the end of a computer-based authentic science inquiry experience. Preliminary results indicate that linguistic analysis reveals differences in domain-general lexical sophistication and in domain-specific verb usage that are consistent with the expertise level of the participant. For example, experts tend to use more hedging language such as may and support during conclusions whereas novices use stronger language such as cause. Using these differences, a simple, rule-based prediction algorithm with LOOCV achieved prediction accuracies of greater than 80%. These data underscore the potential for the use of learning analytics in simulated authentic inquiry to provide a novel and valuable method of assessing inquiry practices and related epistemologies.","pdf":"Assessment of Language in Authentic Science Inquiry  Reveals Putative Differences in Epistemology    Melanie E. Peffer, PhD  University of Northern Colorado  Ross Hall 1556 Campus Box 92   Greeley, CO 80639  +1 970-351-2923   melanie.peffer@unco.edu   Kristopher Kyle, PhD  University of Hawaii at Manoa   587 Moore Hall, 1890 East-West Rd  Honolulu, HI 96822  +1 808-956-8610   kkyle@hawaii.edu       ABSTRACT  Science epistemology, or beliefs about what it means to do  science and how science knowledge is generated, is an integral  part of authentic science inquiry. Although the development of a  sophisticated science epistemology is critical for attaining science  literacy, epistemology remains an elusive construct to precisely  and quantitatively evaluate. Previous work has suggested that  analysis of student practices in science inquiry, such as their use  of language, may be reflective of their underlying epistemologies.  Here we describe the usage of a learning analytics tool, TAALES,  and keyness analysis to analyze the concluding statements made  by students at the end of a computer-based authentic science  inquiry experience. Preliminary results indicate that linguistic  analysis reveals differences in domain-general lexical  sophistication and in domain-specific verb usage that are  consistent with the expertise level of the participant. For example,  experts tend to use more hedging language such as may and  support during conclusions whereas novices use stronger  language such as cause. Using these differences, a simple, rule- based prediction algorithm with LOOCV achieved prediction  accuracies of greater than 80%. These data underscore the  potential for the use of learning analytics in simulated authentic  inquiry to provide a novel and valuable method of assessing  inquiry practices and related epistemologies.    CCS Concepts   Applied Computing Education Interactive Learning   Environments.    Keywords  Science epistemology; Authentic Science Inquiry; Science  Classroom Inquiry Simulations; TAALES; Science Practices;  Lexical sophistication; Assessment      1. INTRODUCTION  1.1 SCI and Authentic Inquiry  Practicing scientists perform authentic science inquiry [1]. Unlike   simple inquiry, authentic inquiry is typically non-linear and  involves a variety of complex features (e.g., proposing  mechanisms, performing multiple studies, revising hypotheses)  [1-2]. Teachers most often employ simple inquiry in classrooms  due to concerns about safety, time, expense and need for  individualized scaffolding [1-3]. Although simple inquiry may be  easier to perform in the classroom, a lack of exposure to authentic  science inquiry results in students developing an understanding of  how science works that is inconsistent with real world science  practices [2].    Science Classroom Inquiry (SCI) simulations were developed to  address pedagogical and research constraints associated with  authentic science inquiry experiences for students [3]. SCI  simulations position students as researchers during a computer- based simulated authentic inquiry activity. We demonstrated in  previous work that after students engage in SCI, their  understanding of what it means to do science is altered to a more  sophisticated stance [3]. On post-intervention metrics, students  commented that completing a simulation helped them realize  science is much more complicated than they originally thought.  They also reported a new understanding that problems do not  always have a single correct answer. We noted that when students  are given the option to engage in non-linear, authentic science  inquiry, their practices are diverse [2]. Diverse inquiry practices,  such as running multiple investigations, seeking additional  information, and coordinating evidence with theory, may provide  insight into students epistemological beliefs about science.   1.2 Epistemology in Authentic Science  Inquiry   An individuals science epistemology encompasses a set of beliefs  about the nature of science and science knowledge that an  individual possesses. Although attaining more sophisticated or  expert-like epistemological beliefs is widely recognized as  important for science literacy [4], epistemology is difficult to  precisely define and consequently measure. For example, current  pen and paper assessments operate under the assumption that the  user is interpreting the questions the same as the researcher [5].  Furthermore, epistemological beliefs about science can vary  widely both within and outside of disciplines. Consequently, there  is no definite consensus about what a correct answer would be  on an assessment. Studying student practices in authentic inquiry,  particularly via the artifacts generated, discourse surrounding  these artifacts, and interrogation into decisions made, is likely to  provide markers of an individuals science epistemology [5].  Furthermore, student performance at particular points during  authentic inquiry and examination of trends over certain tasks or  periods of time may indicate which specific epistemological   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions from Permissions@acm.org.   LAK17, March 13-17, 2017, Vancouver, BC, Canada.  Copyright is held by the owners/author(s). Publication rights licensed to  ACM.   ACM 978-1-4503-4870-6/17/03$15.00    DOI: http://dx.doi.org/10.1145/3027385.3027425     beliefs or constructs (e.g., the tenuous nature of science, lack of a  universal scientific method) are engaged during inquiry [5].    1.3 Learning Analytics for Assessment of  Epistemology  Learning analytic methods embedded in simulated authentic  science inquiry are a possible solution for assessing  epistemological beliefs about science [6]. The value of using  learning analytics to capture data generated in real time by  participants while engaged in authentic inquiry provides  researchers and teachers with greater insight into the participants  underlying epistemological beliefs. Furthermore, learning  analytics allows for capture of data in an authentic environment,  such as during a SCI simulation. We use learning analytics here in  an apprenticeship manner [6] to assess word choice between  experts and novices during the conclusion generating segment of a  SCI simulation. Previous work has suggested that language used  during engagement in scientific practices may be reflective of  epistemology, particularly as discourse around a topic is  associated with the process of making sense about a task [6-7].  Therefore, we used both domain-general and domain-specific  indices of linguistic sophistication. Domain-general indices  included content word frequency calculated by the Tool for the  Automatic Analysis of Lexical Sophistication (TAALES) [8].  TAALES has been used in a number of studies to measure lexical  sophistication in a number of domains (e.g., writing quality [9],  lexical development [10], and sentiment analysis [11].) Domain- specific indices were calculated to determine specific linguistic  items associated with expert and novice writing using natural  language processing (NLP) technology and methods based in  corpus linguistics [12].     1.4 Current Study  Given the limitations with current pencil and paper assessment of  epistemological beliefs about science, we sought to determine a  method for using learning analytic techniques to more quickly and  accurately assess epistemology in an authentic environment,  namely during the authentic science inquiry experience provided  by SCI simulations. Here we describe our preliminary results in  which we examined the language used by novices and experts  when making conclusions at the completion of authentic science  inquiry. We discuss implications of using learning analytics in  simulated authentic science inquiry environments, like SCI, to  produce more fine-grained and robust assessments of science  epistemology, as well as their pedagogical potential.    2. METHODOLOGY  2.1 Participants   20 novices and 8 experts from a large southeastern United States  city participated in this study. Experts and novices were  distinguished by their experience in authentic science practices,  not in their experience with the simulation subject matter. For  example, experts had engaged in independent biology research for  at least 2 years and all were listed as authors on journal  publications submitted or published at the time of the study.  Novices had little if any experience in authentic science inquiry,  and none were authors on primary research manuscripts. Ethnic  backgrounds of participants were diverse. The novice population  was comprised of 15% White/European-American, 45%  Black/African-American, 15% Asian-American, 15% Indian- American, 5% multi-racial, and 5% other. The expert population  was comprised of 62% White/European-American and 38%  Asian-American. Among both novices and experts, participants  were predominately female (novices: 69% female, 31% male;   experts: 88% female, 12% male). All novice participants had  completed at least one year of college education, with the majority  being in their third or fourth year of college (70% seniors, 20%  juniors and 10% sophomores) and experts had passed qualifying  exams to advance to doctoral candidacy or had completed  doctoral training.    2.2 Data Collection  Data was collected over a single meeting in several forms.  Participants were logged into the Unusual Mortality Events SCI  simulation [6]. As the students completed the simulation, the SCI  engine recorded their activity. These logs included both student  generated notes and responses to simulation prompts. For the  current study, we focused on the conclusion sections of the  notebooks, which comprised of responses to the following two  questions: 1. What is/are your final conclusion(s) 2. What  evidence supports these conclusion(s) The average length of  response for participants was 85.89 words (SD = 49.22). Experts  used an average of 107.88 words (SD = 77.11, total words  analyzed 863) in their responses and novices an average of 85.89  (SD = 49.22, total words analyzed 1542).   2.3 Data Analysis   The domain-general analysis was conducted using TAALES 2.0.  We selected content word (i.e., nouns, verbs, adjectives, and  adverbs) frequency based on the academic section of the Corpus  of Contemporary American English (COCA). COCA is a large  (450-million word) corpus of language samples collected in the  United States since 1990. The academic section of COCA  represents general academic language. Content word frequency  has been used successfully to model lexical sophistication in a  number of studies [8].   The domain-specific analysis was conducted using a method  related to keyness analysis. Keyness analysis involves identifying  linguistic items (usually words) that occur statistically  significantly more often in one set of texts than another. We  conducted an analysis that is conceptually similar to keyness  analysis, though our dataset was not large enough to warrant  statistical comparisons. The analysis proceeded as follows. First,  all texts were tagged for part of speech by Stanford CoreNLP [13- 14]. Lemmatized verb type lists were then extracted from the  novice and expert texts, resulting in verb use profiles for each text  type that indicated the percentage of texts each verb occurred in.  The expert and verb texts were then compared. Any verb that  occurred in at least two expert texts and occurred at least 20%  more often in expert texts than in novice texts were considered  expert verbs. Any verbs that met the same criteria with regard to  novice texts were considered novice verbs. Individual texts  were then reanalyzed for their use of domain-specific expert  and novice verbs.   Simple, rule-based algorithms were then used with leave one out  cross-validation (LOOCV) to predict whether a text should be  classified as expert or novice using one of three predictors:  average content word frequency, expert verb use, and novice  verb use.   3. RESULTS  Since understanding science knowledge as tenuous is generally  considered a sophisticated epistemological stance [7], we decided  to examine language used during the conclusion phase of  authentic science inquiry. We would predict that someone with  less sophisticated epistemological beliefs about the tenuous nature  of science would be more likely to use concrete language such as     prove right or correct when making their conclusions and  less hedging language such as may could or support. To  this end our approach included two levels of analysis (1) domain- general lexical sophistication as analyzed using TAALES and (2)  domain-specific language between experts and novices.    3.1 Domain-general lexical sophistication  The results with regard to TAALES content word frequency  suggested that experts tend to use content words that are less  frequent (M = 608.17, SD = 251.76) in normal academic English  use than novices (M = 953.25, SD = 311.64) (Figure 1).       Figure 1. Experts use less frequent content words.  A simple, rule-based prediction algorithm with LOOCV achieved  a prediction accuracy of 82.1%. Table 1 comprises the model  confusion matrix, which compares actual and predicted group  membership.    Table 1. Confusion matrix for domain-general model.       predicted  expert    predicted  novice   accuracy   actual expert 6 2 75.0%   actual novice 2 20 90.0%   overall accuracy   82.1%   3.2 Domain-specific verb sophistication  Table 2 comprises the results from the verb-use comparison  analysis. Expert verb-use tended to be more uniformly distinct  from novice verb use as indicated by the number of expert verbs  versus the number of novice verbs.   Table 2. Verb lists for expert and novice participants.    Expert Verbs Novice Verbs   result  may  do  support  lead  change  increase  decrease  look   make   cause   die   3.2.1 Expert verb use  As expected, conclusions written by experts tended to include  more expert verbs (M = 3.38, SD = 2.56) than conclusions written  by novices (M = 0.85, SD = 0.75) (Figure 2).      Figure 2. Use of expert verbs.  A simple, rule-based prediction algorithm with LOOCV achieved  a prediction accuracy of 89.3%. Table 3 comprises the model  confusion matrix, which compares actual and predicted group  membership.   Table 3. Confusion matrix for the expert verb model.    predicted  expert    predicted  novice   accuracy   actual expert 5 3 62.5%   actual novice 0 20 100%   overall accuracy   89.3%   3.2.2 Novice verb use  As expected, conclusions written by novices tended to include  more novice verbs (M = 0.95, SD = 0.52) than conclusions written  by experts (M = 0.38, SD = 0.52) (Figure 3).      Figure 3. Use of Novice verbs.  A simple, rule-based prediction algorithm with LOOCV achieved  a prediction accuracy of 71.4%. Table 4 comprises the model  confusion matrix, which compares actual and predicted group  membership.   Table 4. Confusion matrix for the novice verb model.    predicted  expert    predicted  novice   accuracy   actual expert 5 3 62.5%   Fr eq  ue nc  y   Fr eq  ue nc  y  Fr  eq ue  nc y     actual novice 5 15 75.0%   overall accuracy   71.4%   4. DISCUSSION  4.1 Experts Differ from Novices in Lexical  Sophistication  In this paper, we performed a preliminary analysis of the language  used by novices and experts during authentic science inquiry  using a learning analytics tool, TAALES, and keyness analysis.  Our goal was to assess if use of learning analytics tools such as  TAALES could be useful in assessing scientific epistemological  beliefs in a more practical, high-throughput, and sensitive manner  than existing pen and paper assessments.  The results indicate that  both domain-general and domain-specific linguistic features can  be used to accurately classify expert and novice investigation  conclusions, although domain-specific indices may be more  construct relevant. These results are discussed below.   4.1.1 Expert v. Novice Content Word Usage  Analysis of content words used during the concluding phase  indicated that experts used words that are less frequent in normal  academic English than novices. Although this was predictive of  expertise, it may be more closely related to overall language  ability. Since experts had spent considerably more time than  novices in an academic environment, it is probable that this is a  reflection of their experience in academic settings. Thus, while  content word frequency demonstrates predictive validity, it may  be construct irrelevant. Content word frequency may be an artifact  of the experts overall language abilities, not an indicator of  underlying epistemologies.   4.1.1.1 Expert v. Novice Verb Usage  Analysis of verbs used by experts and novices resulted in two  independent verb lists (Table 2). Ten verbs met the criteria of  occurring 20% more often in expert texts than novice texts, while  only two verbs met the criteria with regard to novice texts. This  suggests that experts used a more cohesive set of verbs than  novices. This provides preliminary evidence that a) there is an  accepted repertoire of verbs used by expert scientists to discuss  research findings and b) novices have not learned to use these  verbs in this setting. Semantically, many of the expert verbs  represented hedging and tentative language. For example, we  observed usage of the words may and support which is  consistent with our initial hypothesis. See Table 5 for examples of  tentative and hedging verbs in expert texts. This word choice may  be reflective of an understanding of the tentative nature of  science.    Table 5. Examples of tentative and hedging verbs in expert  texts.   This may explain the loss of this particular species from the  estuary.   I observed a temperature change which supports my idea.   Specifically, the growth and ingestion of Gracilaria by Manatees  and the loss of food items for Dolphins and Pelicans which may  have made them more susceptible to infection.   Importantly, using the presence of expert verbs as a variable in  a prediction model was successful and achieved 89.3%  classification accuracy. No novice texts were misclassified as  expert, but 37.5% of expert texts were misclassified. This suggests  that novice scientists rarely use expert verbs when explaining their   findings, but experts also avoid using these verbs occasionally.  Qualitative analysis suggests that experts may also use linguistic  features other than verbs to hedge and express tentativeness. For  example, one of the expert conclusions included no expert verbs.  Instead, it included adverbial phrases that express tentativeness  such as in the sentence Pelican most likely died from the parasite.  This is a potentially fruitful avenue for future research.   4.2 Word Choice May Reflect Epistemology  Since science knowledge is generally accepted as being tentative  and subject to change pending new results and discoveries, the  majority of scientists tend to use tentative language to speak about  their results. Therefore, the use of tentative language may be  reflective of the epistemological belief that scientific knowledge is  not concrete. An alternative interpretation of this observation  could be that experts have simply learned the language of science,  in which their epistemologies may or may not reflected. It may  also be the case that learning scientific language is an important  stepping stone on the way to attaining more sophisticated  epistemological beliefs. Future work that ties an analysis of  lexical sophistication and verb usage to existing metrics of  epistemology and/or nature of science understanding (nature of  science meaning the set of generally accepted facts that influence  ones epistemological beliefs), may be useful for better  understanding how language choices relate to epistemology.  Furthermore, it may be the case that word choice is only one part  of a larger investigation and the greater context in which words  are utilized is more indicative of epistemology. For example, if we  observe that experts are using more tentative language during  their concluding phases, is this also apparent throughout their  scientific investigations Perhaps an understanding of the  tentativeness of science is also reflected in the number or types of  tests a user may perform when engaged in authentic science  inquiry. A user who uses more definitive language may also  perform simple inquiry, only performing one or two tests and  concluding with a final correct answer. Conversely, an expert  may spend more time doing varied tests before coming up with  several possible conclusions that they discuss with tentative  language. Or, perhaps a user that receives conflicting or  inconclusive test results is more likely to use tentative language.  There may also be domain-specific variations in word choice.  Future work is necessary to determine how different practices  during authentic science inquiry relate to language, and how these  different aspects of inquiry combine to reveal insights into the  users epistemological beliefs.    4.3 Limitations  As stated above, the use of tentative language may be reflective of  underlying epistemological beliefs about the tentative nature of  science. However, epistemological beliefs about science include  more than an understanding of the tentative nature of science  knowledge. For example, other important beliefs include how  science knowledge is justified and produced, the lack of a  universal scientific method, and the sources of scientific  information. Epistemologies are also said to be situated, meaning  that an individuals epistemology will vary depending on if the  student is performing inquiry in a classroom, an informal setting,  or in a psychological lab study, such as in this study. Furthermore,  epistemological beliefs vary widely both across and within  disciplines. Therefore, although language may be a useful proxy  of one of these factors, it may only be providing a very small  sliver of a larger picture. Future studies examining practices in a  variety of settings will be useful for determining the key  epistemological episodes by setting.      Our data set for this study is small, particularly among the expert  population. For example, normal sample sizes for TAALES  analysis would be 100-250 participants, with keyness analysis  using even larger data sets. We also note the number of words  collected per participant is also small. However, given that we are  seeing trends that reflect what we expect to see regarding  novice/expert language usage and that the LOOCV model is  predictive of expertise, we feel that a scale-up of this study is  justified and may be informative.    In addition to a small data set, research participants were  predominantly female in both the expert and novice populations.  Since previous work on gender differences in science  epistemology are conflicting, with some reporting an effect of  gender and others not observing gender differences [15], this may  indicate that the observed trends may change with additional male  participants. However, since we are comparing two predominantly  female data sets together, it is likely that the expertise differences  we observe will be maintained.    4.4 Implications and Future Directions  Current pen and paper assessments of epistemology are limited.  Learning analytic techniques, such as TAALES, or TAALES in  combination with other methods of automatically capturing  critical data as students engage in inquiry may be a new way of  assessing amorphous constructs such as epistemology. Tracking  students as they engage in a completely autonomous inquiry  experience provides a level of authenticity that is not provided  through pen and paper assessment. In addition, there is more  freedom for participants to perform many different kinds of  investigation, rather than limiting their responses to pre-defined  survey items. This removes a common limitation cited by those  who critique epistemology or nature of science assessments,  namely that these metrics are based on the assumption that the  user is interpreting the questions the same as the researcher [5].    In addition to utility for assessing of epistemology by researchers,  use of learning analytics techniques in a simulation environment  may also have important roles in instruction and pedagogy. For  example, learning analytics methods could be used to provide  critical feedback to teachers and/or users for instruction. [6] points  out that learning analytics for assessing epistemology can be  embedded in instruction, thereby giving the user real-time  feedback and prompting metacognitive reflection. Alternatively,  identifying important features of epistemology via learning  analytic methods embedded into simulations may be useful for  instructors to identify where student needs are and tailor  instruction appropriately.    Although there is significant potential with learning analytics  techniques for assessment of epistemology, it is necessary to  consider the information garnered via learning analytics within the  greater context of the learning environment. As cautioned by [7],  what is chosen to be assessed and how it is interpreted is a factor  of the views and biases of the researcher/teacher. This raises  particularly important questions in regards to epistemology since  beliefs about the nature of science and science knowledge ranges  widely between scientists, even within the same discipline. In  spite of these caveats, the research potential of learning analytics  in the context of authentic inquiry is rich and exciting.      5. ACKNOWLEDGMENTS  This project was supported through a data consortium fellowship  (NSF 112-1549112). We thank Mike Tissenbaum and Matthew  Berland for choosing our project to receive a fellowship. We   thank Don Davis and the Sona team for support and use of the  Counseling and Psychological Services Research Participant  System. We thank Merrin Oliver for her help with data collection.  6. REFERENCES  [1] Chinn, C. A., & Malhotra, B. A. 2002. Epistemologically   authentic inquiry in schools: A theoretical framework for  evaluating inquiry tasks. Science Education, 86, 2. 175-218.   [2] Peffer, M. E., & Renken, M. 2015. Science Classroom Inquiry  (SCI) Simulations for Generating Group-Level Learner  Profiles. In Exploring the Material Conditions of Learning:  the CSCL Conference 2015, 707-708.   [3] Peffer, M. E., Beckler, M. L., Schunn, C., Renken, M., &  Revak, A. 2015. Science Classroom Inquiry (SCI)  Simulations: A Novel Method to Scaffold Science  Learning. PloS one, 10, 3. e0120638.   [4] Sandoval, W. 2014. Science education's need for a theory of  epistemological development. Science Education, 98, 3. 383- 387.   [5] Sandoval, W. A. 2005. Understanding students' practical  epistemologies and their influence on learning through  inquiry. Science Education, 89, 4. 634-656.   [6] Knight, S., Buckingham Shum, S., Littleton, K. 2014.  Epistemology, assessment, pedagogy: where learning meets  analytics in the middle space. Journal of Learning  Analytics, 1, 2. 23-47.    [7] Deng, F., Chen, D.T., Tsai, C.C., & Chai, C.S. 2011. Students  Views of the Nature of Science: A critical review of  research. Science Education, 95,6. 961-999   [8]  Kyle, K. & Crossley, S.A. 2015. Automatically assessing  lexical sophistication: Indicies, tools, findings, and  application. TESOL Quarterly, 49, 4. 757-786   [9]   Kyle, K. & Crossley, S.A. 2016. The relationship between  lexical sophistication and independent and source-based  writing. Journal of Second Language Writing, 12. 12-24   [10] Crossley, S., Kyle, K., & Salsbury, T. 2016. A usage-based  investigation of L2 lexical acquisition: The role of input and  output. The Modern Language Journal, 100, 3. 702-715.    [11] Skalicky, S., & Crossley, S. 2015. A statistical analysis of  satirical Amazon.com product reviews. The European  Journal of Humour Research, 2, 3. 66-85.    [12] Scott, M. & Tribble, C., 2006. Textual Patterns: Keyword  and corpus analysis in language education. John Benjamins  Publishing Company, Amsterdam, The Netherlands.   [13] Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J.,Bethard,  S.J., & McClosky, D. 2014. The Stanford CoreNLP Natural  Language Processing Toolkit. In Proceedings of the 52nd  Annual Meeting of the Assoication for Computational  Linguistics: System Demonstrations. 55-60   [14] Toutanova, K., Klein, D., Manning, C., & Singer, Y. 2003.  Feature-Rich Part-of-Speech Tagging with a Cyclic  Dependency Network. In Proceedings of HLT-NAACL  2003. 252-259.    [15] Conley, A., Pintrich, P., Vekiri, I., & Harrison, D. 2004.  Changes in epistemological beliefs in elementary science  students. Contemporary educational psychology, 29, 2. 186- 204.       "}
{"index":{"_id":"19"}}
{"datatype":"inproceedings","key":"Bote-Lorenzo:2017:PDE:3027385.3027387","author":"Bote-Lorenzo, Miguel L. and G'omez-S'anchez, Eduardo","title":"Predicting the Decrease of Engagement Indicators in a MOOC","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"143--147","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027387","doi":"10.1145/3027385.3027387","acmid":"3027387","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOC, engagement, supervised machine learning","Abstract":"Predicting the decrease of students' engagement in typical MOOC tasks such as watching lecture videos or submitting assignments is key to trigger timely interventions in order to try to avoid the disengagement before it takes place. This paper proposes an approach to build the necessary predictive models using students' data that becomes available during a course. The approach was employed in an experimental study to predict the decrease of three different engagement indicators in a MOOC. The results suggest its feasibility with values of area under the curve for different predictors ranging from 0.718 to 0.914.","pdf":"Predicting the decrease of engagement indicators in a  MOOC   Miguel L. Bote-Lorenzo   GSIC-EMIC Research Group   Universidad de Valladolid, Spain  miguel.bote@tel.uva.es   Eduardo Gmez-Snchez  GSIC-EMIC Research Group   Universidad de Valladolid, Spain  edugom@tel.uva.es   ABSTRACT  Predicting the decrease of students engagement in typical MOOC  tasks such as watching lecture videos or submitting assignments is  key to trigger timely interventions in order to try to avoid the  disengagement before it takes place. This paper proposes an  approach to build the necessary predictive models using students  data that becomes available during a course. The approach was  employed in an experimental study to predict the decrease of three  different engagement indicators in a MOOC. The results suggest  its feasibility with values of area under the curve for different  predictors ranging from 0.718 to 0.914.   CCS Concepts   Information systems   Clustering analysis and  classification  Applied computing  E-learning.   Keywords  MOOC; engagement; supervised machine learning.   1. INTRODUCTION  Academic engagement is a concept that refers to the observable  behaviors directly related to the learning process that are exhibited  by students participating in course work such as, for instance,  attentiveness and assignment completion [8]. The study of the  academic engagement of students in Massive Open Online  Courses (MOOCs) has already been tackled from different  perspectives in the literature [1, 2, 6, 7, 14, 17, 18]. With this aim,  various engagement indicators that describe different observable  behaviors of MOOC students were employed. Examples of such  engagement indicators include the number of lecture videos  watched, the number of submitted assignments or the number of  posts created by each student.   The analyses of the evolution of engagement indicators along the  course has revealed the existence of different patterns of  engagement among MOOC students [6, 7, 14, 17, 18]. For  example, the learners that follow the so called completing or  all-rounders pattern [2, 6, 7, 14] have high values in the  assignment engagement indicators along the course, while those  belonging to the auditing or viewers group [2, 14] show low  values in assignment engagement indicators and high ones in  video engagement indicators along the course. Interestingly, [14]  discovered that the participants of these two groups reported   similarly high levels of satisfaction with their overall experience  in the course, which suggests the existence of different ways in  which a MOOC can fulfill learner needs.   Another important finding is that the values of engagement  indicators of many MOOC students decay over time often leading  to dropping out [1, 6, 7, 14, 17, 18]. As a consequence, the  educational impact that the MOOC has in these disengaging  learners is reduced, even if they do not eventually drop out. This  is the reason why it is important to maintain and cultivate student  engagement in MOOCs [17].    The identification of students with engagement indicators that are  expected to decrease in the near future would allow making  interventions aimed at preventing it. For example, the suggestion  of an interesting lecture video or a hint to solve an assignment  could be provided to a learner if a decrease in her video or  assignment engagement indicator has been predicted, respectively.  These interventions would thus help to maintain the engagement  of students.   This paper proposes an approach to predict the decrease of  engagement indicators in MOOCs using the students data about  their activity in the course that is available at the moment in which  the prediction must be made. It also presents the results of an  empirical study in which the proposed approach was employed to  make predictions of three different engagement indicators within  the context of a MOOC delivered in the edX platform.   The rest of this papers is organized as follows. Section 2 discusses  the related work that can be found in the literature. Next, section 3  provides a more detailed description of the problem of predicting  the decrease of engagement indicators and proposes an approach  to tackle it. Section 4 presents the experimental study that was  carried out to verify the feasibility of the proposed approach. The  proposed approach is discussed in regard to the results of the  experimental study in section 5. Finally, section 6 includes the  main conclusions of the paper and presents future work.   2. RELATED WORK  There are some works in the literature that have tackled the  detection of disengagement in students within the context of  different learning systems such as intelligent tutoring systems [3,  15] or learning management systems [5]. In these cases, the  proposed solutions aim at predicting the engagement states in  which the students will be in the near feature. However, this paper  proposes an approach to predict the decrease of the individual  indicators that can be used to define such engagement states.   Other contributions such as [4, 9, 20, 21] deal with the problem of  predicting whether a student will eventually drop out a MOOC in  order to enable the possibility of making an early intervention to  try to avoid it. There are also other works such as [11, 13, 18] that  focus on detecting if a student is likely to fail the course so that,  again, an intervention can be made to try to prevent this situation.  The ultimate goal in both cases is thus different to that of the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than the author(s) must be honored. Abstracting  with credit is permitted. To copy otherwise, or republish, to post on servers or  to redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada.   Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027387     problem of predicting the decrease of engagement indicators,  which is maintaining the engagement of students with the  different activities that can be carried out in MOOCs. This should  help not only to have more students that pass the course or that do  not drop out, but also to avoid the decrease of the educational  impact that the course might have on students even if they  participate in different ways that might fulfill their needs.   3. PREDICTING THE DECREASE OF  ENGAGEMENT INDICATORS  3.1 Problem formulation  MOOCs are typically structured as a sequence of chapters. In  many cases the learning materials corresponding to each chapter  are released along the course on a regular basis. The date in which  such release takes place can be considered the chapter start.  Chapters usually include assignments that must be submitted  before a deadline that determines the chapter end.  Students are normally expected to perform different types of tasks  such as video watching or submitting assignments using the  learning materials of every MOOC chapters. It is possible to  define an indicator that quantifies the engagement of a student  with a given type of task from the beginning of the course until  the moment in which the indicator is computed. For example, an  assignment engagement indicator could be obtained by averaging  the percentage of assignments submitted in each chapter that has  already ended. Clearly, the value of an engagement indicator  defined in this way can increase or decrease at the end of every  new chapter depending on work performed by the student in the  tasks of the previous one.    Considering a course with c chapters, and given the learner  activity in the MOOC until the end of chapter i, the prediction of  the decrease of an engagement indicator tries to determine  whether its value at the end of chapter i+1 will be lower or not  than its value at the end of chapter i. Making this prediction in real  time within the context of a MOOC gives a great opportunity to  make interventions aimed at avoiding student disengagement in  the type of task used to define the indicator.    3.2 Proposed approach  A model to predict the decrease of a given engagement indicator  at the end of chapter i+1 can be built using data available in the  MOOC platform at the end of chapter i provided that i2. More  specifically, the predictive model can be generated using a  training set consisting of input vectors that describe the learners  activity until the end of chapter i-1 and output labels stating  whether the value of the engagement indicator decreased at the  end of chapter i with respect to the end of chapter i-1 or not (i.e.  labels with true and false values, respectively). This model can  then be used to predict the decrease of the engagement indicator at  the end of chapter i+1 with data also available at the end of  chapter i. In this case, the predictions would be obtained by using  vectors describing the learners activity until the end of chapter i  as inputs for the predictive model.   The proposed approach thus assumes that the same set of features  that describe the students activity can be computed for both  chapters i-1 and i while the value, and that the target engagement  indicator can be also calculated for these chapters. Furthermore, it  requires each feature to be in the same range in both chapters i-1  and i so that features are comparable across chapters. For  example, the variable percentage of submitted assignments can be  compared in different chapters, but not the number of submitted  assignments since the total number of requested assignments may  vary from one chapter to another.   4. EXPERIMENTAL STUDY  4.1 Course description  The data employed in the experimental study was obtained from  the MOOC  6.002x Circuits and Electronics that was offered on  edX in the spring of 2013 [19]. The course was structured in 14  chapters and included a midterm and a final exam.   The main contents of every chapter are explained in two  sequences of lecture videos interspersed with short and simple  comprehension questions called finger exercises. Chapters 1 to 12  also comprised two types of assignments: homework problems  that included numerical and formula responses, and lab exercises  based on an interactive circuit simulator. In addition, most  chapters provided optional tutorial videos that helped reinforcing  concepts by showing how to solve circuit problems or illustrating  interesting principles. The chapter learning materials were  supplemented by on-line sections of the course textbook, a forum  where students and staff could engage in discussions, a staff- student editable wiki, and ungraded access to the interactive  circuit simulator.   The course schedule comprised 15 calendar weeks. A chapter was  released every Monday except for week 1, when it was made on  Wednesday, and week 8, in which the midterm exam took place  and there was no chapter release. The final exam was made in the  last week of the course. The deadline for the submission of the  assignments included in each of the first 12 chapter was set for the  second Sunday after the corresponding chapter release, except in  the case of chapter 7, in which it was the third Sunday also due to  the occurrence of the midterm exam. This implies that the  deadline for the assignments of a given chapter always took place  nearly one week after the release date of the next chapter.   Final course grades were based on homework sets (15%), online  laboratories (15%), a midterm (30%) and a final exam (40%).  Each chapter had a homework grade and a lab grade. The  homework and laboratories grades used in the final course grades  were obtained by adding the highest 10 out of 12 chapter grades.  Students with a final grade of 50% or greater received a certificate  of accomplishment.   There were 26,947 students enrolled in the course by the deadline  established for the submission of the final exam. Out of these,  only 6,752 watched at least a lecture video or answered at least a  finger exercise or submitted at least an assignment in one of the  first 12 chapters before the corresponding deadline. A certificate  of accomplishment was granted to 1,099 students.   4.2 Datasets  A different engagement indicator was defined for each of the three  main tasks that students were expected to carry out in the course:  watching lecture videos, answering finger exercises and  submitting assignments. The video engagement indicator was  obtained by averaging the percentages of lecture videos that were  totally or partially watched by a student in every chapter before  reaching its end. The exercise engagement indicator was  computed by averaging the percentages of finger exercises  answered by a student in every chapter before its end. The  assignment engagement indicator was calculated by averaging the  percentages of assignments submitted in each chapter that has  already ended.    The values of the three engagement indicators were computed for  every student at the end of each of the first 12 chapters. They were  not calculated for chapters 13 and 14 since they did not include  any assessment. These values were then employed to determine     for every student whether the value of each of her engagement  indicators decreased at the end of chapters 2 to 12 with respect to  the end of the previous chapter.   Table 1 summarizes the 16 features that were computed for every  student at the end of each of the first 11 chapters. It can be noticed  that every feature was defined so that its range of values is the  same across chapters. It is also worth mentioning that the  existence of features related to chapter i+1 calculated at the end of  chapter i is possible since, as explained in the previous subsection,  the end of every chapters 1 to 12 took place nearly one week after  the start of the next chapter. Interestingly, a value of 0 or a  negative value in f14, f15, or f16 implies that the corresponding  engagement indicator will not decrease by the end of chapter i+1  since the learner has already done enough work by the end chapter  i as to assure it.   Table 1. Features derived at the end of each chapter i.   f1 Percentage of lecture videos totally or partially watched in  chapter i   f2 Percentage of finger exercises answered in chapter i  f3 Percentage of assignments submitted in chapter i  f4 Normalized grade of finger exercises in chapter i  f5 Normalized grade of assignments in chapter i  f6 Value of video engagement indicator  f7 Value of exercise engagement indicator   f8 Value of assignment engagement indicator   f9 Normalized total grade of finger exercises in chapters 1 to i  f10 Normalized total grade of assignments in chapters 1 to i  f11 Percentage of lecture videos totally or partially watched in   chapter i+1  f12 Percentage of finger exercises answered in chapter i+1  f13 Percentage of assignments submitted in chapter i+1  f14 Difference between value of video engagement indicator   and percentage of lecture videos totally or partially watched  in chapter i+1 (f6-f11)   f15 Difference between value of exercise engagement indicator  and percentage of finger exercises answered in chapter i+1  (f7-f12)   f16 Difference between value of assignment engagement  indicator and percentage of assignments answered in  chapter i+1 (f8-f13)   A dataset was created for each engagement indicator and chapters  1 to 11. Each dataset contains the students feature vectors  computed at the end of a given chapter along with the  corresponding label stating whether the engagement indicator  decreased or not at the end of the next chapter after filtering out  two types of samples. First, samples in which it is already known  at the end of a given chapter that the target engagement will not  decrease at the end of the next chapter, since there is no need to  make any prediction. Second, samples from students who have  not watched any lecture video, answered any finger exercise of  submitted any assignment in the last three chapters since it is  assumed that they have dropped out. Table 2 shows the number of  samples included in each dataset.   It must be noticed that the fact that chapters 13 and 14 did not  include any assignment implies not only that the assignment  engagement indicator cannot be computed, but also that a chapter  end cannot be defined according to our problem formulation. This  also entails that the video and exercise engagement indicators can  neither be computed for such chapters. As a consequence, datasets  with features gathered at the end of chapters 12 and 13 and  indicators computed at the end of the same chapters 13 and 14  could not be generated.    Table 2. Number of samples in each dataset.     Video   engagement  indicator   Exercise  engagement   indicator   Assignment  engagement   indicator  Chapter 1  Chapter 2  Chapter 3  Chapter 4  Chapter 5  Chapter 6  Chapter 7  Chapter 8  Chapter 9   Chapter 10  Chapter 11   4,637  5,021  5,374  3,206  2,501  2,110  1,759  1,536  1,349  1,186  1,069   2,863  3,218  3,395  2,465  1,974  1,709  1,451  1,281  1,173  1,042     954   3,456  3,895  4,056  3,125  2,295  1,924  1,659  1,417  1,265  1,158  1,120   4.3 Feature selection  The most relevant features of the datasets generated for chapters 1  to 10 were selected using a best first forward search combined  with the Correlation based Feature Selection (CFS) method [10]  to evaluate the quality of feature subsets. CFS was selected  because, unlike other methods, it aims not only at finding subsets  with features that have high individual prediction ability, but also  a low degree of redundancy among them. Furthermore, CFS is not  a computationally intensive method. Feature extraction was not  carried out for chapter 11 since this dataset was not going to be  employed to build any predictive model but just to test one.   Table 3 shows the number of datasets in which each feature was  selected. The values highlighted in bolds correspond to features  that were selected in more than 50% of datasets of a given type of  indicator. It can be seen that all features except for f11, f13, f14  and f15 were selected in most cases for the datasets of at least one  engagement indicator.   Table 3. Number of datasets in which each feature was  selected.    Video datasets  Exercise  datasets   Assignment  datasets   Total   f1  f2  f3  f4  f5  f6  f7  f8  f9   f10  f11  f12  f13  f14  f15  f16   10  3  5  9  7  9  0  0  0  0  5  6  0  2  1   10   7  10  5   10  5  3  6  0  6  0  2  6  1  3  1  6   2  2   10  5   10  1  0  7  0  8  2  2  2  0  0  1   19  15  20  24  22  13  6  7  6  8  9   14  3  5  2   17  4.4 Performance of classification algorithms  Four different classification algorithms were employed to build  models to predict the three engagement indicators according to the  approach proposed in this paper. The selected algorithms were  logistic regression (LR), stochastic gradient descent (SGD),  random forests (RF) and support vector machines (SVM) since  they provided good results in the dropout prediction experiments  described in [20]. More specifically, SVM with radial basis kernel     and SGD with a logistic regression loss function were used. Data  were normalized for these two algorithms but not for LR and RF.   In this way, predictions were made for the datasets of chapters 2  to 11 with predictive models trained with the datasets of chapters  1 to 10, respectively. The trainings were carried out using the  features selected by CFS for each dataset. Obviously, the features  employed in the training phase of each model were also used in  the corresponding test phase. Experiments with SGD, RF and  SVM were repeated 10 times using different random seeds for the  algorithms and presenting the training data in random order.   The results of the prediction tests are reported in Table 4 using  area under the curve (AUC) as performance metric. It is important  to take into account that AUC, unlike other performance metrics  such as accuracy or Cohens kappa, is not affected by imbalanced  distributions of data [12]. This is the reason why the experiments  are compared using AUC, although some accuracy figures are  given with an informative purpose.   The best AUC value obtained for each dataset is highlighted in  bolds in Table 4. It can be observed that the best results are  obtained using SGD in nearly all cases, although LR exhibits a  very similar performance. SGD was able to predict the video  engagement indicator with AUC values ranging from 0.81 to  0.894, exercise engagement indicator with AUC values from  0.837 to 0.906, and assignment engagement indicator with values  from 0.718 to 0.914. The accuracy obtained with SGD ranged  from 81.56% to 89,09% for the video engagement indicator, from  80.04% to 88.79% for the exercise engagement indicator and from  59.56% to 85.39% for the assignment engagement indicator. It is  important to remember that predictions were not made for  students that did not show any activity in three chapters in a row,  thus avoiding the artificial improvement of the results with  predictions that would have been easy to make.   It is noteworthy that the AUC values obtained in the prediction of  the assignment engagement indicator for chapters 10 and 11 are  clearly lower than for the rest of the datasets. This could be  attributed to the fact that the lowest two assignment grades  obtained in any chapter were not taken into account to compute  the final grade. This idea is supported by the fact that an  inspection of the datasets revealed the existence of a relevant  number of students that maintained a high level of assignment  engagement and had obtained high grades in all previous chapters  that significantly reduced their activity in the last two chapters.   It can also be mentioned that the results achieved with SGD in the  prediction of the decrease of engagement indicators are  comparable to the best results reported in [20] for the dropout  prediction problem. However, it can be noted that the experiments  described in such work, as in most contributions dealing with the  dropout prediction problem [4], were made using cross validation   with data that would not have been available in the MOOC at the  time in which the predictions are required. Following [4], this  gives an optimistic estimation on the results that cannot be  achieved with unseen data. In the experiments reported in this  paper, on the contrary, the models were trained using data that  would have been available at the time of making the predictions.   5. DISCUSSION  The experimental study presented above showed that the approach  proposed in this paper yielded good results in the prediction of the  decrease of engagement indicators derived for the three main  tasks that were carried out by students in a MOOC. As noted  before, predictions were made at the end of chapters 2 to 11 using  only information that was available at those moments. It is  noteworthy that during the experiments, predictions were not  made for obvious cases, such as students that at mid-chapter have  already shown better engagement than in the previous chapter or  students that can be considered to have dropped out. Adding these  students would easily improve the prediction performance  metrics, but would not provide any useful information in order to  intervene. The proposed approach would thus have been useful to  identify many students that could have benefited from an  intervention aimed at preventing their disengagement.   One limitation of the proposed approach is that the first  predictions cannot be made before the end of chapter 2.  Predictions cannot be made at the end of chapter 1 since there are  obviously no data from previous chapters that can be employed to  build the predictive model. This problem would not exist in a  different approach that could use data from previous editions of  the course along with transfer learning techniques [16] to build  predictors for chapters 1 to 11. However, such approach would  have the important limitation that it could not be applied to make  predictions in courses that do not have a previous edition. An  alternative approach that might be worth exploring could use data  from previous runs of the course, if available, and transfer  learning techniques to build predictors at the end of chapter 1 and  data from the ongoing course to build them at the end of the rest  of the chapters.   Another limitation of the proposed approach that can be observed  in the experimental study is that predictions regarding the  decrease of the video and exercise engagement indicators could  not be made for chapters 13 and 14. Tackling this limitation could  require not only changing the definition of end of chapter so that it  is only based on the deadlines for submissions. Possibly, it will  also entail defining a method to build the predictors taking into  account that the behavior of students in chapters that do not have  assignments might be very different with respect to chapters in  which they have them. Again, transfer learning techniques could  be useful to build these predictors.   Table 4. Comparison of prediction performance based on area under the curve.    Video engagement indicator Exercise engagement indicator Assignment engagement indicator   LR SGD RF SVM LR SGD RF SVM LR SGD RF SVM   Chapter 2  Chapter 3  Chapter 4  Chapter 5  Chapter 6  Chapter 7  Chapter 8  Chapter 9  Chapter 10  Chapter 11   0.862  0.891  0.868  0.889  0.891  0.859  0.867  0.883  0.855  0.808   0.864  0.892  0.869  0.887  0.894  0.859  0.866  0.887  0.855  0.81   0.809  0.876  0.822  0.872  0.863  0.82  0.854  0.867  0.84  0.803   0.5  0.715  0.5  0.758  0.5  0.729  0.684  0.629  0.737  0.758   0.833  0.884  0.878  0.906  0.903  0.888  0.859  0.898  0.9  0.905   0.837  0.884  0.88  0.906  0.903  0.889  0.864  0.901  0.901  0.912   0.779  0.875  0.837  0.866  0.887  0.883  0.848  0.902  0.87  0.883   0.525  0.742  0.536  0.812  0.5  0.767  0.741  0.772  0.758  0.821   0.848  0.883  0.913  0.908  0.883  0.874  0.818  0.845  0.749  0.715   0.848  0.89  0.914  0.909  0.883  0.875  0.862  0.847  0.751  0.718   0.829  0.864  0.881  0.849  0.848  0.841  0.783  0.823  0.722  0.681   0.749  0.825  0.837  0.845  0.823  0.806  0.59  0.781  0.706  0.652     6. CONCLUSIONS  Early detection of MOOC students with engagement indicators  that are expected to decrease is necessary to make interventions  aimed at preventing it. This paper has presented an approach to  predict the decrease of engagement indicators at the end of  MOOC chapters using information about students behavior that  becomes available during the course. An experimental study was  conducted to predict three engagement indicators derived for the  main tasks that were carried out in a MOOC: watching lectures,  solving finger exercises, and submitting assignments. It was  shown that good results can be achieved for all the indicators  using CFS method for feature selection and SGD algorithm for  classification. This supports the idea that the proposed method  would have been useful to detect disengaging students in that  MOOC and suggests that it could be useful in other MOOCs too.   The next steps in this research will be to evaluate the proposed  approach in other MOOCs and to study its applicability to  engagement indicators derived from other MOOC activities with a  more social nature such as posting in forums or participating in  peer reviews. The possibility of improving predictions using  features that describe aspects of student participation in MOOCs  other than behavior, such as motivation, will be explored. Besides,  the limitation regarding the lack of predictions at the end of the  first chapter or for chapters that do not include assignments will  be tackled. Future work also includes the design of intervention  mechanisms that could be triggered when the decrease of an  engagement indicator is predicted.   7. ACKNOWLEDGMENTS  Access to the data employed in this paper was granted by MIT's  Institutional Research Office. This work has been partially funded  by research projects TIN2014-53199-C3-2-R and VA277U14.    8. REFERENCES  [1] Alario-Hoyos, C., Prez-Sanagustn, M., Delgado-Kloos, C.,   Parada G, H.A. and Muoz-Organero, M. 2014. Delving into  participants profiles and use of social tools in MOOCs.  IEEE Trans. Learn. Technol. 7, 3 (Jan. 2014), 260266.   [2] Anderson, A., Huttenlocher, D.P., Kleinberg, J.M. and  Leskovec, J. 2014. Engaging with massive online courses.  Proceedings of the World Wide Web Conference (Seoul,  Korea, Apr. 2014), 687698.   [3] Beck, J.E. 2005. Engagement tracing: using response times  to model student disengagement. Proceedings of the 12th  Conference on Artificial Intelligence in Education  (Pittsburgh, PA, USA, Jul. 2005), 8895.   [4] Boyer, S. and Veeramachaneni, K. 2015. Transfer learning  for predictive models in Massive Open Online Courses.  Proceedings of the 17th Conference on Artificial Intelligence  in Education (Madrid, Spain, Jun. 2015), 5463.   [5] Cocea, M. and Weibelzahl, S. 2011. Disengagement  detection in online learning: validation studies and  perspectives. IEEE Trans. Learn. Technol. 4, 2 (Jan. 2011),  114124.   [6] Ferguson, R. and Clow, D. 2015. Examining engagement:  analysing learner subpopulations in Massive Open Online  Courses (MOOCs). Proceedings of the 5th International  Conference on Learning Analytics and Knowledge  (Poughkeepsie, NY, USA, Mar. 2015), 5158.   [7] Ferguson, R., Clow, D., Beale, R., Cooper, A.J., Morris, N.,  Bayne, S. and Woodgate, A. 2015. Moving through   MOOCS: pedagogy, learning design and patterns of  engagement. Proceedings of the 10th European Conference  on Technology Enhanced Learning (Toledo, Spain, Nov.  2015), 7084.   [8] Finn, J.D. and Zimmer, K.S. 2012. Student engagement:  what is it why does it matter Handbook of Research on  Student Engagement. Springer. 97131.   [9] Halawa, S., Greene, D. and Mitchell, J. 2014. Dropout  prediction in MOOCs using learner activity features.  Proceedings of the 2nd European MOOCs Stakeholders  Summit (Laussane, Switzerland, Feb. 2014), 5865.   [10] Hall, M.A. 1999. Correlation-based feature selection for  machine learning. PhD dissertation. University of Waikato.   [11] He, J., Bailey, J., Rubinstein, B.I.P. and Zhang, R. 2015.  Identifying at-risk students in Massive Open Online Courses.  Proceedings of the 29th Conference on Artificial Intelligence  (Austin, TX, USA, Jan. 2015), 17491755.   [12] Jeni, L.A., Cohn, J.F. and De la Torre, F. 2013. Facing  imbalanced data-recommendations for the use of  performance metrics. Proceedings of the 5th Conference on  Affective Computing and Intelligent Interaction (Geneva,  Switzerland, Sep. 2013), 245251.   [13] Jiang, S. and Warschauer, M. 2014. Predicting MOOC  performance with week 1 behavior. Proceedings of the 7th  International Conference on Educational Data Mining  (London, UK, Jan. 2014), 273275.   [14] Kizilcec, R.F., Piech, C. and Schneider, E. 2013.  Deconstructing disengagement: analyzing learner  subpopulations in massive open online courses. Proceedings  of the 3rd International Conference on Learning Analytics  and Knowledge (Leuven, Belgium, Apr. 2013), 170179.   [15] Mills, C., Bosch, N., Graesser, A. and DMello, S. 2014. To  quit or not to quit: predicting future behavioral  disengagement from reading patterns. Proceedings of the  12th International Conference on Intelligent Tutoring  Systems (Honolulu, HI, USA, Jun. 2014), 1928.   [16] Pan, S.J. and Yang, Q. 2010. A survey on transfer learning.  IEEE Trans. Knowl. Data Eng. 22, 10 (Jan. 2010), 1345 1359.   [17] Ramesh, A., Goldwasser, D. and Huang, B. 2013. Modeling  learner engagement in MOOCs using probabilistic soft logic.  Proceedings of the NIPS Conference, Workshop on Data  Driven Education (Lake Tahoe, NV, USA, Dec. 2013).   [18] Ramesh, A., Goldwasser, D., Huang, B., Daume, H., III and  Getoor, L. 2014. Uncovering hidden engagement patterns for  predicting learner performance in MOOCs. Proceedings of  the 1st ACM Conference on Learning at Scale (Atlanta, GA,  USA, Mar. 2014), 157158.   [19] Seaton, D.T., Reich, J., Nesterko, S.O., Mullaney, T., Waldo,  J., Ho, A.D. and Chuang, I. 2014. 6.002x Circuits and  Electronics - 2013 spring. MITx Working Paper #8.   [20] Taylor, C., Veeramachaneni, K. and OReilly, U.-M. 2014.  Likely to stop Predicting stopout in Massive Open Online  Courses. arXiv preprint, arXiv:1408.3382.   [21] Xing, W., Chen, X., Stein, J. and Marcinkowski, M. 2016.  Temporal predication of dropouts in MOOCs: Reaching the  low hanging fruit through stacking generalization. Computers  in Human Behavior. 58, (May 2016), 119129.     "}
{"index":{"_id":"20"}}
{"datatype":"inproceedings","key":"Mutahi:2017:SEP:3027385.3027395","author":"Mutahi, Juliet and Kinai, Andrew and Bore, Nelson and Diriye, Abdigani and Weldemariam, Komminist","title":"Studying Engagement and Performance with Learning Technology in an African Classroom","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"148--152","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027395","doi":"10.1145/3027385.3027395","acmid":"3027395","publisher":"ACM","address":"New York, NY, USA","keywords":"developing countries, education, engagement, learning analytics, mobile development","Abstract":"In this paper, we study the engagement and performance of students in a classroom using a system the Cognitive Learning Companion (CLC). CLC is designed to keep track of the relationship between the student, content interaction and learning progression. It also provides evidence-based engagement-oriented actionable insights to teachers by assessing information from a sensor-rich instrumented learning environment in order to infer a learner's cognitive and affective states. Data captured from the instrumented environment is aggregated and analyzed to create interlinked insights helping teachers identify how students engage with learning content and view their performance records on selected assignments. We conducted a 1 month pilot with 27 learners in a primary school in Nairobi, Kenya during their maths and science instructional periods. We present our primary analysis of content-level interactions and engagement at the individual student and classroom level.","pdf":"Studying Engagement and Performance with Learning Technology in an African Classroom  Juliet Mutahi IBM Research Africa  Nairobi, Kenya tjulimuta@ke.ibm.com  Andrew Kinai IBM Research Africa  Nairobi, Kenya andkinai@ke.ibm.com  Nelson Bore IBM Research Africa  Nairobi, Kenya nelsonbo@ke.ibm.com  Abdigani Diriye IBM Research Africa  Nairobi, Kenya a.diriye@ke.ibm.com  Komminist Weldemariam IBM Research Africa  Nairobi, Kenya k.weldemariam@ke.ibm.com  ABSTRACT In this paper, we study the engagement and performance of students in a classroom using a system the Cognitive Learning Companion (CLC). CLC is designed to keep track of the relationship between the student, content interaction and learning progression. It also pro- vides evidence-based engagement-oriented actionable insights to teachers by assessing information from a sensor-rich instrumented learning environment in order to infer a learners cognitive and aective states. Data captured from the instrumented environment is aggregated and analyzed to create interlinked insights helping teachers identify how students engage with learning content and view their performance records on selected assignments. We con- ducted a 1 month pilot with 27 learners in a primary school in Nairobi, Kenya during their maths and science instructional peri- ods. We present our primary analysis of content-level interactions and engagement at the individual student and classroom level.  CCS CONCEPTS Information systems Online analytical processing;  KEYWORDS Education; Engagement; Learning Analytics; Mobile Development; Developing Countries;  1 INTRODUCTION e usage of ICT to improve how education is delivered is visible with the large-scale adoption of tablets in K-12 and Hi-Ed [4]. Rapid improvements in ICT is possible; countries have started to adopt ICT and their school systems are leap-frogging over more archaic and expensive devices that were of most utility during the PC era. As technology currently exists, governments of developing nations that are working toward ICT adoption in the classroom can choose  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. LAK 17, Vancouver, BC, Canada   2017 ACM. 978-1-4503-4870-6/17/03. . .$$15.00 DOI: hp://dx.doi.org/10.1145/3027385.3027395  from a plethora of aordable personal learning devices connected to cheaper and faster networks.  For many, the rst and only computing experience will be mo- bile; and a growing popularity of social computing and educational applications and services on mobile devices drives strong consumer demand. In the midst of this technological shi in education, teach- ers are still expected to perform various activities in tandem from aective recognition and counseling, maintaining engagement of all pupils in the classroom, and intervening to aide poorly perform- ing students. Experienced teachers accomplish these tasks beer than new teachers, but some aspects of the student psyche, aec- tive state, and needs are not readily understood even by the most experienced teacher.  In developing nations, this problem is especially acute: poor facilities, a high student to teacher ratio, and a lack of technology solutions to provide insights and functionality to assist their stu- dents in their learning. is provides the motivation for this paper, and a novel system called the CLC [7] that aides the teacher in understanding their students learning in and out of class.  Teachers are expected to perform various activities within and outside of their classroom. For example, on one hand they require to understand and intervene to their students cognitive or aective ability in relation to their engagement and performance in the classroom. While experienced teachers may accomplish these tasks, but some aspects of the students aective state and needs are not easily understood even by the most experienced teacher.  In this paper, we focus on studying the engagement and perfor- mance of students in a classroom using the CLC system. In prior work, measuring learners engagement and interaction is widely discussed  see, e.g., in [13, 5, 8]. For example, the study in [3] qualitatively demonstrated how learning content (mainly video) production decisions aect student engagement by studying video engagement in online learning. ey measured engagement based on the time spent on watching each video and number of aempts to answer assessment items. While we also measured learners en- gagement, the focus was predominately in a classroom in K-12 seings.  In the following sections, we review the architecture of a novel adaptive learning technology platform called Cognitive Learning Companion in Section 2. We discuss the pilot setup and present research questions in Section 3. Section 4 reports the evaluation of    Event Framework  Notification   Manager Mode Switcher  User Manager  Diary Board Context Manager  CLC Clients (Android/iOS)  App Server  Analytics   Lib  Learning   Content   Hub (LCH)  Student   Master    Data (SMD)  Moodle   Client   (Web   Browser)  Content Catalog Database  Moodle   APIs  Server   Controller  Content Manager  Attention   Manager  Content Viewer Guided Lecture   Planner  Discussion   Facilitator Adaptive Tutor  Digital Learning Environment (DLE) Student   Activity   Info  CLC   APIs  Event   Frame  work  Feedback Dashboard   Manager  Figure 1: Overview of the CLC system as in [7].  the eectiveness of our platform based on a 1 month pilot in real- world classroom. More specically, we examine the content-level interaction, performance of students and the usage of the platform.  2 OVERVIEW OF THE CLC SYSTEM Figure 1 shows a high-level architectural overview of the system (details can be found [7]). CLC is a suite of cognitive capabilities supporting multiple modes of learning enabled on the mobile, and delivered through the cloud. CLC is both a learning and a teaching companion; an essential aid for the student as well as the teacher in the blended learning journey, one that also keeps the parents engaged in the process. An essential feature of CLC is the ability to capture ne-grained user interactions with content and device, and respond to it appropriately based on the context. ese interactions will be aggregated over time and analyzed to develop rich learner models (e.g., knowledge models, learning styles) based on which actionable insights will be provided to the teacher and student in various models of learning.  As a multi-modal system, CLC can operate in school mode, re- mote mode, and an interactive mode, and can eortlessly switch from one mode to another based on the learning context while con- tinuously updating knowledge, interest and interaction models as users interact with content. As it moves from one mode to the next, it elevates its functionality from being an assistant to the teacher (school), to being a partner to the teacher and student (remote), to being personalized tutor supporting adaptive learning (interactive). In each of these roles, CLC provides a suite of cognitive capabilities to assist the end user.  For example, in school mode, CLC can recommend in-classroom teaching materials for a curriculum topic, taking into account stu- dent learning activities and progress outside of class. It can infer when a student is not paying aention, and intervene as needed. It can also suggest personalized homework assignments and tutoring sessions for students.  In remote mode, CLC acts as a partner bridging the inside and outside classroom experience by intelligently facilitating remote discussions between the students and teachers, leveraging previ- ously asked questions and answers in the process. It also captures  contextual information associated with the remote mode, which may impede the students learning and can advise the teacher on appropriate interventions.  In interactive mode, CLC can act as a personalized and adaptive tutor intelligently selecting and sequencing tutoring concepts and learning content, adapting the experience to the students progres- sion, and providing problem solving scaolding to guide the student along the way. Across all the modes, CLC updates learner models related to knowledge, interests and learning styles, and preserves learning history, provides feedback from one mode to the another to propagate the learning context and ensure a seamless experience.  CLC captures ne-grained student learning activity events by in- strumenting a learners mobile learning environment via an event framework. Such event steams form the Student Activity Infor- mation (SAI) which sit on the cloud [6], and is used to develop analytical and cognitive models. e insights that CLC will gen- erate make use of analysis of SAI and will generate actionable insights with factors such as performance, engagement, context, aendance/dropout, teachers eectiveness that characterize learn- ing outcomes, as well as teachers and schools eectiveness. e insights are the basis for identifying at-risk students, create indi- vidual or group proles for each at-risk cluster, and develop pre- liminary intervention plans. For instance, CLC will recommend interventions that improve learning outcomes for underperforming learners (e.g., those with poor reading skills), introduces advanced learning/reading plans for well-performing learners, that provide resources and support for schools, as well as that allow to develop system guided feedback mechanisms for beer transparency and accountability measurement in schools.  Motivated by the above factors, the CLC (Cognitive Learning Companion) has been developed through extensive interactions with primary school teachers from a leading K-12 group of schools in Nairobi, Kenya, that are interested in adopting eective blended teaching/learning practices with their students. Like most online learning systems, CLC supports delivery of content and assessments to students, and reports performance metrics to teachers. What dierentiates CLC from many other systems, however, is the focus on student engagement and interaction. CLC seeks to interpret performance in the light of how the student approached learning, as    inferred from his/her content interaction paerns, comments, and questions, as well as aective states. is is captured through non- intrusive instrumentation of mobile client interfaces and sensors that lets CLC capture ne-grained user interactions with content and sensors (or devices) while learning.  A holistic analysis of performance and engagement yields tar- geted, evidence-based and actionable insights for teachers, and sup- port for integrating those recommendations within the teachers instructional or intervention plans for the class. CLC has thus been designed not as a supplementary, stand-alone system, but with the teacher, student (and eventually parents) and classroom in the loop. Native support for oine learning and periodic synchronization provides additional operational assistance to seamlessly connect diverse blended learning contexts inside and outside the classroom. Finally, while the CLC system is designed and implemented by closely interacting with teachers in Kenya, the underlying concept and system is applicable to any K-12 school system elsewhere in the world.  3 PILOT AND DATA COLLECTION We deployed the system in a local school in Nairobi Kenya for a 4 week pilot program (January  February 2015) in a classroom composed of 27 students aending Grade 5mathematics and science classes, and 4 teachers. e teachers comprised one science teacher, one mathematics teacher and two headteachers who were assigned to physically observe the classroom dynamics and engagement.  Each of the students were equipped with an Android tablet run- ning the CLC student client. We used a Galaxy Tab 4 tablet (spec- ications: CPU Speed 1.2GHz, RAM 1.5GB, ROM 8GB, Standard Baery Capacity 4000 mAh). e client is connected to cloud- enabled backend services (APIs) through a combination of mobile data bundle and School WiFi.  e ocial time allocated for the class lecture was 35 minutes. During this period the teacher has to properly breakdown the lec- ture into two or more sessions (e.g., lecturing, revision, Q&A, and/or classwork sessions) based on prescribed lesson plan, and then be- gins to distribute the 35 minutes to the individual sessions accord- ingly. Note that a lesson plan consists of learning activities, learning materials and group of students and a sequence of learning activities with or without learning materials for groups of students within a dened period of time to arrive at certain learning prociencies.  Twelve lessons were conducted for each subject, and 16 quizzes (composed of a total of 169 questions) were given to students. e students were instructed to use our instrumented platform to carry out their learning while separate teachers taught their re- spective subjects of mathematics and science through the CLC. e total user data collected over the 4 week period for the classroom amounted to 20.6 MB, giving ran average document size of 4KB. We also gathered both quantitative and qualitative data compris- ing quizzes, self-reported performance, usage and engagement of content, video/image data, and aendance data.  While CLC comprises three modes of operation, the pilot was primarily focused on the school mode and remote modes. is was to test our rst hypothesis that learners performance is inuenced by interaction with learning content as well as the class instructor.  4 ANALYSIS In this section, we analyzed content-level interactions analysis at the individual student and classroom level. is allowed teachers to understand how their students are engaged with learning resources, and then to incorporate personalized or group interventions in their strategies. Performance of students on various (sub-) strands (with respects to skills, knowledge and understanding) of tests, quizzes and assessments with results accessible to the classroom teacher, head-teacher and/or administration. Item-level analysis at the individual student and classroom level allows teachers to analyze whether individual or group of students miss the same quiz question items, and then to adjust their teaching strategies.  4.1 Analyzing Classroom Alignment Using the SAI (Student Activity Info) data collected, we analyzed and measured the classroom alignment based on students and teacher engagements in the classroom. For example, Figures 2 and 3 show the alignment between the teacher and the students. e plots are derived from the SAI data of students and teachers interactions with content from a mathematics lesson on Least Common Multiple (LCM) of Numbers.  0  1  2  3  4  5  0 1 2 3 4 5 6 7 8 9 10  Figure 2: Teacher classroom engagement with LCM of Numbers.  0  20  40  60  80  100  120  0 1 2 3 4 5 6 7 8 9 10  Figure 3: Classroom (27 students) engagement with LCM of Numbers content item.  We see on Figure 2 shows that the teacher spent most of their time on page 5 whilst Figure 3 indicates that most of the students were concentrated on Page 3. Further analysis of the content re- vealed that Page 3 was heavily loaded with examples. ese ex- amples can serve as material for the students to beer understand the subject being taught, hence the larger periods of time spent on Page 3. We reviewed the ndings with the classroom teacher and headteacher. Upon reviewing the data, they commented that students at the school typically did not have enough background    knowledge on prerequisite concepts. e teachers noted that it has been a challenge for Grade 5 mathematics teachers in the past stu- dent and their students have shown poor performance in that topic. It was also mentioned that in Grade 4 the students are given a basic introduction on LCM and numbers, and the Class 5 mathematics teacher is expected to start with easy examples to gauge whether the students are able to comprehend the basics on the prerequisite concept before moving on to more complex and dicult examples.  We also found two key take-aways on how the content items are prepared and used from the data. We observed that when there is no signicant update/change to the learning curriculum from one year to the next, then most teachers reuse content from previous years without updating or changing the learning content (e.g. lecture notes and examples). It was also noted that teachers rarely receive feedback on the content items they use in the classroom such as the diculty level, compressing burden, illustrative richness, etc of the content items. None of the participating teachers (including the headteachers) received formal training to create digital lecture materials. Furthermore, there is no standardized guideline to assist them in creating lecture materials, what there though are teacher specic lesson plans derived from generic Scheme of Work.  We analyzed the alignment between students and the teachers in a classroom for the Least Common Multiple (LCM) of Numbers content item (see Figure 2 and 3). e alignment between the student and the classroom teacher shows most of the concentration is in Page 3 and Page 5.  e engagement of a particular student for the same content item but outside a classroom is shown in Figure 4. Interestingly, unlike the the data from the classroom engagement, we found that the students jumped to Page 7 by skipping Pages 1-6. e students were highly engaged on Pages 7-9 (Figure 4) despite the teacher not covering these pages in the classroom.  0  1  2  3  4  5  0 1 2 3 4 5 6 7 8 9 10  Figure 4: Page 7-9 of the Least Common Multiple (LCM) of Numbers content item.  While looking at Pages 7-9 of the content item, the materials pre- sented in these pages are less condensed (low compression burden). is gives an indication as to why the student is highly engaged on those pages. However, we noted that the LCM by Short Method technique was presented aer the examples shown in Pages 7 and 8. Hence, the student has to go back-and-forth between these pages. We also noted that the two examples presented in Page 7 and 8 are identical. We asked a number of probing questions to the teachers such as What intervention would they take if they were to re-teach this content item again or in the future We feedback we received from this was, I would reorganize by re-sequencing the pages based on diculty and density of the page.  4.2 Performance Analysis We analyzed the performance data of the students based on the 16 quizzes for science and mathematics. Our goals were threefold: (i) understand the relationship between the time spent on a quiz and performance, (ii) explore the relationship between quiz diculties, time spent and performance achieved, and (iii) proling students based on quiz aempt paerns. In the quizzes, we do not pre-assign the amount of time it takes to aempt an individual question or a quiz. e teachers, based on their experiences, decide how long a given quiz will take in a classroom.  0 200 400 600 800 1000 1200 1400 1600  0  20  40  60  80  100  120  0  20  40  60  80  100  0 200 400 600 800 1000 1200 1400  Histrogram with Normal Curve  Figure 5: Total time taken to complete assessment against the grades.  Figure 5 shows the total time taken to complete 7 quizzes in mathematics ploed against the score. Wherein on average most students spend about 10 minutes (500-600 seconds) to complete a quiz. We next explored the relationship between time spent on a quiz and performance, and found that there is a positive correlation between the time spent on a quiz and performance  i.e. students who spent longer amounts of time when doing the quiz, had higher performance scores. e local regression for each quiz shows a positive increase in performance over time and then a sudden drop in performance for the classroom.  To further understand what caused the classroom performance to drop, we examined individual topics and their relationship on concept dependency. For instance, Least Common Multiple (LCM), Subtraction of Mixed Numbers, and Addition and Subtraction of Fractions are the three topics being covered in the class to which students were given a quiz for each of these topic at the end of the lesson. e topics are listed with respect to topic sequence as prescribed in the curriculum script, e.g. LCM is the pre-requisite topic for Subtraction of Mixed Numbers. On this pre-requites topic, 69.23% of the classroom (18 students) scored above 75%, 23.08% of the classroom (6 students) scored above 50% but below 75%, and only two students (7.69% of the classroom) scored below 25%. However, the average performance of the classroom dropped on the subsequent advanced topics. As shown in the third row of Table 1, the percentage of the classroomwho scored below the passing mark (50%) increased by a factor. What we do nd underscored across all the quizzes is a positive and consistent relationship between time spent on quizzes and performance score for the three quizzes.  We next study the data gathered from the quiz such as aempts and engagement paerns. In particular, from the performance data we explore ways to best characterize how a student approached a quiz. While analyzing the data, we found that some students    % of Classroom Performance periz Score LCM SMN ASF >= 75% 69.23 26.92 26.92 > 50% & < 75% 23.08 34.62 23.08 < 50% 7.69 38.46 50  Table 1: Classroom performance comparison for three sub- sequent topics.  were consistently repeating the same quiz up to 10 times. ese students appear to be gaming the system by submiing the quiz results multiple times until they achieved the maximum score or until they were identied and told to stop by the teacher.  By analysing the data, we have observed that multiple aempts were recorded for the same quiz. Several students were found to retake and submit the same quiz multiple times. In each aempt, we observed a slight increase in their score. For instance, one student scored 0 in his rst aempt, but aer his 9th aempt he scored a perfect 100%. Further analysis of the data across multiple quizzes revealed characteristic behaviour exhibited by this category of students such as a high number of aempts, and a high number of wrong answers. In Figure 6, we tabulate the performance and the number of wrong aempts during a quiz, and nd distinct category of outliers in dierent subjects. It is therefore possible to dierentiate quiz gamers based on their behaviour against other students taking the same quiz.  0  100  200  300  400  500  600  0 20 40 60 80 100  N o.   W ro  ng   Q  ui z   At te  m pt  s  Scores (Out of 100%)  Student Scores and Wrong Attempts  Mathematics Quiz  Science QuizMathematic Quiz Outliers  Science Quiz  Outliers  Figure 6: Scatter plot outliers for mathematics and science quizzes.  Finally, wemeasured and compared students engagement against performance. Using the performance data, we classied students into high performers (score above 75%), average performers (score between 50 and 74%), and low performers (score below 50%) stu- dents. is was then correlated with the relationship between engagement of students with content and performance. We found that on average that students with high-levels of engagements tend to perform well in the given quizzes.  5 CONCLUSION AND FUTUREWORK Technology is changing how education is delivered inside and outside of the classroom. In this paper, we have presented a novel adaptive learning system called Cognitive Learning Companion (CLC). We found through our study technological interventions are well received by teachers and they agree that this is more ecient and eective. e study shed light on how students would game the system to get beer results that should be addressed in subsequent versions of the platform.  We conducted a one-month longitudinal study with 27 learners in a local school in Nairobi, Kenya to examine the eectiveness of the proposed learning companion. Our ndings showed that there is frequent misalignment between the pace and amount of aention spent on dierent content between the students and teachers. is was due to a lack of feedback on the diculty and amount of content of the teaching material developed by the teachers. We were able to easily and eectively help the teachers identify through the points of misalignment and aspects of their teaching material and curricula that should be edited or receive more/less aention. e study revealed a strong correlation between performance and engagement: where more engaged students would perform on average beer.  Finally, we experienced signicant resource constraints and chal- lenges (e.g. intermient connectivity, baery power, etc) during the deployment experiences, and this should be a consideration when developing systems for resource constraint regions or countries.  Future work would entail enhancing CLC by examining various algorithms to provide real-time insights and predictive capabili- ties for the teachers. We also plan to develop a smart cooperative content downloading module that may combine a novel combi- nation of computed resource level, reliability index, and segment determination to download, aggregate and distribute downloaded chunks. e ability to improve resource utilization for low-end devices while scheduling download contents.  REFERENCES [1] Jaye Clarkes-Nias, Juliet Mutahi, Andrew Kinai, Oliver Bent, Komminist Welde-  mariam, and Saurabh Srivastava. 2015. Towards Capturing Learners Sentiment and Context. In L@S. 217222.  [2] Computing Research Association. 2015. Data-Intensive Research in Ed- ucation: Current Work and Next Steps. (2015). hp://cra.org/ cra-releases-report-on-data-intensive-research-in-education/.  [3] Philip J. Guo, Juho Kim, and Rob Rubin. 2014. How Video Production Aects Student Engagement: An Empirical Study of MOOC Videos. In L@S 14. ACM, New York, NY, USA, 4150. DOI:hp://dx.doi.org/10.1145/2556325.2566239  [4] Nagy K Hanna. 2003. Why National Strategies are needed for ICT-enabled Development. World Bank Sta Paper. Washington, DC: World Bank (2003).  [5] Curtis R. Henrie, Lisa R. Halverson, and Charles R. Graham. 2015. Measuring Student Engagement in Technology-mediated Learning. Comput. Educ. 90, C (Dec. 2015), 3653. DOI:hp://dx.doi.org/10.1016/j.compedu.2015.09.005  [6] Juliet Mutahi, Oliver Bent, Andrew Kinai, Komminist Weldemariam, and Bikram Sengupta. 2015. Capturing Learners Activity Events from a Mobile Learning System Using Adaptive Event Framework. In MOBILESo 2015. 109112.  [7] Juliet Mutahi, Oliver Bent, Andrew Kinai, Komminist Weldemariam, Bikram Sengupta, and Danish Contractor. 2015. Seamless blended learning using the Cognitive Learning Companion: A systemic view. IBM Journal of Research and Development 59, 6 (2015).  [8] Jane Sinclair, Mahew Butler, Michael Morgan, and Sara Kalvala. 2015. Measures of Student Engagement in Computer Science. In ITiCSE 15. ACM, 242247. DOI: hp://dx.doi.org/10.1145/2729094.2742586  http://cra.org/cra-releases-report-on-data-intensive-research-in-education/ http://cra.org/cra-releases-report-on-data-intensive-research-in-education/ http://dx.doi.org/10.1145/2556325.2566239 http://dx.doi.org/10.1016/j.compedu.2015.09.005 http://dx.doi.org/10.1145/2729094.2742586   Abstract  1 Introduction  2 Overview of the CLC System  3 Pilot and Data Collection  4 Analysis  4.1 Analyzing Classroom Alignment  4.2 Performance Analysis   5 conclusion and future work  References   "}
{"index":{"_id":"21"}}
{"datatype":"inproceedings","key":"Gibson:2017:RWA:3027385.3027436","author":"Gibson, Andrew and Aitken, Adam and S'andor, 'Agnes and Buckingham Shum, Simon and Tsingos-Lucas, Cherie and Knight, Simon","title":"Reflective Writing Analytics for Actionable Feedback","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"153--162","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027436","doi":"10.1145/3027385.3027436","acmid":"3027436","publisher":"ACM","address":"New York, NY, USA","keywords":"formative feedback, learning analytics, reflective writing analytics, reflective writing theory","Abstract":"Reflective writing can provide a powerful way for students to integrate professional experience and academic learning. However, writing reflectively requires high quality actionable feedback, which is time-consuming to provide at scale. This paper reports progress on the design, implementation, and validation of a Reflective Writing Analytics platform to provide actionable feedback within a tertiary authentic assessment context. The contributions are: (1) a new conceptual framework for reflective writing; (2) a computational approach to modelling reflective writing, deriving analytics, and providing feedback; (3) the pedagogical and user experience rationale for platform design decisions; and (4) a pilot in a student learning context, with preliminary data on educator and student acceptance, and the extent to which we can evidence that the software provided actionable feedback for reflective writing.","pdf":"Reflective Writing Analytics for Actionable Feedback  Andrew Gibson University of Technology Sydney  Sydney, Australia Andrew.Gibson@uts.edu.au  Adam Aitken University of Technology Sydney  Sydney, Australia Adam.Aitken@uts.edu.au  Agnes Sandor Xerox Research Centre Europe  Meylan, France agnes.sandor@xrce.xerox.com  Simon Buckingham Shum University of Technology Sydney  Sydney, Australia Simon.Buckinghamshum@uts.edu.au  Cherie Tsingos-Lucas University of Technology Sydney  Sydney, Australia Cherie.Tsingos-Lucas@uts.edu.au  Simon Knight University of Technology Sydney  Sydney, Australia Simon.Knight@uts.edu.au  ABSTRACT Reective writing can provide a powerful way for students to in- tegrate professional experience and academic learning. However, writing reectively requires high quality actionable feedback, which is time-consuming to provide at scale. is paper reports progress on the design, implementation, and validation of a Reective Writ- ing Analytics platform to provide actionable feedback within a tertiary authentic assessment context. e contributions are: (1) a new conceptual framework for reective writing; (2) a computa- tional approach to modelling reective writing, deriving analytics, and providing feedback; (3) the pedagogical and user experience rationale for platform design decisions; and (4) a pilot in a student learning context, with preliminary data on educator and student ac- ceptance, and the extent to which we can evidence that the soware provided actionable feedback for reective writing.  CCS CONCEPTS Applied computing  Interactive learning environments; Computingmethodologies Natural language processing; Lex- ical semantics; Human-centered computing  User interface programming;  KEYWORDS Reective Writing Analytics, Formative Feedback, Reective Writ- ing eory, Learning Analytics  ACM Reference format: Andrew Gibson, Adam Aitken, Agnes Sandor, Simon Buckingham Shum, Cherie Tsingos-Lucas, and Simon Knight. 2017. Reective Writing Analytics for Actionable Feedback. In Proceedings of e 7th International Learning Analytics & Knowledge Conference, Vancouver, BC, Canada, March 13-17, 2017 (LAK 17), 10 pages. DOI: 10.1145/3027385.3027436  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 17, Vancouver, BC, Canada  2017 Copyright held by the owner/author(s). 978-1-4503-4870-6/17/03. . .$15.00 DOI: 10.1145/3027385.3027436  1 INTRODUCTION Tertiary institutions are continuously confronted with the issue of assessing students in a way that is authentic, promoting learning for the future. It is generally accepted that when used well in education seings, reective tools enhance lifelong learning and professional practice [30, 39].  When students engage in reection on action [34], they are self-critical, identify and analyse their responses to challenging issues, and are prompted to reect on how this experience reveals knowledge that could be applied in future. In this way reective processes in learning become authentic when they are formative and future-oriented [3], and when higher level, meta-cognitive thinking about experience is required for students to connect their thinking to a wider world [14].  Reective writing is promoted by many researchers as a valid way to assess reective practice in a tertiary context [15, 28]. Re- ective writing allows students to engage with both certainty and with what they do not know about a situation [27]. is typically involves students documenting their experiences, and writing reec- tive essays, journals and blogs based on their personal reections on those experiences. e challenge is in how actionable feedback can be provided at scale when large numbers of students are required to write reectively. For any feedback to be actionable by the student, it needs to be . . . of sucient quantity; timely; it should focus on learning not marks; it should be related to assessment criteria and be understandable, aended to and actually used by students to make improvements on their work. [25, p. 337]  Along with actionable feedback teaching students to write re- ectively requires clear denitions, instructional instruments and rubrics of reection. Reection is thinking with a purpose about complicated or unstructured ideas where there is no obvious so- lution. [23]. From a pedagogical perspective reective writing is directed towards a learning goal [4], and a corrector of distortions in thinking [22].  Because reection is not necessarily an inherent skill the art of writing reectively can be challenging [40], in part because reective writing is dierent in nature and purpose to analytic academic writing, and students can struggle to present a personal stance within an academic context.    LAK 17, March 13-17, 2017, Vancouver, BC, Canada A. Gibson et. al.  1.1 e Project e A3R (Authentic Assessment Analytics for Reection) research project was established to investigate the extent to which auto- mated Reective Writing Analytics (RWA) might oer potential to deliver formative feedback on student reective writing at scale, and whether such feedback might encourage action on the part of the student. RWA involves the automated analysis of reective writing, and the feeding back of analytics to the writer [14]. e key question motivating this research was to investigate how RWA might enable a scalable provision of actionable feedback to students on their reective writing, and thereby augment existing pedagogi- cal approaches to authentic assessment and reective practice.  1.2 Participation Courses across a range of disciplines at the University of Technology Sydney (UTS) use reective writing assessment as it has been shown to be benecial in elds as diverse as Engineering [27], Pharmacy [40], Business [36], Medicine [32], Psychology [10], Nursing [44], and Teaching [24].  For this research, we worked primarily with academics from seven subjects across three disciplines (Science, Business, and Engi- neering). Our academic partners provided reective writing rubrics and samples of student reective writing, primarily in the form of journal entries and reective essays.  e project, which extended over two semesters (March 2016 - November 2016), involved meetings with members of the univer- sitys teaching and learning unit (IML), discussions with academic language and learning (ALL) experts, and workshops with repre- sentatives from all stakeholder groups to discuss the way reective writing is used across the curriculum. e high level of stakeholder engagement was key to establishing the pedagogical objective of actionable feedback, and grounded the research in the learning and teaching of the university.  1.3 Approach Despite the diversity of perspectives that came as a result of working across dierent disciplines, the academic partners reported common problems with reective writing assessment: (1) Large numbers of students, many of whom were unfamiliar with reection and writing reectively; (2) Diculty in clarifying the key features of good reective writing; (3) e need for students to receive timely formative feedback on their writing, but a lack of sta time to meet this need; and (4) A need to ensure that any feedback provided is actionable by students in a way that maximises the benets of the learning task.  e A3R project redesigned and extended (4) the AWA (Aca- demic Writing Analytics) soware platform which was developed through previous research [8, 18, 35] .1 A focus on actionable feedback for students resulted in AWA being redesigned to accom- modate multiple layers of feedback, drawn from a larger set of analytics which were derived using multiple Natural Language Processing (NLP) techniques. e approach to analysing reective  1AWA aims to make visible to learners the ways in which they are using (or failing to use) language, and to do this in a way that is scalable . Designed to complement existing curricula tools AWA does not automate the grading of texts.  writing was also redesigned with the aim of serving actionable feedback.  We took a theory rst approach to the research, synthesising a large body of previous research on reection and reective writing, together with the practical implementation as evidenced by our academic partners. We then implemented the resulting framework computationally in the AWA platform, and applied the soware in a learning context. is approach yielded a number of contributions that we believe advance the eld of Learning Analytics (LA): (1) A theoretical framework for reective writing; (2) a method for using the framework computationally to analyse reective writing; (3) a pedagogically driven design approach to LA; and (4) preliminary evidence supporting our approach. Our approach is detailed in the sections that follow.  2 THEORETICAL FRAMEWORK e A3R approach is grounded in the theory of reection and reective writing [11, 22] and informs a framework (see gure 1) which can be used both for human analysis of reective writing and as a basis for developing methods of computational analysis. A key objective was to accommodate the major ndings of previous research together with the main aspects of the various assessments for the subjects that we worked with. is approach ensures that it is pedagogically useful for the subjects in which it is applied.  2.1 Reective Writing With a brief to synthesise key ideas and relate them to the cur- rent practice of our academic partners, we examined the dominant research on reective writing [17, 23, 29, 37].  A key pedagogical issue is how feedback can guide students to reect more deeply and to learn rhetorically purposeful language to achieve this [19]. A number of researchers characterise depth of reection as a shi from a descriptive style of mere impressionistic reporting of events, through to a more critical style that focuses on integrating, analysing, and restructuring experience [17, 30], and on outcomes [4].  In terms of how feedback can guide learners actions, the frame- work maps ve levels of depth. Students are prompted to reect on these by considering self-directed questions that correspond to these levels. is informed the vertical dimension of the framework (gure 1). e levels range from the lowest simple impressions through to the highest level of an intention to act:  (1) Impression: What is happening around me What is important to me  (2) Interpretation: How do I make sense of my impressions within my current situation  (3) Internalisation: How does this relate to me, to my knowl- edge, my wider context, my learning, my disposition, my emotions How does it make me feel, and what are my reactions  (4) Integration: How does this t with other knowledge, ex- periences, and diering perspectives Can I learn from others  (5) Intention: Why am I concerned with this, and what do I intend to change with regards to myself based on what I have come to understand through being reective    Reflective Writing Analytics for Actionable Feedback LAK 17, March 13-17, 2017, Vancouver, BC, Canada  Figure 1: eReection Frameworkwhich synthesises theories of reection and narrative togetherwith discipline approaches to assessing reection.  ese levels accommodate theoretical models together with ped- agogical descriptions of the actions that might be expected of stu- dents when writing. ese actions were drawn from a range of assessment instruments including rubrics from our academic part- ners, and through the analysis of a range of examples of student writing from across the disciplines.  2.2 Student Writing Analysis For many students reective writing in a tertiary seing is a novel genre, and tutors may be inexperienced in its assessment. While subjects provide rubrics for reective writing assessments, these do not always provide explicit instruction on the language of re- ective writing. UTS academic communications expert Rosalie Goldsmith designed a resource that helps engineering academics recognise linguistic features of reection in student journals [8]. Goldsmith itemised frequently occurring lexico-grammatical fea- tures, e.g. thinking and feeling reporting verbs (I felt, I realised, I became aware), statements of a challenge, a critical incident, expressions of learning intentions and more.  Rhetorical analysis methods [38] and genre theory [21] further informed our analysis of: (1) e lexico-grammatical features of student reective writing; and (2) the discursive structure and cohe- sive paerns most oen found in whole texts. Depth and structural dimensions are marked by rhetorical moves or recurrent discourse elements that characterise wrien and oral genres and which per- form a coherent communicative function [38]. Used with intent by writers, moves realize rhetorical action [13]. To take an exam- ple of analytic writing, a scientic research paper will include a statement of a research gap in the abstract and introductory stages [38]. Structurally, the research paper obeys reader expectations of the genre, and requires a sequenced orderly progression of these moves.  In reective writing rhetorical moves communicate personal shis in perception, express self-critique and self-doubt, and regis- ter changes in belief. Its rhetorical, lexico-grammatical and struc- tural features, considered together, suggest that reective writing is  a genre, by which we mean formalised writing with a goal-oriented communicative purpose, and which is structurally constrained in a series of stages suggesting reporting or narrative form [21]. Dif- ferent models of moves or stages have captured types of reection [41, 42] and also depth of reection [23, 30, 44]. Of particular salience was Birneys [2] study of teacher trainee reection, a ne- grained systemic-functional mapping of reection depth with lin- guistic realisation. is research concludes that strong reective writers are able to draw on up to ten linguistic features ranging from use of feeling and thinking verbs to adjectives (e.g. a positive impact) and reasoning adverbs (e.g. extremely challenging), and future tense modality (e.g. I intend to bring this into my future practice).  In our framework a horizontal dimension represents a narrative- like sequence of rhetorical moves, while a vertical dimension models levels of depth. e intersection of depth and sequential dimen- sions represent the possible stages through which the text moves and the linguistic features used to realise these stages - from shal- low impressionistic description to deep future-minded intentional statements. To prompt students to improve their reection, the frameworks prompt questions encourage them to move from shal- low to deep levels of reection (see gure 1).  Indeed, an analysis that we carried out on student reective texts conrms that similar reective writing moves occur in dierent disciplines. Our work also conrms other studies that show strong writers utilise a wider range of linguistic features than weaker re- ective writers [19]. Highly rated journal writers aempt to make sense of the relationship between themselves and their situations [9], and question their own assumptions that underpin their ac- tions [37]. ey reect on how to proceed in the face of uncertainty [27], and reect on decisions [26]. We found that the rhetorical moves announce a personal response to a learning context (Con- text), acknowledge the challenging nature of learning (Challenge), and indicate a learning experience is self-transformative (Change) (See gure 1).    LAK 17, March 13-17, 2017, Vancouver, BC, Canada A. Gibson et. al.  For example, one student wrote about how habitual ways of thinking lead to problems (obstacles), which then lead to a new or rapid understanding of the situation:  . . . I rapidly understood that language was not the most important barrier. Our ways of thinking, acting, and our values became obstacles we needed to overcome.  When interpreted as a coherent rhetorically staged text, we nd that the writer emphasises a rapid change in understanding (I rapidly understood) as well as the notable challenging element in the learning situation (the barrier, obstacles). ey then note that this situation leads to a change in perception, and a departure from a previous habitual view (language is the main source of misunderstanding). We show how these semantic relations inform computational modelling in section 3.  Feedback from academic partners conrmed that they found the framework to be a helpful lens through which to view their reective writing assessment, and that it would assist in giving feedback to students in a way that might encourage them to improve their reections.  2.3 Simplication Aer assessing the extent to which existing technologies available to us might enable us to operationalise it computationally, we simplied the framework, especially in the way we approached the levels of depth. While we could see the potential for using the linguistic properties of the sentences to identify key moves such as CONTEXT, CHALLENGE, and CHANGE, it was not obvious to us how we might detect levels within the depth dimension. erefore we simplied depth by distilling a dominant common characteristic feature in deeper reection pertaining to all the three moves: that it is applied personally. We identied that this is indicated where the student links one of the key moves to themselves. We referred to this feature as LINK2ME.  We also noted that the framework might inform feedback to the student in ways other than by the rhetorical moves, such as: (1) Indicating aective elements and emotive expressions in the writ- ing; (2) drawing aention to how good reection links to existing knowledge via epistemic expressions; and (3) indicating critique, particularly self-critique, as a key feature of depth.  us, our nal simplied framework for feedback purposes com- prises three moves (CONTEXT, CHALLENGE, CHANGE), a modier of these moves to indicate depth (LINK2ME), and three expression types (EMOTIVE, EPISTEMIC, CRITIQUE). is provided a much more accessible version of the reective writing framework for stu- dents while maintaining its connection to the theory. e simplied version also assisted with the computational implementation which was tackled by two dierent processes: (1) e implementation of the modelling of reective rhetorical moves. is is detailed in section 3; and (2) the aggregation of dierent computational analyses, and implementation of multiple layers of feedback. is is described in section 4.  3 REFLECTIVE RHETORICAL MOVES Building on the theoretical and linguistic description of reective writing moves in Section 2 we have developed a system to compu- tationally detect sentences that convey the three primary rhetorical moves of reective writing determined by the A3R framework: Context, Challenge and Change. Each of these can be augmented by the Link2Me feature. To detect sentence elements indicating the rhetorical moves we used the concept-matching rhetorical analysis framework [33]. e analysis model is implemented in the natural language processing tool Xerox Incremental Parser (XIP) [1].  3.1 e concept-matching analysis framework e concept-matching framework models rhetorical moves as pre- dened paerns of abstract elements called constituent concepts. Constituent concepts are determined by the denitions and de- scriptions of the rhetorical moves. For example, the Challenge move is dened in the A3R framework as the challenge of new surprising or unfamiliar ideas, problems or learning experiences. e constituent concepts suggested by this denition are CONTRAST (cf. challenge, unfamiliar, problem), STANCE (c.f. surprising, expe- riences), ANALYSIS (c.f. ideas, learning) and SUBJECT (the author). Any combination of these concepts constitutes a possible paern for Challenge, as in the following formula:  CHALLENGE = (SUBJECT+ANALYSIS) AND (CONTRAST+ANALYSIS)  e + designates grammatical coherence and AND co-occurrence in the same sentence. is formula will match sentences that de- scribe contrast in the authors thinking. e following sentences are examples that match this paern (constituent concepts in bold):  (1) I reected on this and felt decision making was like second nature, yes I over-thought my decisions whether it was personal or professional but I never thought of the act of having to justify my decisions.  (2) I continued to contemplate how I was going to tactfully address this questionable behaviour.  While traditional approaches to analysing rhetorical writing rely on the detection of lexical and lexico-grammatical features, the distinctive advantage of the concept-matching framework lies with the grammatical coherence constraint coupled with the pre-dened paerns, which is able to lter out noise.  3.2 Implementation in XIP XIP is a natural language analysis tool whose basic function is to provide deep syntactic analysis of sentences. XIP integrates statistical processing and grammar rules, which were developed using its dedicated rule writing mechanism. Basic XIP analysis incrementally executes a chain of treatments for each sentence of a document from segmentation through morpho-syntactic analysis, part-of-speech disambiguation and chunking to syntactic depen- dency extraction, i.e. the identication of syntactic functions, like subject, object, modier, etc.  e concept-matching framework was implemented as an ad- ditional XIP module, building on the general dependency output, and using the rule writing mechanism. e implementation task consisted in detecting sentences that contain a rhetorical paern    Reflective Writing Analytics for Actionable Feedback LAK 17, March 13-17, 2017, Vancouver, BC, Canada  Figure 2: Example rhetorical patterns of the A3R reective moves and illustration sentences.  dened in the concept-matching framework. is involved the fol- lowing steps: (1) e development of a lexical database where the constituent concepts are associated with words and expressions; 2; (2) creating XIP rules that select out of all the dependencies ex- tracted by the general XIP syntactic analysis module those that are relevant for the rhetorical paerns; and (3) creating XIP rules that mark the sentences that contain a set of dependencies of the pre-dened rhetorical paerns. e paern matching rules were developed using a small development corpus of ten reective essays of various domains annotated according to the language analysis methods our reective writing framework.  4 ACTIONABLE FEEDBACK Our moves of Context, Challenge, Change and Link2Me were ac- commodated by the implementation of reective rhetorical moves in XIP. However, much of the reective writing literature had in- dicated that there were other important indicators of reective writing, such as the three expression types (Emotive, Epistemic, and Critique) that we identied in our simplied framework. We also noted that some features might present not at the sentence level, but rather in the text as a whole, such as the general struc- ture of the writing, or the overall sentiment expressed. ere was also research that the situational context in which the reection was based was signicant, indicating that there may be specic vocabulary for dierent writing contexts. ese factors indicated that we needed more than the sentence level reective rhetorical move analytics. We needed to incorporate other NLP techniques that could provide us with varying levels of analysis, and a way of aggregating the resultant analytics.  Further, our objective was to provide actionable feedback to the writer, and so we needed a mechanism for creating feedback from aggregated results, and a way of evaluating the actionability of this feedback.  Responding to these challenges required signicant design de- cisions in terms of both the architecture of the platform and the  2e lexical resources have been acquired by various means: importing from existing rhetorical analysis modules that use the same constituent concepts, and using (morpho)- syntactic and semantic properties and part-of-speech as features, which are provided by the general XIP parser. e lexicon for the STANCE concept has been imported from a sentiment analysis module of XIP [5]  desired user experience. We outline our approach to both of these areas in the following sections.  4.1 Platform Architecture roughout the A3R project, AWA evolved from a web application calling remote analysis service into a platform with multiple ser- vices deployed across a mix of local and cloud infrastructure. e design was driven by Information Architecture (IA), the process of designing, implementing and evaluating information spaces that are humanly and socially acceptable to their intended stakeholders[12]. In our case the intended stakeholders were the students and their teachers, and the information space was AWA, but conceived as learning environment where the students could improve their re- ective writing.  Importantly, this meant a pedagogically driven approach to RWA. at is, architecting the platform in a way that considered action- able feedback in the whole of the user experience: starting from the way that student writing was conceptualised; including the approaches to analysis and the delivery of the feedback; and con- cluding with evaluation of the feedbacks ecacy.  ese architectural changes were facilitated by introducing a Text Analytics Pipeline (TAP), a modular cloud based application. TAP allowed the inclusion of analysis services other than XIP to be included in the platform, facilitated the aggregation of the resultant analytics, and generated multiple levels of feedback. An overview of the key elements of TAP can be seen in gure 3.3  Figure 3: e core modules of the Text Analytics Pipeline (TAP) shown within dashed box, and connections to UI (top le) and XIP (top right).  e design of TAP allowed for the aggregation of multiple sources of analysis from both external services such as XIP, third party libraries like CoreNLP [20], and internal soware. A detailed de- scription of TAP is beyond the scope of this paper, however an overview of the process of generating feedback highlights the way TAP contributes to the AWA platform.  Feedback is provided to the UI in the form of a JSON object composed of the diering feedback types (expression, sentence, paragraph, and document). Sentence level feedback is generated by passing the text through a module that connects to the external XIP service. e resulting analysis is then formaed similarly to 3TAP is wrien in Scala to run on the JVM. It is designed in a modular reactive style with Akka to facilitate exibility and scalability as required.    LAK 17, March 13-17, 2017, Vancouver, BC, Canada A. Gibson et. al.  the other feedback levels before being packaged up and sent back to the UI. Expression level feedback is generated using dierent soware modules, the selection of which depend on the feedback type. Emotive expressions are generated based on lexical com- parisons with the Warriner [43] Corpus selecting high valence and arousal terms, whereas the Critique and Epistemic expressions are derived using techniques for identifying metacognition in reec- tive writing [14]. Paragraph and document level feedback involves complex rules applied to a range of analyses. For example, some document comments are generated based on the ratio of rhetorical moves that have been detected across the document. Other para- graph level comments combine a third party spell checking library4 with named entity recognition from CoreNLP [20] and basic para- graph metrics to determine if a comment on spelling errors should be generated. Further detail on these algorithms can be found by examining the soware.5  Importantly, the choice of modules, third party services, and design of the algorithms, were all driven by the pedagogical imper- ative to provide actionable feedback. us, the very architecture of the platform is shaped by the desired learning outcomes.  4.2 User Experience e objective of actionable feedback based on our conceptual ac- count determined that in addition to a sentence-level annotation (implemented in previous versions of AWA), feedback should also be provided at the sub-sentence (expression), and aggregate lev- els (both paragraph and whole document). ese features were implemented, alongside user support documentation, as outlined below.  4.2.1 User Interface Layout. Actionable feedback requires a UI that prompts users to turn feedback into feed-forward[16]. To prompt re-draing, students are alerted to what is needed in terms of reective expression. Visual feedback has been shown to be more eective than verbal feedback which oen appears complex and multidimensional [31].  Less certain is how and if certain UI features contribute in creat- ing feed forward. A key design change concerned the accommo- dation of feedback to the user other than just sentence labelling and highlighting. ere was a need to accommodate whole of doc- ument textual feedback, feedback associated with paragraphs, and feedback associated with groups of words or expressions. With the addition of at least three more types of feedback, a signicant consideration was to ensure that the interface did not become too cluered. For feedback to be actionable, it rst needs to be compre- hended. We anticipated that too much visual information would result in the user being less certain about which information should be acted up.  Because of this, our aim was a clean UI that maintained the prominence of the original text, but annotated with both sentence and expression level feedback. Paragraph feedback would be pro- vided in a right hand margin aligned with the relevant paragraph, and whole document feedback would be provided above the text as it was intended to provide overall commentary on the writing. Taking this approach also had the advantage of being similar to 4hps://github.com/languagetool-org/ 5e soware can be found on the CIC GitHub Page (hps://github.com/uts-cic)  what students may expect if they were to receive feedback on a printed document, i.e. parts of the text itself underlined and circled with comments in the margins and an overall summary comment. We hoped that this familiar format might assist with the overall usability of the interface. is layout can be seen in gure 4.  Figure 4: e AWA User Interface with a pane (lower right) for collecting a feedback response from the user.  To assist the user with understanding sentence and expression level annotation, a pop-out pane was provided that defaults to being visible when the analysis is displayed, but which can be clicked to reduce to a side tab aer use (gure 5). To assist with evaluation, we also required a way to elicit a response from the user as to whether they thought the feedback that was provided on their text was useful (see boom right of gure 4). e intention was to compare student responses via this mechanism with evidence of changes in subsequent dras.  Figure 5: e UI elements used by sentence and expression level feedback  4.2.2 User Interface Elements. A key factor in the UI design was how the theoretical foundations and their analytics expressions were to be made visible to the user, and the extent to which they might be helpful and useable for actionable feedback.  Sentence annotation was impacted to the greatest extent by changes to the underlying analytics. e framework (2), was sim- plied to ensure that it was easy to comprehend, and the labels derived in the process of developing the analytics (Context, Chal- lenge, and Change) proved to be very ambiguous when presented to users. e previous version of AWA used named labels in the sentences, and our original intention was to maintain this style. However, the ambiguity caused signicant problems in terms of our dierent users who might perceive the feedback and take action on it. Aer much deliberation, we seled on removing the names altogether from the UI, and seled on single sentence descriptions represented by the blue square, pink circle, and green triangle, as shown in gure 5. e nal colour and shapes of the icons were chosen to allow for simplicity in the UI while accommodating users with colour blindness.    Reflective Writing Analytics for Actionable Feedback LAK 17, March 13-17, 2017, Vancouver, BC, Canada  5 STUDENT USE Student use of AWA in multiple subjects is an ongoing process with each subject having dierent due dates for their reective writing assignments. As a result, for a preliminary examination of the data we have selected only those students from Pharmacy who have used the soware at the time of writing this paper (from late August to mid October 2016). Although preliminary, this data has provided some insights on whether the feedback is actionable for these early users of the soware. Other subjects involved in the research are expected to contribute further to the data during the remainder of the spring semester (through to late November).  5.1 e Pharmacy Learning Context Preparing pharmacy students for the complexities and diversity of clinical practice is a consideration with pharmacy educators. A possible solution to bridging the theory/practice gap is through re- ective practice [39]. One of the tools utilised to enhance reective practice is through reective writing. Reective writing skills are paramount for students to think about incidents, their outcomes and how this aects the health of a patient. In 2016, First year Master of Pharmacy students (n=59) were oered the use of AWA to assist with improving their reective writing skills. e Masters of Pharmacy Course is a 2-year intensive program which embeds reective activities in the 520 hours of clinical placements. One aspect of the weekly reective activities involve students writing reective statements and uploading these to their e-portfolios. is process is integrated into the course to prepare students to make beer informed decisions and clinical judgements for future prac- tice. For example, students are required to: (1) reect back on a weekly incident; (2) view the incident from dierent perspectives (eg from the perspective of the patient, carer, pharmacist, and/or other health professional); (3) write about what was learned, what challenges they undertook, what strategies they utilised to over- come these challenges; (4) recognise the strengths and skills they have, how these could further be developed, how their behaviour or approach could be changed to enhance a future similar event; and (5) identify shis in their beliefs and aitudes which have resulted.  5.2 Preliminary results Data was collected from the student use of AWA in Pharmacy, and their responses represent a snapshot at the time of writing the paper.  Total students: 59 AWA users: 30 50.8% Total posts: 120  Posts with response: 63 52.5% Responses feedback helpful 54 85.7%  Table 1: Usage and feedback data  Early discussions between the students and their teacher about their use of AWA anecdotally indicated that it allowed them to use the tool in their own time . . . as oen as they want to further critique their reective writing, learn from the tags and change  their statements prior to submission, and has the potential to familiarise us who are new to the area of reection and reective writing skill development.  More formal responses were collected via the AWA UI. e feed- back tab in AWA poses the question: Did you nd the feedback on your writing helpful Students using the soware at this early stage have generally been positive about its helpfulness with 85.7% of feedback responses being yes - the feedback on their writing was helpful (see table 1 - Note: some students who used AWA multiple times provided multiple responses). Of those who gave more detailed feedback ratings (see gure 6), about half provided a neutral rating (some of which could be explained by the fact that this was the default in the UI). Of those that made a positive or negative choice, the negatives were all just less than neutral (3), whereas the positives were an even combination of just positive (5) and strongly positive (7).  Figure 6: Pharmacy feedback ratings of AWA. 1 is not help- ful and 7 is helpful. 4 is the default.  While we acknowledge that perceptions of helpfulness dont necessarily translate into action, we believe that feedback perceived as not helpful would be highly unlikely to result in any action. Whereas, positive and neutral feedback leaves the way open for action to be taken. erefore, we take these results as an early promising sign.  5.3 Student comments Our cautious optimism was also supported by student comments that suggested that it encouraged them to think about action:  I was fascinated by how it works and can see its implication in future, to determine which phrases need more work/ which can be improved. (Stu- dent A) It details where Ive made reective statements and shows where I can improve as well as add to and ll in aspects to which I have not conrmed. (Student B) Prompted me to follow through with the reection to the last step of the process - i had wrien about    LAK 17, March 13-17, 2017, Vancouver, BC, Canada A. Gibson et. al.  my thoughts and feelings, discussed challenges, but had not followed through with reecting on how this can lead to change. . . .e reports also direct me to write more personally, using lan- guage that evokes emotion, and less descriptively (Student C)  A comment by one student indicated that they saw the potential for AWA to assist them in improving their grade, and even suggested that it should provide a grade:  is system has allowed me to identify the strengths and weaknesses of my reection, highlighting on what criteria I have addressed and which ones I havent. I wish there was feedback on how I could improve to get full marks and wish this reection gave a mark at the end. (Student D)  No Pharmacy students le clearly negative comments. However, some early use of the soware by students from other subjects highlighted a theme in their more negative comments: criticism either at what the soware did not do, or towards a lack of clarity in what needed to change. For example:  Doesnt elaborate on the features that are lacking and oen they appear there but are not recog- nised. Good way of highlighting other points. (Student E)  its not clear what needs improving. (Student F) and comments are not clear enough (Student G)  I dont understand what AWA reproach to my work. It is said that there isnt a good balance but In the text I cant see how not clear (Student H)  is lack of clarity for the student goes to the heart of the ac- tionability of the feedback, suggesting that the students want to use the feedback to take action, but that it is not clear enough for them to do so. However, it is not clear if more data might reveal this as a signicant issue or whether this was more indicative of the immaturity of the platform at the time.  5.4 Evidence of Action Of the 30 users who posted text to AWA, 18 users (60%) posted more than once. We were interested in whether there was evidence of action from feedback in the writing of these users. We postulated that if students were provoked to take action based on the feedback, then they would post a new dra to AWA to check if it addressed the feedback.  From those posting more than once, 5 users (27.8%) showed evidence of modifying dras, and 2 of these users draed more than once on the same text. We view this as a positive indicator that our objective of actionable feedback can be aained. While most of the users (13 users, 72.2%) wrote dierent reections for their multiple posts, this was not unexpected as the subject requires them to write a new reection each week. What is dicult to assess with these users is whether feedback on one reection was taken into account with subsequent reections.  With the exception of addition of new information, most dra modications appeared to improve the quality of the reection. For example:  I made sure to be understanding and not force the customer to purchase just for the sake of receiving a sale (ethics, social responsibility).  was changed to: Initially I was confused as to why this would be an issue like isnt it exactly the same thing But for good pharmacy practice, I decided to be understanding and not force the customer to pur- chase a product just for the sake of receiving a sale (ethics, social responsibility).  e rst of these had no sentence tagging, whereas the second was tagged with a pink circle (Challenge). Students also changed their writing to introduce how they felt about a situation. For example:  is situation didnt sit well with me. was changed to:  is situation didnt sit well with me, I felt as it these patients didnt receive the best care possible.  6 DISCUSSION 6.1 Actionable Feedback Our preliminary results from the use by Pharmacy students sug- gested that the feedback provided by AWA was helpful and ac- tionable. However, we had also received some input from other subjects that suggested that the feedback lacked clarity. Although this is likely to be resolved through the collection of more data from multiple subjects, it raises questions as to whether it is the AWA platform that makes the feedback actionable, or whether there are other factors outside of the system that contribute to this. For exam- ple, the inclusion of teaching on reection, and/or the requirement to write regular weekly reections may assist the students in recog- nising the value of the feedback, and help them understand how to use it. A subject that has lile teaching input and only requires a single piece of reective writing may provide the students with less understanding on how to work with the soware. Regardless of how a subject approaches reective writing, we believe that align- ment between the analytics and pedagogy is critical. e extent to which we can determine if this alignment contributes signicantly to actionable feedback is yet to be found.  6.2 Contextual Feedback rough the discussions with academic partners and the trial of AWA with students, we saw an interesting tension emerge between general and specic feedback. Early in the project, many stake- holders suggested that feedback should be as detailed and specic as possible. While this notion aligns with the educational litera- ture on feedback [16], human feedback is always contextualised to some extent. We found that the lack of reference to context resulted in the rejection of detailed feedback. For example, when the same paragraph feedback was provided for many paragraphs in the text, stakeholders were less likely to appreciate the relevance of the feedback and more likely to criticise the repetitive nature    Reflective Writing Analytics for Actionable Feedback LAK 17, March 13-17, 2017, Vancouver, BC, Canada  of it. A contextual approach would mean that comments for one paragraph would be made in the context of what is stated with other paragraphs.  Similarly, some paragraph feedback is more important in certain parts of the text than in others. For example, seing the context for the reection can tend to be more descriptive and therefore emotive expressions are less important in these sections. An improved system of generating feedback would have an awareness of these contextual interrelationships, and modify the feedback accordingly.  6.3 Modelling Disciplinary Dierences e current AWA platform is missing another contextual feature, and that is the ability to recognise and work with language and reection style dierences across disciplines. While we had hoped to include some modelling of disciplinary specic features in the platform, that has not been implemented to date. e lack of this ability resulted in some signicant disagreement between project stakeholders with regards to structure and relating to knowledge.  In particular, the Engineering subject required reective writing that was highly structured, and a business subject required the use of citations to show how the students were relating their reection to the elds of existing knowledge. In both of these cases, the re- quirements were considered essential for actionable feedback in the subjects concerned, but feedback on these features were problem- atic for other subjects. While providing discipline specic versions of AWA might be a short term solution to this issue, in the long term it is intractable as there would need to be as many versions as there are subject. Instead, we believe that the ability to model dierences within the subject, and allow AWA to make decisions according to combinations of models, might allow a future version of AWA to accommodate these needs. What we are certain of, is that these pedagogical requirements are important for learning, and that allowing them to drive the development of the tool holds much greater value for learning than changing the learning to t the tool.  6.4 Algorithmic Accountability and Integrity e transparency and accountability of algorithms are impor- tant qualities if we are to achieve an acceptable level of analytical system integrity [6] in our educational tools. In the context of this project, this required us to understand the relationship between the computational representation of a text, and the human experience of giving and receiving feedback on writing. e computational representation concerns the ne-grained textual features, and the reasoning behind the algorithms for combining, and acting on, dif- ferent paerns (3). e default educator and student experience deploys a much simpler language, provided by the set of constructs in the document-centric display in which feedback wrapped around, and overlaid onto, the students writing (4). Our assumption is that educators should demand a level of accountability that is less rigorous than a researcher might demand, but more rigorous than a student may demand. at being said, we should also expect and encourage students to demand accountability to whatever level of detail they require, and technically minded students might indeed be more demanding than their tutors as their data literacy grows, and they are encouraged to reect critically how their activity is  tracked in all spheres of their lives [7]. In the previous design it- eration of this project [8] we noted that a key ingredient of any discussion of accountability among design stakeholders is trust: Trust is built through reciprocity, which in learning analytics de- sign means ultimately, that you feel you can inuence the code. We have continued that co-design process in which stakeholders (the UTS academics and Academic Literacies expert, and the Xerox com- putational linguist) shaped the performance of AWA until they felt that it was good enough to pilot with students as an experimental application. e fact that the design team involves academics who trust the tool, because they had the chance to test and give feedback on its performance with their own students writing, should be a source of assurance for students (and indeed other academics con- sidering trialling AWA). Ultimately, however, it comes down to the student experience of the tool, since many emergent phenomena may arise in authentic use. e preliminary feedback from students reported here, coupled with results from another deployment [18] provide encouraging early evidence that students valued AWA, but there remains much more to improve.  7 CONCLUSIONS AND FUTUREWORK rough the A3R project we have created a new conceptual frame- work for reective writing that synthesises the dominant theory. We have used a simplied version of this framework to develop computational approaches to reective writing analysis and in do- ing so have created RWA that can be used for feedback to the writer. A signicant aspect of the research was allowing the pedagogy to drive the design, not just of the framework, but of the platform architecture as well. A pilot of the soware in the subject of phar- macy has shown this pedagogical design approach to be successful, with a early signs of the feedback being actionable by students.  Strengths of the project include the strong theoretical founda- tions of the work, and the maturity of the genre/narrative analysis approach and its instantiation in XIP. We believe that the research has also shown signicant promise and the approach to incorpo- rating multiple analytics for the generation of feedback as imple- mented in TAP. e centrality of actionable feedback, throughout the process has also been a signicant aspect of the work.  However, we also uncovered a number of weaknesses. We found it dicult to meet the needs of all stakeholders, and the lack of data from all participating subjects means that at this point we are unable to ascertain the extent to which this has impacted the students. Nevertheless, early feedback from students suggested that some feedback lacked clarity, and our own analysis showed that in order to improve the quality of the feedback, we need methods for using contextual information.  ese weaknesses set an agenda for future work. Firstly, we plan to develop over coming months a subject specic version of the platform for Engineering. We anticipate that this will test some of our hypotheses about the need for more specic feedback and the use of contextual information in the generation of that feedback. Secondly, we need to collect much more data from a greater variety of disciplines. is would allow us to substantiate our preliminary ndings. Finally, while we have shown the potential of designing LA for actionable feedback, there is much more work to be done in    LAK 17, March 13-17, 2017, Vancouver, BC, Canada A. Gibson et. al.  this space. We expect that ongoing research in RWA will improve our ability to provide actionable feedback to students.  ACKNOWLEDGMENTS We acknowledge the input of Natalia Nikolova, Walter Jarvis, Alan Parr, Andy Leigh, Peter Jones, Jo McKenzie, Rosalie Goldsmith, Susan Hoadley, Isabelle Benne, Sarab Mansoor, Keenan Wilson, and Je Browi. We also acknowledge Xiaolong (Shawn) Wang for the UI soware development, and Xerox Research Centre Europe (XRCE) for the Xerox Incremental Parser (XIP).  REFERENCES [1] S. At-Mokhtar, J.-P. . Chanod, and C. Roux. 2002. Robustness beyond shallowness:  incremental deep parsing. Natural Language Engineering 8, 2-3 (6 2002). DOI: hp://dx.doi.org/10.1017/s1351324902002887  [2] Rosanne Birney. 2012. Reective Writing: antitative Assessment and Identica- tion of Linguistic Features. Ph.D. Dissertation. Waterford Institute of Technology.  [3] David Boud and Nancy Falchikov. 2006. Aligning assessment with long-term learning. Assessment & Evaluation in Higher Education 31, 4 (8 2006), 399413. DOI:hp://dx.doi.org/10.1080/02602930600679050  [4] David Boud, Rosemary Keogh, and David Walker. 1985. What is Reection in Learning Routledge.  [5] Caroline Brun. 2011. Detecting Opinions Using Deep Syntactic Analysis.. In RANLP. 392398.  [6] Simon Buckingham Shum. 2016. Algorithmic Accountability for Learning Ana- lytics.. In Invited Talk, University College London, Institute of Education & London Knowledge Lab. hp://bit.ly/aala2016. hp://bit.ly/aala2016  [7] Simon Buckingham Shum. 2016. Envisioning Learning Analytics for 21st Century Competencies.. In Keynote Address, LASI-Asia 2016: Learning Analytics Summer Institute, Seoul. hp://lasi-asia.org  [8] Simon Buckingham Shum, Agnes Sandor, Rosalie Goldsmith, Xiaolong Wang, Randall Bass, and Mindy McWilliams. 2016. Reecting on reective writing analytics: Assessment challenges and iterative evaluation of a prototype tool. In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge. ACM, 213222.  [9] A. Caetano. 2014. Dening personal reexivity: A critical reading of Archers approach. European Journal of Social eory 18, 1 (9 2014), 6075. DOI:hp: //dx.doi.org/10.1177/1368431014549684  [10] Rudi Dallos and Jacqui Stedmon. 2009. 1 Flying over the swampy lowlands: Re- ective and reexive Practice. Reective practice in psychotherapy and counselling (2009), 122.  [11] John Dewey. 1916. Democracy and education. Courier Corporation. [12] Andrew Dillon. 2002. Information architecture in JASIST: Just where did we  come from Journal of the American society for information science and technology (2002).  [13] John Flowerdew. 2015. John Swaless approach to pedagogy in Genre Analysis: A perspective from 25 years on. Journal of English for Academic Purposes 19 (2015), 102112.  [14] Andrew Gibson, Kirsty Kio, and Peter Bruza. 2016. Towards the Discovery of Learner Metacognition From Reective Writing. Journal of Learning Analytics 3, 2 (2016), 2236.  [15] James E. Grin Jr, Gregory F. Lorenz, and David Mitchell. 2010. A study of outcomes-oriented student reection during internship: e integrated, coordi- nated, and reection based model of learning and experiential education. (2010).  [16] J. Haie and H. Timperley. 2007. e Power of Feedback. Review of Educational Research 77, 1 (3 2007), 81112. DOI:hp://dx.doi.org/10.3102/003465430298487  [17] Neville Haon and David Smith. 1995. Reection in teacher education: Towards denition and implementation. Teaching and teacher education 11, 1 (1995), 3349.  [18] Simon Knight, Simon Buckingham Shum, Phillipa Ryan, Agnes Sandor, and Xi- aolong Wang. in press. Academic Writing Analytics for Civil Law: Participatory Design rough Academic and Student Engagement. International Journal of Articial Intelligence in Education (in press).  [19] Jasmine Luk. 2008. Assessing teaching practicum reections: Distinguishing discourse features of the high and low grade reports. System 36, 4 (12 2008), 624641. DOI:hp://dx.doi.org/10.1016/j.system.2008.04.001  [20] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014. e Stanford CoreNLP Natural Language Processing Toolkit.. In ACL (System Demonstrations). 5560.  [21] James Robert Martin and David Rose. 2003. Working with discourse: Meaning beyond the clause. Bloomsbury Publishing.  [22] Jack Mezirow. 1990. How critical reection triggers transformative learning. Fostering critical reection in adulthood 1 (1990), 20.  [23] Jennifer A. Moon. 2013. Reection in learning and professional development: eory and practice. Routledge.  [24] Chad Morrison, Jill Willis, Leanne Crosswell, and Andrew Gibson. 2014. Turning points in narratives of research design: Research innovation stimulating unique responses to existing challenges for beginning rural teachers. e Journal of Educational Enquiry 13, 1 (2014).  [25] David Nicol. 2009. Assessment for learner self-regulation: enhancing achieve- ment in the rst year using learning technologies. Assessment & Evaluation in Higher Education 34, 3 (6 2009), 335352. DOI:hp://dx.doi.org/10.1080/ 02602930802255139  [26] Joseph M. Paxton, Leo Ungar, and Joshua D. Greene. 2012. Reection and reasoning in moral judgment. Cogn Sci 36, 1 (2012), 16377. DOI:hp://dx.doi. org/10.1111/j.1551-6709.2011.01210.x  [27] Carl Reidsema, Rosalie Goldsmith, and Pam Mort. 2010. Enabling the reective practitioner in engineering design courses. In 2nd International Conference on Design Education (ConnectED2010). e University of New South Wales.  [28] Mary Ryan. 1988. e teachable moment: e Washington center internship program. New Directions for Teaching and Learning 1988, 35 (1988), 3947.  [29] Mary Ryan. 2014. Reexive writers: Re-thinking writing development and assessment in schools. Assessing Writing 22 (10 2014), 6074. DOI:hp://dx.doi. org/10.1016/j.asw.2014.08.002  [30] Mary Ryan and Michael Ryan. 2013. eorising a model for teaching and as- sessing reective learning in higher education. Higher Education Research & Development 32, 2 (2013), 244257.  [31] D. Royce Sadler. 1989. Formative assessment and the design of instructional systems. Instructional science 18, 2 (1989), 119144.  [32] John Sandars. 2009. e use of reection in medical education: AMEE Guide No. 44. Medical Teacher 31, 8 (1 2009), 685695. DOI:hp://dx.doi.org/10.1080/ 01421590903050374  [33] Agnes Sandor. 2007. Modeling metadiscourse conveying the authors rhetorical strategy in biomedical research abstracts. Revue francaise de linguistique appliquee 12, 2 (2007), 97108.  [34] A. Schon Donald. 1983. e reective practitioner: How professionals think in action. (1983).  [35] Duygu Simsek, Simon Buckingham Shum, Agnes Sandor, Anna De Liddo, and Re- becca Ferguson. 2013. XIP Dashboard: visual analytics from automated rhetorical parsing of scientic metadiscourse. (2013).  [36] Peter AC Smith. 2001. Action learning and reective practice in project environ- ments that are related to leadership development. Management Learning 32, 1 (2001), 3148.  [37] Frederick Steier. 1991. Reexivity and Methodology: An Ecological Construc- tionism. In Research and reexivity. London, 257.  [38] John Swales. 1990. Genre analysis: English in academic and research seings. Cambridge University Press.  [39] Cherie Tsingos, Sinthia Bosnic-Anticevich, John M. Lonie, and Lorraine Smith. 2015. A Model for Assessing Reective Practices in Pharmacy Education. Am J Pharm Educ 79, 8 (10 2015), 124. DOI:hp://dx.doi.org/10.5688/ajpe798124  [40] Cherie Tsingos-Lucas, Sinthia Bosnic-Anticevich, and Lorraine Smith. 2016. A Retrospective Study on Students and Teachers Perceptions of the Reective Ability Clinical Assessment. Am J Pharm Educ 80, 6 (8 2016), 101. DOI:hp: //dx.doi.org/10.5688/ajpe806101  [41] Linda Valli. 1997. Listening to other voices: A description of teacher reection in the United States. Peabody Journal of Education 72, 1 (1 1997), 6788. DOI: hp://dx.doi.org/10.1207/s15327930pje7201 4  [42] Max van Manen. 1995. On the Epistemology of Reective Practice. Teachers and Teaching 1, 1 (3 1995), 3350. DOI:hp://dx.doi.org/10.1080/1354060950010104  [43] Amy Beth Warriner, Victor Kuperman, and Marc Brysbaert. 2013. Norms of valence, arousal, and dominance for 13,915 English lemmas. Behavior research methods 45, 4 (2013), 11911207.  [44] Frances KY Wong, David Kember, Lorea YF Chung, and Louisa Yan CertEd. 1995. Assessing the level of student reection from reective journals. Journal of advanced nursing 22, 1 (1995), 4857.  http://dx.doi.org/10.1017/s1351324902002887 http://dx.doi.org/10.1080/02602930600679050 http://bit.ly/aala2016 http://lasi-asia.org http://dx.doi.org/10.1177/1368431014549684 http://dx.doi.org/10.1177/1368431014549684 http://dx.doi.org/10.3102/003465430298487 http://dx.doi.org/10.1016/j.system.2008.04.001 http://dx.doi.org/10.1080/02602930802255139 http://dx.doi.org/10.1080/02602930802255139 http://dx.doi.org/10.1111/j.1551-6709.2011.01210.x http://dx.doi.org/10.1111/j.1551-6709.2011.01210.x http://dx.doi.org/10.1016/j.asw.2014.08.002 http://dx.doi.org/10.1016/j.asw.2014.08.002 http://dx.doi.org/10.1080/01421590903050374 http://dx.doi.org/10.1080/01421590903050374 http://dx.doi.org/10.5688/ajpe798124 http://dx.doi.org/10.5688/ajpe806101 http://dx.doi.org/10.5688/ajpe806101 http://dx.doi.org/10.1207/s15327930pje7201_4 http://dx.doi.org/10.1080/1354060950010104   Abstract  1 Introduction  1.1 The Project  1.2 Participation  1.3 Approach   2 Theoretical Framework  2.1 Reflective Writing  2.2 Student Writing Analysis  2.3 Simplification   3 Reflective Rhetorical Moves  3.1 The concept-matching analysis framework  3.2 Implementation in XIP   4 Actionable Feedback  4.1 Platform Architecture  4.2 User Experience   5 Student Use  5.1 The Pharmacy Learning Context  5.2 Preliminary results  5.3 Student comments  5.4 Evidence of Action   6 Discussion  6.1 Actionable Feedback  6.2 Contextual Feedback  6.3 Modelling Disciplinary Differences  6.4 Algorithmic Accountability and Integrity   7 Conclusions and Future work  Acknowledgments  References   "}
{"index":{"_id":"22"}}
{"datatype":"inproceedings","key":"Nguyen:2017:UDI:3027385.3027409","author":"Nguyen, Quan and Rienties, Bart and Toetenel, Lisette","title":"Unravelling the Dynamics of Instructional Practice: A Longitudinal Study on Learning Design and VLE Activities","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"168--177","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027409","doi":"10.1145/3027385.3027409","acmid":"3027409","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, learning design, longitudinal, panel data analysis, social network analysis","Abstract":"Substantial progress has been made in understanding how teachers design for learning. However, there remains a paucity of evidence of the actual students' response towards leaning designs. Learning analytics has the power to provide just-in-time support, especially when predictive analytics is married with the way teachers have designed their course, or so-called a learning design. This study investigates how learning designs are configured over time and their impact on student activities by analyzing longitudinal data of 38 modules with a total of 43,099 registered students over 30 weeks at the Open University UK, using social network analysis and panel data analysis. Our analysis unpacked dynamic configurations of learning designs between modules over time, which allows teachers to reflect on their practice in order to anticipate problems and make informed interventions. Furthermore, by controlling for the heterogeneity between modules, our results indicated that learning designs were able to explain up to 60% of the variability in student online activities, which reinforced the importance of pedagogical context in learning analytics.","pdf":"Unravelling the dynamics of instructional practice:    A longitudinal study on learning design and VLE activities     Quan Nguyen    Open University the UK  Institute of Educational   Technology   +44 7732599001   quan.nguyen@open.ac.uk      Bart Rienties   Open University the UK  Institute of Educational   Technology   +44 1908332671   bart.rienties@open.ac.uk      Lisette Toetenel   Open University the UK  Institute of Educational   Technology   +44 19083322809   lisette.toetenel@gmail.com      ABSTRACT  Substantial progress has been made in understanding how teachers   design for learning. However, there remains a paucity of evidence   of the actual students response towards leaning designs. Learning   analytics has the power to provide just-in-time support, especially   when predictive analytics is married with the way teachers have   designed their course, or so-called a learning design. This study   investigates how learning designs are configured over time and   their impact on student activities by analyzing longitudinal data of   38 modules with a total of 43,099 registered students over 30 weeks   at the Open University UK, using social network analysis and panel   data analysis. Our analysis unpacked dynamic configurations of   learning designs between modules over time, which allows teachers   to reflect on their practice in order to anticipate problems and make   informed interventions. Furthermore, by controlling for the   heterogeneity between modules, our results indicated that learning   designs were able to explain up to 60% of the variability in student   online activities, which reinforced the importance of pedagogical   context in learning analytics.    CSS Concepts   Applied computing  E-learning; Distance learning      Keywords       Learning analytics, learning design, social network analysis,   longitudinal, panel data analysis                   1. INTRODUCTION   In the last decade, there is a growing body of literature [11, 15, 32]   that seeks to develop a descriptive framework to capture teaching,   and  learning activities so that teaching ideas can be shared and   reused from one educator to another, so called Learning Design   (LD) [16]. A common metaphor of a learning design was a music   notation which contains enough information to convey musical   ideas from one to another over time and space [16]. Extensive   research has been conducted focusing on technological   implementations of LD such as the Educational Modelling   Language (EML) [29], the SoURCE project [31], the Australian   Universities Teaching Council (AUTC) LD project [1], and the   Learning Activity Management System (LAMS) [14]. While the   early work in LD have focused on transferring the design for   learning from implicit to explicit, the relationship between LD and   the actual learners response has been not fully understood. As the   majority of feedback takes forms of assessments, and courses   evaluations, which typically takes place after the learning process   has finished, it prevents teachers from making in-time   interventions. Recently, the advancement in technology has   allowed us to capture the digital footprints of learning activities   from Virtual Learning Environment (VLE). This rich and fine-  grained data about the actual learners behaviors offer educators   potentially valuable insights on how students react to different LDs.    Learning analytics (LA) has the potential to empower teachers and   students by identifying patterns and trends from a wide variety of   learners data. Within the LAK community, substantial progress   has been made both in conceptual development [10, 17] as well as   how to design appropriate predictive learning analytics to support   students [19, 26]. Nonetheless, in line with [26, 44] findings from   LA research in the past have been rather limited to delivering   actionable feedback, while ignoring the context of which the   learning data is situated. Thus, within the LAK community there is   an increasing interest to align LA with LD, as the former facilitates   the transfer of tacit educational practice to an explicit rendition,   while the latter provides educators with pedagogical context for   interpreting and translating LA findings to direct interventions [3,   33, 34, 37, 40]. While there are abundant discussions on the value   and impact of integrating LD into LA to improve teacher inquiry   [3, 37], only a few studies have empirically examined how teachers   actually design their courses [4, 20] and whether LD influences   satisfaction, VLE behavior, and retention [42, 44, 45, 48].   However, most previous studies are limited to interviews and   experimental settings, while others have only explored LD from a   static perspective, without accounting for the differences within   Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for   components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to   post on servers or to redistribute to lists, requires prior specific permission   and/or a fee. Request permissions from Permissions@acm.org.   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027409     and between modules and the possible interaction between different   types of learning activities over time.    This study builds on previous work by Rienties and Toetenel [45],   [48] by dynamically investigating the use of learning design in 38   modules over 30 weeks at one of the largest distance higher   education institutions in Europe using social network analysis and   panel data analysis. Our work contributes to the existing LA   literature by capturing: (1) how teachers configure their course over   time, (2) how learning activities interact with each other across   modules, and (3) how learning designs help to explain VLE   behavior over time.    2. ALIGNING LA WITH LD  In the last five years, LA has attracted a lot of attention from   practitioners, management, and researchers in education by   shedding light on a massive amount of (potentially) valuable data   in education, as well as providing means to explicitly test existing   pedagogical theories. Scholars in the field of LA have exploited   various sources of data, such as activity logs of students [35],   learning dispositions [7, 39], or discussion forum [2, 51]. By taking   advantage of advanced analytical techniques such as predictive   modeling [46], discourse analytics [49], machine learning [30], and   so on, LA has succeeded in uncovering meaningful patterns and   trends occurred during the learning process. While these studies   provide important markers on the potential of LA in education,   critics have indicated a gap between pedagogy and LA [18, 28, 50].   Interesting patterns can be identified from student activities, such   as number of clicks, discussion posts, or essays. However, these   patterns alone are not sufficient to offer feedback that teachers can   put into actions [19, 43]. Without a pedagogically sound approach   to data, LA researchers may struggle with deciding which variables   to attend to, how to generalize the results to other contexts, and how   to translate their findings to actions [28]. Hence, LD can equip   researchers with a narrative behind their numbers, and convert   trends of data into meaningful understandings and opportunities to   make sensible interventions.    The core concepts of LD are best summarized in the Learning   Design Conceptual Map (LD-CM) (Figure 1). It starts with the   main objective of creating learning experiences aligned to   particular pedagogical approaches and learning objectives. How   educators make decision about designing for learning is determined   by Characteristics & Values of the learning environment, the   educational philosophy, and theories and methodologies. In a   interview based study of 30 participants, Bennett, Agostinho and   Lockyer [4] identified three main factors that influenced how   teachers engage in the designing process: student-related factors   (cohort profile, learning objectives, feedback from past sessions),   teachers-related factors (beliefs about teaching, prior experiences),   and context-related factors (colleagues, institutional policies and   culture, resources such as workload, time, and infrastructure).    In the teaching cycle, the reflection phase is limited to insights   generated from assessments, course evaluations, and self-reports.   These channels may suffer from selection bias, response bias, and   hinder educators to make in-time interventions. A potential   contribution of LA in LD is to include real-time learner response to   a LD, such as how much time was spent on a particular activity, or   how often a student visits a concept/topic. These behavioral traces   allow educators to both make personalized interventions to each   student as well as adjust the course according to the overall trends   of a group of students. As illustrated below, LA allows educators   to reflect and compare their practice in a wide range of granularity:   from learning activities to modules, and disciplines. Overall, using   LA in combination with other feedback channels, such as   assessment and evaluation, could empower and speed up the   teaching cycle by generating more feedback, allow educators to   make in-time interventions, to reflect, and to compare their practice   on multiple levels of granularity.    Figure 1: A Learning Design Conceptual Map. Retrieved from   Dalziel, Conole, Wills, Walker, Bennett, Dobozy, Cameron,   Badilescu-Buga and Bower [16].   2.1 Connecting LD and LA  Since the beginning of the 21st century, the term learning design has   emerged as a methodology for enabling teachers/designers to   make more informed decisions in how they go about designing   learning activities and interventions, which is pedagogically   informed and makes effective use of appropriate resources and   technologies [11]. For more discussion on the origins of learning   design and instructional design, we refer readers to Persico and   Pozzi [40]. Several approaches for designing learning have been   proposed, yet, one common stage in almost every approach was the   evaluation of the LD [36, 40]. Persico and Pozzi [40] argued that   the learning process should not only depend on experience, or best   practice of colleagues but also pre-existing aggregated data on   students engagement, progression, and achievement. In a similar    manner, Mor, Ferguson and Wasson [37] suggested that LA could   facilitate teacher inquiry by transforming knowledge from tacit to   explicit, and perceive students and teachers as participants of a   reflective practice. For instance, in a study of 148 learning designs   by Toetenel and Rienties [47], the introduction of a systematic LD   initiative consisting of visualization of initial LDs and workshops   helped educators to focus on the development of a range of skills   and more balanced LDs. Feeding information on how students are   engaged in a certain LD during or post-implementation can provide   a more holistic perspective of the impact of learning activities [34].    Several conceptual frameworks aiming at connecting LA with LD   have been proposed. For example, Persico and Pozzi [40] discussed   three dimensions of LD that can be informed by LA:   representations, tools, and approaches. Lockyer, Heathcote and   Dawson [34] introduced two categories of analytics applications:   checkpoint analytics to determine whether students have met the   prerequisites for learning by assessing relevant learning resources,   and process analytics to capture how learners are carrying out their   tasks. In the recent LAK conference 2016, Bakharia, Corrin, de   Barba, Kennedy, Gaevi, Mulder, Williams, Dawson and Lockyer   [3]  proposed four types of analytics (temporal, tool specific,   cohort, and comparative), and contingency and intervention   support tools with the teacher playing a central role.      In this paper, we will use the conceptual framework developed by   Conole [11] and further developed by Rienties & Toetenel (REF).   Both conceptual and empirical research has found that the Open   University Learning Design Initiative (OULDI) can accurately and   reliably determine how teachers design courses, and how students   are subsequently using these LDs [45, 48]. Seven types of learning   activities can be found in Table 1. Assimilative activities refer to   tasks which require learners attention to information. These   include watching lecture video, reading the text, listening to an   audio file, etc. Finding and handling information activities implies,   for example, searching and filtering for relevant literature in a   particular topic on the internet. Communication activities refer to a   range of practices to communicate such as posting in a discussion   forum and replying to peer comments. Productive activities   represent the construction of an artifact, such as writing a summary   or resolving a problem. Experiential activities provide learners with   opportunities to apply theories in a real-world setting such as case   study, or field trip. Interactive/adaptive activities encourage   learners to apply what they learned in an experietial environment,   or interactng with a simulation. Finally, assessment activities   evaluate the learners understanding such as writing through the   construction of an essay, exam or making a presentation.    Table 1: Learning design taxonomy    Type of activity Example   Assimilative Attending to   information   Read, Watch, Listen,   Think about, Access.   Finding and   handling   information   Searching for and   processing information   List, Analyse, Collate,   Plot, Find, Discover,   Access, Use, Gather.    Communicat  ion   Discussing module   related content with at   least one other person   (student or tutor)   Communicate, Debate,   Discuss, Argue, Share,   Report, Collaborate,   Present, Describe.   Productive Actively constructing   an artefact   Create, Build, Make,   Design, Construct,   Contribute, Complete,.    Experiential Applying learning in a   real-world setting    Practice, Apply, Mimic,   Experience, Explore,   Investigate,.   Interactive   /adaptive   Applying learning in a   simulated setting    Explore, Experiment,   Trial, Improve, Model,   Simulate.    Assessment All forms of   assessment   (summarive, formative   and self assessment)    Write, Present, Report,   Demonstrate, Critique.   Source: Retrieved from Rienties and Toetenel [45]   While there were numerous discussions in aligning LA with LD,   the amount of empirical studies on the subject has been rather   limited. For example, Gaevi, Dawson, Rogers and Gasevic [19]   examined the extent to which instructional conditions influence the   prediction of academic success in nine undergraduate courses   offered in a blended learning model. The results suggested that it is   imperative for LA to taking into account instructional conditions   across disciplines and course to avoid over-estimation or   underestimation of the effect of LMS behavior on academic   success. From our observation, most of the empirical studies   attempting to connect LA and LD are derived from students   activities [34], or differences in discipline [19],  rather than how   teachers actually design their course [24].    In our previous work, we have highlighted explicitly the role of LD   in explaining LMS behavior, student satisfaction, retention, and   differences in prediction of academic success [19, 42, 44, 45, 48].    For example, in our first study linking 40 LDs with VLE behavior   and retention, we found that strongly assimilative designs (i.e., lots   of passive reading and watching of materials) were negatively   correlated with retention [42]. In a large-scale follow-up study   using a larger sample of 151 modules and multiple regression   analyses of 111,256 students at the Open University, UK, Rienties   and Toetenel [45] revealed relations between LD activities and   VLE behavior, student satisfaction, and retention. The findings   showed that taking the context of LD into account could increase   the predictive power by 10-20%. Furthermore, from a practitioners   perspective, the combination of a collaborative, networked   approach at the initial design stage, augmented with visualizations,   changed the way educators design their courses [47]. While these   three studies at the Open University UK (OU) highlighted the   potential affordances of marrying LD with LA on a large scale, two   obvious limitations of these studies were the aggregation of   learning activities in predicting behavior and performance (i.e.,   rather than their interaction), as well as the static rather than   longitudinal perspective of LD. In these studies [42, 44], aggregate   learning design data across the 40 weeks of each module were used,   while in many instances teachers use different combinations of   learning activities throughout the module [24]. While fine-grained   longitudinal data of LD per week were not available during the   initial implementation phase of LD at the OU, in the last year fine-  grained weekly LD data has been added, which would allow us to   potentially identify the optimum mix of LD activities per discipline,   level, and type of students per week and over time.   2.2 Research Questions  Building on previous conceptual and empirical research, we are   particularly interested in how teachers design their learning   activities over time since learning is a dynamic and time-variant   process. Hence, our first research question is:   1) How are learning designs configured across modules  over time   Prior studies of Social Network Analysis (SNA) in e-learning,   particularly in the improvement of LD have concentrated on   examining patterns of learner communication and collaboration in   various situations, such as when discussing, blogging and e-mailing   [8]. Within the last three years in LA, SNA has been shown to be   an effective tool to explore the relationships of learners in online   discussion forum [9, 23, 25, 27, 41], or in eye tracking movements   [52]. However, none has looked at the LD from a social network   perspective on a large scale study. Hora and Ferrare [24] suggested   that teaching practice should be best viewed as situated in and   distributed among features of particular settings. According to the   systems-of-practice theory by Halverson [21], local practices are   informed, constrained, and constituted by the dynamic interplay of   artifact and tasks. Thus, in order to understand how teachers design   their course, it is necessary to consider the inter-relationships   among different learning activities. Thus, our next research   question aims at examining:    2) How do different learning activities interact with each  other across modules   Finally, our previous work has indicated that learning designs are   strong predictors of VLE behaviors [45, 48]. However, we did not   take into account the differences between modules which might   affect the robustness of the analysis [19]. Hence using panel data   analysis, we investigate:      3) How do learning designs affect VLE behavior over time   3. METHOD   3.1 Setting  This study took place at the OU, which is the largest distance   education provider in Europe. Data in this study was generated   from OULDI, which helps teams in defining their pedagogic   approach, choosing and integrating an effective range of media and   technologies, and enable sharing of good practice across the   university [13]. When using data to compare module design across   disciplines and modules, according to our previous work [45, 48] it   is important to classify learning activities in an objective and   consistent manner. In particular, each module goes through a   mapping process by a module team which consists of a LD   specialist, a LD manager, and faculty members. This process   typically takes between 1 and 3 days for a single module, depending   on the number of credits, structure, and quantity of learning   resources. First, the learning outcomes specified by the module   team were captured by a LD specialist. Each learning activity   within the modules weeks, topics, or blocks was categorized under   the LD taxonomy and stored in an activity planner  a planning   and design tool supporting the development, analysis, and sharing   of learning designs. Next, the LD team manager reviews the   resulting module map before the findings are forwarded to the   faculty. This provides academics with an opportunity to comment   on the data before the status of the design was finalized. To sum up,   the mapping process is reviewed by at least three people to ensure   the reliability and robustness of the data relating to a learning   design.    In this study, of 56 modules were selected with all contained LD   data that have been documented on a weekly basis for the academic   years 2014 and 2015, we ended up with 42 modules after excluding   14 modules that were short, intensive training modules. The final   selection of modules were equally distributed across a range of   disciplines with 21% in Art & Social Sciences, 21% in Business &   Law, 12% in Education, Languages, and Health studies, 22% in   Science and Technology, and 24% in other disciplines. Over 90%   of the modules were undergraduate courses. There were 20   modules with 60 credits, 19 modules with 30 credits and 3 modules   with missing information.    In preparation for the panel data analysis, we linked 42 modules   with weekly LD data in 2014 and 2015 with weekly VLE data,   whereby 38 modules were successfully merged. The average   number of students registered in each module was 1134 with the   minimum of 75 and the maximum of 3707.  On average, 91% of   the students who followed the course until the end passed (SD =   0.058) while 63.4% of all the registered students passed the course   (SD=0.086). The retention rate of all the modules was 69% on   average, with a range from 56% to 85%.       3.2 Instruments   3.2.1 Measurement of learning designs  Seven LD variables were measured in terms of workload, which is   the number of hours that students are expected to study. Time spent   on learning activities was restricted based on the size of the module,   such as 30 credits equated to 300 hours of learning, and 60 credits   equated to 600 hours of learning. Of the 38 modules, students were   expected to study on average 8.10 hours per week, of which 3.92   hours were spent on assimilative activities, 0.26 hours on finding   information, 0.29 hours on communication, 1.32 hours on   productive activities, 0.14 hours on experiential activities, 0.17   hours on interactive activities, and 1.99 hours on assessment.    3.2.2 Measurement of VLE  In line with Tempelaar, Rienties and Giesbers [46] and our previous   works [45], two different types of VLE data were gathered per   module in a static and dynamic manner: average time spent (in   minutes) on VLE per week (M=115.4, SD=88.4), and average time   spent per visit (in minutes) on VLE (M=22.5, SD=8.7). Even   though learner activities on VLE were recorded in 40 weeks, we   only used the first 30 weeks data in parallel with 30 weeks data of   learning designs. It should be noted that these crude measurements   of VLE only represented the average time a student spent on VLE   platform, not the actual studying time, as this can be affected by   unobservable factors, such as when students study offline, or using   non-OU systems such as Facebook (which the OU does not   monitor).          3.3 Data analysis   3.3.1 Visualization of learning designs over time  We used Tableau to visualize the LD of 42 modules over 30 weeks   of study time. We displayed both static and dynamic   representations of LD of which the former aggregated all modules,   while the latter was per module basis.    3.3.2 Social network analysis  In line with [24], we used SNA to study the relationships among   learning activities as this technique enables us to quantify and   visualize the interactions and connections between the seven   learning activities. The LD dataset was a weighted two-mode   network as it consisted of different learning activities across several   weeks as illustrated in Figure 2 below. Since we are primarily   interested in the relationships among learning activities, the dataset   was transformed to a one-mode network in line with Hora and   Ferrare [24].       Figure 2: Weighted two-mode network of module X across the   first five weeks   Firstly, two learning activities (blue nodes) become connected if   they were present in the same week (red nodes). Since we captured   how much time students were expected to spent on each LD each   week, the weights of the two learning activities had directed   towards identical weeks could also be measured. In this type of   projected network, the weight of a tie from one LD to another was   not necessarily equal to the weight of the reverse. For example, in   Figure 2, if 2.8 hours were spent on assimilative activities and 1.8   were spent on assessment activities in the same week, then the   weight from assimilative to assessment is recorded as 2.8 and the   weight of the reverse is recorded as 1.8.    Second, the weight of each tie was discounted for the number of   learning activities in the same week [38]. It can be argued that the   tie between two learning activities is weaker when there are more   learning activities that are present in the same week. This can be   generalized as follows:      =     1    where wij is the weight between LD i and LD j, and Np is the number   of learning activities in week p.   After transforming the dataset, we used the Netdraw function of   UCINET [5] to visualize the co-occurrences between each pair of   learning activities across all weeks. The nodes represent the   different learning activities. The tie represents the co-occurrence of   two learning activities in the same week. The thickness of the line   reflects the strength of the ties. Thus, the thicker the line, the higher   the weights of the tie between two learning activities, which was   also represented by the numbers attached along the line.    Finally, in line with Hora and Ferrare [24] configurations of co-  occurring learning activities within each module were used to   determine the repertoires of practice. These were computed as the   combinations of learning activities that occurred most frequently.    3.3.3 Panel data analysis  In preparation for the analysis, the two  datasets on LD and VLE   were transformed from wide to long format. Additional identifiers   were generated as the combination of course code, presentation,   and week. The next step was to merge this dataset according to   these new identifiers. Next, a Hausman test was used to   differentiate between fixed effects and random effects model. It   tests whether the coefficients estimated by the efficient random   effects estimator are the same as the ones estimated by the   consistent fixed effects estimator [22]. Our result supported the   assumption of correlation between observations errors and   predictors, hence, fixed effects model was used as it removes the   effect of time-invariant characteristics to assess the net effect of the   predictors on the outcome. Our analysis was done in Stata.    4. RESULTS   4.1 How are learning designs configured  across modules over time  Figure 3 illustrates the average time students were expected to   spend per module (in hours) on different learning activities over 30   weeks.   At a glance, we can see that there were a lot of fluctuations in   learning activities over time, which indicated a dynamic usage of   LD from teachers (Figure 3). Aligned with previous findings [42,   44, 45, 47, 48], assimilative activities accounted for the majority of   learning time (M=3.9, SD=3.4), which were followed by   assessment activities (M=2.0, SD=3.5). In other words, students   were expected to spend around 6 hours per week on traditional   learning activities of reading and watching materials, and   completing formative and summative assessments. Productive   activities were also adopted constantly over time (M=1.3, SD=1.7).   Communication, experiential, interactive, and finding information   activities were underused most of the time. Interestingly,   assessment and assimilative activities followed opposite paths in   which more assimilative activities were used at the beginning of a   module whereas more assessments were used toward the end. There   seems to be no correlation of any LD with the total time spent   indicating that there is no systematic bias in favoring a particular   learning design.    After capturing the dynamic picture of LD over time, we took a   further step to examine how different learning activities are   configured across different modules. Due to the limited space, we   only reported three exemplar modules across three disciplines with   a variety of configurations and patterns of learning activities   (Figure 4).          Figure 3: Learning designs of 42 modules over 30 weeks in 2014 & 2015           Figure 4: Feature modules   A closer look at each module revealed a diversity of combinations   of LD over time. Module 1 in Arts & Social Science confirmed the   dominance of assimilative (M=4.9, SD= 4.3) and assessment   activities (M=1.4, SD=2.8). Remarkably, there was a surge in the   assimilative activities up to 19.6 hours in week 10 and 14.4 hours   in week 11. On the other hand, module 2 in Business and Law   represented a more balanced learning design. The total workload of   15 hours each week remained constant throughout 30 weeks with   the exception of week 30 where students were expected to spend   more time on assessment activities. There was an assessment   almost every 3 weeks of study. Students in this module engaged in   multiple learning activities: assimilative (M=4.3, SD=1.5),   communication (M=1.6, SD=0.7), finding information (M=1.6,   SD=0.7), productive (M=2.4, SD=1.04), experiential (M=2.1,   SD=1.4), and assessment (M=3.7, SD=5.9). Finally, module 3 used   only three types of LD over time: assimilative, assessment, and   productive. The workload of module 3 in Languages and Education   stayed relatively constant over time, with the majority of studying   time are dedicated to productive activities.    4.2 How do different learning activities  interact with each other across modules over   time      Figure 5: Social network analysis of learning designs     Our social network analysis reveals a variety of combinations of   LD across modules. Again, due to the limited space, we only   reported the three aforementioned exemplar modules (Figure 5).    In Module 1 in Art & Social Sciences, assimilative activities   displayed strong connections with productive and finding   information while there were weak links among other learning   activities. Furthermore, communication acted as a gatekeeper   between experiential and other learning activities which implied   communication was the necessary condition for the existence of   experiential activities. This module confirmed the dominance of   assimilative design as previously illustrated in Figure 3. The   density of the network was 64% with 13 ties in total. The average   distance among reachable pairs was 1.306. The most frequently   used repertoire of practice was assimilative, information, and   productive (38.7% of the time). The relationships among learning   activities in module 2 were more equally distributed in the network,   with the exception of interactive.    Module 2 in Business and Law demonstrated a repertoire of   practice that frequently used assimilative, information,   communication, The network density of this module was 67% with   14 ties in total. The average distance among pairs was 1.2.   Assimilative and assessment shared the strongest connection.   Again, communication played a gatekeeping role in this module in   which it facilitated the use of interactive activities, experiential, and   productive activities (70% of the time).    Module 3 in Languages and Education exhibited a unique setting   which consisted of only three learning activities: assimilative,   assessment, and productive. The network density was 14.3% with   3 ties, and the average distance was 1. Evidently, the most   frequently used repertoire of practice in this module was   assimilative, assessment, and productive (90% of the time).     4.3 How do learning designs affect VLE  behavior over time  In this section, we examined how different learning activities   influence average time spent on VLE per visit (Table 2), and on   VLE per week (Table 3).    For each predictors, four models were applied. First, we ran normal   OLS regression model. Second, we used fixed effect model to the   control of the unobserved heterogeneity of time. Third, we   controlled for the fixed effect across modules. Finally, we   controlled for the fixed effects of both time and modules. The   baseline for LD is the assimilative type. Thus, all the following   results should be interpreted relatively to the module with the   assimilative design. Variance inflation factor (VIF) was computed   after each model to check for multicollinearity. The result indicated   there were no significant correlations among the independent   variables, in other words, there was no overlap of measurements   among seven learning activities. Unstandardized coefficients were   reported because all the explanatory variables were measured in the   same unit (hours). Thus, it is more informative to report the original   metrics.  In the first and second model (Table 2), the effects of each   independent variable remained relatively the same . It implied that   there were no heterogeneity overtime. Assessment,   communication, and productive were positive and significantly   associated with VLE per visit. However, the predictive power of   these models was relatively weak, which only explained 7%-8% of   the variability. In contrast, the predictive power of LD on VLE   increased noticeably when taking into account the differences   across module (model 3 & 4). The effect of assessment became   smaller and insignificant.    Table 2: Panel data analysis of the effect of learning design on   the average time spent on VLE per visit    (1) (2) (3) (4)   VLE_per_visit OLS FE_   week   FE_   module   FE_module  _week          Assessment .51*** .51*** .03 .04    (.08) (.08) (.06) (.06)   Information .25 .32 -.05 .007    (.35) (.35) (.24) (.24)   Communication 2.16*** 2.16*** .69*** .68***    (.35) (.35) (.26) (.26)   Productive .49*** .52*** -.34*** -.32**    (.16) (.16) (.13) (.13)   Experiential -.13 -.13 -.55 -.53    (.53) (.53) (.37) (.36)   Interactive .50 .48 .17 .14    (.34) (.34) (.24) (.24)   Constant 20.19*** 20.11*** 22.74*** 19.29***    (.40) (0.40) (0.31) (1.28)          Observations 1,114 1,114 1,114 1,114   Adjusted   R-squared   0.07 0.08 0.60 0.63   Unstandardized betas, Standard errors in parentheses    *** p<0.01, ** p<0.05, * p<0.1   The effect of communication also decreased to 0.69, which implied   that on average an extra hour spent on communication activities is   associated with 0.69 minutes increase in the time spent per visit on   VLE. In contrast to model 1 & 2, productive activities negatively   impacted VLE per visit. On average, an additional hour spent on   productive activities was associated with 0.34 minutes less in time   spent on VLE per visit. By controlling of the unobservable   heterogeneity across modules, LD can explain up to 60% of the   variability in time spent on VLE per visit. Our results validated the   importance of taking into account the learning context of each   module.   A similar trend was observed in predicting the average time spent   on VLE per week in Table 3. In model 1 & 2, assessment,   communication, and interactive were positive and significantly   related with VLE per week. In model 3 & 4, the effect of assessment   and communication became smaller and insignificant. Productive   activities were negatively associated with VLE per week. Students   who spent one extra hour spent on productive activities on average   spent 4.42 minutes less in VLE. The positive effect of interactive   activities weakened. An additional hour spent in interactive   activities was associated with 6.17 minutes increase in VLE.   Moreover, more time spent on experiential was associated with less   time on VLE per week. An extra hour spent on experiential   activities was associated with 8.43 minutes decrease in VLE. The   predictive power of LD on VLE per week increased largely when   taking into account the differences between modules (Adj-R2 =   40%). Similar models were run again with assessment as the   reference level, however, there was no significant effect of   assimilative activities on both VLE per week and VLE per visit.      Table 3: Panel data analysis of the effect of learning design on   the average time spent on VLE per week    (1) (2) (3) (4)   VLE_per_week OLS FE_   week   FE_   module   FE_module  _week          Assessment 2.96*** 2.35*** -.49 -.98    (.79) (.83) (.74) (.75)   Information 4.442 5.192 .30 .72    (3.60) (3.60) (3.10) (3.04)   Communication 16.53*** 16.40*** 4.32 3.79    (3.60) (3.57) (3.39) (3.31)   Productive .74 1.73 -5.63*** -4.42***    (1.61) (1.60) (1.66) (1.64)   Experiential -4.14 -3.92 -8.81* -8.43*    (5.44) (5.40) (4.77) (4.67)   Interactive 12.02*** 12.44*** 6.03* 6.17**    (3.50) (3.47) (3.13) (3.06)   Constant 102.2*** 101.8*** 122.7*** 99.40***    (4.12) (4.06) (3.98) (16.40)          Observations 1,114 1,114 1,114 1,114   Adjusted    R-squared   0.04 0.08 0.36 0.40   Unstandardized betas, Standard errors in parentheses   *** p<0.01, ** p<0.05, * p<0.1      To sum up, by taking the differences between modules, LD   activities were strong predictors of the average time spent on VLE   platform. In particular, students spent less time on VLE if they were   required to do more productive and experiential activities while the   opposite is true when they engaged in communication and   interactive learning activities.        5. DISCUSSION    5.1 Implications  Firstly, our longitudinal visualization at a static level of LD   suggested that teachers designed learning activities differently over   time. In line with our previous work [45, 48], assimilative and   assessment activities accounted for the majority of learning   activities followed by productive activities, whereas experiential,   interactive, communication, and finding information were less   common. In line with basic principles of LD, more assimilative   activities were employed at the beginning of the course: students   were required to acquire and obtain new knowledge and   information about a particular module, such as reading course   syllabus, watching the introductory lecture, and so forth. Towards   the end of a module, fewer assimilative activities were used,   whereas more formative and summative assessments were made to   evaluate the understanding of learners [6]. Multiple peaks in   assessment activities also indicated that the learning process was   continuously assessed over time, rather than relying solely on a   large final exam. Continuous formative and summative assessment   plays a very important role in distance courses, since small and   constant assessments can both motivate learners and provide an   accurate evaluation of their understanding over time, in order to   intervene in time [46].    Secondly, our dynamic inspection on the LD of each module over   time revealed that the use of LD varied considerably across   modules and disciplines. A balanced approach of LD can be seen   in module 2 in the Business and Law faculty, in which it consists   of six out of seven LDs with equally distributed workloads for each   activity and each week. When there was an assessment, the   workload on other activities were reduced to avoid the   overwhelming workload on students (see Figure 4). This is a very   important remark for teachers and course designers since learners   can be sensitive to peaks and troughs in workload, which in turn   may damage their learning experience. Such example could be   observed in module 1 in Art and Social Science discipline, in which   there was a huge surge in the workload in week 10, which was more   than 20 hours for all learning activities, compared with the average   of 9 hours per week. Another example of a potentially unbalanced   design was module 3 in the Faculty of Education and Language   studies, which only used three types of LD throughout the course   (i.e., assimilative, assessment, and productive). Evidently, we do   not judge which design is good or bad, but this dynamic   visualization of LD across modules can help educators reflect on   their LD to anticipate whether their design best serves the learning   objectives and learner experience.    Thirdly, using social network analysis we were able to observe how   different learning activities were connected to each other. Our   results suggested that if we concentrate on a single component of   learning design in isolation, we might omit the complexity and   critical features of the instructional dynamic. By adopting the view   of system of practice [21], our empirical evidence strengthened the   view of Hora and Ferrare [24] which indicated that teachers   perceive certain learning activities as being meant for each other   (i.e. assimilative & productive, communication & experiential) and   these perceptions varies across disciplines. Interestingly, even   though certain disciplines exhibited favorable practice towards a   particular learning activity, each module utilized it with other   learning activities in different ways. For example, it is apparent that   assimilative activities were the most common learning activities in   all three exemplar modules. However, the repertoire of practice in   module 1 (assimilative, information, and productive) was different   from module 2s (assimilative, information, communication,   experiential, and productive) and module 3s (assimilative,   assessment, and productive). Overall, learning activities are best   viewed in relation to one another in multiple dimensions   throughout time.   Our final takeaway is by taking into account the context of learning   across 38 modules, learning designs were strong predictors of the   time spent on VLE platform. Even though significant effects of   certain learning activities on VLE activities were identified in our   analysis, we advise readers to interpret them with cautions. As   discussed above, learning activities should be perceived in relation   with one another rather than in isolation. For example, our results   showed that students spent less time on VLE when they engaged in   productive activities. However, this did not imply that by simply   cutting down productive activities, students will be more likely to   engage. It is because each module employed productive activities   in relation with different learning activities in different ways at   different points in time. Students who engaged in productive   activities which include building, constructing, and creating a   knowledge nugget may work offline. If they are required to share   these knowledge nuggets with other students then a rise in   communication activities is expected as they post their thoughts   and creations to the discussion forum.    From a researchers perspective, by acknowledging the distinctive   features of each discipline, we can considerably increase the     accuracy of predicting student engagement in VLE. From a   practitioners point of view, our results highlighted the need to   appropriately balance learning design that fit with specific learning   outcomes and disciplinary practice.    5.2 Limitations  First, the measurements of the average time spent on VLE were   crude indicators. Capturing the time spent on actual learning   activities while control for which VLE activities are dedicated to   which learning activities, and other unobservable non-studying   activities is difficult. This problem has also been addressed in   LAK15 in which Joksimovi, Gaevi, Loughin, Kovanovi and   Hatala [26] confirmed that the choice of the time-on-task   estimations (assignment, reading, discussion, adding a post, or   updating post) played an important role in the overall model fit and   subsequent model interpretation.    Second, in a time-series model, time lag issue may occur [12]. For   example, students who anticipated an assessment in week 10 would   start preparing in week 9. Thus, assessment should be discounted   one week in order to accurately reflect its effect on VLE activities.   However, determining time lag is challenging given the variances   of LD and the inconsistencies across modules.     Third, the LD taxonomy has certain limitations. On one hand, it   could be over-simplify the actual LD since there are sub-categories   in each types of learning activities (i.e. there are many kinds of   assessment such as tutor-marked assessment, and computer-based   assessment). On the other hand, some learning activities are overlap   between different categories (i.e. watching a lecture while replying   to the chat could be both assimilative and communication).    Finally, at the time this paper was written, the OU does not model   learning designs across a programme or curriculum perspective.   Therefore, we are limited to what we can actually conceptually   define and empirically test LD at a program level.    6. CONCLUSION AND FUTURE WORK  This study investigated how learning designs are configured over   time and its effect on student activities in VLE by analyzing 38   modules over 30 weeks at the Open University UK. By visualizing   how learning design changed over time, teachers can explicitly   reflect on their practice as well as compare and contrast with others.   Using social network analysis, we illustrated how different learning   activities interact with each other and which repertoire of practice   was frequently adopted. Our results indicated a wide variance in the   number of learning activities was used as well as the workload   balance across modules. When the workload is unbalanced   according to the OULDI framework, teachers can anticipate   potential problems in their design to make informed interventions.    Moreover, our panel data analysis on the effect of learning designs   on VLE activities indicated that by controlling for the differences   across modules, learning designs proved to be strong indicators of   student activities. In particular, communication and interactive   activities had a positive effect on VLE engagement whereas   productive and experiential were associated with lower levels of   VLE activities. Our findings reinforced and provided new   empirical evidence of the importance of understanding pedagogical   context in LA in order to translate the findings to sensible actions.    Our research contributes to the existing literature in LA & LD by   providing visualizations of elements of LD, and empirically   examining the actual student learning behaviors in relation with the   teachers pedagogical intentions. By analyzing the actual learning   behaviors of students across a large number of online modules, our   work addressed the issue of ecological validity of experimental   studies in LD while enhanced the external validity of the findings.   By connecting the LD (input) with students learning behaviors   (progress), our work also supports previous LA findings which   were mainly based on students learning behavior (progress) and   learning outcomes (output).    Future scholars are recommended to consider the inter-  relationships between learning activities in doing research on. For   instance, social network metrics of LD can be incorporated in the   prediction models. When more fine-grained data (i.e. how much   time students are expected to spend on writing essays, watching   video, listening to audio, etc.) become available, researchers can   unfold the complexity of LD in a more specific manner. Multi-level   analysis can be conducted on a large scale study to account for the   heterogeneity across faculties, levels of study, modules, and   configurations of learning design.        7. REFERENCES  [1] AUTCLearningDesign PredictObserveExplain: Designers Voice  Context. Retrieved 9 Jan, 2016, from  http://www.learningdesigns.uow.edu.au/exemplars/info/LD44/more/03Co  ntext.html   [2] Bakharia, A. and Dawson, S. 2011. SNAPP: a bird's-eye view of  temporal participant interaction. In Proceedings of the Proceedings of the   1st international conference on learning analytics and knowledge, 2011.   ACM, New York, NY, USA, 2011, 168-173.   [3] Bakharia, A., Corrin, L., de Barba, P., Kennedy, G., Gaevi, D.,   Mulder, R., Williams, D., Dawson, S. and Lockyer, L. 2016. A conceptual  framework linking learning design with learning analytics. In Proceedings   of the Sixth International Conference on Learning Analytics &   Knowledge, 2016. ACM, New York, NY, USA, 2016, 329-338.   [4] Bennett, S., Agostinho, S. and Lockyer, L. 2015. Technology tools to   support learning design: Implications derived from an investigation of  university teachers' design practices. Computers & Education, 81. 211-  220.   [5] Borgatti, S. P., Everett, M. G. and Freeman, L. C. 2002. Ucinet for  Windows: Software for social network analysis.   [6] Boud, D. and Falchikov, N. 2006. Aligning assessment with longterm   learning. Assessment & Evaluation in Higher Education, 31 (4). 399-413.   [7] Buckingham Shum, S. and Crick, R. D. 2012. Learning dispositions   and transferable competencies: pedagogy, modelling and learning  analytics. In Proceedings of the 2nd International Conference on Learning   Analytics and Knowledge, 2012. ACM, New York, NY, USA, 2012, 92-  101.   [8] Cela, K. L., Sicilia, M. . and Snchez, S. 2015. Social network   analysis in e-learning environments: A Preliminary systematic review.  Educational Psychology Review, 27 (1). 219-246.   [9] Chen, B., Chen, X. and Xing, W. 2015. Twitter archeology of learning   analytics and knowledge conferences. In Proceedings of the Fifth  International Conference on Learning Analytics And Knowledge, 2015.   ACM, New York, NY, USA, 2015, 340-349.   [10] Clow, D. 2013. An overview of learning analytics. Teaching in   Higher Education, 18 (6). 683-695.   [11] Conole, G. Designing for learning in an open world. Springer  Science & Business Media, 2012   [12] Cromwell, J. B. Multivariate tests for time series models. Sage, 1994   [13] Cross, S., Galley, R., Brasher, A. and Weller, M. Final Project Report   of the OULDI-JISC Project: Challenge and Change in Curriculum Design  Process, Communities, Visualisation and Practice. York: JISC. Retrieved   October 16th, 2016, from http://www.open.ac.uk/blogs/OULDI/wp-  content/uploads/2010/11/OULDI_Final_Report_Final.pdf   [14] Dalziel, J. 2003. Implementing learning design: The learning activity   management system (LAMS). In Proceedings of the 20th Annual   Conference of the Australian Society for Computers in Learning in  Tertiary Education, Adelaide, 2003. University of Adelaide, 593-596.   [15] Dalziel, J. Learning design: Conceptualizing a framework for  teaching and learning online. Routledge, New York, NY, USA, 2015     [16] Dalziel, J., Conole, G., Wills, S., Walker, S., Bennett, S., Dobozy, E.,   Cameron, L., Badilescu-Buga, E. and Bower, M. 2016. The Larnaca  declaration on learning design. Journal of Interactive Media in Education,   2016 (1). 1-24.   [17] Ferguson, R. 2012. Learning analytics: drivers, developments and   challenges. International Journal of Technology Enhanced Learning, 4 (5-  6). 304-317.   [18] Gaevi, D., Dawson, S. and Siemens, G. 2015. Lets not forget:   Learning analytics are about learning. TechTrends, 59 (1). 64-71.   [19] Gaevi, D., Dawson, S., Rogers, T. and Gasevic, D. 2016. Learning   analytics should not promote one size fits all: The effects of instructional   conditions in predicting academic success. The Internet and Higher  Education, 28. 68-84.   [20] Goodyear, P. 2015. Teaching as design. HERDSA Review of Higher   Education, 2. 27-50.   [21] Halverson, R. R. 2003. Systems of practice: How leaders use artifacts   to create professional community in schools. Education Policy Analysis  Archives, 11 (37). 1-35.   [22] Hausman, J. A. 1978. Specification tests in econometrics.   Econometrica: Journal of the Econometric Society, 46 (6). 1251-1271.   [23] Hecking, T., Chounta, I.-A. and Hoppe, H. U. 2016. Investigating   social and semantic user roles in MOOC discussion forums. In  Proceedings of the Proceedings of the Sixth International Conference on   Learning Analytics & Knowledge, 2016. ACM, New York, NY, USA,   2016, 198-207.   [24] Hora, M. T. and Ferrare, J. J. 2013. Instructional systems of practice:   A multidimensional analysis of math and science undergraduate course  planning and classroom teaching. Journal of the Learning Sciences, 22   (2). 212-257.   [25] Joksimovi, S., Dowell, N., Skrypnyk, O., Kovanovi, V., Gaevi,  D., Dawson, S. and Graesser, A. C. 2015. How do you connect: Analysis   of social capital accumulation in connectivist MOOCs. In Proceedings of   the Fifth International Conference on Learning Analytics And Knowledge,  2015. ACM, New York, NY, USA, 2015, 64-68.   [26] Joksimovi, S., Gaevi, D., Loughin, T. M., Kovanovi, V. and  Hatala, M. 2015. Learning at distance: Effects of interaction traces on   academic achievement. Computers & Education, 87. 204-217.   [27] Joksimovi, S., Kovanovi, V., Jovanovi, J., Zouaq, A., Gaevi, D.  and Hatala, M. 2015. What do cMOOC participants talk about in social   media: a topic analysis of discourse in a cMOOC. In Proceedings of the   Fifth International Conference on Learning Analytics And Knowledge,  2015. ACM, New York, NY, USA, 2015, 156-165.   [28] Kirschner, P. Keynote: Learning Analytics: Utopia or Dystopia.  Retrieved October 10th, 2016, from http://lak16.solaresearch.org/wp-  content/uploads/2016/05/lak16keynotelearninganalytics-utopiaofdystopia-  160428103734.pdf   [29] Koper, R. and Manderveld, J. 2004. Educational modelling language:   modelling reusable, interoperable, rich and personalised units of learning.   British Journal of Educational Technology, 35 (5). 537-551.   [30] Kuzilek, J., Hlosta, M., Herrmannova, D., Zdrahal, Z. and Wolff, A.   2015. OU Analyse: analysing at-risk students at The Open University.  Learning Analytics Review. 1-16.   [31] Laurillard, D. and McAndrew, P. 2001. Virtual Teaching Tools:   Bringing academics closer to the design of e-learning. In Proceedings of   the Third International Conference on Networked Learning, Sheffield,   England, 2001, 11-16.   [32] Lockyer, L., Bennett, S., Agostinho, S., Harper, B., Lockyer, L.,   Bennett, S., Agostinho, S. and Harper, B. Handbook of Research on   Learning Design and Learning Objects: Issues, Applications and  Technologies. IGI Global, New York, NY, USA, 2008   [33] Lockyer, L. and Dawson, S. 2011. Learning designs and learning  analytics. In Proceedings of the 1st international conference on learning   analytics and knowledge, 2011. ACM, New York, NY, USA, 2011, 153-  156.   [34] Lockyer, L., Heathcote, E. and Dawson, S. 2013. Informing   pedagogical action: Aligning learning analytics with learning design.  American Behavioral Scientist, 57 (10). 1439 - 1459.   [35] Macfadyen, L. P. and Dawson, S. 2010. Mining LMS data to develop  an early warning system for educators: A proof of concept. Computers   & education, 54 (2). 588-599.   [36] MacLean, P. and Scott, B. 2011. Competencies for learning design: A  review of the literature and a proposed framework. British Journal of   Educational Technology, 42 (4). 557-572.   [37] Mor, Y., Ferguson, R. and Wasson, B. 2015. Editorial: Learning   design, teacher inquiry into student learning and learning analytics: A call   for action. British Journal of Educational Technology, 46 (2). 221-229.   [38] Newman, M. E. 2001. Scientific collaboration networks. II. Shortest   paths, weighted networks, and centrality. Physical review E, 64 (1).   016132.   [39] Nguyen, Q., Tempelaar, D. T., Rienties, B. and Giesbers, B. 2016.   What learning analytics based prediction models tell us about feedback  preferences of students. Quarterly Review of Distance Education, 17 (3).   13-33.   [40] Persico, D. and Pozzi, F. 2015. Informing learning design with  learning analytics to improve teacher inquiry. British Journal of   Educational Technology, 46 (2). 230-248.   [41] Poquet, O. and Dawson, S. 2016. Untangling MOOC learner   networks. In Proceedings of the Sixth International Conference on   Learning Analytics & Knowledge, 2016. ACM, New York, NY, USA,  2016, 208-212.   [42] Rienties, B., Toetenel, L. and Bryan, A. 2015. Scaling up learning  design: impact of learning design activities on lms behavior and   performance. In Proceedings of the Fifth International Conference on   Learning Analytics And Knowledge, 2015. ACM, New York, NY, USA,  2015, 315-319.   [43] Rienties, B., Boroowa, A., Cross, S., Kubiak, C., Mayles, K. and   Murphy, S. 2016. Analytics4Action Evaluation Framework: A Review of  Evidence-Based Learning Analytics Interventions at the Open University   UK. Journal of Interactive Media in Education, 2016 (1).   [44] Rienties, B. and Toetenel, L. 2016. The impact of 151 learning   designs on student satisfaction and performance: social learning   (analytics) matters. In Proceedings of the Sixth International Conference  on Learning Analytics & Knowledge, Edinburgh, United Kingdom, 2016.   ACM, New York, NY, USA, 2016, 339-343.   [45] Rienties, B. and Toetenel, L. 2016. The impact of learning design on  student behaviour, satisfaction and performance: A cross-institutional   comparison across 151 modules. Computers in Human Behavior, 60. 333-  341.   [46] Tempelaar, D., Rienties, B. and Giesbers, B. 2015. In search for the   most informative data for feedback generation: Learning Analytics in a  data-rich context. Computers in Human Behavior, 47. 157-167.   [47] Toetenel, L. and Rienties, B. 2016. Learning Designcreative design   to visualise learning activities. Open Learning: The Journal of Open,  Distance and e-learning, 31 (3). 233-244.   [48] Toetenel, L. and Rienties, B. 2016. Analysing 157 learning designs  using learning analytic approaches as a means to evaluate the impact of   pedagogical decision making. British Journal of Educational Technology.   [49] Whitelock, D., Twiner, A., Richardson, J. T., Field, D. and Pulman,   S. 2015. OpenEssayist: a supply and demand learning analytics tool for   drafting academic essays. In Proceedings of the Fifth International  Conference on Learning Analytics And Knowledge, 2015. ACM, 208-212.   [50] Wise, A. F. and Shaffer, D. W. 2015. Why theory matters more than   ever in the age of big data. Journal of Learning Analytics, 2 (2). 5-13.   [51] Wise, A. F., Cui, Y., Jin, W. and Vytasek, J. 2017. Mining for gold:   Identifying content-related MOOC discussion threads across domains  through linguistic modeling. The Internet and Higher Education, 32. 11-28.   [52] Zhu, M. and Feng, G. 2015. An exploratory study using social   network analysis to model eye movements in mathematics problem  solving. In Proceedings of the Fifth International Conference on Learning   Analytics And Knowledge, 2015. ACM, 383-387.     "}
{"index":{"_id":"23"}}
{"datatype":"inproceedings","key":"Adjei:2017:SCA:3027385.3027412","author":"Adjei, Seth A. and Botelho, Anthony F. and Heffernan, Neil T.","title":"Sequencing Content in an Adaptive Testing System: The Role of Choice","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"178--182","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027412","doi":"10.1145/3027385.3027412","acmid":"3027412","publisher":"ACM","address":"New York, NY, USA","keywords":"ASSISTments, PLACements, mastery learning, remediation assignments, student choice","Abstract":"The effect of choice on student achievement and engagement has been an extensively researched area of learning analytics. Current research findings suggest a positive relationship between choice and varied outcome measures, but little has been reported to indicate whether these findings hold in the context of Intelligent Tutoring Systems (ITS). In this paper, we report the results of a randomized controlled experiment in which we investigate the effect of student choice on assignment completion and future achievement in an ITS. The experimental design uses three conditions to observe the effect of choice. In the first condition, students are able to choose the order in which to complete assignments, while in the second condition, students are prescribed an intuitive order in which to complete assignments. Those in the third condition were prescribed a counter-intuitive order in which to complete assignments. Results indicate that allowing students to choose the order in which to work on assignments leads to higher completion rates and better achievement at posttest. A post-hoc analysis also revealed that even considering students with similar completion rates, those given choice had higher posttest scores than those observed in any other condition. These results seem to support the many theories of the positive effect of choice on student achievement.","pdf":"Sequencing Content in an Adaptive Testing System:   The Role of Choice  Seth A. Adjei  Worcester Polytechnic Institute   100 Institute Road  Worcester   Telephone number, incl. country code  saadjei@wpi.edu    Anthony F. Botelho  Worcester Polytechnic Institute   100 Institute Road  Worcester   Telephone number, incl. country code  abotelho.777@gmail.com   Neil T. Heffernan  Worcester Polytechnic Institute   100 Institute Road  Worcester   Telephone number, incl. country code  nth@wpi.edu        ABSTRACT  The effect of choice on student achievement and engagement has  been an extensively researched area of learning analytics. Current  research findings suggest a positive relationship between choice  and varied outcome measures, but little has been reported to  indicate whether these findings hold in the context of Intelligent  Tutoring Systems (ITS). In this paper, we report the results of a  randomized controlled experiment in which we investigate the  effect of student choice on assignment completion and future  achievement in an ITS. The experimental design uses three  conditions to observe the effect of choice. In the first condition,  students are able to choose the order in which to complete  assignments, while in the second condition, students are  prescribed an intuitive order in which to complete assignments.  Those in the third condition were prescribed a counter-intuitive  order in which to complete assignments. Results indicate that  allowing students to choose the order in which to work on  assignments leads to higher completion rates and better  achievement at posttest. A post-hoc analysis also revealed that  even considering students with similar completion rates, those  given choice had higher posttest scores than those observed in any  other condition. These results seem to support the many theories  of the positive effect of choice on student achievement.   CCS Concepts   General and reference~Empirical studies    General and  reference~Experimentation.   Keywords  Mastery Learning; Student Choice; PLACEments; ASSISTments;  Remediation Assignments.    1. INTRODUCTION  The concept of mastery learning is based on a philosophy that  states that all students have the ability to learn anything and that  this ability is a function of time.  In other words, given a new  topic, it is merely a matter of time and practice before one can  reach a state of understanding. It has also been suggested that   mastery learning is purely teacher-paced, where teachers  determine the order in which students must learn specific  knowledge components or skills.    An opposing philosophy to mastery learning is known as the  personalized system of instruction (PSI), in which students decide  on their pace and the amount of content they learn. [2] Ritter,  Yudelson, Fancsali, and Berman conducted a study in which  mastery learning of content in the Cognitive Tutor was compared  to teachers prescriptions of the order in which to present content.  [10] It was found in this work that the systems determination of  ordering caused significant improvements in student learning.  Their findings suggested that using ITS to prescribe the order in  which students were presented a set of knowledge components or  skills was a better approach to learning than allowing teachers to  determine or prescribe content order. From a different perspective,  these results seemed to show that choice, at least at the teacher  level, did not cause learning gains.    The effect of choice on various aspects of human life has been  studied for many decades. Watanabe & Sturmey performed a  meta-analysis of publications in the area of metacognition and the  effect of choice on student performance and found that,  particularly for students with disability, allowing choice has many  benefits. [13] Choice was shown to improve student engagement  on tasks as well as propensity of completion. Additionally, it has  been shown that intrinsic motivation to carry out general tasks can  be improved when students are given choice. [3] Other  researchers have observed the positive effect of choice on  outcome measures in a number of varied activities. [6, 9, 14]  Wang & Stiles showed that students completed more tasks when  asked to choose when and how to complete the tasks. [12] This  phenomenon is evident in preschoolers [1], high-schoolers [11],  and college undergraduates [14]. Across ages, the primary  contributing factor causing the increase in performance and rate of  completion has been attributed to the motivational effects of  choice.  The extent of the effects of choice are sometimes conflicting  across different studies. Flowerday and Schraw [4], for example,  show that choice had a positive effect on attitude and effort,  however the effect on cognitive engagement was minimal or non- existent.  While not unanimous across all domains and studies,  there is compelling argument to pursue the study of choice for its  potential benefits in learning. Understanding how the positive  aspects of choice can best be implemented to improve students  learning experiences is a topic still in need of research.    Despite the many benefits that seem to be derived from choice,  ITSs rarely offer features that allow students to make choices   Permission to make digital or hard copies of all or part of this work for personal or  classroom use is granted without fee provided that copies are not made or  distributed for profit or commercial advantage and that copies bear this notice and  the full citation on the first page. Copyrights for components of this work owned by  others than the author(s) must be honored. Abstracting with credit is permitted. To  copy otherwise, or republish, to post on servers or to redistribute to lists, requires  prior specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada  Copyright is held by the owner/author(s). Publication rights licensed to ACM.  ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027412     regarding what they learn, and when and how to remediate  content that they may be lacking. Ostrow & Heffernan conducted  a randomized controlled experiment in which they investigated  allowing students to choose the type of feedback received while  working on an assignment and its effect on assignment  completion and future performance. [8] They compared students  who were given the choice to decide on the type of feedback  received with those who randomly received a particular type of  feedback. They found that students given choice had significantly  better achievement than those in the control group, lending  credence to the notion that choice has a positive impact on student  performance within an ITS.   In this study our goal is to investigate the effect of choice on  student assignment completion and learning gains when given the  opportunity to choose the order in which to complete assignment  tasks. We report on a randomized controlled experiment in which  students were placed into three conditions. In one condition,  students were asked to choose the order in which to complete the  assignments, whereas students in the other two conditions  followed different prescribed content orders. We also report a  post-hoc analysis of the study in which we find that, for students  with similar assignment completion rates, those in the choice  group performed better at posttest than those in either prescribed  condition.   1.1 Research Question  The following research question is addressed in the present study:   Does allowing students to choose the order in which to remediate  skills improve adherence in the form of assignment completion  rates, and/or Math achievement   In other words, does choice matter What is the relationship  between student choice and mastery learning   2. METHODS  This section describes the methods employed in answering the  research questions stated above. We ran the randomized  controlled experiment in PLACEments, an adaptive testing  system. This system is described briefly in this section. We then  present the experimental design, the participants used in the  experiment, and the outcome measures of interest.     2.1 An introduction to PLACEments  PLACEments is a computer-aided adaptive testing feature of  ASSISTments, an online learning platform powered by Worcester  Polytechnic Institute. [5] PLACEments uses a prerequisite skill  graph that underlies the system, created based on the  Massachusetts Common Core State Standards for Mathematics.  [7] All PLACEments tests are teacher-driven, in that teachers  choose what and when to assign. These tests are composed of an  initial set of skills selected by the teacher, and once assigned,  students are tested on questions related to the initial skills. If a  student performs poorly on any of the initial skills, the system  traverses the skill graph to select questions from the immediate  prerequisite skills of the incorrect items. These items are then  included on the test, and the graph traversal for item selection  continues until the system determines that there are no further  prerequisite skills to be shown or the traversal reaches a  predefined end point in the graph; the predefined end-point is set  at test creation time. In this manner, the system can isolate and  map the depth of gaps in students knowledge, while providing  opportunity for remediation.    2.1.1 Progressing through the Test  For the sake of simplicity, we use the hypothetical graph shown in  Figure 1 to explain how the test proceeds. The nodes in the graph  represent skills or knowledge components. The arrows between  skills represent the order in which students need to learn these  skills/concepts in order to succeed in the subsequent skills. They  therefore show the prerequisite relationships between skills (thus,  skill D is one of the prerequisites of skill A). The correctness  indicators attached to each node in the graph are a representation  of a given students performance during the test. In this  configuration, the student is assigned A, B, and C as initial  skills in the test. The system presents the student with questions  from these skills, and since the student performs poorly on skill A  (as shown in the graph), the student is further tested on skills D, E,  and then subsequently H since the student did not demonstrate  mastery of skills E and H respectively.       Figure 1 A sample skill graph and a sample students   response configuration  Generally, the tests are meant to identify students lack of specific  skill knowledge and to find which prerequisite skills to blame for  that missing knowledge.    2.1.2 Remediation Assignment Creation and Release  Once the knowledge gaps are determined from the test,  PLACEments attempts to help students close that gap. Once the  test is completed, students are assigned remedial practice  questions on the skills in which they performed poorly. The  release of these assignments is staggered and is based on the  underlying prerequisite skill graph that PLACEments depends  upon, and the number of these remediation assignments given is  dependent on the students performance during the test. The  remediation assignments of the lowest grade level skills on which  the students performed poorly are released first and, once  completed, subsequent post-requisite skill remediation  assignments follow. In the configuration depicted in Figure 1, the  remediation assignment for skill H is released and completed  before the assignment for E is released. The assignment for skill A  will be held back until the student completes skill E.  All  remediations are mastery-based assignments referred to as skill  builders, in which students are given similar skill-based items  until a predefined threshold of understanding is reached; this  threshold is usually met by answering three consecutive items  correctly.   2.2 Experiment Design  We ran a randomized controlled trial in PLACEments in which  we experimented with the order in which remediation assignments  were released. Figure 2 illustrates the experimental design for this  study.   As shown in Figure 2, each participant is given a predefined  PLACEments test which has various initial skills. These     assignments are teacher-assigned and may have varying degrees  of difficulty. After the tests, students are randomly placed in one  of three conditions. In the first condition, Prerequisite to  Postrequisite, participants are assigned remediation skill builder  assignments beginning with the skills of the lowest grade level  and the graph is traversed in the pre-to-post direction. Participants  are required to complete all released remediation assignments for  a given test before the subsequent post-requisite skill related  assignments are released. This condition typifies the current graph  traversal direction for remediation assignments that are released in  PLACEments (See section 2.1.2 for more details).     Figure 2 Experimental Design   The Post-requisite to Prerequisite condition has a similar  behavior as the Prerequisite to Post-requisite condition with the  exception that the graph is traversed in reverse, from the post- requisite to the prerequisite skills, which is counter intuitive to  most teachers. In the third condition, graph traversal is not  considered. For all participants in this third condition, the release  of remediation assignments is not staggered, nor is it based on the  prerequisite skill graph. Instead, all remediation assignments are  released to the students at once and they get to choose the order in  which to complete their assignments.    A month after the initial test, students had the opportunity to  retake their initial PLACEments test as a posttest to gauge the  amount of learning that had occurred from the remediation  assignments and, ultimately, the effect of condition.   We also performed a post-hoc analysis of the data collected from  this experiment. In this post-hoc analysis, we investigated the  effect of other PLACEments test features and the condition  assignment on students performance gain over the study period.   2.3 Participants   For this experiment, there were 410 student participants, each of  whom was assigned the initial PLACEments test as well as the  reassignment that served as the outcome measure. All students  were 7th and 8th grade users of ASSISTments. The participants  had varying levels of math competence and were randomly  assigned to one of the three previously described conditions in the  study. The Prerequisite-to-Post-requisite, Post-requisite-to- Prerequisite, and Release All conditions had 129, 145, 136  students respectively. Random assignment to condition was  performed after the initial PLACEments test was completed. The  results of the tests in no way impacted random assignment.    2.4 Outcome measures  To determine the effectiveness of choice, the following outcome  measures were used: remediation completion rate, performance  on posttest, and the learning gain from the initial to the reassigned  PLACEments tests (i.e., from pre to posttest).   The completion rate, in this context, is the ratio of remediation  assignments completed to the number of remediation assignments  assigned. This outcome measure was intended to help determine  whether the order in which remediation assignments were released  had an impact on students assignment completion rates.  Additionally, we use students performance on the second  PLACEments test as a second outcome measure (i.e., posttest).  We also considered the gain in PLACEments test performance.  This gain was the mathematical difference between the initial test  performance (expressed as percent of items answered correctly)  and that of the second PLACEments test.   3. RESULTS   In this section, we present an initial intent-to-treat analysis of all  the participants in the experiment and further describe an analysis  of students who participated in the post-test. We then proceed to  answer the proposed research questions using data from students  who were actually treated. The dataset for this experiment can be  found at http://tiny.cc/palsrct5data.    3.1 Effect of Choice on Remediation  Assignment Completion Rate  Though all 410 students in the study were expected to complete  the posttest, we found that a high percentage of students did not  have the opportunity to do so. In some cases teachers prevented  entire classes from completing the posttest, while in other cases,  the school year ended before students had the opportunity to take  the posttest. In view of this, only one of our research questions  can be answered using the entire population of the study.   In regards to the impact of choice on assignment completion,  Table 1 shows the remediation completion rate for each of the  conditions in the study. There was no significant difference in  remediation completion rates between conditions (p-value > 0.05).  Though students in the counter intuitive condition (i.e. post-to-pre  condition) seemed to have a slight edge over students in other  conditions, the difference was not significant. The observed  difference may be due to the fact that the post-to-pre condition  encouraged students to complete more assignments because they  presumably navigated from difficult assignments to easier  assignments. Generally, there was a low average remediation  assignment completion rate of 0.38 across the entire population.   Table 1: Remediation Completion Rates by Condition  Condition Participants Mean Completion   Rate   Pre-to-post 129 0.38   Post-to-pre 145 0.42   StudentChoice 136 0.35      3.2 Effect of Choice on Post-test Performance  Of the 410 initial students, 70 students completed the post-test. As  Table 2 shows, the students were randomly and almost equally  distributed across the different conditions. This section describes  the effect of choice on performance of these students on the post- test.     Table 2 Completion Rates and Learning Gains   Condition Number of  Participants   Average  Completion  Rate*   Learning  Gain*   Pre-to-post 22 0.457 0.038   Post-to-pre 26 0.476 0.120   Release All/  Student Choice 22 0.512 0.310   Total 70     * Significance with p-value <0.05  Among others, Table 2 shows that students in the pre-post  condition who completed the posttest also completed far more  remediation assignments than those in the other two groups.  Additionally, students in the choice condition were not  completing as many assignments as those in either prescriptive  condition. These results suggest that choice in this setting did not  necessarily increase assignment completion rates for these  students, as described in section 3.1 above. However, Table 2  suggests that this same group of students performed better on the  posttest than students in the two prescriptive conditions. Their  gain in achievement from the pre-test to the post-test was more  than twice the gain for the counter intuitive group and 10 times  that of the intuitive group. This seems to suggest that students in  this condition may have recognized the skills they performed  poorly on and were therefore able to make intelligent choices  regarding which skills required remediation.   We also performed a one-way ANOVA and the results show that  there was a significant difference in math achievement for  students who had the chance to complete the experiment (p-value  < 0.05).   4. ANALYSIS OF RESULTS  4.1 Post-hoc Analyses  Across our population, 32 students had an assignment completion  rate of 100%. (see Table 3) We analyzed these 32 participants and  found that, first of all, they were equally distributed among the  conditions. Secondly, students in the choice condition achieved  huge learning gains over those from the other prescriptive  conditions. This result seems to suggest that even among students  who are consistent in completing their assignments, prescribing  the order in which to complete assignments is not ultimately  helpful to learning. When there are multiple tasks to be performed  by students, it is best to allow them to choose the order in which  to work on the assignments, as suggested by our results here.  Allowing students to choose the order in which they work on  assignments appears to provide better gains than when the  systems make the choice for them, especially for students who  have high assignment completion rates.   Table 3: Learning Gains among students with comparable  assignment completion rates   Condition Number of  Participants   Learning  Gain*   Pre-to-post 11 0.056   Post-to-pre 11 0.086   Student Choice 10 0.333   Total 32    * Significance with p-value <0.05   5. DISCUSSION  Student choice has been found to be helpful for encouraging  learners to perform well on certain outcome measures of interest.  Research has shown that giving students opportunities to make  choices regarding the pace and sequence of math content has  many positive effects on students. These findings informed our  quest to determine whether the phenomenon would hold true in  the context of PLACEments, the adaptive testing system that  leverages the ASSISTments learning platform.    In the current study, we set out to determine whether the touted  benefits of student choice could be replicated in our testing  system, and if so, to what degree it mattered. Contrary to the  established notion that choice improves assignment completion,  the present study showed that assignment completion rates were  not significantly different among conditions. These findings  reveal that though there were differences in student completion  rates, these differences were not significant and their magnitudes  were minimal at best. We think this may be the result of several  factors, the most prominent of which is the possibility that the  lengths of the PLACEments tests in these classes were too short.   However, of the students who completed the posttest, we found  that differences in assignment completion were significant. We  also found that among those students who completed the  experiment, there were significant differences in learning gains.  Post-hoc analysis of the results seemed to suggest that choice was  very important amongst students with comparable assignment  completing behavior.  This is an impactful finding, as it suggests  that choice increases performance. Of note here, the observed  performance boost could not be attributed to students completing  more assignments than those in the other groups; the assignment  completion rates were not significantly different, and yet the  difference in performance remains.   The contributions of this paper support that in every learning  analytics study that tries to model students learning and behavior,  the effect of choice cannot be ignored. Additionally, designers of  ITSs must look for ways to incorporate opportunities for students  with comparable abilities and assignment completion rates to  make choices in the order in which they complete assignments  while using the system. This consideration will contribute to an  improved learning experience for students.   6. FUTURE WORK  The study we report in this paper has one clear limitation in that  there was a considerably high dropout rate among all  experimental conditions. We presume this high attrition may have  been caused by a number of possible factors which require further  scrutiny. We think that this may be an artifact of the PLACEments  system and the size and difficulty of the assignments used in the  study. Further investigations into the causes of this high dropout  rate are necessary to help rectify the issue in future analyses, and  to boost teacher and student fidelity of the PLACEments system.  The current study investigated the effect of choice on the release  and completion of remediation assignments. Another feature of  the system in which we can implement choice is in the test itself.  Additional experiments are being planned to determine how  choice can be incorporated in this aspect. An illustrative example  of this involves providing choice in completing the initial skills  for the test.    We intend to run additional experiments to replicate these  findings and improve upon the current results. If the results hold  in replication trials, we will modify the PLACEments system to  allow students to choose the order in which to complete their     remediation assignments as it is shown here to significantly  benefit student learning.   7. ACKNOWLEDGMENTS  We acknowledge funding from multiple NSF grants (ACI- 1440753, DRL-1252297, DRL-1109483, DRL-1316736 & DRL- 1031398), the U.S. Department of Education (IES R305A120125  & R305C100024 and GAANN), the ONR, and the Gates  Foundation.   8. REFERENCES  [1] Amabile, T. M., and Gitomer, J. (1984) , Childrens Artistic   Creativity: Effects of Choice in Task Materials, Personality  and Social Psychology Bulletin, vol. 10, 1984, pp. 209-15.   [2] Block, James H., & Burns, Robert B. (1976). Mastery  Learning. Review of Research in Education, 4, 3-49.  doi:10.2307/1167112   [3] Cordova, D. I., & Lepper, M. R. (1996). Intrinsic motivation  and the process of learning: Beneficial effects of  contextualization, personalization, and choice. Journal of  Educational Psychology, 88(4), 715-730. doi:10.1037/0022- 0663.88.4.715   [4] Flowerday, Terri, & Schraw, Gregory. (2003). Effect of  Choice on Cognitive and Affective Engagement. The Journal  of Educational Research, 96(4), 207-215.  doi:10.1080/00220670309598810   [5] Heffernan, N. & Heffernan, C. (2014). The ASSISTments  Ecosystem: Building a Platform that Brings Scientists and  Teachers Together for Minimally Invasive Research on  Human Learning and Teaching. International Journal of  Artificial Intelligence in Education. 24(4), 470-497 DOI  10.1007/s40593-014-0024-x.   [6] Langer, E. J., & Rodin, J. (1976). The effects of choice and  enhanced personal responsibility for the aged: A field  experiment in an institutional setting. Journal of Personality  and Social Psychology, 34, 191198   [7] National Governors Association Center for Best Practices,  Council of Chief State School Officers Title: Common Core   State Standards; Publisher: National Governors Association  Center for Best Practices, Council of Chief State School  Officers, Washington D.C.   [8] Ostrow, K. & Heffernan, N. (2015). The Role of Student  Choice Within Adaptive Tutoring. In Conati, Heffernan,  Mitrovic & Verdejo (eds.) Proceedings of the 17th  International Conference on Artificial Intelligence in  Education (AIED 2015). Springer International Publishing.  Madrid, Spain. June 22-26. pp. 752- 755.   [9] Perlmuter, L. C., & Monty, R. A. (1977). The importance of  perceived control: Fact or fantasy American Scientist, 65,  759765.   [10] Ritter, Steve, Yudelson, Michael, Fancsali, Stephen E., &  Berman, Susan R. (2016). How Mastery Learning Works at  Scale.   [11] Rainey, R. G. (1965), The Effects of Directed Versus Non- Directed Laboratory Work on High School Chemistry  Achievement, Journal of Research in Science Teaching,  vol. 3, 1965, pp. 286-92.   [12] Wang, M. C.  and Stiles, B. (1976), An Investigation of  Childrens Concept of Self-Responsibility for Their School  Learning, American Educational Research Journal, vol. 13,  1976, pp. 159-79.   [13] Watanabe, Mari, & Sturmey, Peter. (2003). The Effect of  Choice-Making Opportunities During Activity Schedules on  Task Engagement of Adults with Autism. Journal of Autism  and Developmental Disorders, 33(5), 535-538.  doi:10.1023/A:1025835729718   [14] Zuckerman, M., Porac, J., Lathin, D., Smith, R., & Deci, E.  L. (1978). On the importance of self-determination for  intrinsically motivated behavior. Personality and Social  Psychology Bulletin, 4, 443446.Bowman, M., Debray, S.  K., and Peterson, L. L. 1993. Reasoning about naming  systems. ACM Trans. Program. Lang. Syst. 15, 5 (Nov.  1993), 795-825. DOI=  http://doi.acm.org/10.1145/161468.16147.            "}
{"index":{"_id":"24"}}
{"datatype":"inproceedings","key":"DiMitri:2017:LPM:3027385.3027447","author":"Di Mitri, Daniele and Scheffel, Maren and Drachsler, Hendrik and Borner, Dirk and Ternier, Stefaan and Specht, Marcus","title":"Learning Pulse: A Machine Learning Approach for Predicting Performance in Self-regulated Learning Using Multimodal Data","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"188--197","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027447","doi":"10.1145/3027385.3027447","acmid":"3027447","publisher":"ACM","address":"New York, NY, USA","keywords":"biosensors, learning analytics, machine learning, multimodal data, wearable enhanced learning","abstract":"Learning Pulse explores whether using a machine learning approach on multimodal data such as heart rate, step count, weather condition and learning activity can be used to predict learning performance in self-regulated learning settings. An experiment was carried out lasting eight weeks involving PhD students as participants, each of them wearing a Fitbit HR wristband and having their application on their computer recorded during their learning and working activities throughout the day. A software infrastructure for collecting multimodal learning experiences was implemented. As part of this infrastructure a Data Processing Application was developed to pre-process, analyse and generate predictions to provide feedback to the users about their learning performance. Data from different sources were stored using the xAPI standard into a cloud-based Learning Record Store. The participants of the experiment were asked to rate their learning experience through an Activity Rating Tool indicating their perceived level of productivity, stress, challenge and abilities. These self-reported performance indicators were used as markers to train a Linear Mixed Effect Model to generate learner-specific predictions of the learning performance. We discuss the advantages and the limitations of the used approach, highlighting further development points.","pdf":"Learning Pulse: a machine learning approach for predicting performance in self-regulated learning using  multimodal data  Daniele Di Mitri Daniele.Dimitri@ou.nl  Maren Scheffel Maren.Scheffel@ou.nl  Hendrik Drachsler Hendrik.Drachsler@ou.nl  Dirk Brner Dirk.Boerner@ou.nl  Stefaan Ternier Stefaan.Ternier@ou.nl  Marcus Specht Marcus.Specht@ou.nl  Welten Institute, Research Centre for Learning, Teaching and Technology Open University of the Netherlands  Valkenburgerweg 177, 6401 AT Heerlen  ABSTRACT Learning Pulse explores whether using a machine learning approach on multimodal data such as heart rate, step count, weather condition and learning activity can be used to pre- dict learning performance in self-regulated learning settings. An experiment was carried out lasting eight weeks involving PhD students as participants, each of them wearing a Fitbit HR wristband and having their application on their com- puter recorded during their learning and working activities throughout the day. A software infrastructure for collecting multimodal learning experiences was implemented. As part of this infrastructure a Data Processing Application was de- veloped to pre-process, analyse and generate predictions to provide feedback to the users about their learning perfor- mance. Data from different sources were stored using the xAPI standard into a cloud-based Learning Record Store. The participants of the experiment were asked to rate their learning experience through an Activity Rating Tool indicat- ing their perceived level of productivity, stress, challenge and abilities. These self-reported performance indicators were used as markers to train a Linear Mixed Effect Model to generate learner-specific predictions of the learning perfor- mance. We discuss the advantages and the limitations of the used approach, highlighting further development points.  CCS Concepts Applied computing  Computer-assisted instruction;  Keywords Learning Analytics, Biosensors, Wearable Enhanced Learn- ing, Multimodal data, Machine Learning  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027447  1. INTRODUCTION The permeation of digital technologies in learning is open-  ing up interesting opportunities for educational research. Flipped classrooms, ubiquitous and mobile learning as other technology-enhanced paradigms of instruction are enabling new data-driven research practices. Mobile devices, social networks, online collaboration tools as well as other digi- tal media are able to generate a digital ocean of data [9] which can be explored to find new patterns and insights. The opportunities that data opens up are unprecedented to educational researchers as they allow to analyse and under- stand aspects of learning and education which were difficult to grasp before.  The disruption lies primarily in how the evidence is gath- ered: data collection is embedded, on-the-fly and ever- present [5]. Collecting data is not enough to extract useful information: the data must be pre-processed, transformed, integrated with other sources, mined and interpreted. Re- porting on historical raw data only does not bring, in most of the cases, added value to the final user. As Li points out [18] individuals are already exposed to so many data they risk to drawn into data. What is instead more desirable is re- ceiving support in-the-moment which can prescribe positive courses of action, especially for twenty-first century learners which need to orient themselves continuously in an ocean of information with very little guidance [13].  Machine learning and predictive modelling can play a ma- jor role in extracting high-level insights which can provide valuable support for learners. Such ability highly depends whether the attributes taken in consideration to describe the learning experiences (the Input space) are descriptive for the learning process, they carry enough information to be able to accurately predict a change in the learning performance (the Output space). The relation between these two dimen- sions is further described in section 3.1.  The standard data sources in the reviewed predictive ap- plications are most of time Learning Management Systems (LMS) and the Student Information Systems. Looking only at clickstreams, keystrokes and LMS data alone gives a par- tial representation of the learning activity, which naturally occurs across several platforms [25]. Several authors have pointed out the need to explore data beyond the LMS [15] to be able to get more meaningful information of the learn-    ing process. We believe that an interesting alternative could be found in the Internet of Things (IoT) and sensor com- munity. Schneider et al. [24] have listed 82 prototypes of sensors that can be applied for learning. The employment of IoT devices allows collecting real-time and multimodal data about the context of the learning experience.  These considerations have shaped the motivation for the Learning Pulse experiment. The challenges it seeks to an- swer are the following: (1) define a set of data sources be- yond the LMS; (2) find an approach to couple multimodal data with individual learning performance; (3) design a sys- tem which collects and stores learning experience from dif- ferent sensors in a cloud-based data store; (4) find a suitable data representation for machine learning; (5) identify a ma- chine learning model for the collected multimodal data.  Learning Pulses main contribution to the Learning An- alytics community consists in outlining the main steps for a new practice to design automated multimodal data col- lection to provide personalised feedback for learning with the ultimate aim to facilitate prediction and reflection, the two most relevant objectives of learning analytics [14]. This proposed practice borrows the modelling approach from the machine learning field and uses it to model, investigate and understand human learning.  2. RELATED WORK Learning Pulse belongs to the cluster of Predictive Learn-  ing Analytics applications. The scope of this sub-field in Learning Analytics was framed by the American research in- stitute Educause with a manifesto [10] reporting some exam- ple applications, including Purdues Signals [1] or the Stu- dent Success System (S3) by Desire To Learn (D2L) [12]. These applications rely solely on LMS data for predicting academic outcomes or student drop-outs. Learning Pulse goes beyond those Predictive Analytics Applications by us- ing multimodal data from sensors to investigate the learning process.  The field of multimodal data was given more prominence in the last Conference Learning Analytics and Knowledge (LAK16) with the workshop Cross-LAK: learning analytics across physical and digital spaces [21]. The concept behind Learning Pulse was presented at the Cross-LAK workshop [8]. In this workshop, several topics were touched: data synchronisation [11], technology orchestration [20] or face to face collaboration settings [30].  With a mission similar to Learning Pulse, a data challenge workshop on Multimodal Learning Analytics (MLA16)1 took place at LAK16 for investigating learning happening on the physical or virtual world through multimodal data including speech, writing, sketching, facial expressions, hand gestures, object manipulation, tool use, artifact building.  Finally, there has been a paper by Pijeira Diaz et. al [22] who used a mutimodal data for Computer Supported Collab- orative Learning in a school setting. Although not focused on using machine learning, the link made with psychophys- iology theory introduce a novel research question, i.e. the possibility to infer psychological states including cognitive, emotional and behavioural phenomena from physiological responses such as sweat regulation, heart beat or breath [4].  1http://www.sigmla.org/mla2016/  3. METHOD The background exposed in the previous chapter has led  to the formulation of an overarching research question:  How can we store, model and analyse multimodal data to predict performance in human learning ((RQ-MAIN)  This main research question leads to three sub questions:  (RQ1) Which architecture allows the collection and storage of multimodal data in a scalable and efficient way  (RQ2) What is the best way to model multimodal data to apply supervise machine learning techniques  (RQ3) Which machine learning model is able to produce learner specific predictions on multimodal data  To further investigate these research questions, we de- signed the Learning Pulse experiment that involved nine PhD students as participants and generated a multimodal dataset of approximately ten thousands records.  3.1 Approach While frameworks already exist for standard within-the-  LMS Predictive Learning Analytics, e.g. the PAR Frame- work [27], there are no structured approaches to treat beyond- the-LMS data in the context of multimodal data. For this reason, in this work, a novel approach for predictive ap- plications inspired by machine learning is proposed. The objective is to learn statistical models out of the learning experiences and outcomes. Using a mathematical formalism that corresponds to learning a function f in the equation y = f(X), where X is a vector containing the attributes of one learning experience which work as the input of the function and, y is a particular learning outcome.  By using such an approach, three elements need to be further clarified: (1) the scope of investigation (the learn- ing context); (2) the attributes encompassed by multimodal data (the Input space); (3) the learning performance object of the predictions (the Output space).  3.1.1 Learning context The learning context investigated is self-regulated learning  (SRL) which is defined as the active process whereby learn- ers set goals for their learning and monitor, regulate, and control their cognition, motivation, and behaviour, guided and constrained by their goals and the contextual features of the environment [23]. Self-regulated learners are able to monitor their learning activity by defining strategic goals and that drive them not only to academic success, but lead to an increased motivation and personal satisfaction [31]. There is an overarching difference between self-regulated and non-self-regulated learners: the former are generally more engaged with their learning activities and desire to improve their learning performance [3]. On the contrary, the latter are less experienced, they do not perceive the relevance of their learning program and for this reason need to be fol- lowed closer by a tutor.  3.1.2 Input space Learning is a complex human process and its success de-  pends on several endogenous (e.g. psychological states) and    exogenous factors (e.g. learning contexts). Defining the In- put space consists of selecting the relevant attributes of the learning process and structuring them into a correct data representation. This modelling task is non-trivial: accord- ing to Wong [29] modern seamless learning encompasses up to ten different dimensions. In this project, two of them are of main interest: Space and Time. The Input space can be imagined as the sequence of events happening throughout the learning time across digital and physical environments as shown on the left of figure 1.  Learning in a digital space means mediated by a digital medium i.e. by technological devices like laptops, smart- phones or tablets. Digital learning data are easier to collect as most of the digital tools leave traces of their use. On the contrary, learning happening in the physical space refers to the learning not mediated by digital technology, like read- ing a book or discussing with a peer. Although the line between Digital and Physical gets blurred with the perva- siveness of technology, the bulk of the learning activities still happens offline and should be projected into data through a sensor based approach to be able to take advantage of those moments.  Time is also a relevant dimension: the data-driven ap- proach works best whenever the data collection becomes continuous and unobtrusive for the learner. This require- ment inevitably limits the scope of investigation only to tan- gible events whose values are easy to measure over time. If on the one hand, this constraint makes data collection eas- ier as there is no need to employ time-consuming surveys and questionnaires, on the other hand, this approach does not make it possible to directly capture psychological states which manifest during the learning.  Besides spanning across physical and digital space, the Input space of Learning Pulse can be grouped into three layers as shown in figure 1: those are 1) Body encompassing physiological responses and physical activity, 2) Learning Activities 3) and Learning Context.  Figure 1: Bi-spatial and three-layered Input Space  3.1.3 Output space The Output space of the prediction models corresponds to  the range of possible learning performances. These outputs are crucial for the machine learning algorithms to distinguish between successful learning moments from the unsuccessful ones. As self-regulated learners decide on their own learning goals and required learning activities, we need performance indicators which go beyond common course grades.  An interesting approach to measure learning productivity is the concept of Flow theorised by the Hungarian psycholo- gist Csikszentmihalyi. The Flow is a mental state of opera- tion that individuals experience whenever they are immersed  in a state of energised focus, enjoyment and full involvement with their current activity. Being in the Flow means feeling in complete absorption with the current activity and being fed by intrinsic motivation rather than extrinsic rewards [6]. In the model theorised by Csikszentmihalyi depicted in fig- ure 2, the Flow naturally occurs whenever there is a balance between the level of difficulty of the task (the challenge level is high) and the level of preparation of the individual for the given activity (the abilities are high).  Low High  High  Low  ArousalAnxiety Flow  Worry  Apathy  Control  RelaxationBoredom  C ha  lle ng  e  le  ve l  Skill level  Figure 2: Csikszentmihalyis Flow model  To measure the Flow we applied experience sampling [17]: the participants reported about their self-perceived learn- ing performance. As self-assessment is strictly subjective it has the advantage to be exclusively based on the learners personal feelings. If carefully designed, self-assessment can lead to models tailored on personal dispositions. This brings clear advantage in the context of self-regulated learning: what is perceived as good (or productive, stressful etc.) is classified as such, meaning that what is good is only what the learner thinks is good.  3.2 Participants and Tasks The experiment took place at the Welten Institute of the  Open University of the Netherlands involving nine doctoral students as participants, five males and four females, aged between 25 and 35 with a background in different disciplines including computer science, psychology and learning science. PhD students are good self-regulated learners, as they are generally experienced learners and have strong engagement and motivation with their tasks.  All participants were provided with a Fitbit HR wrist- band and installed the tracking software on their laptops. As sensitive data were collected, every participant signed an informed consent. In addition, to ensure their privacy, their personal data were anonymised making use of the alias ARLearn plus an ID between 1 and 9.  The experimental task requested from the study partici- pants was to continue their typical research activity through- out the day: the only additional action consisted in rating their learning activity every working hour between 7AM and 7PM (for the amount of hours they worked) through the Ac- tivity Rating Tool (described in sec. 3.4.1).  The actual experiment lasted for eight weeks and consisted of three phases: 0) Pre-test, 1) Training and 2) Validation. Phase 0: Pre-test . System infrastructure was tested in all its functionalities. A presentation was rolled out to intro-    duce the experimental setting and the studys rationale to the participants. Participants were instructed to set-up the data collection software on their laptop as well as the fitness wristband. Phase 1: Training . The first phase of the experiment lasted three weeks and consisted of the rating collection: participants have rated their activities hourly. The only vi- sualisation they could see at that point were the ratings during that day. The first phase was named training be- cause the collected data and ratings were necessary to train the predictive models. Phase 2: Validation . After two weeks of break, the sec- ond phase started lasting for another two weeks. In the Validation phase, the activity rating collection continued in a Learner Dashboard visualisation. The second phase was called Validation as its purpose was to compare the pre- dicted Performance indicators with the actual rated ones and to determine the prediction error.  3.3 Data sources  3.3.1 Biosensors The physiological responses and physical activity (Biosen-  sor data for short) in this study are represented by heart rate and step count respectively. The approach used to track these bodily changes consisted in making use of wearable sensors. The decision of the most suitable wearable tracker was dictated by following criteria: 1) heart rate tracking sensor; 2) price per single device; 3) accuracy and reliabil- ity of the measurements; 4) comfort and unobtrusiveness; 5) openness of the APIs and data for analysis.  The choice converged to Fitbit Charge HR2: standing out on the cost-quality trade off, Fitbit HR complied with all the requirements, in particular by offering open access to the collected data through the Fitbit API. Such way of ac- cessing data was beneficial on the one hand, as the software application developed for the project had to communicate exclusively with the Fitbit cloud datastore - while being ag- nostic to sensor trackers and their interfaces. The downside on the other hand was the dependence to the API specifi- cations: the maximum level of detail available was a heart rate value update every five seconds and step count update every minute.  It is relevant to point out the difference of the heart rate and step count signals: while the heart rate values are a continuous time-series, also called fixed event, the number of steps per minute is a random event as it represents a voluntary human activity and not an involuntary process as the heart beat. The value of step count at one time point is not dependent on the previous ones (i.e. is random) while the heart rate value at time t surely depends on the value at time t 1.  3.3.2 Learning Activities To monitor self-directed learning we decided to track PhD  students activities on their laptops, being those the main learning medium in which they perform their PhD activities. Given the variety of learning tasks executed by the partic- ipants during the experiment, the actual learning happens across different platforms including software applications, websites, web tools. To capture and represent this heteroge- neous complex of digital activities a software tracking tool  2https://www.fitbit.com/chargehr  was installed on the working laptop of the participants. The idea is that the use of a particular software or application adds up a valuable piece of information to consider when abstracting the learning process.  The tool chosen to monitor working efficiency was Res- cueTime, a time management software tool. RescueTime stores every five minutes (maximum level of detail allowed by its API specifications) into a proprietary cloud database an array containing the applications in use by the learner, weighted by their duration in seconds. Each activity in one interval has an activity ID and duration in seconds. The duration ranges between 1 and 300 (max seconds in five minutes), as the zero valued entries are the applications not used in an interval.  Given the diversity of research topics and learning tasks there is a high intersubject difference on the set of appli- cations used during the learning experience; apart from a few common applications, the majority of applications used are very sparse. To mitigate this problem applications were grouped into categories by hand. The name of the categories chosen were: 1) Browsing, 2) Communicate and Schedule, 3) Develop and Code, 4) Write and Compose, 5) Read and Consume, 6) Reference Tools, 7) Utilities, 8) Miscellaneous, 9)Internal Open Universiteit, 10) Sound and Music.  In figure 3, the distribution of the applications is compared with their categories. The height of the bars represents the number of executions that application had during the exper- iment, which equals to the presence of that application in one of the five-minute intervals. While in the left-hand chart the long tail effect due to the sparsity is quite noticeable, on the right hand side that does not appear.  Figure 3: Plots showing the number of executions per Applications (left), per Application category (right).  3.3.3 Performance indicators The indicators used in Learning Pulse are four: Stress,  Productivity, Challenge and Abilities. The four indicators were collected with the following questions.  1. Stress: how stressful was the main activity in this time frame  2. Productivity: how productive was the main activity in this time frame  3. Challenge: how challenging was the main activity in this time frame  4. Abilities: how prepared did you feel in the main ac- tivity in this time frame  Each participant had to rate each of these indicators retroac- tively with respect to the main activity performed in the    time frame being rated. The participants were expected to answer these questions at the end of every working hour from 7AM to 7PM using for each of them a slider in the Activity Rating Tool described in section 3.4.1 which translated the rating into an integer ranging from 0 to 100.  The Flow The Flow is operationalised trhough a single numerical in- dicator calculated based on the Challenge and Abilities in- dicators, as indicated by formula 1. i identifies a specific learners, while j references a specific time frame. Fij is the Flow score for the learner ith at the time frame jth; Aij and Cij is the level of Abilities and Challenge rated by the learner ith at the time frame jth.  Fij = (1 |Aij  Cij |)  |Aij + Cij |  2 (1)  Figure 4 plots the ratings of all the participants through- out the whole experiment in a two-dimensional space, where the x-axis are the level of Abilities and the y-axis is the level of Challenge. Both indicators are expressed as percentages. The dots in the scatter plot are coloured depending to their Flow-value calculated with the formula 1.  Figure 4: Scatter plot of the Flow of all study par- ticipants.  The colour scale used for the Flow goes from red over yel- low to green recalling the metaphor of a traffic light: high Flow values are green, medium ones are yellow and low Flow values are red. The plot visualises how the formula 1 works. The Flow is higher if two conditions apply: 1) the difference between Abilities and Challenge is small, meaning they are close to line x = y; 2) the mean between Abilities and Chal- lenge is close to one, meaning the observation falls into the top-right corner of the plot, which corresponds to the Flow zone, as in the original definition of Flow (see figure 2).  Besides the four questions also the Activity Type was sam- pled along with the GPS coordinates. The Activity Type was a categorical integer representing the following labels 1) Reading, 2) Writing, 3) Meeting, 4) Communicating, 5) Other.  The rationale behind this labelling was to have a hint on the nature of the main learning task executed during that  Figure 5: Plot showing the ratings given by one par- ticipant in one day.  time frame. Finally, the GPS coordinates consisted of two floating points which are the latitude and longitude of the location where the rating was submitted with the Activity Rating Tool.  Figure 5 shows the ratings of the four indicators of one participant during one day of the experiment, as well as the calculated Flow indicator. The background colours repre- sent the different activity types, as the legend visually indi- cates.  3.3.4 Environmental context The third data source is made up by the surrounding  context of learning as the environment might also have an impact on the final learning outcomes. The ideal solution would be to track information about the indoor surrounding environment, such as measuring the light intensity, humid- ity and heat inside the office, thus combining these with the information about the weather.  Given the lack of adequate sensors to employ in the of- fice environment, only the outdoor weather conditions were monitored. For each participant, the GPS coordinates were stored that allowed to call the weather data API through the online service OpenWeatherMap3 and to store weather data specific to the location from where each participant was operating. The weather API was called automatically every ten minutes for each of the nine participants. The attributes extracted from these statements were 1) Temperature, 2) Pressure, 3) Precipitation, 4) Weather Type, with the first three being floating points while the latter is a categorical integer.  3.4 Architecture Combining different Data Sources into a central data store  and processing them in real time is not a trivial task. Fig- ure 6 presents a transversal view of the system architecture which is divided into three layers.  At the top level, the Application Layer groups all the ser- vices that the end-user interfaces with including the Fitbit wristband and the RescueTime application here referred as Third Party Sensors. The Activity Rating Tool (ART) be- longs to the same level.  The middle level is the Controllers Layer which gathers the back-end components of the Applications. In this layer,  3https://openweathermap.org/    as figure 6 shows, the software is running on two server in- frastructures: the Cloud and the Virtual Machine. Not re- ported here are the controllers of the Third Party Sensors and the Learner Dashboard as the System Architecture de- scribed here is agnostic towards their implementation. On the Cloud side, there are the Learning Pulse Server, a script- ing software responsible for importing data from different APIs and storing them into the Learning Record Store. In addition, also running on the Cloud, there is the server soft- ware of the Activity Rating Tool which connects the client user interface with the database. The scripting software run- ning on the Virtual Machine is the Data Processing Server, which as the name indicates, implements the post-processing operations including data transformation, model fitting and predictions.  The lowest level is the Data layer. While the Third Party Services use their own APIs which receive regular queries by the importers of the Learning Pulse Server, the main datastore is the Learning Record Store. Consisting of a Fact Table and a Big Query Index, the Learning Record Store is the cloud-based database which collects the data about the learning experience of all participants. It also runs on the Cloud infrastructure and is further described in section 3.4.2.  Even though they are not directly part of the Learning Record Store, also the results of the Data Processing server are pushed into a datastore which is also shown in the Data Layer. This datastore is developed with a non-relational database and collects the predictions (also referred as fore- casts) and the transformed representation of the historical data, namely the learning experience data in the Learning Record Store opportunely processed and transformed. Fi- nally, the Data Processing Server makes use of further per- sistent data, as for example the Learners Models, which are stored locally, reused constantly and regenerated once a day.  3.4.1 Activity Rating Tool Responsible for collecting the participants ratings about  their learning experience, designed and developed as a scal-  Figure 7: Two screenshots of the Activity Rating Tool : on left side the list of time frames available for rating, on the right the rating form of a time frame.  able web application, the Activity Rating Tool runs App Engine using webapp2 lightweight Python web framework. While the back-end was written in pure Python, the front- end uses Bootstrap4.  The interface of the tool was designed to be as intuitive as possible and with the aim to make the rating action quick and easy for the participants considering they needed to use it several times a day. Figure 7 shows two screenshots of the applications main page; on left-hand side, it shows the list of all the past time frames between 7AM and the hour previous to the current. To rate a time frame the form shown on the right-hand side of figure 7 opened. There users are asked to  4http://getbootstrap.com/  Learning Pulse Server  Learning Record Store  Data Processing Application  Activity Rating Tool  Third party APIs  ART  serverImporter  Third party Sensors  Fitbit Rescue Time  Fitbit  API  RescueTime  API  ART client  BigQuery  index  Facts  Table  DATA LAYER  Learner  Dashboard  User  models  Transformer  Prediction  engine  Model  updater  History  Forecasts  CONTROLLERS LAYER  APPLICATION LAYER  Synchroni zer  OpenWeather API  Cloud Virtual Machine  Copyright   Daniele Di Mitri   Figure 6: System architecture of Learning Pulse.    select the Activity Type through five different icons; below, users can input the rating for the four indicators through four sliders, differently coloured for each indicator. Once the desired values are chosen, the sliders translate the position of the slide into an integer between 0 and 100. To prioritise straightforwardness and to avoid information overload, the guiding questions were hidden into a help tool-tip at the right-hand side of the sliders.  Once the participant pressed Submit the time frame turned green coloured in the time frame list. The partic- ipant could also delete ratings or resubmit in case of errors. Additionally, a Daily Rating Plot is shown just before the Submit button which shows the past ratings recorded that day with the purpose of reminding the participant their pre- vious ratings that day in order to support a coherent overall rating.  3.4.2 Learning Pulse Server The Learning Pulse Server is the script component respon-  sible for pulling the data from the third party APIs and transforming them into learning records and handing out their identifiers. The learning records are first stored into the Fact Table by assigning a UUID (Universally Unique Identifiers). The Learning Pulse Server script and the Fact Table were implemented as application and data store in the Cloud, which allowed to balance the load of data on a distributed architecture for scalability purposes. From the Fact Table, the data were synchronised into a Query Index, implemented with a scalable non-relational database, which contrarily to the Fact Table, allowed to query the distributed learning statements with SQL language. The synchronisa- tion between the Fact Table and the Query Index happens using a queue, such that no learning record could get lost.  While the Learning Pulse Server is the application script responsible for pushing and pulling the learning records, the Fact Table and the Query Index together form the LRS. Implementing the LRS with a cloud-based solution allowed to achieve properties such as (1) high availability: the LRS could be reached at any time, with respect to the privileges of the client; (2) high scalability: although the size of the data collected was about 1 Gigabyte the number of learn- ing statements could easily scale up tens or even hundreds of times more; (3) high reliability: the cloud infrastructure chosen provided performance and security.  3.4.3 Experience API The chosen data format for the learning records was the  Experience API (or xAPI) data standard, an open source API language through which systems send learning informa- tion to the LRS. XAPI is a RESTful web service, with a flex- ible standard which aims at interoperability across systems. The XAPI standard has the format actor-verb-object and are generated and exchanged in JSON format, opportunely validated by and stored in the LRS. The main advantage of using xAPI is interoperability: learning data from any sys- tem or resource can be captured and eventually queried by the third party authenticated services. For each event cap- tured in Learning Pulse, an xAPI statement template was designed following the Dutch xAPI specification for learning activities [2] 5.  5A list of the statements can be found here http://bit.ly/ DutchXAPIreg  3.5 Data processing After being stored in the LRS, learning records were pro-  cessed, transformed and mined in order to generate predic- tions to be shown to the learners. Data collection and Data processing can be seen as two legs which walk side by side, complementing each others role. The data processing soft- ware was named Data Processing Application6 (DPA) and its main responsibilities consisted in (1) fetching the data from the Learning Record Store; (2) transforming the new data by time resampling and features extraction; (3) learn- ing and exploiting different regression models; and (4) stor- ing the results of the regression.  The DPA needed to run continuously on a server always- on without the need of human interaction. Other important requirements for the DPA were the possible integration with other software components (e.g. interfacing with the LRS) and availability of statistical and Machine Learning tools. The final choice converged on using Python as the main programming environment, mainly because of its flexibility and wide support for data analysis.  Learning  Record  Store  User  models  Importer Model fitting  OpenWeatherMap  Virtual Machine  History Forecasts  Prediction  Update  modelNew data  Scheduler  NO YES  Transformer  YES NO  DATA  CONTROLLERS  Third party API  BigQuery  Copyright   Daniele Di Mitri   Figure 8: The data processing workflow  For the Data Processing Server, namely the computer in- frastructure which hosted the DPA, cloud options were con- sidered including popular cloud IaaS solutions. For financial reasons, the choice directed towards an in-house server solu- tion constituting of a Virtual Machine running an OpenSuse Linux distribution.  The diagram in figure 8 shows the data processing work- flow, a close-up of the system architecture shown in section 3.4. The figure is divided into three layers: the controllers, the data and the visualisations.  3.5.1 Data fetching A cron-job on the Virtual Machine activated the scheduler  every ten minutes, every working day, from 7AM to 7PM. The main task of the scheduler was to query the Learning Record Store and to realise whether new intervals could be formed based on the learning records retrieved. In order to be valid, the learning intervals have to be completed for Biosensor, Activity and Weather data. If any of these data are not available, the execution of the Data Processing Ap- plication is interrupted and postponed to the next round. To connect to the Learning Record Store, the DPA uses Pan- das Big Query connector. This interface can authenticate 6The source code of the Data Processing Appli- cation is available at https://github.com/WELTEN/ learning-pulse-python-app    the client (the DPA Python script) to the Big Query service, submit a query and fetch the results that are returned into a data frame, the popular data format for structuring tabular data in Pandas.  3.5.2 Multi-instance representation Each data source had its own frequency of data genera-  tion: the ratings were submitted every hour, the heart rate was updated every five seconds, the step count every minute, the activities every five minutes and the weather every ten minutes. That resulted in the so-called relational represen- tation as for each participant a different number of relations corresponded with all the other entities depending on how frequent their values were updated. Relational representa- tions are not ideal for machine learning as the input space which needs to be examined can become very broad [7].  The problem was therefore translated into a multiple in- stance representation where each training sample is a fixed length time interval. The interval length is determined by how frequently the labels i.e. the ratings, are updated. As the ratings here equal the working hours (say 8 hours), if multiplied by the experiment days (say 15), that would re- sult in the best-case scenario of 120 samples for each par- ticipant, which is too small in size for a training set. To overcome this problem the compromise was found selecting 5 minutes long intervals. This decision, however, triggered an- other problem, what to do with those attributes that are up- dated more or less frequently. The approach used was differ- ent for each entity. Ratings, which are updated hourly, were linearly interpolated; the step count, which is updated every minute, was aggregated with a sum function; the weather, which was updated every 10 minutes, was copied backwards; the activities came already with a five minutes frequency, therefore no action was required. Finally, to represent a five minutes heart rate signal into one or more features, the best solution was to use different aggregate functions, namely: 1) the minimum of the signal, 2) the maximum, 3) the mean, 4) the standard deviation and 5) the average change - i.e. the mean of the absolute value of the difference between two consequent data points. This naive approach consists in plugging in several different features and letting the ma- chine learning algorithm decide which ones are the most in- fluential on predicting the output. It is, however, useful to point out that more sophisticated techniques for feature ex- traction from the heart rate exist, such as the Heart Rate Variability [28] or the Sample Entropy.  3.5.3 Data storing Similarly to the data collection, also the data processing  had to be the same. In order not to repeat the processing step of the same data multiple times, it was convenient to store the results of the transformation in a permanent data store, to be able to retrieve it when necessary. To do so a Big Query table was created called History : the name was used to differentiate the transformed historical data with the forecast about the future, whose table is called Forecasts.The Big Query was preferred over other solutions since the LRS was developed with the same technology. In addition, Pan- das offers an easy Big Query interface, which allows to push and pull data easily from the Cloud Database.  3.6 Regression approach As the collected data were longitudinal, the fixed effects  showed stochastic behaviour implying that the observations were highly dependent on one another. In formal terms, this means that observing the behaviour of one participant at time t, the output variable yt is described by the equa- tion yt = +Xt + et. The dependence among the samples means that given a later observation at time t + 1, the co- variance cov(et, et+1) 6= 0 with t 6= t+ 1.  As the samples were intercorrelated it was not possible to employ common regression models, as most of these tech- niques assume that the residuals are independent and iden- tically distributed normal random variables. Treating cor- related data as if they were independent can yield wrong p-values and incorrect confidence intervals. To overcome this problem the approach chosen was to Linear Mixed Ef- fect Models (LMEM).  LMEM relax the dependency constraint of the data and they can both treat data of mixed nature, including fixed and random effects, plus they describe the variations of the response variables with respect to the predictor variables with coefficients that can vary for each group [19]. In for- mal terms, the LMEM as described by [16] consist in a ni- dimensional vector y for the i-th subject:  yi = Xi + Zii + i, i = 1, ...,Mi  N(0,) (2)   ni is the number of samples for subject i  Y is a ni dimensional vector of response variables  X is a ni  kfe dimensional matrix of fixed effects coeffi- cients   is a kfe-dimensional vector of fixed effects slopes  Z is a ni  kre dimensional matrix of random effects coef- ficients   is a kredimensional random vector with mean zero and covariance matrix; each subject gets its own independent    is a nidimensional within-subject error with mean 0 and variance 2 with a spherical Gaussian distribution.  4. ANALYSIS AND RESULTS At the end of the experimental phase, the transformed  dataset presented the following characteristics: a total of 9410 five-minute learning samples, counting for all nine par- ticipants. The biggest sample size was ARLearn5 with 1725 samples, while the one with the smallest number of samples was ARLearn4 with 514. There were 29 attributes in total.  As a single-output LMEM implementation was chosen, five different models were learnt each of them having as re- sponse variable one of the five performance indicators (Abili- ties, Challenge, Productivity, Stress and Flow). The models were initialised with the following parameters:   Fixed Effects: timeframe, latitude, longitude, weath- erConditionId, pressure, temp, humidity, hr min, hr avc, hr mean, hr std, hr max  Random Effects: Browsing, Communicate Schedule, De- velop Code, Internal OU, Miscellaneous, Read Consume, Ref- erence, Sound Music, Utilities, Write Compose, Steps.  As the way of rating of each participant was different, the predicted values were normalised with respect to the learner- specific historical min and max using the following formula.    xnew = (xmax  xmin)  xi  100 + xmin  For the evaluation of the predicted results we used R- squared, a statistical measurement which scores how close the data are to the regression line and outputs a number from 0 and 1 which measures the goodness-of-fit of the model. The results obtained were the following: Stress: 0.32, Chal- lenge: 0.22, Flow score: 0.16, Abilities: 0.08, Productivity: 0.05.  5. DISCUSSION The first question (RQ1) focused on the best architectural  setup to process multimodal data. The answer found to the question was satisfactory as architecture design discussed in section 3.4 was capable of: (1) importing a great num- ber of learning statements from the sensors and their APIs; (2) feeding the statements into a cloud-based LRS avoiding collisions among them and information loss; (3) combining the statements with the self reports regularly provided by the learners; (4) programmatically transforming the learn- ing statements by extracting relevant attributes and by re- sampling into uniform intervals; (5) fitting the predictive model on historical observations and saving for the reuse with the newer observations and (6) saving the predictions in a separate store to be able to compare with the actual val- ues. On the other hand, the architectural design had some limitations. First of all, it exhibited a real-time syncing issue: the data synchronisation with the wearable trackers was slower than expected; in the best case scenario, the data about the heart rate and the steps were available in the LRS only 15 to 20 minutes later. Secondly, the Data Processing Server hosting the Data Processing Application was poor in performance: the weak processing power slowed down the data processing and that resulted in long job cycles.  The second research question (RQ2) was concerned with finding the best way to model multimodal data suitable for machine learning. The solution found was to treat the prob- lem using a Multiple Instance Representation as detailed in section 3.5.2, i.e. using a tabular representation where each row represents a five minute learning interval and each col- umn a different attribute. This representation helped to overcome the problems derived from the relational nature of the collected data. Additionally, third party APIs influenced a lot the type of data that is possible to be retrieved from the sensors. An example is the Fitbit Charge HR, whose API only allows to get values of the heart rate every five seconds and no inter-beat distance. This scarcity of available data did not allow to calculate useful measurements on the heart rate, like the Heart Rate Variability which has been proven to be a good predictor for workload stress [26].  The third research question (RQ3) asked which machine learning model for regression is best suited for the hetero- geneous type of data. The solution discussed in section 3.6 consisted in using the Linear Mixed Effect Models as they al- low (1) taking into account data specific to each learner; (2) distinguishing between fixed and random effects; (3) tak- ing categorical data into account. Despite LMEM being the appropriate model for the intended task, the R-squared evaluation test yielded poor prediction accuracies for the five outputs. One possible reason might be the sparsity of random effects, especially those that refer to the least used  activity categories (whose distribution is shown in figure 3). We observed that while adding up sparse attributes (ran- dom effects) as predictors decreases the prediction accuracy, fixed effects improve the general accuracy.  The answers to the three sub research questions provide an answer to the main research question (RQ-MAIN): a way to store, model and analyse multimodal data was successfully found. Nevertheless the limited significance of the prediction results does not allow us to assert that accurate and learner- specific predictions can be generated. This might have been caused by: 1) the combination of multimodal data selected in the experiment; 2) no clear learning task to be executed, high variance of the learning context explored; 3) sparse ran- dom effects were still too many as opposed to fixed effects.  6. CONCLUSIONS This paper described Learning Pulse, an exploratory study  whose aim was to use predictive modelling to generate timely predictions about learners performance during self-regulated learning by collecting multimodal data about their body, ac- tivity and context. Although the prediction accuracy with the data sources and experimental setup chosen in Learning Pulse led to modest results, all the research questions have been answered positively and have lead towards new insights on the storing, modelling and processing multimodal data.  We raise some of the unsolved challenges that can be con- sidered a research agenda for future work in the field of Pre- dictive Learning Analytics with beyond-LMS multimodal data. The ones identified are: 1) the number of self-reports vs unobtrusiveness; 2) the homogeneity of the learning task specifications; 3) the approach to model random effects; 4) alternative machine learning techniques.  There is a clear trade-off between the frequency of self- reports and the seamlessness of the data collection. The number of self-reports cannot be increased without worsen- ing the quality of the learning process observed. On the other side, having a high number of labels is essential to make supervised machine learning work correctly.  In addition, a more robust way of modelling random ef- fects must be found. The found solution to group them man- ually into categories is not scalable. Learning is inevitably made up by random effects, i.e. by voluntary and unpre- dictable actions taken by the learners. The sequence of such events is also important and must be taken into account with appropriate models.  As an alternative to supervised learning techniques, also unsupervised methods can be investigated, as with those methods fine graining the data into small intervals does not generate problems with matching the corresponding labels also the amount of labels is no longer needed.  Regarding the experimental setup, it would be best to have a set of coherent learning tasks that the participants of the experiment need to accomplish, contrarily to as it was done in Learning Pulse, where the participants had com- pletely different tasks, topics and working rhythms. It would be also useful to have a baseline group of participants, which do not have access to the visualisations while another group does have access; that would allow to see the difference of performance, whether there is an actual increase.  To conclude, Learning Pulse set the first steps towards a new and exciting research direction, the design and the de- velopment of predictive learning analytics systems exploiting multimodal data about the learners, their contexts and their    activities with the aim to predict their current learning state and thus being able to generate timely feedback for learning support.  Acknowledgements The Learning Pulse project was partially funded by the Learning Analytics Community Exchange (LACE) project (grant no. 619424).  References [1] K. E. Arnold. Signals: Applying Academic Analytics.  EDUCAUSE Quarterly, 33:8792, 2010. [2] A. Berg, M. Scheffel, H. Drachsler, and M. Specht.  Dutch Cooking with xAPI Recipes The Good, the Bad, and the Consistent. In Proceedings of the Interna- tional Conference on Advanced Learning Technologies (ICALT16), pages 234236, 2016.  [3] D. L. Butler and P. H. Winne. Feedback and Self- Regulated Learning: A Theoretical Synthesis. Review of Educational Research, 65(3):245281, 1995.  [4] J. Cacioppo, L. G. Tassinary, and G. G. Berntson. The Handbook of Psychophysiology, volume 44. Cambridge University Press, 2007.  [5] B. Cope and M. Kalantzis. Interpreting Evidence-of- Learning: Educational research in the era of big data. Open Review of Educational Research, 2(1):218239, 2015.  [6] M. Csikszentmihalyi. Finding flow: The psychology of engagement with everyday life. Basic Books, 1997.  [7] L. De Raedt. Logical and relational learning, volume 5249 LNAI. Heidelberg, Springer-Verlag Berlin, 2008.  [8] D. Di Mitri, M. Scheffel, H. Drachsler, D. Borner, S. Ternier, and M. Specht. Learning Pulse: using Wear- able Biosensors and Learning Analytics to Investigate and Predict Learning Success in Self-regulated Learn- ing. CEUR proceedings, pages 16, 2016.  [9] K. DiCerbo and J. Behrens. Impacts of the digital ocean on education. In London: Pearson, number February. London: Pearson, 2014.  [10] ECAR. The Predictive Learning Analytics Revolution: Leveraging Learning Data for Student Success. ECAR working group paper., pages 123, 2015.  [11] V. Echeverra, F. Domnguez, and K. Chiluiza. Towards a distributed framework to analyze multimodal data. CEUR Workshop Proceedings, 1601:5257, 2016.  [12] A. Essa and H. Ayad. Improving student success using predictive models and data visualisations. Research in Learning Technology, 5:5870, 2012.  [13] R. Ferguson and S. B. Shum. Social learning analyt- ics. Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK 12, pages 2333, 2012.  [14] W. Greller and H. Drachsler. Translating Learning into Numbers : A Generic Framework for Learning Analyt- ics Author contact details :. Educational Technology & Society, 15(3):42  57, 2012.  [15] K. Kitto, S. Cross, Z. Waters, and M. Lupton. Learn- ing Analytics beyond the LMS: the Connected Learn- ing Analytics Toolkit. Proceedings of the Fifth Interna- tional Conference on Learning Analytics And Knowl- edge (LAK 15). ACM, New York, NY, USA, pages 1115, 2015.  [16] N. M. Laird and J. H. Ware. Random-effects models for longitudinal data. Biometrics, 38(4):96374, 1982.  [17] R. Larson and M. Csikszentmihalyi. The Experience Sampling Method., 1983.  [18] I. Li. Beyond Reflecting on Personal Data: Predictive Personal Informatics. In Beyond Personal Informatics: Designing for Experiences with Data CHI 2015, pages 15, 2015.  [19] M. Lindstrom and D. Bates. Newton-Raphson and EM Algorithms for Linear Models for Repeated-Measures Data. Journal of the American Statistical Association, 83(404):10141022, 1988.  [20] R. Martinez-Maldonado. Seeing learning analytics tools as orchestration technologies: Towards support- ing learning activities across physical and digital spaces. CEUR Workshop Proceedings, 1601:7073, 2016.  [21] R. Martinez-Maldonado, D. Suthers, N. R. Aljohani, D. Hernandez-Leo, K. Kitto, A. Pardo, S. Charleer, and H. Ogata. Cross-LAK: Learning analytics across phys- ical and digital spaces. In ACM International Confer- ence Proceeding Series, pages 486487, 2016.  [22] H. J. Pijeira-Daz, H. Drachsler, S. Jarvela, and P. A. Kirschner. Investigating collaborative learning success with physiological coupling indices based on electro- dermal activity. Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK 16, pages 6473, 2016.  [23] P. R. Pintrich Zusho, A. Student motivation and self- regulated learning in the college classroom. In The scholarship of teaching and learning in higher educa- tion: An evidence-based perspective, pages 731810. Springer, 2007.  [24] J. Schneider, D. Borner, P. van Rosmalen, and M. Specht. Augmenting the Senses: A Review on Sensor-Based Learning Support. Sensors, 15(2):4097 4133, 2015.  [25] D. Suthers and D. Rosen. A unified framework for multi-level analysis of distributed learning. In Pro- ceedings of the 1st International Conference on Learn- ing Analytics and Knowledge - LAK 11, pages 6474. ACM, 2011.  [26] J. Taelman, S. Vandeput, a. Spaepen, and S. V. Huffel. Influence of Mental Stress on Heart Rate and Heart Rate Variability. Ecifmbe 2008, 29(1):13661369, 2009.  [27] E. Wagner and B. Davis. The Predictive Analytics Re- porting ( PAR ) Framework , WCET. Educase Review Online, pages 18, 2014.  [28] H. M. Wang and S. C. Huang. SDNN/RMSSD as a surrogate for LF/HF: A revised investigation. Modelling and Simulation in Engineering, 2012, 2012.  [29] L.-H. Wong. A learner-centric view of mobile seamless learning. British Journal of Educational Technology, 43(1):E19E23, 2012.  [30] M. Wong-Villacres, R. Granda, M. Ortiz, and K. Chiluiza. Exploring the impact of a tabletop- generated group work feedback on students collabora- tive skills. CEUR Workshop Proceedings, 1601:5864, 2016.  [31] B. Zimmerman. Becoming Learner: Self-Regulated Overview. Theory into Practice, 41(2):6470, 2002.    "}
{"index":{"_id":"25"}}
{"datatype":"inproceedings","key":"Lau:2017:TSL:3027385.3027443","author":"Lau, Clarissa and Sinclair, Jeanne and Taub, Michelle and Azevedo, Roger and Jang, Eunice Eunhee","title":"Transitioning Self-regulated Learning Profiles in Hypermedia-learning Environments","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"198--202","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027443","doi":"10.1145/3027385.3027443","acmid":"3027443","publisher":"ACM","address":"New York, NY, USA","keywords":"information-processing theory, latent transition analysis, metacognition, metamemory, self-regulated learning","Abstract":"Self-regulated learning (SRL) is a process that highly fluctuates as students actively deploy their metacognitive and cognitive processes during learning. In this paper, we apply an extension of latent profiling, latent transition analysis (LTA), which investigates the longitudinal development of students' SRL latent class memberships over time. We will briefly review the theoretical foundations of SRL and discuss the value of using LTA to investigate this multidimensional concept. This study is based on college students (n ","pdf":"Transitioning self-regulated learning profiles in  hypermedia-learning environments   Clarissa Lau1, Jeanne Sinclair2, Michelle Taub3, Roger Azevedo3, Eunice Eunhee Jang1     1Department of Applied Psychology and Human Development, Toronto, ON, Canada  2Department of Curriculum, Teaching and Learning, University of Toronto, Toronto, ON, Canada   3Department of Psychology, North Carolina State University, Raleigh, NC, USA     {clarissa.lau, jeanne.sinclair, eun.jang} @mail.utoronto.ca; {mtaub, razeved} @ncsu.edu    ABSTRACT  Self-regulated learning (SRL) is a process that highly fluctuates as  students actively deploy their metacognitive and cognitive  processes during learning. In this paper, we apply an extension of  latent profiling, latent transition analysis (LTA), which  investigates the longitudinal development of students SRL latent  class memberships over time. We will briefly review the  theoretical foundations of SRL and discuss the value of using  LTA to investigate this multidimensional concept. This study is  based on college students (n = 75) learning about the human  circulatory system while using MetaTutor, an intelligent tutoring  system that adaptively supports SRL and targets specific  metacognitive SRL processes including judgment of learning  (JOL) and content evaluation (CE). Preliminary results identify  transitional probabilities of SRL profiles from four distinct events  associated with the use of SRL.      CCS Concepts   Applied computing~Interactive learning environments     Computing methodologies~Latent variable models    Keywords  Self-regulated learning; Information-processing theory;  Metacognition; Metamemory; Latent transition analysis                 Permission to make digital or hard copies of all or part of this  work for personal or classroom use is granted without fee  provided that copies are not made or distributed for profit or  commercial advantage and that copies bear this notice and the full  citation on the first page. Copyrights for components of this work  owned by others than ACM must be honored. Abstracting with  credit is permitted. To copy otherwise, or republish, to post on  servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13-17, 2017, Vancouver,  BC, Canada   2017 ACM. ISBN 978-1-4503-4870- 6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027443   1 INTRODUCTION  In dynamic learning environments such as intelligent tutoring  systems and other advanced learning technologies, students  continuously adapt to internal (e.g., prior knowledge) and external  (e.g., effectiveness of systems; feedback and scaffolding)  conditions by rapidly changing as they interact with the tasks and  the system, based on accurate use of cognitive and metacognitive  processes.     This progressive change occurs over multiple stages that require  high-degree of regulation; however, previous research has  indicated that students do not automatically develop an ability to  regulate their SRL knowledge and skills [1, 3]. This warrants a  need to clarify how students initially develop regulatory behaviors  and how they shift towards more advanced levels of regulation.  This paper will investigate the transitional profiles of two aspects  of self-regulated learning in the context of an online learning  environment, MetaTutor.      1.1 Self-regulated learning  Over the course of engaging with learning in a technologically  rich environment, students are engaged in high complex learning  processes. Students are expected to acquire knowledge, respond to  feedback, and evaluate progress  processes that require high- levels of regulation. In order to achieve success during the  complex learning process, students must gain agency in directing  their own learning. Self-regulated learning (SRL) describes  students accurate monitoring and regulation of effective  cognitive, metacognitive, and motivational knowledge, skills, and  strategies to learn, problem solve, and perform across tasks [12,  19]. SRL is a multi-phase process with the following assumptions:  (1) students plan, monitor, control, and reflect their learning  strategies and progress [12], and (2) this involves a three-stage  cyclical process where students engage in orientation and  planning phase, monitoring and performance phase, and  evaluation and adaptation phase [18]. These theoretical models of  SRL collectively suggest that a combination of knowledge, skills,  and strategies are essential and occur in a sequential and/or  cyclical manner during learning and problem solving. While these  models do describe SRL as phases, only one model considers SRL  as an event that unfolds over time  information-processing theory  (IPT) [16, 17]. Since this study is examining how SRL profiles  transition over time, this study will draw specifically from the  information-processing theory of SRL.           1.2 Information-processing theory of SRL  According to the information-processing theory of SRL [16, 17],  learning occurs through a series of four cyclical (yet not  necessarily sequential) stages, in which information processing  and SRL occur. In phase 1, task understanding, students must be  able to clearly understand the task they are being asked to  complete. In phase 2, goals and plans, students set appropriate  goals for accomplishing the given task, along with plans for how  they expect to achieve those goals. Phase 3, using learning  strategies, involves students using the learning strategies (i.e.,  cognitive, metacognitive, and motivational SRL processes) they  planned in the previous phase. In phase 4, adaptation, students  can make changes to their goals, plans, and use of learning  strategies to make sure they are using them efficiently.  Furthermore, within each phase, information processing occurs as  the interaction of ones conditions, operations, products,  evaluations, and standards (COPES). Thus, according to this  model, students control and monitor their use of information  processing and SRL strategies during each phase of learning.    1.3 Model of metamemory   Nelson & Narens [10] model of metamemory suggests that  metacognition occurs at a meta-level, which involves three phases  of memory-related processes. Thus, engaging in these  metacognitive processes at the meta-level can influence task  performance at the object level. Specifically, at the encoding  phase, students engage in ease of learning (EOL) judgments,  where they anticipate how easy it will be to accomplish a task. At  the retrieval phase, students are making judgments of their  understanding of the material (i.e., judgment of learning or JOLs).  At the retention phase, students make a retrospective confidence  judgment (RCJ), where they judge their confidence in their  performance on the task. According to this model, making these  metacognitive judgments may impact students task performance,  via memory processes.    1.4 Latent profiling of SRL   A challenge in understanding SRL is capturing the interactive  nature of information processing and SRL strategies that students  deploy in real time during learning. Constructs such as SRL are  often difficult to observe directly, so traditional research methods  (e.g., self-reports) will typically infer this behavior after the task is  completed. However, these methods can hardly capture the  dynamic and progressive stages of SRL. Recent advances have led  researchers to use on-line trace methods (e.g., log-files, concurrent  think-alouds) to measure the real-time deployment of SRL  processes during learning [1, 2]. The primary difference between  traditional techniques and latent profiling is: the common factor  model decomposes the covariances to highlight relationships  among the variables, whereas the latent profile model decomposes  the covariances to highlight relationships among students [4, p.  6]. Latent profiling is a latent variable model that identifies  unobserved subgroups in a population. One study established  distinct latent profiles of SRL that exist across students and these  profiles are associated with varying academic outcomes [3].  Findings from this study suggested that individuals with minimal  and disorganized profiles of SRL might demonstrate poorer  academic outcomes [3]. This approach is a person-centered  analysis that is more appropriate for understanding a multi-phased  concept such as SRL.     This study is situated to investigate the unique characteristics of  students self-regulatory learning behaviors at different stages of  interaction in the online learning environment, MetaTutor. During  MetaTutor, metacognitive judgments during learning are  measured by assessing students level of confidence in making  these judgments. This study aims to elucidate unique student  profiles of SRL and describe the changing states across time.     2 METHOD  2.1 Participants  A total of 194 students are included in the study with 99 in the  control group and 95 in the experimental group. From the  experimental group, a total of 75 students are included in this  ongoing study. Students were recruited from three major  universities across North America. Background information was  available for only 70 students. Of the 70 students, 53% were  males and 47% were females. Mean age was M = 20.11 (SD =  .21) and mean GPA was M = 3.31 (SD = .09).    2.2 MetaTutor  This study is set in the context of MetaTutor, an intelligent  tutoring system that is built upon theoretical models of SRL [1,  13, 16, 17, 18]. The system engages students in biological  concepts (e.g., the human circulatory system) and provides  support through offering various strategies to plan, monitor, and  reflect upon their learning. Students are prompted to set goals  with respect to their learning and engage in cognitive and  metacognitive SRL strategies while the system provides feedback  based on student scores on quizzes and responses to making  metacognitive judgments, such as JOLs. During the process of  reading and inspecting hypermedia materials, students are given  an option to select cognitive learning strategies, such as  summarizing (SUMM), taking notes (TN), planning (PLAN),  prior knowledge activation (PKA), and making inferences (INF),  and metacognitive monitoring processes such as monitoring  progress towards goals (MPTG), judgment of learning (JOL),  feeling of knowing (FOK), and content evaluation (CE) from an  SRL palette provided by the systems interface. These processes  include: This study will examine two of these processes: judgment  of learning (JOL) and content evaluation (CE). Below is a  screenshot of the MetaTutor main interface (See Figure 1). The  interface includes elements designed to foster effective SRL, such  as a timer (top left), a table of contents (left, below the timer),  content (center), overall learning goal and sub-goals (top center),  a pedagogical agent (top right), and the SRL palette (right, below  the agent).    Participants background knowledge of biological concepts  ranged from no experience to some experience (i.e., taken 1-5  undergraduate science courses). Prior to learning, participants  were randomly assigned to one of two experimental conditions:  prompt and feedback, or control. In the prompt and feedback  condition, participants were prompted to use cognitive and  metacognitive SRL strategies, and were provided with immediate  feedback on their performance. Additionally, participants could  self-initiate the use of these SRL processes, and were still  provided feedback on their performance. In the control condition,  participants were not prompted to engage in SRL strategies, nor  were they provided with feedback if they self-initiated the use of  these processes. As such, participants in the control condition  were left to work independently without being provided with any  guidance from the system.           2.3 Metacognitive monitoring process  For this analysis, we selected the metacognitive monitoring  processes: judgment of learning (JOL) and content evaluation  (CE), as research has shown these processes to be influential in  learning about complex science topics [1, 5, 7].    2.3.1 Judgment of learning  Judgments of learning (JOLs) require students to assess their  understanding of the material they are currently reading [1, 2, 5,  7]. This strategy requires learners to monitor their comprehension  of the material, which is an effective strategy as it makes them  actively aware of their progress during learning. Moreover, once  this judgment is made, students can select the appropriate  subsequent cognitive learning strategies to use, based on the  valence of that judgment (i.e., JOL+: I understand this or JOL-: I  do not understand this). For example, a student who does  understand the material can go on to take notes on the content  (i.e., cognitive strategy of taking notes) compared to a student  who does not understand the material, who should re-read the  material as the subsequent cognitive learning strategy. It is  important to emphasize that valence is associated with the  accuracy of a metacognitive judgment and we can therefore  assume that a metacognitive-strategy dyad can either be adaptive  (e.g., a JOL- followed by re-reading the same materials) or  maladaptive (e.g., a JOL- followed by reading additional  research).     2.3.2 Content evaluation  Content evaluations (CEs) involve students assessing whether the  current content is relevant to the sub-goal they are working on [1,  2, 6, 7] making them aware of the relevancy of the material they  are reading. When making a CE, the student evaluates the  relevancy of the text and the diagram on the content page, thereby  monitoring the material they are reading to accomplish their sub- goals. Similar to JOLs, there is also a valence component  associated with making this metacognitive evaluation, such that  the content can be relevant to the current sub-goal (i.e., CE+),  which should be followed by continuing to read that page, inspect  the diagram, and take notes or summarize the material. In  contrast, an evaluation of the content as being irrelevant to the  current sub-goal (i.e., CE-), would be followed by changing pages  to a relevant one.     Many research studies investigating the impact of JOLs on  learning have revealed that making these judgments can positively  impact learning [1, 5]. One main focus of research on JOLs has   been on assessing how students levels of confidence (i.e., the  judgments they make) impact their subsequent learning [5].  Additionally, research investigating the use of CEs has suggested  that selecting more relevant content pages obtain higher  performance scores [14] Although this research demonstrates the  importance of judging ones understanding of content and  selecting relevant pages for completing sub-goals, research is  lacking on assessing how the accuracy of these judgments change  over time.    2.4 Latent transition analysis  Latent transition analysis (LTA) is an extension of latent class  analysis (LCA) that identifies underlying traits longitudinally.  LCA is a statistical model that groups individuals together based  on unobserved traits (i.e. latent traits) that are inferred with  categorical variables [6]. LCA outputs latent classes that describe  distinct behaviors shared by a group of individuals. Extending  from LCA, LTA describes changes in these latent classes across  time and transitional probabilities are estimated. These latent  classes are not assumed to be static, but rather, individuals may be  described with different classes at different times.     The most appropriate LTA model for describing a population of  individuals sharing similar behaviors is estimated with three sets  of parameters. First, latent status membership probabilities are  estimated for subsequent times after the first time point. These  probabilities recognize the proportion of individuals that would  belong in any given latent class. Second, transition probabilities  are calculated to identify the chance of students changing from  one latent class from one time point to another latent class at the  next time point. Third, model-fit indices are used to determine the  relationship between the observed indicates and the latent class.  Three model-fit indices are typically used: likelihood-ratio G2 fit  statistic, Akaike Information Criteria (AIC), and Bayesian  Information Criteria (BIC). For interpretation, larger values of  likelihood-ratio G2 value indicate rejection of the null-hypothesis  [9] while smaller values of AIC and BIC are indicators of better  model fit [3]. For this study, LTA was conducted in SAS with the  PROC LTA procedure [8].    3 RESULTS  Latent transition analysis for 2-, 3-, and 4- class solutions was  conducted. The two-class model was considered due to low AIC  and BIC values, but classes and transitions lacked interpretability.  The selected model is the three-class solution, as it was the most  parsimonious, had the best goodness of fit statistics of  interpretable models, and offered the most useful interpretation.  Table 1 indicates the model fit statistics for the 2-, 3-, and 4-class  models.    According to this model, all members of Class 1 (n=20) correctly  identified content relevant to their sub-goal (CE). Approximately  58% of this class judged their learning (JOL) with correct  confidence or under-confidence in light of their page quiz results,  while 42% were overconfident. Thus this class has strength in CE  and emerging skills in JOLs. The members of Class 2 (n=39) have  the contrasting profile: all of Class 2 exhibited accurate or under- confident JOL, but only approximately 55% of Class 2 correctly  identified the relevance of page/image content for their sub-goal.  As such, class 2 exhibits strength in JOL and emerging CE. Class  3 consists of learners who exhibit challenges in both JOLs and  CEs. 100% of Class 3 (n=16) learners exhibited overconfidence in  JOL, and 100% also incorrectly identified content relevant to their   Figure 1. Main interface of MetaTutor tutoring system.     current learning sub-goal (CE). Classes are constrained to have  the same composition over the two time points.     Table 1. Comparison of LTA models.  Number  of classes G  2 df AIC BIC   2 6.813 8.000 20.813 37.035   3 1.833 1.000 29.833 62.278   4 0.000 -8.000 46.000 99.302  Note: Bold font indicates the selected model.    Table 2. Item-response probabilities (probability of item  response given latent status), prevalence of latent statuses, and  transition probabilities in latent status membership.   Latent Status    Class 1  (n=20):  Strong  CE,  emerging  JOL   Class 2  (n=39):  Strong  JOL,  emerging  CE   Class 3  (n=16):  Overconfident  JOL,   incorrect   CE   Item-response  probabilities:        JOL      Under or correct  confidence   0.576 >0.999 <0.001   Overconfidence 0.424 <0.001 >0.999   CE      Incorrect <0.001 0.455 >0.999   Correct 0.999 0.545 <0.001   Prevalence of statuses  at:        Time 1 0.269 0.517 0.213   Time 2 0.454 0.478 0.066   Transitions from Time  1 (rows) to   Time 2 (columns):            Class 1: Strong       CE, emerging JOL   0.370 0.630 <0.001   Class 2: Strong  JOL, emerging CE   0.617 0.280 0.103   Class 3:  Overconfident JOL  and   incorrect CE   0.168 0.769 0.062   Note: Item-response probabilities are constrained to be equal at  time points 1 and 2. Bold font indicates membership in the same  latent status at both time points.    According to Table 2, there were more than 75% of students in  Class 3 who demonstrated difficulty with both JOL and CE. Of  this group, more than 75% improved their judgment of learning,  and approximately 17% improved their content evaluation skills.  Just 6% of the original Class 3 remained in Class 3. Overall, the  probability of being in this class decreased approximately 15%   from the initial measurement in time point 1 to the final  measurement in time point 2. This is a remarkable finding,  considering their time with the MetaTutor program was limited to  only a few hours at most.    Table 2 also indicates that approximately 63% of students in Class  1 (Strong CE/Emerging JOL) transitioned to Class 2 (Strong  JOL/Emerging CE) at time point 2, while 61% of students did the  reverse. There was less than 0.001 probability that students in  Class 1 transitioned to Class 3 (Overconfident JOL/Incorrect CE),  but approximately 10% of Class 2 did transition to Class 3,  indicating that they were more likely than Class 1 to exhibiting  overconfidence and incorrect CE after engaging with the  MetaTutor system.     4 DISCUSSION   This ongoing research is intended to elucidate the transitioning  profiles of SRL in the context of MetaTutor. Extending from  previous literature investigating the construct and latent nature of  SRL [3, 11, 12], this study investigates the changing latent SRL  profiles over two time points.     Preliminary results indicated that a three-class solution best fit the  model. It is important to remember that transitioning from Class 1  (strong CE, emerging JOL) to 2 (strong JOL, emerging CE), or  the reverse, does not indicate that there has been a weakening of  the other skill. Students transitioning from one class to another  class are now demonstrating a different profile of JOLs and CEs.  The findings of this study indicate that students exhibit different  combinations of SRL behaviors across short time periods. Further  inferential statistics are required to clarify which combinations  maximize performance outcomes.  Class 3 (i.e., learners who demonstrate challenges in both JOL  and CE) is an interesting profile to mention. More than 75% of  this class later demonstrated the significant improvements in their  JOL and approximately 17% of these students improved their CE.  An explanation may be found in the tailored feedback provided by  MetaTutor. The feedback given by MetaTutor is tailored  specifically to students input, with over 20 different feedback  responses. The time that students spend engaging in MetaTutor  improves their outcomes relative to their area(s) of weakness. For  example, of those who were in Class 1 at time point 1 (emerging  JOL), more than half made gains in JOL, ending up in Class 2. A  similar result is exhibited for Class 2, with over half of the  members of Class 2 at time point 1 transitioning to Class 1 at time  point 2. This can potentially be attributed to the tailored feedback  that students received in their area of weakness as they engaged  with MetaTutor. In support of this thought, an earlier study by  Thompson [15] found that general feedback did not significantly  improve accuracy of students confidence-recall, but instead  found that students confidence-recall improved when feedback  was given for specific types of questions.    We recognize a limitation to this preliminary study. JOL and CE  events were only included when feedback was provided.  Therefore, students who were in the control condition were not  included in the preliminary analysis and the sample size was  reduced.     4.1 Next Steps  We do intend to continue the analysis by investigating predictive  relationships between the transitioning SRL profiles and  performance outcomes (e.g., setting sub-goals, time spent on task,     and outcome on quiz and post-test scores). Next, the current  constructs of JOL and CE have been dichotomized for the  preliminary analysis; however, JOL and CE constructs do consist  of more than one dimension (e.g., relevancy of text vs. relevancy  of diagram). Further investigation would be required to  appropriate categorize these constructs so these constructs can be  more accurately represented in the analysis. Finally, the nature of  feedback is particularly interesting and would require further  investigation to understand how different students demonstrating  various SRL profiles would respond to tailored feedback.    5 ACKNOWLEDGMENTS  The research presented in this paper has been supported by  funding from National Science Foundation (DRL1431552) and  the Social Sciences and Humanities Research Council of Canada  (SSHRC 895-2011-1006). Any opinions, findings, and  conclusions or recommendations expressed in this material are  those of the author(s) and do not necessarily reflect the views of  the National Science Foundation or Social Sciences and  Humanities Research Council of Canada. The authors of this  study would like to acknowledge Nicholas Mudrick for his  involvement with MetaTutor.       6 REFERENCES  [1] Azevedo, R., Harley, J., Trevors, G., Duffy, M., Feyzi-  Behnagh, R., Bouchet, F., and Landis, R. S. 2013. Using  trace data to examine the complex roles of cognitive,  metacognitive, and emotional self-regulatory processes  during learning with multi-agent systems. In R. Azevedo &  V. Aleven, Eds. International handbook of metacognition  and learning technologies, Amsterdam, The Netherlands:  Springer 2013, 427-449.    [2] Azevedo, R., Moos, D. C., Johnson, A. M., and Chauncey,  A. D. 2010. Measuring cognitive and metacognitive  regulatory processes during hypermedia learning: Issues and  challenges. Educational Psychologist, 45, 4, 210-223.   [3] Barnard-Brak, L., Lan., W. Y., and Paton, V. O. 2010.  Profiles in self-regulated learning in the online learning  environment. The International Review of Research in Open  and Distributed Learning, 11, 1, 61-80.   [4] Bauer, D. J., and Curran, P. J. 2004. The integration of  continuous and discrete latent variable models: Potential  problems and promising opportunities. Psychological  Methods, 8, 338-363.   [5] Dunlosky, J., and Lipko, A. 2007. Metacomprehension: A  brief history and how to improve its accuracy. Current  Directions in Psychological Science, 16, 228232.    [6] Goodman, L. A. 1974. Exploratory latent structure analysis  using both identifiable and unidentifiable  models. Biometrika., 61, 215231.   [7] Greene, J. A., and Azevedo, R. 2009. macro-level analysis of  SRL processes and their relations to the acquisition of   sophisticated mental models. Contemporary Educational  Psychology, 34, 1829.   [8] Lanza, S. T., and Collins, L. M. 2010. A new SAS procedure  for latent transition analysis: Transitions in dating and sexual  risk behavior. Dev Psychol, 44, 2, 446-456.   [9] Lanza, S. T., Dziak, J. J., Huang, L., Xu, S., and Collins, L.  M. 2011. Proc LCA & Proc LTA users guide (Version  1.2.6). University Park, PA: The Methodology Center, Penn  State University. Retrieved from http://methodology.psu.edu   [10] Nelson, T. O., and Narens, L. 1990. Metamemory: A  theoretical framework and new findings. The Psychology of  Learning and Motivation, 26, 125-173.    [11] Pintrich, P. R. 2000. The role of goal orientation in self- regulated learning. In M. Boekaerts, P. Pintrich, & M.  Zeidner, Eds. Handbook of self-regulation, New York, NY:  Academic Press, 452-502.   [12] Pintrich, P. R., and De Groot, E. V. 1990. Motivational and  self-regulated learning components of classroom academic  performance. Journal of Educational Psychology, 82, 1, 33- 40.   [13] Schunk, D. 2005. Self-regulated learning: The educational  legacy of Paul R. Pintrich. Educational Psychologist, 40, 85 94.   [14] Taub, M., Azevedo, R., Bouchet, F., Clodfelter, E., and  Mudrick, N. 2014. Can scaffolds from pedagogical agents  influence effective completion of sub-goals during learning  with a multi-agent hypermedia-learning environment In  Proceedings of the 11th International Conference of the  Learning Sciences (Boulder, Colorado 2014), 1052-1056.   [15] Thompson, W. B. 1998. Metamemory accuracy: Effects of  feedback and the stability of individual differences.  American Journal of Psychology, 111, 1, 33-42.   [16] Winne, P., and Hadwin, A. 1998. Studying as self-regulated  learning. In D. Hacker, J. Dunlosky, & A. Graesser, Eds.  Metacognition in educational theory and practice, Mahwah,  NJ: Erlbaum, 227-304.   [17] Winne, P., and Hadwin, A. 2008. The weave of motivation  and self-regulated learning. In D. Schunk & B. Zimmerman,  Eds. Motivation and self-regulated learning: Theory,  research, and applications, Mahwah, NJ: Erlbaum, 297-314.   [18] Zimmerman, B. J. 1998. Developing self-fulfilling cycles of  academic regulation: An analysis of exemplary instructional  models. In D. H. Schunk, & B. J. Zimmerman, Eds. Self- regulated learning: from teaching to self-reflective practice ,  New York: Guilford Press, 1-99.   [19] Zimmerman, B. J. 2008. Investigating self-regulation and  motivation: Historical background, methodological  developments, and future prospects, American Educational  Research Journal, 45, 166-183.       "}
{"index":{"_id":"26"}}
{"datatype":"inproceedings","key":"Spann:2017:ESL:3027385.3027427","author":"Spann, Catherine A. and Schaeffer, James and Siemens, George","title":"Expanding the Scope of Learning Analytics Data: Preliminary Findings on Attention and Self-regulation Using Wearable Technology","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"203--207","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027427","doi":"10.1145/3027385.3027427","acmid":"3027427","publisher":"ACM","address":"New York, NY, USA","keywords":"attention, heart-rate variability, psychophysiology, self-regulation, wearable technology","Abstract":"The ability to pay attention and self-regulate is a fundamental skill required of learners of all ages. Learning analytics researchers have to date relied on data generated by a computing system (such as a learning management system, click stream or log data) to examine learners' self-regulatory abilities. The development of wearable computing through fitness trackers, watches, heart rate monitors, and clinical grade devices such as Empatica's E4 wristband now provides researchers with access to biometric data as students interact with learning content or software systems. This level of data collection promises to provide valuable insight into cognitive and affective experiences of individuals, especially when combined with traditional learning analytics data sources. Our study details the use of wearable technologies to assess the relationship between heart rate variability and the self-regulatory abilities of an individual. This is relevant for the field of learning analytics as methods become more complex and the assessment of learner performance becomes more nuanced and attentive to the affective factors that contribute to learner success","pdf":"Expanding the Scope of Learning Analytics Data:  Preliminary Findings on Attention and Self-Regulation   using Wearable Technology     Catherine A. Spann  University of Texas at Arlington   Learning Innovation and Networked  Knowledge (LINK) Research Lab   caspann@uta.edu   James Schaeffer  University of Texas at Arlington   Learning Innovation and Networked  Knowledge (LINK) Research Lab   Department of Psychology  james.schaeffer@mavs.uta.edu   George Siemens  University of Texas at Arlington   Learning Innovation and Networked  Knowledge (LINK) Research Lab   gsiemens@gmail.com       ABSTRACT  The ability to pay attention and self-regulate is a fundamental skill  required of learners of all ages. Learning analytics researchers  have to date relied on data generated by a computing system (such  as a learning management system, click stream or log data) to  examine learners self-regulatory abilities. The development of  wearable computing through fitness trackers, watches, heart rate  monitors, and clinical grade devices such as Empaticas E4  wristband now provides researchers with access to biometric data  as students interact with learning content or software systems.  This level of data collection promises to provide valuable insight  into cognitive and affective experiences of individuals, especially  when combined with traditional learning analytics data sources.  Our study details the use of wearable technologies to assess the  relationship between heart rate variability and the self-regulatory  abilities of an individual. This is relevant for the field of learning  analytics as methods become more complex and the assessment of  learner performance becomes more nuanced and attentive to the  affective factors that contribute to learner success.   CCS Concepts   HCI Theory, Concepts and Models   Keywords  Attention; self-regulation; heart-rate variability;  psychophysiology; wearable technology.   1. INTRODUCTION  A comprehensive assessment of a learner should account for the  many contributions of cognitive, affective, and behavioral factors.  Broadening data collection in the field of learning analytics [13] is  required in order to provide insight into how individuals function  in a complex and changing world. Snapshots of learner behavior  generated from learning management systems, click stream data,    and self-report provide data that lacks depth and generally limited  to one point in time. Wearable technologies, defined as a device  worn on the body that incorporates wireless connectivity for the  purposes of accessing, interacting with, and exchanging  contextually relevant information [4], offer a new mode of data  collection for learning analytics researchers. Expected to reach  over four billion dollars in revenue in 2017 [7], wearable devices  come in a variety of forms, including smart watches, glasses, and  smart jewelry, which produce a wealth of data. The benefits of  this technology to teaching and learning is only beginning to be  explored.   Existing research in learning analytics has emphasized the  importance of self-regulation for effective learning [12]. An  emerging area of interest in learning analytics involves the  physiological component of human learning and self-regulation. It  is clear that learners must have both the cognitive and  physiological flexibility to face changing environmental demands.  An under-regulated autonomic nervous system can mean that  learners are at the whim of their surroundings, unable to regulate  their thoughts, feelings, and behaviors. Wearable technologies can  capture autonomic nervous system activity.    Wearable technology can provide psychophysiological data that  have until recently been prohibitively challenging to collect.  Psychophysiology concerns embodied intelligencethe idea that  through sensations that arise within the body itself, we learn and  organize our experiences. The developments of ambulatory  recording devices and mobile computing make it possible to now  measure psychophysiological phenomena in naturalistic settings.   This paper explores how an individuals ability to pay attention  and self-regulate can be assessed through wearable technologies  that collect psychophysiological data. This work is important to  the learning analytics field in providing a) broadened data  collection capabilities beyond log files and click streams, b)  assessing psychophysiological factors that influence learner self- regulatory abilities, and c) providing a foundation for future  research around affect and self-regulation through wearable  devices.    1.1 Attention and Self-Regulation  The ability to control ones attention, thoughts, emotions, and  behaviors is essential for a healthy and successful life [10].  Attentional control and the ability to inhibit inappropriate or  maladaptive behavior is critical for learning. Attention and self-  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions  from Permissions@acm.org.  LAK 17, Month 13-17, 2017, Vancouver, BC, Canada   2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00  DOI: http://dx.doi.org/10.1145/3027385.3027427     regulation is related to numerous outcomes including academic  success [3], better mental and physical health [10], success in the  workplace [5], and healthier and more positive social relationships  [1]. A longitudinal study of over 1,000 individuals spanning 32  years found that, after controlling for socioeconomic status and  intelligence, childhood self-regulation predicted better health, less  criminal behavior, and greater wealth in adulthood [10]. Failures  of attentional control and self-regulation led to a variety of  negative outcomes including school dropout and violent crime  [10].    The opposing construct to paying attention and self-regulating  ones behavior is mind-wandering. Mind-wandering occurs when  attention moves from the immediate external environment to  internal thoughts. The act of mind wandering is incredibly  common. One large-scale investigation using experience-sampling  techniques estimating that we are engaged in mind wandering  approximately 50% of our waking life [8]. There are many costs  to mind wandering including reduced working memory [11],  increased impulsivity [11], impaired reading comprehension [8],  and negative mood [8]. Certainly, uncontrolled mind wandering  impairs the learning process and can disrupt performance on a  number of activities. Thus, the ability for a learner to sustain  attention on the task at hand and self-regulate their behavior is  critically important for their success as a student.   1.2 Heart-Rate Variability  Attention and self-regulation is closely linked to internal bodily  states [9], largely because the brain and heart are in constant  communication. The Neurovisceral Integration Model [15] puts  forth the idea that cognitive, affective, and physiological  regulation may be associated with each other to serve the purpose  of goal-directed behavior. In the context of physiological  regulation, and specifically the regulation of the heart, a balanced  system is necessary. The balanced body can respond to physical  and environmental demands.   Activities that require cognitive effort are known to influence the  autonomic nervous system (ANS), which is divided into the  sympathetic nervous system (SNS) and parasympathetic nervous  system (PNS). While the SNS is devoted to prepare the body for  engaging with or avoiding an activity, the PNS is concerned with  slowing down and calming the nervous system. The two  components of the ANS play antagonistic roles and, through their  synergy, are responsible for maintaining homeostasis and a  regulated bodily system.   Mental effort is associated with physiological arousal and, thus,  increased SNS activity [15]. Increases in sympathetic activity  correlates with increases in heart rate while increases in  parasympathetic activity correlates decreases in heart rate.  Consequently, SNS increases cause the time between heart beats  to shorten while PNS increases cause the time between heart beats  to lengthen. Sympathetic influences are slow, occurring over  seconds, while the parasympathetic effects are fast, occurring over  milliseconds.    Heart-rate variability (HRV) is regularly used as a biomarker of  the autonomic nervous system. HRV is defined as the variability  of the beat to beat timing of the heart and the data required for  calculating HRV is the sequence of time intervals between  successive heart beats. Because SNS activity occurs over seconds,  the parasympathetic effects are the only ones that could produce  rapid changes in the time between heart beats. Thus, the high  frequency component of HRV, typically measured at .15 to .40  Hz, reflects the parasympathetic influence on the ANS.    The parasympathetic nervous system of the ANS is critical for  regulating the body and plays a role in attention, working  memory, and emotion regulation [15]. Greater high frequency  HRV indicates an individuals ability to maintain homeostasis and  responsiveness to complex and changing demands. HRV is related  to physiological, affective, and cognitive processes. Individuals  with higher resting HRV are better able to perform cognitive and  behavioral tasks that require attentional control and inhibition  across a wide-range of situations in the lab and real-world  situations [15].    A critical idea is that HRV may provide a marker of the degree to  which the brains integrative system for successful regulation  provides flexible control over the body [15]. HRV may serve as  one simply measured output that may provide important  information regarding the ability of a person to function  effectively in a complex environment.    Researchers from diverse fields are studying HRV. With the  advancement and proliferation of wearable technologies, this  physiological signal can now be measured in real-time and in real- world environments.   1.3 The Current Study  The present investigation is a report of preliminary findings on the  relationship between HRV (measured with wearable technology)  and attention and self-regulation (measured in a real-world  setting). The study employed a correlational design in which  individuals at a local museum participated in a twenty-minute  research study. Our goal was to take psychophysiological  assessments via wearable technology out of the laboratory and  into real-world settings. Participants in this study self-reported to  us their global assessment of their own attention and self- regulation and they also completed an objective measure of  attention and self-regulation on an iPad. A wearable wristband  designed for physiological data collection was worn throughout  the entire study.    1.3.1 Hypotheses  Our first hypothesis was that higher HRV would be associated  with greater attention and self-regulation. We also expected that  higher HRV would be related to greater self-reported attention  and self-regulation.    2. METHODS  All study procedures were reviewed and approved by the  Institutional Review Board at the University of Texas at  Arlington.   2.1 Recruitment  Individuals ages nine and older were recruited from the Forth  Worth Museum of Science and History, in a partnership with the  Research and Learning Center located within the museum. Study  staff approached participants at the museum and asked if they  would like to participate in a research study on attention and self- regulation. Participation was voluntary and individuals were not  compensated for their time.   2.2 Participants  The current sample includes 52 participants, 23 of which were  between the ages of 9 and 17, 15 were between the ages of 18 and  45, and 14 participants were 46 or older.     2.3 Measures  2.3.1 Attention and Self-Regulation  The Dimensional Change Card Sort (DCCS) is the most widely  used measure of executive cognitive control, tapping into  attentional control, inhibitory control, and cognitive flexibility.  The DCCS is part of the National Institutes of Health Toolbox  Cognition Battery. The DCCS is a five-minute computerized task  in which two presented target pictures vary along two dimensions  (shape and color; Figure 1). Participants are asked to match a  series of test pictures (bunny and sailboat) to the target pictures,  first according to one dimension (shape) and then according to  another dimension (color). Switch trials are employed, where the  participant must change the dimension being matched. For  example, after four consecutive trials matching on shape, the  participant may be asked to match on color during the next trial  and then return back to shape. This requires attentional and  behavioral control to quickly choose the correct stimulus. This  task was administered on an iPad. The participant is instructed to  answer as quickly as possible and to try not to make any mistakes.     Figure 1. Sequence of trials for the National Institutes of   Health (NIH) Toolbox Dimensional Change Card Sort Test.   2006-2012 National Institutes of Health and Northwestern   University.  Scoring of the DCCS is based on a combination of accuracy and  reaction time. Higher scores indicate better performance. A two- vector scoring method is employed, where the vectors range from  0 to 5. This forms a computed score ranging from 0 to 10. For any  given participant, accuracy is considered first, and if accuracy  levels are less than or equal to 80%, the final computed score is  equal to the accuracy score. If accuracy levels for the participant  reached more than 80%, the reaction time score and accuracy  score are combined.   The accuracy vector was comprised of accuracy points. For every  correct behavioral response, a participant received a value of  0.167 (5 points divided by 30 trials). Thus, DCCS Accuracy Score  = (0.167 * Number of Correct Responses).    The reaction time vector was generated from the median reaction  time score on switch trials (non-dominant dimension/dimension  cued less frequently). Median reaction time values were computed  using only correct trials with reaction times greater than or equal  to 100ms and reaction times no larger than 3 standard deviations  away from the childs mean reaction time for switch trials.  Because reaction time data tends to have a positively skewed  distribution, a log (Base 10) transformation was applied to each  childs median reaction time score. Participants with median  reaction times that fell outside of the validated minimum (500ms)   and maximum (3,000ms) reaction time, but within the allowable  range (100ms-10,000ms) were truncated (i.e., reaction times  between 3,000ms and 10,000ms were set to 3,000ms). Log values  were then algebraically rescaled from a log(500)-log(3000) range  to a 0-5 range using the following formula:     DCCS Reaction Time Score = 5   5  %&'()*%&' (,--)  %&' /--- *%&' (,--)      Again, these reaction time scores were combined with the  accuracy scores for children who achieved the accuracy criterion  of greater than 80%. Children who failed to reach the criterion  received their accuracy score as their total computed score.    2.3.2 Heart-Rate Variability  HRV was taken through a wearable Empatica E4 wristband  (Figure 2, https://www.empatica.com/e4-wristband). The E4 is  worn on the wrist and collects data from the surface of the skin.  The wristband contains a photoplethysmography (PPG) sensor,  which measures blood volume pulse. Heart rate and HRV were  derived from blood volume pulse.      Figure 2. Empatica E4 Wristband worn on the non-dominant   wrist during 20-minute experiment.   https://www.empatica.com/e4-wristband   Heart rate data collected during the three-minute baseline period  was processed using Kubios software [11]. The digital signal  underwent a fast Fourier transform to extract the high frequency  component (0.15 to 0.40 Hz) of HRV from the inter-beat interval  data. A low-level artifact correction was used to account for  missing data, where intervals differing more than 0.35s of the  local average were determine to be artifacts and interpolated.  Power values (ms2) were transformed with a natural logarithm to  remove a positive skew.   2.3.3 Self-Reported Attention and Self-Regulation  Three questionnaires were administered depending on the  participants age. For adolescents aged 9-15, the Early Adolescent  Temperament Questionnaire was used. They responded to eleven  items, where six items assessed their attentional control and five  items assessed their self-regulation. Internal consistency  reliabilities for attentional control and self-regulation were .39 and  .41, respectively. For individuals age 16 and older, they  completed the Adult Temperament Questionnaire. Five items  assessed their attentional control and 7 items assessed their self- regulation. Internal consistency reliabilities for attentional control  and self-regulation were .53 and .40, respectively.   2.4 Procedure  Data collection occurred at the Research and Learning Center at  the Fort Worth Museum of Science and History. First, participants  were provided the informed consent. Parental approval was     provided for all individuals under the age of 18. After providing  consent, the E4 wristband was placed on the participants non- dominant wrist. Individuals then completed the demographic  questionnaire where they reported gender, birthdate, race, and  ethnicity. Participants then completed the self-report questionnaire  on their attention and self-regulation.  Following this, the experimenter informed the participant that  they would collect baseline assessments of the physiological  measurements while they were at rest. They were asked to place  noise-cancelling headphones on, sit in an upright and relaxed  posture, close their eyes. Nature sounds were played through the  headphones for three minutes while the participant relaxed.    After the baseline procedures, participants completed the DCCS  on the iPad. This lasted approximately four minutes. The entire  procedure took between fifteen and twenty minutes to complete.   2.5 Statistical Analyses  Using an a priori power analysis assuming a small to moderate  effect size of .06, an alpha of .05, and power of .80, a sample size  of 133 is required. The present results are preliminary findings  from this proposed sample.   Prior to formal hypothesis testing, data were screened for outliers  and skewness to ensure that all assumptions were met for  subsequent analyses. The sample was parsed into three age  categories: 9-17, 18-45, 46 and older. Bivariate relationships and  gender differences were examined with Pearsons R correlations  and independent samples t-tests.   3. RESULTS  3.1 Descriptive Statistics  Table 1 provides mean and standard deviations for the primary  variables of interest. HF-HRV refers to the high frequency  component of HRV, or the parasympathetic influence on  autonomic activity. DCCS refers to participant scores on the  cognitive task requiring attention and self-regulation. The  columns for attention and self-regulation refer to the self-reported  attention and self-regulation scores. Scores for these two variables  were standardized within each age group. Higher scores reflect  better self-reported attention and self-regulation.    Table 1. Means and standard deviations for study measures      3.2 Gender Differences  Using independent samples t-tests, no gender differences were  found across all objective and self-reported measures from any  age group.   3.3 Correlational Analyses  Correlations among all variables are presented in Table 2. Results  partially supported our first hypothesis, which predicted that HF- HRV would be significantly positively related to DCCS scores.   This was only apparent among adolescents. The relationship  between HF-HRV and DCCS scores attenuated among older  individuals and reversed among the oldest participants. That is,  there was an inverse relationship between HF-HRV and DCCS  scores among those 46 years of age and older. This relationship,  however, was not significant.  Results did not support our second hypothesis, which predicted  that HF-HRV would be significantly positively related to self-  reported attention and self-regulatory abilities. There was a  relationship found between HF-HRV and self-reported attention  in older adults, but this relationship did not reach significance.   Table 2. Correlations among variables   Note. *significant at p < .05.   4. DISCUSSION  This study examined the relationship between autonomic nervous  system activity and behavioral and self-reports of attention and  self-regulation. We used wrist worn wearable technology to  collect psychophysiological data in a real-world learning  environment. Preliminary findings from this study indicated that  higher HRV is associated with an objective measure of attention  and self-regulation among adolescents.   The small sample significantly limits any strong conclusions at  this time. The internal consistency of the questionnaires was poor,  and significant conclusions should not be made with the current  findings related to self-reported attention and self-regulation.  However, the current results are promising in that the biological  indicator of HRV assessed with wearable technology was  significantly associated with an objective measure of attention and  self-regulation.   4.1 Future Directions  Data collection will continue on this project. This work will also  extend into other physiological indicators of autonomic activity  taken during this study, including heart rate and galvanic skin  response. Our research will extend to longer term, continuous data  collection in real-world settings along with a focus on intervention  work. Previous research has suggested that moderate practice in  meditation and deep reflection increases HRV [9] and ones  ability to pay attention, self-regulate, and control mind wandering  [11]. Wearable and mobile technology now enables us to monitor  HRV and self-regulatory abilities over longer time periods in a      HF-HRV  DCCS  Attention  Self-Regulation   Ages  9-17   7.18  (0.65)   123.73  (23.67)   -0.76  (1.06)   -0.09  (.98)   Ages  18-45   6.22  (1.06)   112.38  (17.61)   -0.10  (1.03)   0.11  (0.95)   Ages  46+   5.64  (0.87)   125.64  (20.93)   0.25  (0.82)   0.03  (1.12)      HRV  DCCS  Attention  Self-Control   9- 17   y rs .   Age  .25  .26  -.12  -.13   HRV        .48*  -.23  -.29   DCCS       -.21  -.30   Attention         .42   18 -4 6  yr s.   Age  -.67*  -.18  -.29  -.16   HRV      .11   .38   .16   DCCS        .46     .75*   Attention           .54*  46  +  yr s.   Age  -.30   .30  -.23   .22   HRV     -.27   .57   -.001   DCCS        .27     .60*   Attention         .33     variety of settings. These technologies will also increase  understanding of the boundary conditions under which the  influence of autonomic nervous system activity on behavior  fluctuates.   4.2 Conclusion  This study introduces a promising area of research for the field of  learning analytics: psychophysiological measures through use of  wearable technologies. As learning analytics continues to evolve  as a bricolage field [6] that draws from numerous disciplines in  maturing analysis techniques, researchers can benefit from  evaluating the physiological conditions that underpin self- regulation, motivation, and related attributes. In our study, we  detail preliminary results of HRV and self-regulation - research  that is relevant to learning designers, software programmers, and  educators. By recognizing the contribution of physiological states  to cognitive and learning performance, LA as a field will be better  able to guide system designers in creating nudging mechanisms,  assess the impact of learning activity design on self-regulation,  and provide direction on scaffolding and support needed for  learners whose self-regulatory capabilities are not sufficiently  developed to enable mastery of complex learning material.   5. ACKNOWLEDGMENTS  Thank you to the Research and Learning Center at the Fort Worth  Museum of Science and History for partnering with us on this  project and to the families who participated in this research.   6. REFERENCES  [1] Ayduk, O., Mendoza-Denton, R., Mischel, W., Downey, G.,   Peake, P.K. and Rodriguez, M. 2000. Regulating the  interpersonal self: Strategic self-regulation for coping with  rejection sensitivity. Journal of Personality and Social  Psychology. 79, 5 (2000), 776792.   [2] Bailey, C.E. 2007. Cognitive Accuracy and Intelligent  Executive Function in the Brain and in Business. Annals of  the New York Academy of Sciences. 1118, 1 (Sep. 2007),  122141.   [3] Blair, C. and Razza, R.P. 2007. Relating effortful control,  executive function, and false belief understanding to  emerging math and literacy ability in kindergarten. Child  development. 78, 2 (2007), 647663.   [4] Bower, M. and Sturman, D. 2015. What are the educational  affordances of wearable technologies Computers &  Education. 88, (Oct. 2015), 343353.   [5] Critchley, H.D. and Harrison, N.A. 2013. Visceral Influences  on Brain and Behavior. Neuron. 77, 4 (Feb. 2013), 624 638.   [6] Gaevi, D., Dawson, S. and Siemens, G. 2015. Lets not  forget: Learning analytics are about learning. TechTrends.  59, 1 (2015), 6471.   [7] Global wearable technology market 2012-2018 | Statistic:  https://www.statista.com/statistics/302482/wearable- device-market-value/. Accessed: 2017-01-07.   [8] Killingsworth, M.A. and Gilbert, D.T. 2010. A Wandering  Mind Is an Unhappy Mind. Science. 330, 6006 (Nov.  2010), 932932.   [9] Lumma, A.-L., Kok, B.E. and Singer, T. 2015. Is meditation  always relaxing Investigating heart rate, heart rate  variability, experienced effort and likeability during  training of three types of meditation. International Journal  of Psychophysiology. 97, 1 (Jul. 2015), 3845.   [10] Moffitt, T.E., Arseneault, L., Belsky, D., Dickson, N.,  Hancox, R.J., Harrington, H., Houts, R., Poulton, R.,  Roberts, B.W., Ross, S., Sears, M.R., Thomson, W.M. and  Caspi, A. 2011. A gradient of childhood self-control  predicts health, wealth, and public safety. Proceedings of  the National Academy of Sciences. 108, 7 (Feb. 2011),  26932698.   [11] Mrazek, M.D., Franklin, M.S., Phillips, D.T., Baird, B. and  Schooler, J.W. 2013. Mindfulness Training Improves  Working Memory Capacity and GRE Performance While  Reducing Mind Wandering. Psychological Science. 24, 5  (May 2013), 776781.   [12] Roll, I. and Winne, P.H. 2015. Understanding, evaluating,  and supporting self-regulated learning using learning  analytics. test. 2, 1 (2015), 712.   [13] Siemens, G. 2012. Learning analytics: envisioning a research  discipline and a domain of practice. Proceedings of the 2nd  International Conference on Learning Analytics and  Knowledge (2012), 48.   [14] Tarvainen, M.P., Niskanen, J.-P., Lipponen, J.A., Ranta-aho,  P.O. and Karjalainen, P.A. 2014. Kubios HRV  Heart rate  variability analysis software. Computer Methods and  Programs in Biomedicine. 113, 1 (Jan. 2014), 210220.   [15] Thayer, J.F., Hansen, A.L., Saus-Rose, E. and Johnsen, B.H.  2009. Heart Rate Variability, Prefrontal Neural Function,  and Cognitive Performance: The Neurovisceral Integration  Perspective on Self-regulation, Adaptation, and Health.  Annals of Behavioral Medicine. 37, 2 (Apr. 2009), 141 153.             "}
{"index":{"_id":"27"}}
{"datatype":"inproceedings","key":"Poquet:2017:EYF:3027385.3027404","author":"Poquet, Oleksandra and Dawson, Shane and Dowell, Nia","title":"How Effective is Your Facilitation?: Group-level Analytics of MOOC Forums","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"208--217","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027404","doi":"10.1145/3027385.3027404","acmid":"3027404","publisher":"ACM","address":"New York, NY, USA","keywords":"ERGM, MOOCs, facilitation, forum, indicators of social activity","Abstract":"The facilitation of interpersonal relationships within a respectful learning climate is an important aspect of teaching practice. However, in large-scale online contexts, such as MOOCs, the number of learners and highly asynchronous nature militates against the development of a sense of belonging and dyadic trust. Given these challenges, instead of conventional instruments that reflect learners' affective perceptions, we suggest a set of indicators that can be used to evaluate social activity in relation to the participation structure. These group-level indicators can then help teachers to gain insights into the evolution of social activity shaped by their facilitation choices. For this study, group-level indicators were derived from measuring information exchange activity between the returning MOOC posters. By conceptualizing this group as an identity-based community, we can apply exponential random graph modelling to explain the network's structure through the configurations of direct reciprocity, triadic-level exchange, and the effect of participants demonstrating super-posting behavior. The findings provide novel insights into network amplification, and highlight the differences between the courses with different facilitation strategies. Direct reciprocation was characteristic of non-facilitated groups. Exchange at the level of triads was more prominent in highly facilitated online communities with instructor's involvement. Super-posting activity was less pronounced in networks with higher triadic exchange, and more pronounced in networks with higher direct reciprocity.We investigate automatic detection of teacher questions from audio recordings collected in live classrooms with the goal of providing automated feedback to teachers. Using a dataset of audio recordings from 11 teachers across 37 class sessions, we automatically segment the audio into individual teacher utterances and code each as containing a question or not. We train supervised machine learning models to detect the human-coded questions using high-level linguistic features extracted from automatic speech recognition (ASR) transcripts, acoustic and prosodic features from the audio recordings, as well as context features, such as timing and turn-taking dynamics. Models are trained and validated independently of the teacher to ensure generalization to new teachers. We are able to distinguish questions and non-questions with a weighted F1 score of 0.69. A comparison of the three feature sets indicates that a model using linguistic features outperforms those using acoustic-prosodic and context features for question detection, but the combination of features yields a 5% improvement in overall accuracy compared to linguistic features alone. We discuss applications for pedagogical research, teacher formative assessment, and teacher professional development.","pdf":"How Effective is Your Facilitation Group-Level Analytics  of MOOC Forums   Oleksandra Poquet  University of South Australia   Delft University of Technology  Adelaide, Australia   sspoquet@gmail.com   Shane Dawson  Teaching Innovation Unit   University of South Australia  Adelaide, Australia   shane.dawson@unisa.edu.au   Nia Dowell  Institute for Intelligent Systems   University of Memphis  Memphis, USA   ndowell@memphis.edu     ABSTRACT  The facilitation of interpersonal relationships within a respectful  learning climate is an important aspect of teaching practice.  However, in large-scale online contexts, such as MOOCs, the  number of learners and highly asynchronous nature militates  against the development of a sense of belonging and dyadic trust.  Given these challenges, instead of conventional instruments that  reflect learners affective perceptions, we suggest a set of  indicators that can be used to evaluate social activity in relation to  the participation structure. These group-level indicators can then  help teachers to gain insights into the evolution of social activity  shaped by their facilitation choices. For this study, group-level  indicators were derived from measuring information exchange  activity between the returning MOOC posters. By conceptualizing  this group as an identity-based community, we can apply  exponential random graph modelling to explain the networks  structure through the configurations of direct reciprocity, triadic- level exchange, and the effect of participants demonstrating  super-posting behavior. The findings provide novel insights into  network amplification, and highlight the differences between the  courses with different facilitation strategies. Direct reciprocation  was characteristic of non-facilitated groups. Exchange at the level  of triads was more prominent in highly facilitated online  communities with instructors involvement. Super-posting activity  was less pronounced in networks with higher triadic exchange,  and more pronounced in networks with higher direct reciprocity.   CCS Concepts  Applied computing EducationE-learning   Keywords  MOOCs; forum; facilitation; indicators of social activity; ERGM.   1. INTRODUCTION  With the massification and commercialization of higher  education, universities place more demands on educators to  deliver quality teaching to more students at lower costs. As a  result, educators need to adapt and modify their pedagogical    approaches to better accommodate a significantly larger and more  diverse group of students. Well-designed assessment to some  extent can help teachers determine if their efforts to help students  learn were effective. However, student performance data alone,  does not provide sufficient information about other aspects of the  learning and teaching context. An alternate approach is required  to explore if a teachers implemented facilitation processes were  successful in promoting interpersonal relationships and peer-to- peer learning.  Various instruments have been designed to capture the evolution  of in-class social relations related to the positive impact on  learning. Examples include the community of inquiry  questionnaire [45], surveys of social presence [34] and sense of  belonging [46]. These instruments evaluate the state of trust and  community often at the group-level as the ultimate outcome of  interpersonal relationship formation.  The importance of trust-based relationships has been associated  with small online and face-to-face classes where the group  boundaries are fixed. However, there remains a significant  challenge in leveraging the benefits of such interpersonal  relationships in large learner cohorts particularly in the non- formal contexts. In massive open online courses (MOOCs) learner  participation in social activities is intermittent [55], and early  interactions can amass to a level of chaos [3] that impedes an  individuals propensity for developing relationships. For instance,  according to Gillani, MOOC forums assemble and disperse as  crowds [10]. Additionally, as stated by Poquet et al. [39] the  relatively short time frames associated with MOOC offerings  further diminish the opportunities for learners to develop  interpersonal trust. Simply put, large course size, short duration of  teaching and the non-formal nature of MOOCs militate against  the development of community. In such instances the use of  established instruments evaluating if learner-learner bonds have  been forged becomes irrelevant.  The amplification of communication between participants could  serve as an effective and proactive indicator for group processes.  The promotion of connections between learners, concepts and  artefacts is favoured by networked learning, connectivism and  socio-material approaches [1, 28, 48]. For example, a connectivist  approach to teaching in networked systems includes controlling  the network of learners through a facilitating role  by amplifying,  curating, way-finding and socially-driven sense-making,  aggregating, filtering, modelling and being persistently present  [49]. In this context, the role of the teacher becomes strongly  aligned with promotion and facilitation of networks  interconnectedness. Thus, an amplified network of learner  communications may indicate if a structure and a climate  conducive to egalitarian participation has evolved.  This present study uses network indicators to explore social  activity in an open online course. Through these indicators the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions  from Permissions@acm.org.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027404     study examined amplification of communication within the  network of learners in ten MOOCs. Exponential random graph  modelling (ERGMs) was applied to understand the extent of  direct and triadic-level reciprocity and the effects of participants  posting activity within each network. ERGMs required a  theoretically-driven hypothesis to be interpreted meaningfully.  Thus, based on our prior work we conceptualized groups of  regularly posting forum users, i.e. forum cores, as identity-based  online communities. Communication within the communities  were abstracted into network graphs for ERGM analysis. Results  highlighted the differences in propensity for tie formation and the  probability of information exchange across networks of different  size and course design. The examined MOOCs were conducted in  STEM disciplines, and delivered by the Delft University of  Technology via the edX platform in 2013-2015.   2. REVIEW OF LITERATURE  2.1 Analytics for Group-Level Social Activity  The social fabric of a learning group evolves from the structures  and processes created as the collective completes the assigned  work. The development of effective group strategies requires  teachers to adopt strong facilitation skills and techniques. In this  context, facilitation skills refer to the ability to design and manage  learning group structures and processes, and employ appropriate  and timely strategies to minimize the common problems that arise  when people work together [29]. Facilitation skills, learning  design and support of knowledge construction are all required to  establish an effective online teaching program [7]. However,  facilitation can be seen as distinct from the other two. Learning  design is geared towards teachers decisions about learning tasks,  course structure and assessment [6]. The support of knowledge  construction, or content facilitation [13], refers to a teachers  ability to promote students epistemic fluency. Facilitation is  content-neutral in that it promotes a structure and a culture of  participation built on individual dispositions to the group and one  another. Measuring learning outcomes that result from effective  facilitation is particularly challenging because they fall within the  socio-emotional domain.   Group-focused analyses can provide novel insights into how  social activity unfolds. For example, Huang et al [24] examined  the health of forums at the group level in 44 Coursera MOOCs in  relation to super-posting behaviours of its learners. The authors  found that overall higher activity from super-posters is positively  correlated with higher activity from other forum users, concluding  that the super-posters do not drown out the silent majority  (p.118). It should be noted, however, that such an evaluation of  the health of the forum was limited to the correlation of individual  counts of content-related messages between different types of  learners. The study is a reflective of how social activity on forums  is approached in MOOC research to date.  In current research of platform-based MOOCs, most indicators of  social activity are not well suited to evaluate the state of social  activity in a group. Analyses have been predominantly conducted  at the level of the individual, often as a complement to academic  performance (grades). Numerous studies have analysed the  association of a number of forum posts in relation to an  individuals performance [12]. For example, Kizilcec et al. [31]  demonstrated that completing learners who attempted a majority  of the assessments were also more active on the forum. Vu et al.  [52], however, found that the relationship between learning  performance and social interactions is not bi-directional. Although  high learning performance was positively correlated with active   social interactions, active social interactions were not necessarily  associated with performance. The importance of these analyses  lies in grasping the value of social activity for individual student  certification, however, they offer little insight into the group-level  forum dynamics.   Studies measuring individual engagement with others in the form  of social capital offer an alternative to evaluating the role of social  activity through post counts. Such an approach is based on  quantifying an individual learners position within the network of  posters [8, 25, 26]. However, such an approach does little to  reveal how the network emerges from the collective contributions.  To explain, although individual learning outcomes such as social  capital do provide information about social activity, they are not  sound indicators of group-level activities.   Where social activity in MOOCs has been evaluated at the group- level, the evaluations have tended to privilege the content- dimension of interaction. For instance, Hecking et al. [21]  encapsulated social interaction within content-related posts of the  network into blockmodels describing the overall network pattern.  Or, Kellogg et al. [30] reported knowledge construction within the  network of learners through qualitative analysis of learner posts.  Similarly, Wang et al. [53] identified and evaluated groups in the  course based on higher order thinking behaviours reflected  through posted text. These examples demonstrate that the  socioemotional dimension of learning is often taken for granted.  As argued by Kreijns et al. [33], when a need for the  socioemotional is recognized, the dimension is examined in the  context of a content-loaded learning task or as targeting cognitive  processes.   2.2 Social Network Analysis in MOOCs   Social network analysis (SNA) is widely used for evaluating  group-level social activity. SNA as a methodology combines  learner agency with the context where it operates. Thus, it offers  group-level insights into social activity in MOOCs. Early on  Ferguson & Shum [9] suggested the use of SNA for investigating  interpersonal relations mediated by social platforms. They  proposed to examine social relations through the prism of strong  and weak ties  theoretically loaded terms borrowed from social  science theory. This proposition aligns with the suggested focus  on inquiries into learning ties by Haythornthwaite & de Laat [20].  However, any inferences about group-level activity in MOOCs  drawn from current SNA research in learning analytics needs to  be enacted with caution.   On the one hand, SNA requires its methodological decisions to be  theoretically driven. Network research is relational, that is, its  main unit of analysis is a theoretically defined relation between  two actors. The meaning assigned to this unit of analysis directly  impacts the interpretations of the results. In short, a network is a  representation of a phenomenon, and consequently, this  phenomenon needs to be conceptually represented in the data [2].  In practical terms this is done by providing justification for  inclusion and exclusion of ties and edges, as well as drawing  network boundaries. According to Laumann, Marsden and  Prensky [35], errors in defining these boundaries reach beyond the  consequences of slightly biased estimates of population means,  proportions, or inefficiency in statistical estimation. Flawed  boundary specification may lead to the fundamental  misrepresentation of the process under study, since errors of  omitting one actor may distort the overall configuration of actors  in the system and render the entire analysis meaningless (p.19).     On the other hand, there is a lack of theorising social processes in  open online courses. As a result, there is a divergence in the  research resulting from the differing conceptualizations of  network ties. This largely stems from how post-reply structures  are interpreted. For instance, researchers examining Twitter  networks in MOOCs analyzed communication represented  through person-to-person information flow [15, 50]. Twitter  communication interactions are possibly the least ambiguous to  interpret as they are by design identical with the name networks  where there is little doubt as to who spoke to whom [16]. Post- reply communication structures appear to be replicated in studies  of platform-based MOOCs delivered via edX and Coursera.  However, the approaches adopted to analyse the forum data  differ. For instance, Joksimovic, et al., [27] used a conventional  directed post-reply structure commonly used in online education  forums: if author A2 replied to a message posted by author A1,  we would add a directed edge A2->A1. Further, if A3 posted a  comment on A2s post, we would include A3->A2 edge as well.  In contrast, Brown and colleagues [4], [5] used a post-reply  network structure based on communication going from one person  to many prior posters. Gillani, et al., [11] adopted another  alternate approach in a Coursera MOOC. In this instance, the  authors represented communication through an edge connecting  two learners simply if they co-posted in at least one discussion  thread. The diversity in how researchers are conceptualising  information/ communication flow as network structures impedes  our capacity to make broader inferences about group-level  learning activity. Hence, to draw meaningful interpretations from  graph analytical techniques employed in SNA of MOOCs, it is  imperative to theorise and conceptualise the phenomenon that is  being studied.     3. THE CURRENT STUDY  3.1. Our Approach  The aim of the current study is to develop indicators to evaluate  social activity in MOOCs, particularly in relation to forum  facilitation. Given that interpersonal relationships at scale may be  unattainable, we propose to examine the amplification of  communication within the networks of MOOC learners.   In our earlier research we argued that teaching online approaches  are derived from formal educational contexts, and there is a strong  need for new analytical frameworks that are distinct for MOOC  settings [39]. While open and formal online learning appear  similar, processes in formal online modes are streamlined by the  strict starting dates and learners motivation to complete. The  formal boundaries and learner motivations lie in stark contrast to  the more informal and open MOOC contexts. Yet, both research  and practice use theoretical and methodological frameworks  inherited from formal settings to analyse open online courses. To  overcome the challenge of compatibility of the tools with the  object of analysis, social processes emerging on MOOC forums  first need to be examined for forum sub-groups comparable in  dynamics to formal education settings. In contrast to many  MOOC learners who drop into the forums and leave, persistent  forum contributors experience a greater sense of continuity and  established history in their relationships typical for social groups  [40]. Thus, a sub-set of forum participants, termed as the forum  core, was established.  The forum core are posters who contributed to the forum in at  least any three weeks of the course and received replies to their  posts within the same course week. The underlying characteristic  of this sub-group is repeated presence and timeliness of the   reciprocated communication exchanges. The number of posts  does not distinguish this group from other posters. Some forum  core members could have as few as four to five posts, and some  non-forum core posters could have over a hundred.  To meaningfully capture indicators of participation patterns we  conceptualized forum core, i.e. a group of returning MOOC  posters, as identity-based community. Then, building on prior  research examining the properties of network formation in social  exchange networks, we inquired about the effects different  facilitation strategies may have on network configurations. The  underlying theoretical assumptions were evaluated by statistical  network analyses applied to ten cases of MOOCs. The following  section offers an overview of the prior work and relevant  theoretical and empirical literature that shaped the research  question.    3.2 Identity-Based Communities  Education research has tended to define community development  within two broad categories  community formed through a  common-identity or formed via a common-bond. The differences  between these categories lie within the origin of the group- attachment. One-to-one attachment among members is a  consequence of people liking each other. When approaching  attachment from a bond-based perspective, the focus is therefore  on interpersonal, dyadic relationships of trust. In contrast,  common-identity attachment is a result of self-identification with  groups purpose as in topic-based groups, such as sports team or a  school newspaper. These differences between common-identity  and common-bond communities are best captured by what  happens when a member leaves. When attachment is motivated by  identity, other members are perceived as interchangeable, and  turnover of membership does not impact individual behaviour as  much as in common-bond communities where members will leave  following their friends.  A common-identity approach has previously been applied to  informal online groups [41, 47, 51]. Postmes, Spears & Lea [44]  argued that the power of anonymity in groups where individuals  do not have cognitive representations of other individuals could  enhance the salience of a groups identity. The authors suggested  that paradigms in which group members do not meet face-to-face  provide precisely those conditions predicted to maximize social  influence exerted by social norms and social identities (p. 7).  Furthermore, in a series of experiments, Ren et al. [42]  demonstrated that facilitation of identity-based attachment had  stronger effects on the frequency of engagement than conditions  targeting interpersonal attachment development. Therefore,  defining a community through identity-based attachment of one- to-many is of high relevance in educational settings such as  MOOCs where the scale and non-privacy of the environment  aggravate the development of bond-based relationships.  In light of identity-based community theory, this present study  approached forum core as a social entity where attachment is  dictated by self-identification with a group without necessarily  investing in specific person-to-person relationships. Examination  of the discourse produced by forum core indicated traces of social  processes that forum core posters engaged in [40]. More  specifically, through qualitative analysis of forum content, Poquet  [38] identified the presence of discourse negotiating situational  norms, shared domain of practice, i.e. MOOC forum, and shared  experience, and, MOOC learning and assessment. Furthermore, in  a complementary study of the perceptions among those  contributing regularly Poquet et al. [39] found that forum core     participants reported the lack of dyadic trust and dyadic  familiarity in interpersonal perceptions but presence of higher  group cohesion perceptions. In short, forum cores in some courses  engaged in collective social processes, and developed perceptions  of group cohesion without underpinning interpersonal trust. This  empirical evidence allows to conceptualize the forum core as an  identity-based community based on information exchange.   3.4 Network Exchange Patterns in Online  Communities  Insights into the baseline properties of similar networks are  required to transition from theoretical analysis of forum core as  identity-based communities to applied analysis of the  amplification of its communication exchange. Studies of  electronic networks of practice offer information about network  topography in identity-based communities that help modelling  forum core participation structure.  Electronic networks of practice are conceptually aligned with the  major premises of identity-based communities. They are informal  groups that share knowledge about specific subject of interest  from photography groups on flickr to Wikipedia contributors to  software developers blogging community. According to Wasko et  al., [54], participants of public electronic networks reported trust  in the quality of information from active members (i.e. attachment  to the group), and that those actively participating were perceived  as trustworthy (i.e. belief in the group), thereby eliciting a  stronger motivation to continue participation (i.e. commitment to  participating in groups activity). Faraj & Johnson theorised such  online communities as social exchanges between participants  situated in a network context. That is, regardless of the content of  exchange, the interactions are more than just information queries,  but also have social nature and social purpose. In other words, in  their activities posters are conscious of the potential use by  readers, as well as current and future contributors through what  becomes a part of shared history. Simply put, electronic networks  of practice appear to represent a particular kind of an identity- based community.  The work of Faraj & Johnson served to empirically validate the  consistent and predictable network exchange patterns that emerge  from divergent motivations. They proposed three network patterns  that characterize the formation of the network of practice on the  micro-level: direct reciprocity (A replies to B because B helped A  in the past), indirect reciprocity (B helped to A, and thus A will  help C), and preferential attachment where new actors choose to  interact with already well-connected actors.  Through a large- scale longitudinal research of Yahoo! Bulletin boards, they found  a significant positive high propensity of direct reciprocation (A to  B to A), significant positive low propensity for indirect exchange  happening (A to B to C), and a negative propensity for  preferential attachment. The negative propensity for preferential  attachment has also been supported in studies of MOOC forum  networks [27, 30]. In summary, network configurations of  reciprocity lend themselves nicely to modelling baseline network  properties in forum core networks for further examination of the  differences between them in different facilitation contexts.   3.4. Focus of the Study  Building on the discussed theoretical and empirical literature,  MOOC forum cores are assumed to reflect natural patterns  occurring in identity-based communities characterized by network  exchange. Accordingly, forum cores that emerge and evolve in  courses with pedagogical interventions constituted by moderation   strategies would demonstrate different dynamics, varying along  with the intensity of facilitation. These differences in network  structure in courses with moderate and higher moderation would  then indicate the effects of intended forum strategies. In line with  this argument, the following research question was posed:  Are there differences in patterns of direct and triadic-level  reciprocity in forum core networks under different facilitation  strategies   To address this research question forum core communication  networks were analysed using models of social selection based on  the p*class models  [44].   4. METHODS  4.1. Network Construction  Forum core networks were constructed based on the premise that  they represented identity-based communities. That is, the analyses  were constructed from the forum core posters that were  committed to participating in forums activity, and driven by the  mix of altruistic and selfish reasons to engage in information  exchange. Their commitment was captured by the extended  participation criterion that distinguished forum core posters from  intermittent posters.   Network ties were non-valued and directed representing the  direction of the service offered. The ties were directed to all the  posters in the same thread who preceded the actor based on  timestamps. If A posted, B replied, and C commented, and D  replied, each of the subsequent actors would have a directed tie to  everybody else prior to them. That is, B -> A, C-> B, C->A, D- >C, D->B, D->A. Such tie inclusion was based on the principle of  collective reciprocity where an individual contributed information  to the group, rather than provided a service to an identified  individual.   The network boundaries were set around the course delivery time:  between the week the first video lecture was released, and the  week when the last lecture in the course was released. As  communication between actors can potentially continue well past  the course lectures due to exams or assignments, any established  closing date for the forum analysis would be arbitrary. The  discrete temporal limits applied in this study made the comparison  of courses more feasible.   4.2 Data Description  This present study analysed the forum interactions evolving from  ten (10) STEM courses delivered by the Delft University of  technology via edX platform in 2013-2015. Due to the evaluative  nature of the study, the courses were de-identified. Table 1 offers  an overview of the disciplines, sizes of posting cohort against the  forum core, course duration and qualitative description of the  forum facilitation strategies.   The dataset included five large MOOC with forum core higher  than one hundred people, and five smaller courses with forum  cores ranging from 2370 people. Three courses were highly  facilitated, i.e. instructor was among active posters, along with  designated staff members and TAs. Five courses had moderate  facilitation, i.e. although staff and TAs helped the information  flow, instructor was not involved with the forum. Finally, two  courses had low facilitation, i.e. nobody moderated the forums  except one or two staff announcement were posted.            Table 1. Course overview with forum facilitation strategies.   Cours e    Area Posters Foru m   Core   Course  Duratio  n   Forum  Facilitatio  n  A Soft Systems 752 48 10 Moderate  B Engineering 4384 340 10 Low   C Engineering 668 38 6 Moderate   D Soft Systems 1859 70 6 Moderate  E Engineering 2480 231 9 High   F Engineering 6775 254 10 Moderate  G Engineering 360 23 5 Low  H Soft Systems 1255 43 5 Moderate  I Data Analysis 2825 220 8 High  J Computer   Science  1433 161 8 High     4.3 Exponential Random Graph Modelling  Exponential random graph modelling (ERGM) was used to  analyse the network properties describing the structure of forum  core networks. ERGM is a methodology for the analysis of social  selection models. These models assume that characteristics of  actors influence them to select or be selected by others as social  partners [44]. These micro-level interactions resulting from social  selection aggregate into group-level patterns that describe the  network. Additionally, p*/ERGM presumes that multiple  processes can take place simultaneously, and social networks are  both structured and stochastic [36 p.10].    ERGM is a probability model for network analysis [17, 19, 36,  37]. In modelling ERGM estimates the likelihood of a parameter  for a theoretically justified network configuration (form) to occur  beyond chance. This is implemented by examining the likelihood  of a studied configuration (network form) to occur in a generated  distribution of random networks modelled on the network of  interest. Additionally, multi-level analysis within ERGM controls  for the tendency of studied parameters against one another, due to  their theoretical dependency. The output includes a parameter  estimate where zero indicates that the modelled effect is not  different from random. Estimated parameters are considered of  value after the goodness of fit is conducted upon the best fitting  model.    4.4 Modelled Configurations and Effects  This study investigated three configurations within forum core  networks: direct reciprocity characterized by mutuality statistic,  triadic-level exchange characterized by simmelianties statistic,  and the effect of learner super-posting activity on the propensity  of sending and receiving ties. Figure 1 depicts three network  configurations. The first two represent configurations of direct  and indirect reciprocity that, according to Faraj & Johnson are  baseline properties of network formation in identity-based  communities. We further hypothesised that forum core members  who overtime co-occur within direct (A to B to A, Figure 1a) and  indirect reciprocity (A to B to C, Figure 1b) will overtime form  triadic network configurations representing mutual and triadic  exchange that eventually could extend to a cycle if interaction  should continue. Joksimovic, et al., [27] previously fitted similar  network configuration (Figure 1c) using simmelianties statistic in  ERGM for MOOC forums. The authors interpreted it as  Simmelian ties [32]. From the analytical perspective, we take  inspiration and build on their approach. Yet, we interpret this   network configuration as amplification of parts of the network  due to generalized information exchange between the actors. Only  direct reciprocation and triadic-level exchange configurations  were therefore modelled.       a) Direct  Reciprocity b) Indirect Reciprocity c) Triadic-level exchange   Figure 1. Network Features Modelled in ERGMs. Adapted from  Faraj & Johnson (2010).   Markov network configuration were not used in the study to  model triadic exchange. Instead we use simmelainties statistic  available in R statnet package [18]. Morris, Hunter & Handcock  [37] explain that simmelian triads (Figure 1c) can overlap in terms  of nodes and ties, thus simmelianties is rather a measure of the  clustering of Simmelians (given the number of Simmelians).  Although social circuit parameters (gwesp) were fitted on some of  the networks, they were not characteristic of all networks, and  thus not used to maintain comparability in the modelled outputs.  Table 2 presents the counts of modelled network configurations  for each of the networks.  Table 2. Counts of network configuration in analyzed  networks.    Course/  Facilitation   Node s   Edges  (density*)   Reciprocity  (mutual)    Ties embedded in  triadic-level   exchange   (simmelianties)   A/Moderate 48 968 306 602  B/Low 340 4574 725 1164  C/Moderate 38 367 111 196  D/Moderate 70 713 164 264  E/High 231 3743 1003 1938  F/ Moderate 254 3059 629 1060  G/ Low 23 86 19 0  H/ Moderate 43 249 50 34  I/ High 220 2260 516 882  J/ High 161 2978 880 1734   Note: *equivalent statnet name for the parameters   Additionally, in ERGMS we controlled for the activity level of  each learner within the network using the nodefactor parameter.  To establish the overall level of activity, we applied in-reach and  out-reach measures of each individual activity as suggested by  Hecking, Hoppe & Harrer [22]. The authors developed measures  of entropy to calculate the diversity of outgoing and ingoing  relations for a node, and account for the weight of an edge, i.e.  frequency of dyadic ties between two given actors. We have  replicated their measure1, and applied it to the entire forum  network. We then applied k-means clustering to divide all forum  posters into three groups: with highest, moderate and low forum  posting activity in the entire network. These attributes of posting  activity were used to control for tie propensity formation in forum  core ERGMs. Table 3 presents the number of actors within each  cluster. Table 3 demonstrates the number of people with different  posting activity in each of the networks. Cluster 1 refers to the  learners with low posting activity. Cluster 2 refers to the posters  with moderate posting activity, and Cluster 3 represents  hyperposters. These interpretations do not fit to describe Course  C, where Cluster 3 and Cluster 4, in fact have similar activity                                                                        1https://github.com/hecking/socio_semantic_blockmodelling/blob  /master/scripts/centrality_over_time.R     levels, but Cluster 2 has higher in-reach measures, and Cluster 3  has higher out-reach measure.  Table 3. Number of people in the cluster representing  different posting activity after k-means clustering of in-reach  and out-reach measures of posting activity in the entire  network   Course Cluster1 Cluster2 Cluster3  A 503 65 3  B 2998 273 6  C 381 75 50  D 856 398 1  E 1297 459 4  F 4082 40 7  G 262 58 7  H 849 61 11  I 1994 38 3  J 1317 41 4     Labelling learners as moderate, or of high and low activity was  relative to one another within the course. That is, a hyperposter in  one course may have similar activity numerically as the moderate  poster in another course. Appendix 1 presents the measures for  cluster representatives.   5. RESULTS  This study examined the extent of communication exchange  amplification in the forum core networks of multiple MOOC  courses. Using an ERGM approach, we modelled the baseline  density of networks, direct reciprocity of ties, and triadic-level  exchange as the propensity of simmelianties network  configuration to cluster. We have also controlled for the level of  activity distinguishing between individuals with low, moderate  and high posting behaviour. The research question inquired if any  differences were observed between the measures of direct and  triadic-level exchange in networks where staff implemented  different facilitation strategies.  All fitted ERGMs have converged, were not degenerate, and  showed acceptable goodness of fit. All models were fitted using  the described network configurations. The two exceptions were  course G where models did not converge with the effects of  posting activity, and Course D where no super-posters were a part  of the forum core. Table 4, 5 and 6 outline the results grouped  together for courses with similar facilitation strategies.   First and foremost, our hypothesis that courses with no or low  facilitation reflect the network structures typical for online public  electronic groups was supported. In courses with low facilitation  (Table 4) we observed higher propensity for direct exchange, and  low or no propensity for exchange at the triadic-level. Course B,  as a large course, had posters with high level of activity, and they  were somewhat more likely to form ties than the posters with  moderate participation, though the difference was not stark.  Course G was very small, had no triadic-level exchange features,  and the null model was only slightly improved by adding the  direct reciprocity configuration.   In courses with moderate facilitation (Table 5), where teaching  assistants, staff or community assistants were advancing the  information flow, the dynamics had some similarities and  differences with unmoderated settings. For the similarities, theta  estimates for reciprocity in Courses D and F were higher than  those for triadic-level exchange mirroring the dynamics of  unmoderated courses. In three small courses (A, F and H) triadic- level exchange parameters were not significant, or even negative.   Interestingly, the propensity of super-posters to form ties as  compared to those posting moderately was more pronounced in  courses with lower or no triadic exchange, and heightened  reciprocity.  Table 4. ERGMs outputs for courses with low facilitation  Features/Courses B G   Density -4.21*** (0.03) -1.44*** (0.3)                                                 Structural Properties    Reciprocity 1.29*** (0.1) 1.84*** (0.3)  Triadic-Level Exchange 0.56*** (0.04) --    Main Effects    Moderate Participation 0.83*** (0.02) --  High Participation 1.66*** (0.05) --   AIC Null 38485 463  AIC Final 32926 442     In contrast, course F with highest propensity for triadic-level  exchange features to cluster, indicating higher network  amplification, has least difference between the propensity for tie  formation between those with moderate and high activity.   Table 5. ERGM outputs for courses with moderate facilitation   Features/          A   Courses   C D F H   Density           -1.66***                    (0.06)   -2.34***  (0.11)   -2.93***  (0.08)   -3.8***  (0.03)   -3.06***  (0.16)   Structural Properties   Reci-               1.82***  procity              (0.3)                  1.35***  (0.35)   0.09***  (0.22)   1.57***  (0.1)   1.45***  (0.26)   Triadic-level    -0.55***  Exchange        (0.13)   0.21.  (0.13)   0.46***  (0.09)   0.66***  (0.06)   0.04  (0.09)   Main Effects on Ties Formation   Moderate        1.12***  Participation  (0.01)     0.67***  (0.1)   0.068***  (0.06)   0.85***  (0.04)   0.59***  (0.1)   High                3.17***  Participation   (0.23)   1.19***  (0.13)   -- 1.3***  (0.05)   1.12***  (0.14)   AIC Null 3084 1616 4045 24601 1451   AIC Final 2461 1334 3523 20451 1307     These differences between a moderated Course A and  unmoderated Course B can be interpreted as follows. In an  unmoderated network information exchanges are random,  distributed and decentralised. In moderated networks, moderators  have much higher posting activity, sometimes dominating in  offering information to other learners. When this occurs, the  triadic-level exchange features seem to be either low, negative or  non-significant. By translating the log odds, we observe that the  high participation posters in course A were 22 times more likely to  form a tie as compared to low posters, while moderate posters  were three times more likely. At the same time, in an unmoderated  course B: posters with higher activity were six times as likely to  make a post than those with low posting behavior, and moderate  posters were three times more likely. We can speculate that in  cases where super-posters do not over-dominate, activity of those  with moderate posting behavior overtime grows, and these actors  engage in more conversations with one another as well as     interacting with super-posters, thereby resulting in an  amplification of communication exchange.  Finally, network structures in courses with high facilitation  differed from both moderated and unmoderated courses without  instructor participation (Table 6). Overall, it appears that in  courses with high facilitation, triadic exchange configuration has  a higher likelihood to occur than direct reciprocity. Such  dynamics are demonstrated in courses E and J, where clustering  of reciprocal triads are more characteristic of the network than  person-to-person reciprocations. In a course I, however, direct  reciprocity is still more characteristic of the network, which we  interpret as the lack of amplification within the information  exchange. Again, similar to the pattern observed with the  moderately facilitated courses, in course I, high posting  individuals are much more likely to form ties. In fact, by  converting log odds, we found that a super poster in course I was  eleven times more likely to form network ties than those with a  low level of forum activity. In courses E and J the likelihood for  super-posters to form ties was three times more than participants  with a low posting behavior. To note, the individual number of  posts by super-posters in course I was actually lower by count  than that of super-posters in the other two courses.    Table 6. ERGM outputs for courses with high facilitation   Features  /Courses   E I J   Density -3.69*** (0.02) -4***(0.03) -3.21***(0.04)  Structural Properties   Reciprocity 0.93*** (0.17) 1.91***(0.15) 0.65*(0.27)  Triadic-level  Exchange 1.13*** (0.08) 0.31***(0.06) 1.05***(0.13)   Main Effects  Moderate  Participation  0.53*** (0.023) 1.33***(0.04) 0.73***(0.03)   High  Participation 0.98*** (0.05) 2.41***(0.07) 1.32***(0.06)   AIC Null 27098 18244 18450  AIC Final 22005 13812 14347     To extrapolate, the network dynamics is opposite between courses  with high facilitation and low or no facilitation. Direct reciprocity  of knowledge exchange seems to be inherent in identity-based  communities, such as forum core. Overtime and with facilitative  efforts network structure starts being defined by core and  periphery: with both random reciprocation and low clustering of  more generalized, triadic, exchange, likely at the core of the  group. This clustering may also be interpreted as clique  formation, or polarization of power and access that takes place as  the network shifts from distributed to amplified. Facilitated forum  core networks appear to be characterized by higher degree of  clustering of reciprocated triads, and lower level of random  reciprocation. This could mean that random direct exchanges  within the group decrease, as information flow gets to amplify  across more and more members, shifting from cliquish core and  distributed periphery to an amplified interconnected cluster. We  also observed that super-posting activity does seem to be  associated with lower level of exchange between triads. In other  words, a moderator may be taking over what somebody else could  address by offering her services too much or too fast, therefore  not allowing other members to indirectly reciprocate to the group.   To conclude, according to ERGM results, network features  modelled to gain insight into network amplification were useful in   highlighting differences between the courses with different  facilitation strategies. More specifically, direct reciprocation was  characteristic of non-facilitated groups, while triadic-level  exchange was more prominent in highly facilitated online  communities with instructors involvement. Finally, super posting  activity was less pronounced in networks with higher triadic-level  exchange, and more pronounced in networks with higher direct  reciprocity.   6. DISCUSSION  The aim of the current study was to establish potential indicators  for evaluating social activity in MOOCs, particularly in relation to  forum facilitation. Thus, in lieu of measuring affective individual  perceptions as indicators of socio-emotional processes, the extent  of amplification in communication exchanges between MOOC  learners was examined. A sub-group of MOOC forum posters was  conceptualized as an identity-based community. Building on the  prior research on the properties of network formation in social  exchange networks, we modelled ten forum core networks using  network configurations of direct reciprocity and triadic-level  exchange, while controlling for the overall level of individual  activity.  Results support our hypotheses about the nature of the forum core  in MOOCs, and suggest that the chosen indicators of forum core  networks may be useful in evaluating the effects of facilitation.  That is, courses with varying levels of moderation differ in the  likelihood of direct and triadic exchanges that take place. In non- facilitated courses, the dynamics of the networks was similar to  electronic networks of practice, and largely characterized by  direct reciprocity. In highly moderated networks, triadic-level  exchange configurations were more prominent than direct  reciprocity. That is, these networks were amplified by the  propensity of reciprocated ties in triads to cluster. Furthermore, a  higher degree of triadic-level exchange seemed to take place in  networks where activity of super-posting participants was less  pronounced as compared to those posting moderately or at low  levels.  The main contribution of the study is that its indicators of group- level social activities differentiated between the network  structures in forums with different facilitation strategies. Our  results demonstrated that moderating the forum per se is  insufficient for the effective evolution of participation. Courses  with teaching assistants and staff demonstrate different patterns  than those with instructor involvement. In courses with moderate  facilitation interventions reciprocal triads have lower likelihood to  occur than in highly facilitated forums with instructors  involvement. These findings re-iterate the importance of teachers  social presence and its impact on the level of engagement in open  online communities.  It is not surprising to learn that an instructors presence motivates  learners to participate - even in large scale courses. This finding  has been widely discussed and supported in formal online  education research. This study offers additional insights into the  roles, played by the staff, teaching and community assistants.  MOOC research claimed their importance in supporting the spirit  of the forums. Our findings further demonstrate that moderated  forums appear to transition from their baseline properties, as the  structure shifts from distributed to more hierarchical network.  The results of the present study also support Huang et al.s  conclusion that super-posters do not drown out the silent majority.  The observed interplay between the prominence of moderator  involvement and the development of exchange at the triadic-level     features suggest that the health of the forum may still be affected.  In courses where super-posting activity is more distinct than that  of moderate posters, features of triadic-level exchange are not  more pronounced than direct reciprocity features. Such dynamics  may indicate that super-posters are limiting engagement  opportunities for learners who are moderately active. In other  words, if there is a moderator who always replies, and no teacher  to stimulate the dialogue, then the community has less reason to  engage with one another. It seems that super-posters, who are  staff and teaching assistants at early stages, help the network to  develop, but they are not always skillful at decreasing their  activity at an optimal time.  This studys analysis yielded many promising results. However,  further work is still required. Much of the interpretation of this  work and the associated implications are at this point speculative.  Future work should seek to account for the temporal aspect of  how the network unfolds and interacts, as well as include forum  core interactions with the rest of the forum posters in MOOCs. An  extrapolation of the findings relates to the time based dynamics of  network formation. We suggest that facilitation shifts the forum  core network structure from a distributed to cliquish to egalitarian  structure. Here an egalitarian structure is the product of combined  direct and indirect reciprocity. Forum core is in its turn situated  within the network exchanges of intermittent posters, and the  relationship of forum core to those posters is based on the features  of indirect reciprocity. That is, if a forum core member gets  reciprocated by a staff member at an early stage, she would be  more likely to offer answer to an intermittent poster later on. In  other words, measuring the extent of network amplification over  time offers partial explanation of the nature of social exchanges,  while indirect reciprocation between the forum core and the other  poster explains the remaining dynamics. These speculations form  the basis of our future research.  Several direct implications stem from this studys analyses. First,  empirical validation of forum core networks as identity-based  communities is valuable as MOOC forum research can build on  the established research agenda both in social science and in  studies of human-computer interaction. For instance, Ren et al.  [43] developed a set of design principles for the facilitation of  identity-based communities, and these could be applied in  MOOCs. Yet, as the dataset of courses was limited, more diverse  sets of courses need to be analyzed to see if the insights  demonstrate consistent patterns across disciplines, student cohorts  and class sizes.  While this study did not include text mining and analysis of  learner posts, identity-based online communities have been  researched using text analysis along with SNA. Particularly  [14]  suggested that measures of entropy capture the lack of topical  diversity in identity-based communities, what they refer to as  topical groups, and the divergence of discussion themes in what  they refer to as social groups. Furthermore, granular text mining  for the concepts related to power and authoritativeness [e.g. 23]  may offer insights as to where the shifts in network structure are  reflected in the text features of the various learner posts. On a  final note, the indicators proposed from this work should be  validated in formal online education environments, to assess for  comparability between formal and open online settings.   7. ACKNOWLEDGMENTS  Our thanks to TU DELFT and Open Online Learning at DelftX.   8. REFERENCES  [1] Bell, F. (2011). Connectivism: Its place in theory-informed   research and innovation in technology-enabled learning. The  International Review of Research in Open and Distance  Learning, 12(3).   [2] Brandes, U., Robins, G., McCranie, A. and Wasserman, S.  (2013). What is network science Network Science, 1(1), 1 15. http://dx.doi.org/10.1017/nws.2013.2   [3] Brinton, C.G., Chiang, M., Jain, S., Lam, H., Liu, Z. and  Wong, F.M.F. (2013). Learning about social learning in  MOOCs: From statistical analysis to generative model. IEEE  Transactions on Learning Technologies, 7(4).  http://dx.doi.org/10.1109/TLT.2014.2337900   [4] Brown, R., Lynch, C.F., Eagle, M., Albert, J., Barnes, T.,  Baker, R., Bergner, Y. and McNamara, D. (2015). Good  communities and bad communities: Does membership affect  performance Proceedings of the 8th International  Conference on Educational Data Mining, 612614.   [5] Brown, R., Lynch, C., Wang, Y., Eagle, M., Albert, J.,  Barnes, T., ... McNamara, D. (2015). Communities of  performance & communities of preference. In CEUR  Workshop Proceedings. (Vol. 1446). CEUR-WS.   [6] Conole, G. (2013). Designing for learning in an open world.  New York: Springer.   [7] De Laat, M., Lally, V., Lipponen, L. and Simons, R.-J.  (2007). Online teaching in networked learning communities:  A multi-method approach to studying the role of the teacher.  Instructional Science, 35(3), 257286.  http://dx.doi.org/10.1007/s11251-006-9007-0   [8] Dowell, N. M., Skrypnyk, O., Joksimovi, S., Graesser, A.  C., Dawson, S., Gaevi, S.,  Kovanovi, V. (2015).  Modeling learners social centrality and performance through  language and discourse. In C. Romero & M. Pechenizkiy  (Eds.), Proceedings of the 8th International Conference on  Educational Data Mining (pp. 250257). International  Educational Data Mining Society.   [9] Ferguson, R., Shum, S.B. (2012). Social learning analytics:  five approaches. Proceedings of the 2nd international  conference on learning analytics and knowledge, 2333.  http://dx.doi.org/10.1145/2330601.2330616   [10] Gillani, N., & Eynon, R. (2014). Communication patterns in  massively open online courses. The Internet and Higher  Education, 23, 18-26.  http://dx.doi.org/10.1016/j.iheduc.2014.05.004   [11] Gillani, N., Yasseri, T., Eynon, R., & Hjorth, I. (2014).  Structural limitations of learning in a crowd: communication  vulnerability and information diffusion in MOOCs. arXiv  preprint arXiv:1411.3662 [physics.soc-ph].  http://dx.doi.org/10.1038/srep06447   [12] Goldberg, L. R., Bell, E., King, C., OMara, C., McInerney,  F., Robinson, A., & Vickers, J. (2015). Relationship between  participants level of education and engagement in their  completion of the Understanding Dementia Massive Open  Online Course. BMC medical education, 15(1), 1.  http://dx.doi.org/ 10.1186/s12909-015-0344-z   [13] Goodyear, P., Salmon, G., Spector, J. M., Steeples, C., &  Tickner, S. (2001). Competences for online teaching: A  special report. Educational Technology Research and  Development, 49(1), 65-72. http://dx.doi.org/  10.1007/BF02504508     [14] Grabowicz, P. A., Aiello, L. M., Eguluz, V. M., & Jaimes,  A. (2013). Distinguishing topical and social groups based on  common identity and bond theory. In Proceedings of the  sixth ACM international conference on Web search and data  mining, 627-636. http://dx.doi.org/  10.1145/2433396.2433475    [15] Gruzd, A., & Haythornthwaite, C. (2013). Enabling  community through social media. Journal of medical  Internet research, 15(10), e248.   [16] Gruzd, A. & Haythornthwaite, C. (2008). Automated  discovery and analysis of social networks from threaded  discussions. Paper presented at the International Network of  Social Network Analysis, St. Pete Beach, FL, USA.   [17] Handcock, M., Hunter, D., Butts, C., Goodreau, S.,  Krivistky, P. and Morris, M. 2015. ergm: Fit, Simulate and  Diagnose Exponential-Family Models for Networks. The  Statnet Project. http://www.statnet.org.   [18] Handcock, M. S., Hunter, D. R., Butts, C. T., Goodreau, S.  M., & Morris, M. (2003). statnet: Software tools for the  Statistical Modeling of Network Data. Seattle, WA. Version,  2.   [19] Harris, J. K. (2013). An introduction to exponential random  graph modeling (Vol. 173). Sage Publications.   [20] Haythornthwaite, C. and De Laat, M. (2012). Social network  informed design for learning with educational technology. In  A.D. Olofsson, J.O. Lindberg, K. Klinger, C. Shearer (Eds.),  Informed Design of Educational Technologies in Higher  Education: Enhanced Learning and Teaching (pp.352374).  IGI Global.   [21] Hecking, T., Chounta, I.-A. and Hoppe, H.U. (2016).  Investigating social and semantic user roles in MOOC  discussion forums. Proceedings of the Sixth International  Conference on Learning Analytics & Knowledge, 198207.  http://dx.doi.org/10.1145/2883851.2883924   [22] Hecking, T., Hoppe, H.U. and Harrer, A. (2015). Uncovering  the Structure of Knowledge Exchange in a MOOC  Discussion Forum. Proceedings of the 2015 IEEE/ACM  International Conference on Advances in Social Networks  Analysis and Mining 2015, 16141615. http://dx.doi.org/  10.1145/2808797.2809359   [23] Howley, I., Mayfield, E. and Ros, C.P. (2011). Missing  something Authority in collaborative learning. In A.  Dimitracopolou, C. OMalley, D- Suthers, P-Reimann (Eds.),  Proceedings of the 9th Computer Supported Collaborative  Learning Conference (pp.336373).   [24] Huang, J., Dasgupta, A., Ghosh, A., Manning, J. and  Sanders, M. (2014). Superposter behavior in MOOC forums.  Proceedings of the first ACM conference on  Learning @ Scale, 117126.   [25] Jiang, S., Warschauer, M., Williams, A.E., ODowd, D. and  Schenke, K. (2014). Predicting MOOC performance with  week 1 behavior. Proceedings of the 7th International  Conference on Educational Data Mining, 273275.   [26] Joksimovi, S., Dowell, N., Skrypnyk, O., Kovanovi, V.,  Gaevi, D., Dawson, S. and Graesser, A.C. (2015). How do  you connect Analysis of social capital accumulation in  connectivist MOOCs. Proceedings of the Fifth International  Conference on Learning Analytics and Knowledge, 6468.  http://dx.doi.org/10.1145/2723576.2723604   [27] Joksimovi, S., Manataki, A., Gaevi, D., Dawson, S.,  Kovanovi, V. and de Kereki, I.F. (2016). Translating  network position into performance: importance of centrality  in different network configurations. Proceedings of the Sixth  International Conference on Learning Analytics &  Knowledge, 314323.  http://dx.doi.org/10.1145/2883851.2883928   [28] Jones, C. (2004). Networks and learning: communities,  practices and the metaphor of networks. Research in  Learning Technology, 12(1).   [29] J Justice, T., & Jamieson, D. W. (2012). The facilitator's  fieldbook. AMACOM Div American Mgmt Assn.    [30] Kellogg, S., Booth, S. and Oliver, K. (2014). A social  network perspective on peer supported learning in MOOCs  for educators. The International Review of Research in Open  and Distributed Learning, 15(5).   [31] Kizilcec, R.F., Piech, C. and Schneider, E. (2013).  Deconstructing disengagement: Analyzing Learner  Subpopulations in Massive Open Online Courses.  Proceedings of the Third International Conference on  Learning Analytics and Knowledge - LAK 13.   [32] Krackhardt, D. (1998). Simmelian ties: Super strong and  sticky. In R. Kramer, M. Neale (Eds.), Power and influence  in organizations (pp. 2138). SAGE Publications: USA.   [33] Kreijns, K., Kirschner, P. A., & Jochems, W. (2003).  Identifying the pitfalls for social interaction in computer- supported collaborative learning environments: a review of  the research. Computers in human behavior, 19(3), 335-353.  http://dx.doi.org/10.1016/S0747-5632(02)00057-2   [34] Kreijns, K., Kirschner, P. A., Jochems, W., & Van Buuren,  H. (2011). Measuring perceived social presence in  distributed learning groups. Education and Information  Technologies, 16(4), 365-381.  http://dx.doi.org/10.1007/s10639-010-9135-7   [35] Laumann, E., Marsden, P. and Prensky, D. (1983). The  boundary specification problem in network analysis. In  L.Freeman, D. White (Eds.), Research methods in social  network analysis (pp.6186). Transaction Publishers: London,  UK.   [36] Lusher, D., Koskinen, J. and Robins, G. (2012). Exponential  random graph models for social networks: Theory, methods,  and applications. Cambridge University Press: USA.   [37] Morris, M., Handcock, M. S., & Hunter, D. R. (2008).  Specification of exponential-family random graph models:  terms and computational aspects. Journal of statistical  software, 24(4), 1548 7660.   [38] Poquet, O. (2016, September). Needle in a haystack:  Analysis of social processes in MOOCs. Presentation at the  EASS HDR Forum, School of Education, University of  South Australia.   [39] Poquet, O., Kovanovic, V., de Vries, P., Hennis, T.,  Joksimovic, S., Gasevic, D. and Dawson, S. (n.d.) Social  presence in Massive Open Online Courses. Manuscript  submitted for review.   [40] Poquet, O. and Dawson, S. (2016). Untangling MOOC  learner networks. Proceedings of the Sixth International  Conference on Learning Analytics & Knowledge, 208212.  http://dx.doi.org/10.1145/2883851.2883919   [41] Postmes, T., Spears, R., & Lea, M. (1999). Social identity,  normative content, and  deindividuation  in computer-    mediated groups. In R. Spears, B. Doosjie & N. Ellemans  (Eds.), Social identity: Context, commitment, content (pp.  164183). Oxford: Blackwell.    [42] Ren, Y., Harper, F. M., Drenner, S., Terveen, L. G., Kiesler,  S. B., Riedl, J., & Kraut, R. E. (2012). Building Member  Attachment in Online Communities: Applying Theories of  Group Identity and Interpersonal Bonds. Mis Quarterly,  36(3), 841-864.   [43] Ren, Y., Kraut, R., & Kiesler, S. (2007). Applying common  identity and bond theory to design of online communities.  Organization studies, 28(3), 377-408.  http://dx.doi.org/10.1177/0170840607076007   [44] Robins, G., Pattison, P., & Elliott, P. (2001). Network  models for social selection processes. Psychometrika,  66(161). http://dx.doi.org/10.1007/BF02294834   [45] Rourke, L., Anderson, T., Garrison, D. R., & Archer, W.  (2007). Assessing social presence in asynchronous text- based computer conferencing. International Journal of E- Learning & Distance Education, 14(2), 50-71.    [46] Rovai, A. P. (2002). Building sense of community at a  distance. The International Review of Research in Open and  Distributed Learning, 3(1).   [47] Sassenberg, K. (2002). Common bond and common identity  groups on the Internet: Attachment and normative behavior  in on-topic and off-topic chats. Group Dynamics: Theory,  Research, and Practice, 6(1), 27.  http://dx.doi.org/10.1037/1089-2699.6.1.27   [48] Siemens, G. (2004, December). Connectivism: A learning  theory for the digital age. [Web log post]. Retrieved from  http://www.elearnspace.org/Articles/connectivism.htm   [49] Siemens, G. (2008). Learning and Knowing in Networks:  Changing Roles for Educators and Designers. Presented at  ITFORUM.    [50] Skrypnyk, O., Joksimovic, S., Kovanovic, V., Gasevic, D.  and Dawson, S. 2014. Roles of course facilitators, learners,  and technology in the flow of information of a cMOOC. The  International Review of Research in Open and Distributed  Learning, 16(3).    [51] Utz, S. (2003). Social identification and interpersonal  attraction in MUDs. Swiss Journal of  Psychology/Schweizerische Zeitschrift fr  Psychologie/Revue Suisse de Psychologie, 62(2), 91.  http://dx.doi.org/10.1024//1421-0185.62.2.91   [52] Vu, D., Pattison, P. and Robins, G. (2015). Relational event  models for social learning in MOOCs. Social Networks, 43,  121135. http://dx.doi.org/10.1016/j.socnet.2015.05.001   [53] Wang, X., Wen, M. and Ros, C.P. (2016). Towards  triggering higher-order thinking behaviors in MOOCs.  Proceedings of the Sixth International Conference on  Learning Analytics & Knowledge, 398407.  http://dx.doi.org/10.1145/2883851.2883964   [54] Wasko, M., Teigland, R. and Faraj, S. (2009). The provision  of online public goods: Examining social structure in an  electronic network of practice. Decision Support Systems,  47(3), 254265. http://dx.doi.org/10.1016/j.dss.2009.02.012   [55] Yang, D., Sinha, T., Adamson, D. and Rose, C. (2013).  Turn on, Tune in, Drop out: Anticipating student dropouts  in Massive Open Online Courses. Presented at the NIPS  Workshop on Data Driven Education, Lake Tahoe, Nevada,  USA.   Appendix 1.     Table A1. Results of k-means clustering of in-reach and out-reach measures derived from the entire MOOC forum network in each  of the courses. Attributes for clusters were used to control for activity in forum core ERGM. In most courses clusters were  interpreted as posters with high, moderate and low posting activity (except for C)   Course A B C D E F G H I J   Cluster 3 - High Posting Activity  Posters, N 3 6 50 1 4 7 7 11 3 4  In-reach Centroid 13525 6045 252 262089 50415 18797 601 641 14630 69788   Out-reach Centroid 18892 7808 835 178378 58515 22576 917 672 12994 81533   Cluster 2 - Moderate Posting Activity  Posters, N 65 273 75 398 459 40 58 61 38 41  In-reach Centroid 2612 655 578 2748 2337 4484 214 182 2054 16114   Out-reach Centroid 2536 710 221 2900 2375 6116 212 223 2864 21084   Cluster 1 - Low Posting Activity  Posters, N 503 2998 381 856 1297 4082 262 849 1994 1317  In-reach Centroid 475 32 20 144 116 170 15 14 65 743  Out-reach Centroid 463 35 20 142 91 159 19 12 58 619        "}
{"index":{"_id":"28"}}
{"datatype":"inproceedings","key":"Donnelly:2017:WMA:3027385.3027417","author":"Donnelly, Patrick J. and Blanchard, Nathaniel and Olney, Andrew M. and Kelly, Sean and Nystrand, Martin and D'Mello, Sidney K.","title":"Words Matter: Automatic Detection of Teacher Questions in Live Classroom Discourse Using Linguistics, Acoustics, and Context","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"218--227","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027417","doi":"10.1145/3027385.3027417","acmid":"3027417","publisher":"ACM","address":"New York, NY, USA","keywords":"automatic speech recognition, classroom analytics, natural language processing, question detection","Abstract":"We investigate automatic detection of teacher questions from audio recordings collected in live classrooms with the goal of providing automated feedback to teachers. Using a dataset of audio recordings from 11 teachers across 37 class sessions, we automatically segment the audio into individual teacher utterances and code each as containing a question or not. We train supervised machine learning models to detect the human-coded questions using high-level linguistic features extracted from automatic speech recognition (ASR) transcripts, acoustic and prosodic features from the audio recordings, as well as context features, such as timing and turn-taking dynamics. Models are trained and validated independently of the teacher to ensure generalization to new teachers. We are able to distinguish questions and non-questions with a weighted F1 score of 0.69. A comparison of the three feature sets indicates that a model using linguistic features outperforms those using acoustic-prosodic and context features for question detection, but the combination of features yields a 5% improvement in overall accuracy compared to linguistic features alone. We discuss applications for pedagogical research, teacher formative assessment, and teacher professional development.","pdf":"Words Matter: Automatic Detection of Teacher Questions   in Live Classroom Discourse using Linguistics,    Acoustics, and Context   Patrick J. Donnelly   University of Notre Dame  Notre Dame, IN, 46556, USA   pdonnel4@nd.edu      Sean Kelly  University of Pittsburgh  Pittsburgh, PA 15260  spkelly@pitt.edu   Nathaniel Blanchard   University of Notre Dame   Notre Dame, IN, 46556, USA  nblancha@nd.edu      Martin Nystrand  University of Wisconsin, Madison   Madison, WI 53715  nystrand@wisc.edu   Andrew M. Olney  University of Memphis  Memphis, TN 38152   aolney@memphis.edu     Sidney K. DMello  University of Notre Dame   Notre Dame, IN, 46556, USA  sdmello@nd.edu  ABSTRACT  We investigate automatic detection of teacher questions from audio  recordings collected in live classrooms with the goal of providing  automated feedback to teachers. Using a dataset of audio recordings  from 11 teachers across 37 class sessions, we automatically  segment the audio into individual teacher utterances and code each  as containing a question or not. We train supervised machine  learning models to detect the human-coded questions using high- level linguistic features extracted from automatic speech  recognition (ASR) transcripts, acoustic and prosodic features from  the audio recordings, as well as context features, such as timing and  turn-taking dynamics. Models are trained and validated  independently of the teacher to ensure generalization to new  teachers. We are able to distinguish questions and non-questions  with a weighted F1 score of 0.69. A comparison of the three feature  sets indicates that a model using linguistic features outperforms  those using acoustic-prosodic and context features for question  detection, but the combination of features yields a 5% improvement  in overall accuracy compared to linguistic features alone. We  discuss applications for pedagogical research, teacher formative  assessment, and teacher professional development.    CCS Concepts   Social and professional topics~K-12 education  Computing  methodologies~Discourse, dialogue and pragmatics    Computing methodologies~Supervised learning by  classification  Information systems~Information extraction   Keywords  Automatic Speech Recognition; Natural Language Processing;  Classroom Analytics; Question Detection      1. INTRODUCTION  Teachers employ a variety of pedagogical practices in their  classrooms. Instructional activities may include lectures, asking  questions and evaluating student responses, or assigning students  individualized seatwork or group work. A growing body of  research indicates that certain activities, such as asking particular  types of questions or engaging in classroom-wide discussion,  predicts increased levels of student engagement and achievement  gains net of socio-demographics [4, 20, 42]. Furthermore,   providing teachers with training [18] and data-driven analysis [23]  about their use of such instructional strategies can have positive  downstream effects on student achievement.   But just how do we generate this feedback to share with teachers  Currently, efforts to assess classroom practices rely on observations  by trained human judges [19]. For example, the Nystrand and  Gamoran coding scheme [14, 30] provides a general template for  documenting teachers activities and can be used to analyze their  instructional strategies. Unfortunately, this is an expensive and  time-consuming practice that cannot be deployed uniformly at  scale.  In order to facilitate wide-scale analysis of teachers practices,  computational methods that can automatically analyze classroom  instruction are needed. We take a step in this direction by  automatically detecting teacher questions in live classrooms. We  focus on questions because they are a central component of dialogic  instruction, often serving as a catalyst for in-depth classroom  discussions and so called dialogic spells, characterized by student  questions that spawn reflection, debate, and deviation from pre- scripted lesson plans [31]. We acknowledge that all questions are  not created alike. The so called authentic questions (questions  without prescripted answers) and questions with uptake (follow-up  on the respondents answer) are much more highly predictive of  achievement compared to test questions, where the answers are  known apriori [30, 31]. Nevertheless, we focus on detecting all  teacher questions in this early stage of work with an eye for  categorizing among different types of questions in future work (as  we have begun to do from text transcripts [36]).  Classrooms provide a unique set of challenges for automatic  detection of questions. Questions in the classrooms often vary from  traditional information-seeking questions in other contexts, such as  office meetings or conversational speech. For instance, teachers ask  different types of questions, such as managerial questions (e.g.,  who hasnt finished the assignment yet), rhetorical questions      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027417   mailto:Permissions@acm.org   (e.g., that is indeed relevant, isnt), closed questions (what is  the capital of Pakistan), and open questions (why do you think  the author makes this argument) [7]. Some of these questions,  such as attendance-taking or teacher test questions, often do not  exhibit the prosodic rises in inflections typically associated with  question-asking and may instead sound like declarative statements  when taken out of context.  Furthermore, a system designed to automatically analyze teachers  practices must fulfill a number of practical constraints [10]. For  one, it cannot be disruptive to either teacher or students. Secondly,  the approach must be cost-effective and easy to setup in order to  enable widespread adoption. Additionally, for privacy concerns,  video recordings are not possible unless students can be de- identified.  We attempt to overcome these challenges by designing a system  that includes a low cost, wireless headset microphone to record  teachers as they freely move about the classroom. Our system  accommodates various seating arrangements, classroom sizes, and  room layouts, and attempts to mitigate complications due to  ambient classroom noise, muffled speech, or classroom  interruptions [5], factors that reflect the reality of real-world  environments.   2. RELATED WORK  There has been a large-body of work on detecting questions from  text-based corpora spanning several decades [1]. However, there  has been comparably little research on question detection from  audio recordings. A few key studies are discussed below.  Boakye et al. [8] examined the ability of machine learning models  to automatically detect questions from the ICSI Meeting Recorder  Dialog Act (MRDA) corpus [40], a dataset of 75 hour-long  meetings recorded with headset and lapel microphones. Using the  SRI CTS automatic speech recognition (ASR) engine, they  achieved a word error rate (WER), a measure of edit distance  comparing the ASR hypothesis to the original transcript, of 0.38 on  the corpus. They then trained an AdaBoost classifier to detect  questions from the corpus using word, natural language, and parse  tree features derived from the ASR transcriptions, achieving F1  scores of 0.52, 0.35, and 0.50, respectively. Adding contextual and  acoustic features improved the F1 score to 54.0. This modest  improvement suggests that linguistic information may be more  important for question detection compared to contextual or acoustic  information.   Stolcke et al. [41] presented a statistical approach for modeling  dialogue acts in conversational telephone speech. The authors used  a hidden Markov model on the Switchboard corpus [15] to identify  speech acts, such as questions, statements, or apologies, achieving  an accuracy of 65% on ASR transcriptions (WER 0.41) and 71%  based on human transcriptions (chance level 35%; human  agreement 84%).   The authors also attempted to distinguish questions from  statements, two dialogic components often confused by their  model. The authors improved their accuracy to 75% on a subset of  their dataset containing equal proportions of questions and  statements. This result, based on a balanced dataset limited to a  small number of pre-specified topics and segmented by human  observers, achieved only modest accuracy, highlighting the  difficulty of question detection in real world environments.   More recently, Orosanu and Jouvet [33] used ASR transcriptions to  train models to differentiate between statements and questions  using three French language corpora consisting of 7,005 statements  and 831 questions. They classified 72.6% of questions and 77.7%   of statements correctly using linguistic and prosodic features  derived from human transcriptions. However, when they  substituted ASR transcriptions in two corpora (WERs of 0.22 and  0.28), they found a 3% reduction in classification accuracy.  Additionally, the F1 score was only 0.40 for questions compared to  a weighted F1 score of 0.63, demonstrating greater difficulty in  identifying questions compared to statements. The authors found  that models trained on linguistic features significantly  outperformed those trained with prosodic features, and the  combination of both provided only trivial improvements.  Furthermore, the models trained on ASR transcriptions of  unscripted, spontaneous speech from the third corpus was slightly  less accurate (accuracy of 70%) compared to a corpus of scripted  dialogue from radio interviews (73%) (chance=50%). This  underscores the potential added difficulty of automatically  detecting questions based on spontaneous speech, as in our study.   Additionally, Orosanu and Jouvet examined classification using  imperfect sentence boundaries. The authors manipulated the  boundaries of the human segmented utterances based on the longest  silence preceding or following an utterance. Following this  perturbation of the manual segmentation, they observed a 3% drop  in accuracy. This reveals an additional difficulty in question  detection based on imperfectly segmented utterances, a challenge  we also confront in this work.   Other studies have attempted to identify questions using prosodic  information, often in tonal languages such as Chinese [45] or  Vietnamese [34, 44]. Another study used prosodic features and a  decision tree to detect questions from a dataset of Arabic language  audio lectures containing an equal number of questions and  statements [21]. Although the authors reported 76% accuracy  (chance = 50%), the dataset consisted of only three speakers and  the results were not validated independent of the speaker.  Nevertheless, the authors found that the fundamental frequency and  the energy level were the most useful features, features we also  consider in this work.   One English language study combined acoustic, lexical, and  syntactic features to identify questions from Wikipedia talk pages  [24]. The authors noted that models using lexical and prosodic  features (AU-ROC 0.92) only slightly outperformed models using  lexical features alone (AU-ROC 0.91). This study benefited from  perfect transcripts and their corpus did not contain properties of  spontaneous speech (e.g., backchannels, disfluencies, or  interruptions), as in our study.   In preliminary work, we explored question detection from  automatically-segmented utterances derived from live-recordings  of classroom audio. Using leave-one-speaker-out cross-validation,  we achieved an overall weighted F1 score of 0.69 using only lexical  and syntactic features [28] demonstrating that question detection  was possible from noisy classroom audio. Presently, we expand  upon this work to explore the potential of acoustic and contextual  features in conjunction with linguistic features.    3. CONTRIBUTIONS AND NOVELTY  We describe an approach to automatically identify teacher  questions solely from an audio recording of the teacher in a real- world classroom. Given the noisy environment, we face a number  of technical challenges. Classroom speech is particularly noisy as  there are disruptions, accidental microphone contact, sounds of  students shuffling papers or moving desks, alarms, background  media, and so on. Classroom speech is also more informal and  conversational compared to more formal settings such as meetings.  Furthermore, we must automatically segment utterances from the     audio stream, an analysis itself that is prone to error. Lastly, ASR  transcription is imperfect, thereby adding additional noise.  We make several contributions while addressing these challenges.  First, we examine a dataset of full length recordings of real world  class sessions, drawn from multiple teachers and schools. Second,  we only use teacher audio because it is the most practical option  given privacy and scalability concerns. Third, we automatically  segment audio recordings into individual teacher utterances in a  fully automated pipeline. Fourth, we combine multiple ASR  engines at the feature level to ameliorate errors.  Fifth, we consider  high-level linguistic features, acoustic and prosodic features, and  contextual features, all derived from the audio stream. Finally, we  design our models to generalize across teachers rather than  optimizing to individual teachers.   4. METHODS  We begin by reviewing our data collection procedure (Section 4.1),  followed by discussion of our approach to automatically segment  the audio streams (Section 4.2). Next, we describe the process used  to manually label the detected speech as containing a question or  not (Section 4.3) and transcribe the audio using ASR (Section 4.4).  Finally, we discuss our feature sets (Section 4.5) and machine  learning approach (Section 4.6).    4.1 Data Collection  A dataset of audio recording was collected at six rural Wisconsin  middle schools during literature, language arts, and civics classes  taught by 11 different teachers (three male; eight female). A total  of 37 class sessions were recorded over 17 separate days in the  course of a year. The length of each class session varied depending  on the school, lasting between 30 and 90 minutes. The dataset  contains a total of 32 hours and five minutes of audio.  In order to capture unbiased samples of discourse in real-world  classrooms, each teacher was asked to carry out their normal lesson  plan. The teachers were recorded using a wireless microphone,  which allowed them to move about the classroom freely. A Samson  77 Airline wireless microphone (Figure 1) was chosen based on  previous work [10] and for its noise-canceling abilities and  relatively low-cost ($300 in 2014). The recording of the teachers  speech was saved as a 16 kHz, 16-bit single channel audio file  (Figure 2).      Figure 1. Samson 77 Airline Microphone     Each class session was live coded by observers trained in the  Nystrand and Gamoran coding scheme (see below), using  specialized software developed for this task (Figure 3) [25]. Coders  were also trained on proper recording techniques to ensure  consistent levels of quality in the recordings. Following the class  session, the coded annotations were reviewed and refined by the  original coder until reaching full agreement with a second coder.   The Nystrand and Gamoran coding scheme [14, 30] tracks  classroom activity across the following (in order of increasingly  fine granularity) three parallel levels:  (1)  episodes, which denote     Figure 2. Excerpt of classroom recording marked with   utterances, instructional segments, and dialogic questions         Figure 3. Screenshot of the Class 4 program     the current activity/topic; (2) instructional segments, 17 categories  that represent possible classroom activities (e.g., Lecture, Group  Work, Discussion) used to implement an episode; and (3) certain  questions (e.g., non-procedural, non-rhetorical) asked by teachers  and students [30]. We focus on questions asked by the teacher here.    4.2 Teacher Speech Extraction  Recordings of the teachers speech was segmented into utterances  using a voice activity detection (VAD) technique [5]. First, a low- pass filter was applied to the recordings. Next, the amplitude  envelope of the signal was examined in non-overlapping 20- millisecond windows. Whenever the amplitude of the signal  exceeded a preset threshold, it was assumed that the teacher was  speaking, otherwise, silence was assumed. Consecutive windows  of teacher speech were considered part of a potential teacher  utterance until silence was detected continuously for 1 second.   We set the VAD threshold low enough to prioritize capturing all  instances of the teachers speech. However, this caused a high rate  of false alarms in the form of non-speech utterances, such as  classroom noise, body movements, coughing, or heavy breathing.  In order to filter out these false alarms, we processed the potential  utterances with the Bing ASR system [27]. We considered any  potential utterances rejected by the ASR as non-speech.     Additionally, utterances shorter than 125 milliseconds were  removed as they were unlikely to contain meaningful speech.  We evaluated the effectiveness of this utterance detection approach  in prior work [10]. Briefly, we extracted a random subset of 1,000  potential utterances and manually coded them as containing speech  or not speech. We observed high levels of both precision (96.3%)  and recall (98.6%) and an F1 score of 0.97, which we deemed  sufficient for our purpose. We extracted a total of 10,080 utterances  from the 37 classroom recordings. The average utterance length  was 5.01 seconds with a standard deviation of 7.64 seconds.    4.3 Question Coding  Many of the previous studies in automatic question detection used  datasets that contained questions and statements that had been  segmented manually by humans. However, in our study we rely on  automatic, and thus imperfect, segmentation. We also used fixed  amplitude envelope and silence thresholds while segmenting  utterances as opposed to learning teacher-specific thresholds in  order to increase generalizability to new teachers. A side-effect of  this procedure is that each utterance may contain multiple  questions, or conversely, a question may be spread across multiple  utterances [22].   To address this concern, we manually coded the 10,080 extracted  utterances as either containing a question or not containing a  question rather than question or statement. This distinction,  although subtle, addressed the cases in which a question phrase was  embedded within a longer utterance, coding these as containing a  question. Similarly, we also handled cases in which a question  phrase spans one or more consecutive utterances, also coding these  utterances as containing a question.  We define a question following a coding scheme that was  specifically designed to analyze questions in classroom discourse  [31]. Here, questions are defined as utterances in which the teacher  solicits information from a student either procedurally (e.g., Is  everyone ready), rhetorically (e.g., Oh good idea James, why  dont we just have recess instead of class today), or for knowledge  assessment/information solicitation purposes (e.g., What is the  capital of Indiana, Michael). Likewise, the teacher calling on a  different student to answer the same question (e.g., Nope.  Shelby) would also be considered a question. In some coding  schemes, the previous example would be classified as turn  eliciting questions [3]. Cases in which the teacher reads from a  novel in which a character asked a question or calls on a student for  other reasons (e.g., such as to discipline them) would not be  considered questions.   The coders were seven research assistants and researchers whose  native language was English and who had no experience with the  Nystrand and Gamoran [25] coding scheme. The coders first  engaged in a training task by labeling a common evaluation set of  100 utterances. These 100 utterances were manually selected to  exemplify questions that were difficult to identify. Once coding of  the evaluation set was completed, the expert coder who initially  selected and coded the example utterances reviewed the codes for  any discrepancies. Coders were required to achieve a minimal level  of agreement with the expert coder (Cohens kappa,  = 0.80). If  the agreement was lower than 0.80, mistakes were identified and  explained to the coders.   After this training task was completed, each coder coded a subset  of utterances from the complete dataset across multiple sessions.  Coders listened to the utterances in temporal order and assigned a  label to each, based on the words spoken by the teacher, the  teachers tone (e.g., prosody, inflection), and the context of the   previous utterance. Coders could also flag an utterance for review  by the expert, although this occurred only rarely.   Of the 10,080 utterances, 3,584 were labeled as containing a  question (36%) and 6,496 as not containing a question (64%),  across teachers. To ensure reliability, a random subset of 117  utterances from the full dataset were selected and coded by the  expert coder, which resulted in high agreement (Kappa  = 0.85).   4.4 Automatic Speech Recognition  In previous work [6, 28] we examined three different automatic  speech recognition (ASR) systems for this task: Bing [27], AT&T  Watson [16] and the Azure [26]. Both the Bing and AT&T ASR  systems transcribed individual utterances segmented as described  in Section 4.2. The Azure system, however, processed full-length  classroom recording to produce a set of time-stamped words, from  which we reconstructed the individual utterances.  We evaluated performance of the three ASR systems on a subset of  1,000 utterances chosen randomly without replacement,  considering two metrics commonly used in speech recognition:  word error rate (WER) and simple word overlap (SWO) [28]. WER  accounts for word order between ASR and human transcripts and  was computed by summing the number of substitutions, deletions,  and insertions required to transform the human transcript into the  ASR transcript, divided by the total number of words in the human  transcript. SWO, however, does not account for word order and was  computed by dividing the number of words that appear in both the  human and ASR transcripts by the total number of words in the  human transcript.   In Table 1 we show the WER and SWO for the three different ASR  systems. We note that all three systems achieved moderate  accuracy, despite the complexity of the task of automatically  transcribing noisy conversational speech recorded in a real-world  environment.      Table 1. ASR word error rate (WER) and simple word  overlap (SWO) averaged by teacher for 1,000 utterances, with   standard deviations shown in parentheses  ASR WER  SWO  Bing Speech 0.45 (0.10) 0.55 (0.06)  AT&T Watson 0.63 (0.11) 0.42 (0.11)  Azure Speech 0.49 (0.07) 0.64 (0.16)     The Azure ASR engine failed to return transcriptions for 201 of  these 1,000 utterances. This resulted in a WER of 1.0 and SWO of  0.0 for those utterances. Discarding those instances improves WER  for Azure to 0.37 (SD = 0.07) and SWO to 0.68 (0.06). Thus Azure,  when able to transcribe an utterance, is the best performing ASR.  However, this failure to return a transcription for 20% of utterances  requires that we consider it only in conjunction with another ASR  engine.   We also compared ASR transcriptions with each other across our  full dataset of 10,080 utterances. The results in terms of SWO were  as follows: Bing versus AT&T (0.43), Bing versus Azure (0.51),  and Azure versus AT&T (0.48). On average the ASR engines only  agree on half of the words in each transcript. Therefore, the  combined use of multiple ASR systems could provide additional  information, as the strengths and weaknesses of individual ASRs  may vary across teachers, class sessions, and instruction types. We  combined them at the feature-level as discussed in the next section.     4.5 Features  We extracted 218 linguistic, acoustic, and contextual features to  train our classification models.   Linguistic Features. We considered a set of natural language  features generated from ASR transcripts for each utterance  obtained from Bing Speech, AT&T Watson, and Azure Speech  engines. The majority of these features (n=34) were obtained by  processing each utterance with the Brill Tagger [9] and analyzing  each token with a question type classifier [32]. The question type  classifier, developed for speech act classification in educational  systems, used a cascaded finite state transducer to tag utterances  according to a taxonomy of potential question types, as well as part  of speech tags. Features included the presence of particular  question words (e.g., what, why, how), simple disambiguation rules  (e.g., the presence of words that start with wh-), part of speech tags  (e.g., presence of nouns, presence of adjectives), and hypothesized  categories based on simple keywords (e.g., definition, comparison,  procedural). We used these features in prior work to detect domain- independent question properties from human-transcribed questions  [36] and for automatic classification of teacher questions [6, 28].  We included three additional features: proper nouns (e.g., student  names), pronouns associated with uptake (teacher questions that  incorporate student responses), and pronouns not associated with  uptake, as recommended by a domain expert on teacher questions.  In all, we extracted 37 binary linguistic (NLP) features for each  ASRs transcription. Next, we computed a combined NLP feature  set by taking the mean of the individual ASR binary features. For  example, if the what feature was detected by Bing and AT&T, but  not Azure, then it would take on a value of 0.67. All subsequent  analyses focus on this combined feature set as it has been shown to  be superior to the individual feature sets in our prior work [28]  Acoustic Features. We extracted prosodic, spectral, and voice  quality features with the OpenSmile audio feature extraction tool  [13] using the feature set from the Interspeech Emotion Challenge  [38]. The low-level audio descriptors were: zero-crossing-rate  (ZCR) from the time signal, root mean square (RMS) frame energy,  Mel-frequency cepstral coefficients (MFCC) 1-12, fundamental  frequency computed from the cepstrum (normalized to 500 Hz),  and a voicing probability computed from the autocorrelation of the  power spectrum. For each feature, 12 statistical functionals were  computed: mean, standard deviation, kurtosis, skewness, minimum  and maximum value, relative position of the minimum and  maximum values (in frames), range, two linear regression  coefficients (slope, offset), as well as the linear regression mean  square error (MSE). Additionally, for each feature, we considered  the smoothed moving averaged window (length 3) and the 1st order  delta coefficient (differential) of the smoothed low-level descriptor.  This results in 16  2  12 = 384 features for each utterance. Given  the large number of acoustic features compared to our other feature  sets, we used tolerance analysis to eliminate features with high  multicollinearity (variance inflation factor > 5) [2], after which, 168  acoustic features remained.  Context Features. We considered a number of context features for  each utterance. These included: the length of utterance, lengths of  the previous and subsequent utterances, the duration of the pause  preceding and following the utterance, the position of the utterance  within the class session normalized to (0,1), verbosity (the number  of words from the Bing ASR transcript), and the rate of speech  (number of words in the Bing ASR transcript divided by the length  of the utterance in seconds).   We also considered the likelihood that each utterance occurred  during one of the instructional segments from the Nystrand and   Gamoran coding scheme [30]. We considered the five most  commonly occurring instructional segments in the dataset:  Question & Answer, Procedures & Directions, Group Work,  Supervised Seatwork, and Lecture. In prior work, we predicted the  occurrence of the segments by training supervised machine  learning models using timing, acoustic, and linguistic features,  resulting in F1 scores ranging from 0.64 to 0.78 [12]. For each  utterance, the probability it is contained within each of the  instructional segments was considered, resulting in five additional  context features. In total, we extracted 13 context features.    4.6 Classification Models  We trained and tested supervised classification models to predict if  an utterance contained a partial or complete question (question), or  did not contain a question at all (non-question). The model building  process involved the following steps.  Feature Standardization. We z-scored standardized all 218  features, resulting in a mean of 0.0 and a standard deviation of 1.0  for each feature. Features were standardized within each teacher.  Classification Models. We explored a number of common  machine learning classifiers using implementations from the  WEKA toolkit [43]: Nave Bayes, logistic regression, random  forest, J48 decision tree, J48 with Bagging, Bayesian network, k- nearest neighbor (k = 7, 9, and 11). We also combined the  classifiers with MetaCost [11], which penalized misclassifications  of the minority class (weights of 2 and 4).   Validation. We validated the classification models with leave-one- teacher-out cross-validation. Each model was built on data from 10  teachers (the training set) and validated on the held-out teacher (the  testing set). The process was repeated for 11 folds so that each  teacher appeared in the testing set once and the results were  calculated from a confusion matrix aggregated across teachers.  This cross-validation technique tests the potential of our models to  generalize to new teachers despite variations in speaking patterns,  word-choice, and rate of question-asking.    5. RESULTS  5.1 Classification Accuracy  The best performing model was the J48 decision tree with bagging  and MetaCost (miss weight of 2) and used all features. This model  achieved the highest F1 score for the classification of utterances  containing questions and was consistent with respect to precision  (0.69) and recall (0.70). We selected the best performing model  based on the F1 score for questions to prioritize the models ability  to detect the class of interest, which was always the minority, rather  than prioritizing the dominant class label (i.e. non-questions).  Figure 4 shows the results of the J48 decision tree with bagging and  MetaCost for each set of features: linguistic (NLP; n = 37), acoustic  (n = 168), and contextual (n = 13), as well as the combination of all  features (n = 218). The remainder of the results given in this work  use this classifier.   With respect to individual feature sets, we found that linguistic  features outperformed acoustic and context features in the  classification of both questions and non-questions, a result that is  consistent with previous studies in question detection from  automatic transcriptions from audio [8, 24, 33]. However, we note  that while acoustic-prosodic features were not particularly  successful in identifying questions (0.36), they were significantly  more successful in identifying non-questions (0.67). This suggests  that acoustic-prosodic features may be useful in identifying other  types of statements compared to questions themselves, albeit less  useful than the linguistic features alone.               Table 2. Confusion matrix showing agreement and disagreement between the three feature sets: linguistic (NLP),   acoustic (Aco), and context (Con) in classifying questions (Q) and non-questions (NQ)    Predicted     Agreement     Disagreement     NLP Q NLP NQ     NLP Q NLP Q NLP Q NLP NQ NLP NQ NLP NQ     Aco Q Aco NQ     Aco Q Aco NQ Aco NQ Aco NQ Aco Q Aco Q   Actual Con Q Con NQ     Con NQ Con Q Con NQ Con Q Con NQ Con Q   Q 0.64 0.36     0.37 0.35 0.40 0.32 0.35 0.35   NQ 0.36 0.64     0.63 0.65 0.60 0.68 0.65 0.65                Figure 4. Results for question detection comparing linguistic (NLP), acoustic (Aco), and context (Con) features, pairwise  combinations of feature sets, and the combination of all features     For the question class, the combined model demonstrated no  improvement compared to using only linguistic features (both F1 =  0.56). However, we found that the additional features did result in  improvement for the classification of non-questions (0.76 vs. 0.71),  a 7% improvement. This resulted in a 5% improvement to the  overall weighted F1 score (0.69 vs. 0.66). Table 2 provides a  confusion matrix that shows agreement and disagreement between  the three feature sets. In general, there was no clear pattern and the  results were similar to the base rates in the dataset (see Section 4.2).   Our best performing classification model (J48 with bagging and  MetaCost) returns a confidence rate with each prediction, allowing  comparison of the confidence of predictions between feature sets  (Pearsons r): acoustic vs. linguistic (0.15), context vs. linguistic  (0.22), and acoustic vs. context (0.30), indicating a lack of  consensus between the individual feature set models.  Unsurprisingly, given our findings on the importance of the  linguistic features, the prediction confidence of the linguistic model  and the combined model was strongly correlated (r =.80) compared  to acoustic (r = .25) and context (r = .27).   Additionally, we compared pairwise combinations of the three  feature sets, shown in Figure 4. We found that combining linguistic  features with acoustic (0.55) or context (0.55) features resulted in  no improvement over linguistic features alone (0.56) for the  detection of questions. However, the combination of acoustic  (0.76) or context (0.73) features with linguistic features did yield a  7% and 3% improvement, respectively, over linguistic features  alone (0.71) for the detection of non-questions. When taken  together, these results indicate that a combination of acoustic and  linguistic features may be sufficient for this task, with the context  features not contributing much more.    5.2 Feature Analysis  Motivated by the observation that linguistic features were the most  useful in identifying questions, we analyzed the diagnosticity of  each feature individually. We hypothesized that linguistic features  documenting the use of certain question words (e.g., what, why,  how) will be the most useful to discriminate between questions and  non-questions. We re-ran our models by considering only a single  feature, one at a time, using a J48 decision tree with bagging and  Meta-Cost. In Figure 5 we show the F1 scores for each individual  feature ranked by the overall weighted F1 score. For completeness,  we focused on all 218 features rather than the NLP features alone.      Figure 5. Analysis of individual features ranked by overall   weighted F1 score   Of the top 100 features, 91 were acoustic features, eight were  linguistic features, and only one was a context feature (the length  of the utterance; ranked 13th). The dominance of acoustic features  is unsurprising as they constitute 79% of the features. However, the  top seven features were all linguistic, perhaps explaining the  success of models trained with linguistic features alone compared  to models using acoustic or context features. We show the top 10  features sorted by their overall weighted F1 scores in Table 3.  The top ranked feature was the presence or absence of the question  word what in the ASR transcript. We note that this feature alone  achieved an overall F1 score of 0.68, rivaling the models  performance using all features (0.69). However, this feature only  achieved a F1 of 0.46 for the question class compared to 0.56 using  all features. An analysis of the ASR transcriptions showed that the  what feature appeared in 5.9% of non-questions and 12.8%  questions, indicating this one word alone was insufficient to  distinguish questions from non-questions. Contrary to our  expectations, we found that no other question-word features were  as successful: how (ranked 198th), why (ranked 203rd), wh- (ranked  207th), should (ranked 218th). We also note that no single feature  model exceeded an F1 of 0.50 for question detection, implying that  the combination of features was needed.     Table 3. Top ten features ranked by overall F1 score      5.3 Analysis by Class Session  The models were trained using leave-one-teacher-out cross- validation, but we performed additional post-hoc analyses  exploring the models accuracy across the 37 individual class  sessions. These analyses allow an investigation of the stability of  our models for individual class sessions, which will be essential for  generalizability to future class sessions with different topics.   In Figure 6 we show histograms of F1 scores for questions, non- questions, and the overall weighted average by individual class  session. We note that model accuracy was distributed across class  sessions, rather than a bimodal distribution of successes and  failures. The model yielded an interquartile range 0.44 to 0.65 for  the question class. In the classification of questions, we observed  that the model had greater difficulty with some class sessions, and  that this was most often associated with classes that contained  relatively fewer questions. For example, the best performing class  session (0.76) contained 57% questions while the poorest- performing class session (0.19) contained only 29% questions.  Over the 37 class sessions, the rate of questions and the F1 scores  for questions was strongly correlated (Pearsons r = 0.76). This  demonstrates that our model had greater difficulty in identifying  questions for class sessions that contained a low proportion of   Type Feature F1   NLP Presence of word  what  0.68  NLP Presence of phrase  dohave  0.60  NLP Presence of a pronoun 0.59  NLP Presence of word  be  0.59  NLP Presence of a proper noun 0.57  NLP Pronouns associated with uptake 0.57  NLP Presence of a verb 0.55  Acoustic MFCC [1] - maximum value 0.54  Acoustic Voice probability - maximum position 0.54  Acoustic MFCC [11] - linear regression MSE 0.54     questions to non-questions. detection of questions might have  important consequences for both research on effective instructional  strategies and on teacher professional development. Thus, our  current work centers on a fully-automated process for predicting  teacher questions in a noisy real-world classroom environment,  using only a full-length audio recording of teacher speech.   6. DISCUSSION  The importance of teacher questions in classroom discourse is  widely acknowledged in both policy (e.g., [39]) and research [4, 29,  31]. Teacher questions play a central role in promoting student  engagement and achievement, suggesting that automating the    6.1 Summary of Contributions  We present encouraging results with our automated processes,  consisting of voice activity detection to automatically segment  teacher speech, combining three different ASR transcriptions, three  different features sets (linguistic, acoustic-prosodic, and context),  and machine learning with teacher-independent validation.   A key contribution of our work over previous research is that our  models were trained and tested on automatically, and thus  imperfectly, segmented utterances. This builds upon the work of  Orosanu and Jouvet [33] which artificially explored perturbations  of a subset of utterance boundaries using automatic detection of  silence within human-segmented spoken sentences. We note that  despite automatic segmentation, which may split individual  questions across utterances, we outperformed the previous work   (weighted F1 scores 0.69 vs. 0.63). To our knowledge, our work is  the first to detect spoken questions using a fully automated process.   Our best performing model using all features achieved an F1 score  of 0.56 for the question class and an overall weighted F1 score of  0.69. Furthermore, we demonstrated that models built using only  linguistic features outperformed those built using either acoustic or  context features, consistent with similar findings in the literature [8,  24, 33]. Although models built using a combination of features did  not improve the identification of questions, they were more  successful at detecting non-questions. Here, also linguistics and  paralinguistics seemed to suffice; contextual features had little  more to add  at least for the small set investigated here.  Additionally, we investigated the utility of each feature. We found  that the top seven individual features were all linguistic features,  underscoring the importance of the specific content of the spoken  utterances compared to paralinguistic or contextual clues for  identifying questions.   We validated our models using leave-one-teacher-out cross- validation, demonstrating generalizability of our approach across  teachers in this dataset. Furthermore, we analyzed model  performance by class session, finding that our model was consistent  across class sessions, an encouraging result supporting our goals of  class session-independent question detection.    6.2 Limitations and Future Work  This study is not without limitations. We were also unable to record  individual students for practical reasons and extraction of student  speech was not feasible from the teachers microphone. This  precluded us a potentially key feature that signals a question  the  student response. Fortunately, additional data collection includes a  second microphone that captures general classroom activity. This  second channel of audio when combined with the recording of the  teacher, will afford modeling patterns of teacher-student  interactions, potentially revealing question-response patterns  between teachers and students.  We designed our approach to avoid overfitting to specific classes,  teachers, or schools. However, all of our recordings were collected  in Wisconsin, a state that has adopted the common core standard  [39]. It is possible that the common core may impose aspects of a  particular style of teaching that our models may overfit to.  Similarly, although we used speaker-independent ASR and teacher- independent validation techniques to improve generalizability to  new teachers, our sample of teachers were from a single region with  traditional Midwestern accents and dialects. Therefore, broader  generalizability across the U.S. and beyond remains to be seen [17].  Finally, our models are likely English language specific. However,  because we limited the linguistic features to high-level part of  speech features, it may be possible to adapt these features to other  languages that share similar linguistic structures. Our finding that  the most useful features are linguistic features is encouraging as  these features could be readily tagged in many languages.  We acknowledge that our method for teacher utterance  segmentation may potentially be improved using proposed  techniques in related works. For example, Komatani et al. [22]  explored detecting and merging utterances segmented mid- sentence, allowing analysis to take place on a full sentence, rather  than a fragment, which may improve detection of questions split  across utterances. An alternative approach would be to  automatically detect sentence boundaries within utterances, and  extract features from each detected sentence. Furthermore, Raghu  et al. [35] explored using context to identify non-sentential  utterances (NSUs), defined as utterances that are not full sentences          Figure 6. Histogram of F1 scores by class session       but convey complete meaning in context. Identification of NSUs  may improve our models ability to differentiate between difficult  cases (e.g., calling on students, saying a students name to  discipline them).   In this work, we compared the performance of three different  features sets (linguistic, acoustic-prosodic, and context). While this  approach allowed us to compare the utility of individual feature  sets, the ideal set of features may derive from a subset drawn from  different feature types. More sophisticated fusion methods in lieu  of the simple feature level fusion explored here might also be  needed. In future work, we will examine empirical feature selection  as well as explore decision- and model-based fusion techniques that  combine the three feature sets. We will also explore temporal  models, such as hidden Markov models and conditional random  fields, that might better capture questions in the larger context of  the classroom dialogue. Such a temporal analysis may help find  sequences of consecutive questions, such as those present in  question and answer sessions or in classroom discussions.   Lastly, informed by our observation of the utility of linguistic  features, and more specifically those that capture certain question  words, we will explore additional linguistic feature to identify  additional words important to the detection of questions. Because  the topics varied between individual class sessions, a traditional  bag-of-word analysis may not be useful since the course material is  not likely to repeat between teachers and sessions. However, this  approach may yield insight into additional key words which could  be aggregated into high-level features that may be useful to detect  questions, similar to our binary question word features.  Finally, as noted in the Introduction, it is not merely the amount of  questions asked but the types of questions that correlate with  achievement. Thus, future work will focus on classifying question  properties defined by Nystrand and Gameron [25], such as  authenticity, uptake, and cognitive level. We have explored these  properties in previous work [36, 37] using perfectly segmented and  human transcribed text. We will continue this work using our  approach that employs automatic segmentation, ASR  transcriptions, and question detection.   6.3 Applications  The ability to identify questions asked by teachers in the classroom  is necessary in order to generate personalized formative feedback  for the teacher about their use of class time. Our approach would  permit automating such analysis, enabling a cost-effective scalable  deployment that would be accessible to many schools and teachers.  Using our system, teachers could record their class and receive  automated feedback following the class session. Such feedback will  afford teachers reflection on their teaching style and better enable  collaboration with professional development personnel towards  improvement of their pedagogy with the ultimate goal of increasing  student engagement and achievement. It will also facilitate research  into effective pedagogy by providing educational researchers with  an automated approach to collect and code classroom discourse.   6.4 Concluding Remarks  We took steps towards fully-automated detection of teacher  questions in noisy real-world classroom environments using  linguistic, acoustic-prosodic, and context features. The present  contribution is one component of a broader effort to automate the  collection and coding of classroom discourse.    7. ACKNOWLEDGMENTS  We thank Marci Glaus, Xiaoyi Sun, and Brooke Ward of the  University of Wisconsin-Madison for their help with the collection   and annotation of the classroom data. This research was supported  by the Institute of Education Sciences (IES) (R305A130030). Any  opinions, findings and conclusions, or recommendations expressed  in this paper are those of the author and do not represent the views  of the IES.    8. REFERENCES  [1] Aichroth, P., Bjrklund, J., Stegmaier, F., Kurz, T. and   Miller, G. 2015. State of the art in cross-media analysis,  metadata publishing, querying and recommendations.  Media in Context (MICO). 1, (2015).   [2] Allison, P.D. 1999. Multiple regression: A primer. Pine  Forge Press.   [3] Allwood, J., Cerrato, L., Jokinen, K., Navarretta, C. and  Paggio, P. 2007. The MUMIN coding scheme for the  annotation of feedback, turn management and sequencing  phenomena. Language Resources and Evaluation. 41, 3-4  (2007), 273287.   [4] Applebee, A.N., Langer, J.A., Nystrand, M. and Gamoran,  A. 2003. Discussion-based approaches to developing  understanding: Classroom instruction and student  performance in middle and high school English. American  Educational Research Journal. 40, 3 (2003), 685730.   [5] Blanchard, N., Brady, M., Olney, A.M., Glaus, M., Sun, X.,  Nystrand, M., Samei, B., Kelly, S. and DMello, S. 2015. A  study of automatic speech recognition in noisy classroom  environments for automated dialog analysis. Artificial  Intelligence in Education (2015), 2333.   [6] Blanchard, N., Donnelly, Patrick J, Olney, A.M., Samei, B.,  Ward, B., Sun, X., Kelly, S., Nystrand, M. and DMello,  S.K. 2016. Automatic detection of teacher questions from  audio in live classrooms. Proceedings of the 9th  International Conference on Educational Data Mining  (EDM 2016), International Educational Data Mining  Society (2016).   [7] Blosser, P.E. 1975. How to ask the right questions. National  Science Teachers Association Press.   [8] Boakye, K., Favre, B. and Hakkani-Tr, D. 2009. Any  questions Automatic question detection in meetings.  Automatic Speech Recognition & Understanding, 2009.  ASRU 2009. IEEE Workshop on (2009), 485489.   [9] Brill, E. 1992. A simple rule-based part of speech tagger.  Proceedings of the workshop on Speech and Natural  Language (1992), 112116.   [10] DMello, S.K., Olney, A.M., Blanchard, N., Samei, B., Sun,  X., Ward, B. and Kelly, S. 2015. Multimodal capture of  teacher-student interactions for automated dialogic analysis  in live classrooms. Proceedings of the ACM on International  Conference on Multimodal Interaction (2015), 557566.   [11] Domingos, P. 1999. Metacost: A general method for making  classifiers cost-sensitive. Proceedings of the fifth ACM  SIGKDD international conference on Knowledge discovery  and data mining (1999), 155164.   [12] Donnelly, P.J., Blanchard, N., Samei, B., Olney, A.M., Sun,  X., Ward, B., Kelly, S., Nystrand, M. and DMello, S.K.  2016. Automatic teacher modeling from live classroom  audio. Proceedings of the 24th Conference on User  Modeling, Adaptation and Personalization, 4553.   [13] Eyben, F., Wllmer, M. and Schuller, B. 2010. OpenSmile:  the Munich versatile and fast open-source audio feature  extractor. Proceedings of the 18th ACM international  conference on Multimedia (2010), 14591462.   [14] Gamoran, A. and Kelly, S. 2003. Tracking, instruction, and  unequal literacy in secondary school English. Stability and     change in American education: Structure, process, and  outcomes. (2003), 109126.   [15] Godfrey, J.J., Holliman, E.C. and McDaniel, J. 1992.  SWITCHBOARD: Telephone speech corpus for research  and development. Acoustics, Speech, and Signal  Processing, 1992. ICASSP-92., 1992 IEEE International  Conference on (1992), 517520.   [16] Goffin, V., Allauzen, C., Bocchieri, E., Hakkani-Tr, D.,  Ljolje, A., Parthasarathy, S., Rahim, M.G., Riccardi, G. and  Saraclar, M. 2005. The AT&T WATSON Speech  Recognizer. ICASSP (1) (2005), 10331036.   [17] Hall, J.K. 2008. Language education and culture.  Encyclopedia of language and education. Springer. 4555.   [18] Juzwik, M.M., Borsheim-Black, C., Caughlan, S. and  Heintz, A. 2013. Inspiring dialogue: Talking to learn in the  English classroom. Teachers College Press.   [19] Kane, T. and Staiger, D. 2012. Gathering feedback for  teachers: Combining high-quality observations with student  surveys and achievement gains. Research Paper. MET  Project. Bill & Melinda Gates Foundation.   [20] Kelly, S. 2007. Classroom discourse and the distribution of  student engagement. Social Psychology of Education. 10, 3  (2007), 331352.   [21] Khan, O., Al-Khatib, W.G. and Lahouari, C. 2007.  Detection of questions in Arabic audio monologues using  prosodic features. Multimedia, 2007. ISM 2007. Ninth IEEE  International Symposium on (2007), 2936.   [22] Komatani, K., Hotta, N., Sato, S. and Nakano, M. 2015.  User adaptive restoration for incorrectly segmented  utterances in spoken dialogue systems. 16th Annual Meeting  of the Special Interest Group on Discourse and Dialogue  (2015), 393.   [23] Lai, M.K. and McNaughton, S. 2013. Analysis and  discussion of classroom and achievement data to raise  student achievement. Data-based decision making in  education. Springer. 2347.   [24] Margolis, A. and Ostendorf, M. 2011. Question detection in  spoken conversations using textual conversations.  Proceedings of the 49th Annual Meeting of the Association  for Computational Linguistics: Human Language  Technologies: short papers-Volume 2 (2011), 118124.   [25] Martin Nystrand 2016. Class 4.5 users manual: A windows  laptop-computer system for the in-class analysis of  classroom discourse. The Wisconsin Center for Education  Research, University of Wisconsin-Madison.   [26] Microsoft 2016. Azure Speech API.  [27] Microsoft 2014. The Bing Speech Recognition Control.  [28] Blanchard, N., Donnelly, P.J., Olney, A.M., Samei, B., Sun,   X., Ward, B., Kelly, S., Nystrand, M., and DMello, S.K.  2016. Identifying teacher questions using automatic speech  recognition in live classrooms. Proceedings of the Special  Interest Group on Discourse and Dialogue (SIGDIAL)  (2016), 191201.   [29] Nystrand, M. and Gamoran, A. 1991. Instructional  discourse, student engagement, and literature achievement.  Research in the Teaching of English. (1991), 261290.   [30] Nystrand, M., Gamoran, A., Kachur, R. and Prendergast, C.  1997. Opening dialogue: Understanding the dynamics of  language and learning in the English classroom. Language  and Literacy Series. Teachers College Press.   [31] Nystrand, M., Wu, L.L., Gamoran, A., Zeiser, S. and Long,  D.A. 2003. Questions in time: Investigating the structure  and dynamics of unfolding classroom discourse. Discourse  Processes. 35, 2 (2003), 135198.   [32] Olney, A., Louwerse, M., Matthews, E., Marineau, J., Hite- Mitchell, H. and Graesser, A. 2003. Utterance classification  in AutoTutor. Proceedings of the HLT-NAACL 03 workshop  on building educational applications using natural  language processing-Volume 2 (2003), 18.   [33] Orosanu, L. and Jouvet, D. 2015. Detection of sentence  modality on French automatic speech-to-text transcriptions.  Proceedings ICNLSP'2015, International Conference on  Natural Language and Speech Processing (2015).   [34] Quang, V.M., Besacier, L. and Castelli, E. 2007. Automatic  question detection: prosodic-lexical features and cross- lingual experiments. Proc. Interspeech (2007), 22572260.   [35] Raghu, D., Indurthi, S., Ajmera, J. and Joshi, S. 2015. A  statistical approach for non-sentential utterance resolution  for interactive QA system. Special Interest Group on  Discourse and Dialogue (SIGDIAL) (2015), 335343.   [36] Samei, B., Olney, A., Kelly, S., Nystrand, M., DMello, S.,  Blanchard, N., Sun, X., Glaus, M. and Graesser, A. 2014.  Domain independent assessment of dialogic properties of  classroom discourse. Proceedings of the 7th International  Conference on Educational Data Mining. International  Educational Data Mining Society. (2014), 223236.   [37] Samei, B., Olney, A.M., Kelly, S., Nystrand, M., DMello,  S., Blanchard, N. and Graesser, A. 2015. Modeling  classroom discourse: Do models that predict dialogic  instruction properties generalize across populations  Proceedings of the 8th International Conference on  Educational Data Mining, International Educational Data  Mining Society. (2015), 444447.   [38] Schuller, B., Steidl, S., Batliner, A. and others 2009. The  INTERSPEECH 2009 emotion challenge. INTERSPEECH  (2009), 312315.   [39] Schutz, D. 2011. The Common Core State standards for  English language arts & literacy in history/social studies,  science, and technical subjects: An analysis and an  alternative. Social Studies, Science, and Technical Subjects:  An Analysis and an Alternative. (2011), 19.   [40] Shriberg, E., Dhillon, R., Bhagat, S., Ang, J. and Carvey, H.  2004. The ICSI meeting recorder dialog act (MRDA)  corpus. In Proceedings of the 5th SIGdial Workshop on  Discourse and Dialogue, 97100.   [41] Stolcke, A., Coccaro, N., Bates, R., Taylor, P., Van Ess- Dykema, C., Ries, K., Shriberg, E., Jurafsky, D., Martin, R.  and Meteer, M. 2000. Dialogue act modeling for automatic  tagging and recognition of conversational speech.  Computational Linguistics. 26, 3 (2000), 339373.   [42] Sweigart, W. 1991. Classroom talk, knowledge  development, and writing. Research in the Teaching of  English. (1991), 469496.   [43] Witten, I.H., Frank, E., Trigg, L.E., Hall, M.A., Holmes, G.  and Cunningham, S.J. 1999. Weka: Practical machine  learning tools and techniques with Java implementations.  Workshop on emerging Engineering and Connectionnist- based Information Systems. (1999), 192196.   [44] Xiong, S., Guo, W. and Liu, D. 2014. The Vietnamese  speech recognition based on rectified linear units deep  neural network and spoken term detection system  combination. 9th International Symposium on Chinese  Spoken Language Processing (ISCSLP) (2014), 183186.   [45] Yuan, J. and Jurafsky, D. 2005. Detection of questions in  Chinese conversational speech. IEEE Workshop on  Automatic Speech Recognition and Understanding (2005),  4752.     "}
{"index":{"_id":"29"}}
{"datatype":"inproceedings","key":"Knight:2017:TMS:3027385.3027433","author":"Knight, Simon and Martinez-Maldonado, Roberto and Gibson, Andrew and Buckingham Shum, Simon","title":"Towards Mining Sequences and Dispersion of Rhetorical Moves in Student Written Texts","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"228--232","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027433","doi":"10.1145/3027385.3027433","acmid":"3027433","publisher":"ACM","address":"New York, NY, USA","keywords":"academic writing, learning analytics, process mining, rhetorical moves, sequence mining, temporal analysis, text mining, writing analytics","Abstract":"There is an increasing interest in the analysis of both student's writing and the temporal aspects of learning data. The analysis of higher-level learning features in writing contexts requires analyses of data that could be characterised in terms of the sequences and processes of textual features present. This paper (1) discusses the extant literature on sequential and process analyses of writing; and, based on this and our own first-hand experience on sequential analysis, (2) proposes a number of approaches to both pre-process and analyse sequences in whole-texts. We illustrate how the approaches could be applied to examples drawn from our own datasets of 'rhetorical moves' in written texts, and the potential each approach holds for providing insight into that data. Work is in progress to apply this model to provide empirical insights. Although, similar sequence or process mining techniques have not yet been applied to student writing, techniques applied to event data could readily be operationalised to undercover patterns in texts.","pdf":"Towards Mining Sequences and Dispersion of   Rhetorical Moves in Student Written Texts    Simon Knight, Roberto Martinez-Maldonado, Andrew Gibson, and Simon Buckingham Shum  Connected Intelligence Centre, University of Technology Sydney, Sydney, Australia   {Simon.Knight, Roberto.Martinez-Maldonado, Andrew.Gibson, Simon.BuckinghamShum}@uts.edu.au     ABSTRACT  There is an increasing interest in the analysis of both students   writing and the temporal aspects of learning data. The analysis of   higher-level learning features in writing contexts requires analyses   of data that could be characterised in terms of the sequences and   processes of textual features present. This paper (1) discusses the   extant literature on sequential and process analyses of writing;   and, based on this and our own first-hand experience on   sequential analysis, (2) proposes a number of approaches to both   pre-process and analyse sequences in whole-texts. We illustrate   how the approaches could be applied to examples drawn from our   own datasets of rhetorical moves in written texts, and the   potential each approach holds for providing insight into that data.   Work is in progress to apply this model to provide empirical   insights. Although, similar sequence or process mining techniques   have not yet been applied to student writing, techniques applied to   event data could readily be operationalised to undercover patterns   in texts.    CCS Concepts    Applied computing~E-learning    Computing   methodologies~Natural language processing    Keywords  Learning analytics, writing analytics, temporal analysis,   sequence mining, process mining, text mining, rhetorical   moves, academic writing   1. INTRODUCTION  There is an increasing interest in the analysis of both students   writing [3] and the temporal aspects of learning data [5, 25]. In   order for text-based analytics to support higher level learning    such as the sharing of ideas together, or the communication and   critical analysis of those ideas  analysis of data with a temporal   or sequential character is necessary [24, 25]. In the context of   writing, this analysis might include data regarding: writing   processes, the behaviours they involve, and the outputs they   produce (for example, note taking, drafting, copy-editing); or of   sequences, both of events (for example, the co-occurrence of   inserting a citation and inserting language from the referred to   paper), or of linguistic information within a text (for example,   argumentative moves that recur in consistent sequential forms).   These analyses are further complicated by the potential for them   to be multi-modal (for example, across online and paper-based   documents), and multi-agent (for example, including interaction   with peers, tutors, and others). This paper first discusses extant   literature on what we characterise as temporal writing analytics   (2) highlighting cohesion (2.1) and rhetorical move (2.2)   analyses. We illustrate by our developing work on a corpus   annotated with rhetorical moves and their analysis (4).   Secondly, based on literature on temporal analysis of writing,   developing analysis and sequence pattern mining, and our own   first-hand experience of sequential analysis, we propose four   approaches to pre-process and four approaches to mine frequent   sequences and dispersion of rhetorical moves in authentic student   writing, highlighting their potential for  analysis of features of that   writing  (4.2-4.3).   2. SEQUENCE AND PROCESS ANALYSES  OF STUDENT WRITING  A small body of work has explored sequence and process features   of student writing. For example, to study writing processes,   analysis has been conducted on the editing features in Google   Docs to investigate the revisioning and editing processes students   undertake in writing tasks [4, 27, 41], with similar work in an   automated writing tutor tool [37]. In other work, researchers have   explored the ways that topic modelling techniques, and social   network analyses, can be used to investigate the emergence,   divergence, and convergence of voices throughout a document   [10], describing an essays flow.   Similarly, argument mining refers to the automatic identification   of the argumentative structure contained within a piece of natural   language text [26]. The presence and combination of specific   elements of arguments (e.g. premise, argumentative move, claim)   are fundamental to definitions of argument and the   computational modelling of argumentation [e.g., 15, 45], thus   lending itself to sequence analysis for identification of argument   structures and arguments in use through the analysis of argument   constituent parts [e.g., 28].   Obviously the syntactic dependencies, represented as sequences of   parts of speech in essays, can also be used to infer structures  for   example, causal relations [18], or term-definition pairs [14]    present in a text, with the additional analysis of temporal   references in a text supporting identification of question-answer   pairs [2], and types of discourse [38]. Association rule and   sequence mining approaches have been similarly used to identify   relationships between constructs in a text [for example, 1, 20, 35],   and to detect erroneous sentences [42]. Linguistics research has   used these approaches to investigate the ways that language is   structured, and used in everyday contexts.   2.1 Text Cohesion  In research specifically on student written products, a limited   body of work has investigated the sequential structure of those   texts using natural language processing (NLP) techniques. Using   linguistic features related to textual cohesion and lexical   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for   components of this work owned by others than the author(s) must be   honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior   specific permission and/or a fee. Request permissions from   Permissions@acm.org.   LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada   Copyright is held by the author(s). Publication rights licensed to ACM.      DOI: http://dx.doi.org/10.1145/3027385.3027433   mailto:Permissions@acm.org http://dx.doi.org/10.1145/3027385.3027433   sophistication, the position of a paragraph  introductory, body, or   conclusion  can be predicted with a similar degree of accuracy to   human judgements (65% against 66% accuracy), indicating that   different sections of texts tend to contain particular features [11].   Moreover, higher quality paragraphs were easier to rate   (presumably because they best exemplified the rhetorical structure   of the intended paragraph type), and paragraph-position specific   information regarding body paragraphs may inform pedagogic   strategies [32].    In addition to textual indicators being used to identify paragraph-  position, indicators can be constructed with small but significant   predictive value for ratings of introductory (r2=0.25), body   (r2=0.10), and concluding (r2=0.11) paragraph quality [36]. This   analysis indicated that good introductions tended to be longer,   using more infrequently occurring words, plus indicators of   temporal cohesion (consistent tense, aspect, etc.) and causal   cohesion (i.e. indicators of how events and actions are connected).   Body paragraphs, then, displayed deep vocabulary alongside   locational nouns, more varied sentence structures, and an internal   cohesion. Finally, conclusions express more specific ideas using   accessible, yet varied syntax. This pattern is consistent with the   rhetorical goal of concluding an essay with a straightforward   summary of ones idea that provides the reader with a big   picture understanding [36].    2.2 The Rhetorical Structure of a Text  That work [36] hypothesised that, for example, relationships   identified in introductory paragraphs indicate authors previewing   of arguments and provision of reasons for their position.   However, the measures used provide limited insight into   rhetorical structure. Indeed, building on this work, Crossley et al.,   [12] further analysed the use of key n-grams (unigrams, bigrams,   and trigrams) in poor and high quality introductory, body, and   concluding paragraphs, by grouping the key n-grams into clusters   of rhetorical, grammatical, syntactic, and cohesion features   [12]. Across all three paragraph positions, rhetorical features were   identified as most important, indicating the potential of analysing   rhetorical n-grams within paragraph-contexts to inform automated   approaches to assessing whole-essay quality [12].   An alternative approach to understanding sub-patterns within texts   has emerged from work informed by Swales analysis of   rhetorical moves in academic writing [43]. In research paper   introductions, these moves mark the: introduction of a topic and   its background; raising questions, contrasts or the need to extend   the topic; and establishing the contribution or novelty of the given   text (its niche or the gap it occupies). Swales thus offers a   template for thinking about the kinds of linguistic patterns   (rhetorical moves) that occur in texts, where in texts they might   appear and in what kind of sequence. Specifically, we would   anticipate that texts make moves between: (1) introducing topic   background; (2) establishing a space within that topic through its   critique, contrast, discussion of its potential for extension, etc.;   and (3) filling that space by taking a particular position or stance   with regard to it. These moves might be dispersed both within a   particular section of text (i.e., we would expect more 1 moves   earlier, and more 3 moves later in an introduction), and within   particular sub-section-sequences such as individual paragraphs.   Thus, we might expect paragraphs to introduce background (move   1), evaluate that information (move 2), and point to the potential   for future research (move 3). In related work on Argumentative   Zoning, location is used as a feature in order to determine the   rhetorical move being made in a scientific text [17, 44].   One tool that has investigated automated approaches to the   detection of these moves at a sentence level, and specifically in a   higher education context, is the Intelligent Academic Discourse   Evaluator (IADE) [7], research on which has demonstrated the   variation in the weight of particular rhetorical moves in different   disciplinary texts. Of course, analysis of rhetorical moves above   the sentence level (or, moves as comprising many sentences) is   important too [6, 13], and indeed a tool based on these rhetorical   moves has been developed to give feedback on the moves within   abstracts [13], with a newer tool (the Research Writing Tool    RWT) developed to support students in reflecting on the rhetorical   moves that should be present in each section of their text, in their   disciplinary context [8, 9].    2.3 Summary: Developing Analytic  Approaches  Across the body of work reviewed above that explores this issue,   analysis has indicated that sections of a text (for example,   introduction, body, conclusion) contain different linguistic   features, and that the presence of these features may be an   indicator of the appropriateness (or quality) of those sections. That   is, good introductions tend to have particular characteristics as   compared to other text sections and poorer quality introductions.   Across this work, the rhetorical structure of a text has emerged as   a particularly salient feature. Extant analyses of student writing   have investigated the weight of particular rhetorical moves, or   textual features, in sections of a text. These analyses have been   related to: human assessments of the type of text observed (i.e.,   whether it was introduction, body, or concluding text);   disciplinary genre; and  in a few cases  the quality of the text   observed. This analysis has tended to focus on describing the   nature of particular text sections, but less on relating these to   assessment criteria judgements, or feedback to students.   Moreover, extant prior analyses have not, to our knowledge,   extended to the potential of sequence and process mining   approaches for understanding textual data.    3. SEQUENCE AND PROCESS MINING  ON STUDENT DATA  Sequential mining and process mining are techniques that have   been used to identify patterns in educational datasets by   considering the order of students actions in learning system   activity logs [21, 29, 33]. Examples of sequential pattern   extraction and modelling have examined the temporality of   students actions in order to gain insights into: the development of   strategies, or to differentiate or group students who show similar   behaviours [21]; students strategies [e.g., 31], students profiles   [e.g., 34], conversation patterns [e.g., 30], the temporal evolution   of students strategies [e.g., 22], or to compare cohorts of students   by identifying the actions that differentiate them according to their   expertise [19]. To a lesser extent, sequence mining techniques   have also been used to focus on understanding the evolution of the   artefacts created by the students [e.g., 29, 34].    The potential of these techniques for learning analytics is that   analysing sequences of events can be a quite generic approach to   consider the temporality for distilling interesting patterns where   the sequence of occurrence (absence or dispersion) of certain   event can be crucial for learning. We propose that such techniques   could have potential to provide insights into student writing, and   in the light of the above literature analysis, to provide feedback on   the rhetorical structure of a text. Although, similar sequence or   process mining techniques have, to our knowledge, not yet been   applied to student writing, some techniques applied to event data     could easily be operationalised to undercover patterns in texts. To   summarise, the preliminary work reported here motivates a   theoretically sound rationale for the application of sequence   mining to student writing, which we hope other researchers can   use as a reference, and establish the steps needed to prepare data   to maximise the opportunities of finding useful insights.    4. APPROACH  This section presents work analysing sequences and dispersion of   moves in student writing. Below, we illustrate our approaches for   preparing (pre-processing) and analysing (mining) the dataset   based on our specific case of rhetorical moves in student writing.   However, our claim is that the general approach described is   applicable to other contexts, and is agnostic regarding the kind of   analytic approach taken to identifying moves in a text. We   finalise the section with an illustrative example of cohort analysis   of rhetorical moves dispersion.   4.1 Dataset: Annotation of Rhetorical Moves  Analysis is underway on a dataset from multiple disciplinary   genres (law, accounting, and biology), annotated using the Xerox   Incremental Parsers (XIP) instantiation in tools for feedback on   analytical academic writing [23, 39, 40]. Within the Academic   Writing Analytics (AWA) tool developed at UTS, the analytic   parser is designed to detect rhetorical steps, inspired by Swales   moves, that indicate specific rhetorical functions falling under the   general moves described above. These steps are then  through   AWA  highlighted within a submitted document, in order to give   students feedback on the rhetorical structure of their text.    The analytical module labels thus include: Summarising issues   (describing the articles plan, goals, and conclusions) (S),   describing Background knowledge (B), Contrasting ideas (C),   Emphasising important ideas (E), mentioning Novel ideas (N),   pointing out Surprising facts, results, etc. (P), describing an open   Question or insufficient knowledge (Q), and recognising research   Trends (T). Thus, as in Figure 1, each sentence in a document is   labelled with none, or with one or more of these steps.        Figure 1  An Example AWA Report   4.2 Dataset Preparation Approaches  Texts submitted to AWA are processed, with the marked up   documents displayed to students in a report which highlights   individual sentences that display indicators of particular rhetorical   moves. In order to process the student texts the original files (docx   or pdf) were cleaned to remove: student IDs and names; headings;   preface (such as cover sheets, or the essay prompt); end-matter   (such as reference lists); and figures, tables, and captions. In   addition, lists were converted to paragraph text. These texts were   analysed using the AWA tool, with outputs in JSON format    Thus, documents submitted to AWA can be represented as sets of   ordered sentences, with each sentence annotated with particular   features  including which (if any) rhetorical moves it appears to   exemplify. These texts may then be represented using a few   different approaches, with important implications for the unit of   analysis  for example, whether to treat the whole-text as a single   unit, or to analyse meaningful sub-sections (such as the paragraph,   or headed-section).    Essays can thus be represented as ordered lists of sentence-types.   We can consider all the sentences or rather only those sentences   for which moves are identified (i.e., ignoring blank sentences)    levels 1 and 2 in Figure 2. In addition, the repetition of   particular features (including blank sentences) might be   collapsed into a single item, or treated separately. In addition,   representations may capture elements of the process of writing a   text  the key sections or elements that a text is built up from, for   example, the paragraph, or introduction/body/conclusion  as in   the right most two representations in Figure 2 (levels 3 and 4). In   this latter model, sentences, then, might either be treated as   ordered or unordered elements within the constituent parts. That   is, they can be treated as components of the sequences seen in the   left most columns, or treated such that the order of the items   within the sub-section does not matter.     Figure 2  Levels for Representing/Aggregating Students   Writing: 1) Sentences (all), 2) Sentences marked as moves, 3)   Paragraphs, and 4) Sections   4.3 Analysis (Mining) Approaches  Given the relationship between the XIP identified rhetorical   moves within the AWA tool, and Swales rhetorical moves, our   hypothesis is that methods to treat the sequences of rhetorical   moves temporally may yield insight and aid us in designing   actionable feedback for learners. Specifically, we are orienting our   analysis around four approaches to the data analysis (which  as   described below  may not be mutually exclusive):    Analysis of Sub-sequences commonly occurring throughout a text    for example, the recurrence of background, contrast,   novel moves in sequences indicating a typical analysis style   sequence. Analysis based on this approach  for example using   TraMineRs analysis of frequent sub-sequences [16]  could   indicate patterns in the ways in which groups of texts use   particular structures in their text as indicated by their patterns of   rhetorical move. This analysis could be conducted on any of the   levels of data segmentation in Figure 2.   Analysis of Dispersion or weighting of moves across sub-sections   of a text (e.g. introduction, body, conclusion)  for example, a   tendency for background moves to appear more in the   introduction, with emphasizing moves more in a conclusion.   This type of analysis could also indicate the dispersion of sub-  sequences throughout a text. Analysis based on this approach   could indicate the ways that texts are structured to show how   sections of text fulfil particular functions. Analysis of this sort   could be conducted on levels 3 & 4 of representing the data, as it   requires a way for weighting the moves within particular sub-  sections.     Analysis of Associations between moves that co-occur in   meaningful sub-sections of text (e.g. the paragraph level), in   non-ordered ways. For example, a tendency for novel and   surprise moves to co-occur. This type of analysis could   indicate that particular sub-sections of text have   characteristic moves  but that these moves can occur in a   variety of orders. Analysis of this sort could be conducted on   any of the levels of representing the data.    Analysis of Processes of moves occurring over a whole text    for example, a text might include a cycle of sections   weighted towards a particular move (as in the dispersion   analysis), in sequences  for example, more background   moves, followed by more question moves, followed by   more novel, although within each element other moves   might occur. For example, we might see a repeated pattern   of shifts from mostly background steps to mostly question   steps, to mostly trends steps, as in discussion papers that point to   emerging research directions based on questions in the earlier   literature. Analysis of this sort could be conducted on any (1-4) of   the levels of representing the data.   4.4 Illustrative Example: Cohort Analysis of  Rhetorical Moves Dispersion  Here we present a preliminary example of the potential of   analysing the dispersion of rhetorical moves in students texts   Figure 3 shows the rhetorical moves for 6 students texts of two   cohorts: those that received high distinction (HD) and pass (P)   marks. For illustrative purpose, this simple visualisation shows all   the rhetorical moves of the texts divided at a section level, with   each students text arbitrarily divided in quantiles to explore the   dispersion of the rhetorical moves.    The HD texts present a larger mix of rhetorical moves,   particularly in the first and last quintiles (see particularly Q1 and   Q2 for texts B and C). Notably, they all commenced with an   EMPHASIS move followed by a combination of SUMMARY,   CONTRAST sentences and other moves. In Q4 and Q5 these texts   also showed a quite varied combination of moves. By contrast, the   P texts, overall, showed fewer, and more dispersed, rhetorical   moves. Notably, the three P examples present some moves in the   middle of the text (at Q3) which are not observed in the HD texts.   Additionally, these texts include a smaller range of moves either   at the beginning (Q1 for F) or the end (Q5 in D and E) of the text.   This illustrates the potential insights that can be gained by   analysing sequencing and dispersion of rhetorical moves. Finding   patterns of this type in larger cohorts may be useful to generate   the means for providing automated or hybrid feedback to the   students about their writing.    5. CONCLUSIONS  This paper has sought to present an account of sequential and   process analyses of writing, using data from our own research to   highlight the ways in which student-texts might be treated using   these approaches. In doing so, we foreground the potential of   different types of analysis and data representation. There is   potential to draw on these analyses to feedback to students and   instructors regarding the structure of their written work. Analyses   are ongoing to demonstrate this potential empirically.   6. REFERENCES  [1] Bchet, N., Cellier, P., Charnois, T. and Crmilleux, B.   2012. Discovering linguistic patterns using sequence mining.   International Conference on Intelligent Text Processing and   Computational Linguistics (2012), 154165.   [2] Bruce, B.C. 1972. A model for temporal references and its   application in a question answering program. Artificial   intelligence. 3, (1972), 125.   [3] Buckingham Shum, S., Knight, S., McNamara, D., Allen, L.,   K.., Betik, D. and Crossley, S. 2016. Critical Perspectives on   Writing Analytics. (Edinburgh, UK, 2016), 481483.   [4] Calvo, R.A., ORourke, S.T., Jones, J., Yacef, K. and   Reimann, P. 2011. Collaborative writing support tools on the   cloud. IEEE Transactions on Learning Technologies. 4, 1   (2011), 8897.   [5] Chen, B., Wise, A.F., Knight, S. and Cheng, L., K.. 2016.   Its About Time: Putting Temporal Analytics into Practice:   The 5th International Workshop on Temporality in Learning   Data. (Edinburgh, UK, 2016), 488489.   [6] Cortes, V. 2013. The purpose of this study is to: Connecting   lexical bundles and moves in research article introductions.   Journal of English for academic purposes. 12, 1 (2013), 33  43.   [7] Cotos, E. 2009. Designing an intelligent discourse evaluation   tool: Theoretical, empirical, and technological   considerations. Iowa State University.   [8] Cotos, E. and Huffman, S. 2013. Learner fit in scaling up   automated writing evaluation. International Journal of   Computer-Assisted Language Learning and Teaching   (IJCALLT). 3, 3 (2013), 7798.   [9] Cotos, E. and Pendar, N. 2016. Discourse classification into   rhetorical functions for AWE feedback. Calico Journal. 33,   1 (2016), 92.   [10] Crossley, S., Dascalu, M., Trausan-Matu, S., Allen, L. and   McNamara, D. 2016. Document Cohesion Flow: Striving   towards Coherence. Cognitive Science Society (2016).   [11] Crossley, S., Dempsey, K. and McNamara, D. 2011.   Classifying paragraph types using linguistic features: Is   paragraph positioning important Journal of Writing   Research. 3, 2 (Dec. 2011), 119143.   [12] Crossley, S.A., Defore, C., Kyle, K., Dai, J. and McNamara,   D.S. 2013. Paragraph Specific N-Gram Approaches to   Automatically Assessing Essay Quality. EDM (2013), 216  219.   [13] Dayrell, C., Candido Jr, A., Lima, G., Machado Jr, D.,   Copestake, A.A., Feltrim, V.D., Tagnin, S.E. and Alusio,   S.M. 2012. Rhetorical Move Detection in English Abstracts:   Multi-label Sentence Classifiers and their Annotated   Corpora. International Conference on Language Resources   and Evaluation (Istanbul, Turkey, 2012).   [14] Denicia-Carral, C., Montes-y-Gmez, M., Villaseor-Pineda,   L. and Hernndez, R.G. 2006. A text mining approach for   definition question answering. Advances in Natural   Language Processing. Springer. 7686.     Figure 4  Visualisations of 6 pieces of Science students writing that   represent the potential of cohort analysis of rhetorical moves dispersion     [15] Feng, V.W. and Hirst, G. 2011. Classifying arguments by   scheme. Proceedings of the 49th Annual Meeting of the   Association for Computational Linguistics: Human   Language Technologies-Volume 1 (2011), 987996.   [16] Gabadinho, A., Ritschard, G., Mueller, N.S. and Studer, M.   2011. Analyzing and visualizing state sequences in R with   TraMineR. Journal of Statistical Software. 40, 4 (2011), 1  37.   [17] Guo, Y., Korhonen, A. and Poibeau, T. 2011. A weakly-  supervised approach to argumentative zoning of scientific   documents. Proceedings of the Conference on Empirical   Methods in Natural Language Processing (2011), 273283.   [18] Hastings, P., Hughes, S., Britt, A., Blaum, D. and Wallace,   P. 2014. Toward Automatic Inference of Causal Structure in   Student Essays. Intelligent Tutoring Systems. S. Trausan-  Matu, K.E. Boyer, M. Crosby, and K. Panourgia, eds.   Springer International Publishing. 266271.   [19] Jiang, Y., Paquette, L., Baker, R.S. and Clarke-Midura, J.   2015. Comparing Novice and Experienced Students within   Virtual Performance Assessments. International Educational   Data Mining Society. (2015).   [20] Kale, M.S., Palshikar, G.K., Chhajed, S. and Deshpande, L.   2005. Data Mining over Textual Data. TACTiCS  TCS   Technical Architects Conference (London, UK, 2005).   [21] Kinnebrew, J. and Biswas, G. 2012. Identifying learning   behaviors by contextualizing differential sequence mining   with action features and performance evolution. Educational   Data Mining 2012 (2012).   [22] Kinnebrew, J.S., Segedy, J.R. and Biswas, G. 2014.   Analyzing the temporal evolution of students behaviors in   open-ended learning environments. Metacognition and   Learning. 9, 2 (2014), 187215.   [23] Knight, S., Buckingham Shum, S., Ryan, P., Sndor, . and   Wang, X. Forthcoming. Academic Writing Analytics for   Civil Law: Participatory Design Through Academic and   Student Engagement. International Journal of Artificial   Intelligence in Education. (Forthcoming).   [24] Knight, S. and Littleton, K. 2015. Discourse-Centric   Learning Analytics: Mapping the Terrain. Journal of   Learning Analytics. 2, 1 (2015), 185209.   [25] Knight, S., Wise, A.F., Chen, B. and Cheng, B.H. 2015. Its   About Time: 4th International Workshop on Temporal   Analyses of Learning Data. (Poughkeepsie, NY, USA, Mar.   2015), 388389.   [26] Lawrence, J. and Reed, C. 2015. Combining argument   mining techniques. NAACL HLT 2015. (2015), 127.   [27] Liu, M., Calvo, R.A. and Pardo, A. 2013. Tracer: A Tool to   Measure and Visualize Student Engagement in Writing   Activities. 2013 IEEE 13th International Conference on   Advanced Learning Technologies (Jul. 2013), 421425.   [28] Madnani, N., Heilman, M., Tetreault, J. and Chodorow, M.   2012. Identifying high-level organizational elements in   argumentative discourse. Proceedings of the 2012   Conference of the North American Chapter of the   Association for Computational Linguistics: Human   Language Technologies (2012), 2028.   [29] Maldonado, R.M., Yacef, K., Kay, J., Kharrufa, A. and Al-  Qaraghuli, A. 2010. Analysing frequent sequential patterns   of collaborative learning activity around an interactive   tabletop. Educational Data Mining 2011 (2010).   [30] Martinez-Maldonado, R., Dimitriadis, Y., Martinez-Mons,   A., Kay, J. and Yacef, K. 2013. Capturing and analyzing   verbal and physical collaborative learning interactions at an   enriched interactive tabletop. International Journal of   Computer-Supported Collaborative Learning. 8, 4 (2013),   455485.   [31] Martinez-Maldonado, R., Yacef, K. and Kay, J. 2013. Data   Mining in the Classroom: Discovering Groups Strategies at   a Multi-tabletop Environment. Educational Data Mining   2013 (2013).   [32] Myers, J.C., McCarthy, P.M., Duran, N.D. and McNamara,   D.S. 2011. The bit in the middle and why its important: a   computational analysis of the linguistic features of body   paragraphs. Behavior research methods. 43, 1 (2011), 201  209.   [33] Pechenizkiy, M., Trcka, N., Vasilyeva, E., van Aalst, W. and   De Bra, P. 2009. Process mining online assessment data.   Educational Data Mining (2009), 279288.   [34] Perera, D., Kay, J., Koprinska, I., Yacef, K. and Zaane, O.R.   2009. Clustering and sequential pattern mining of online   collaborative learning data. Knowledge and Data   Engineering, IEEE Transactions on. 21, 6 (2009), 759772.   [35] Plantevit, M., Charnois, T., Klema, J., Rigotti, C. and   Crmilleux, B. 2009. Combining sequence and itemset   mining to discover named entities in biomedical texts: a new   type of pattern. International Journal of Data Mining,   Modelling and Management. 1, 2 (2009), 119148.   [36] Roscoe, R., Crossley, S., Weston, J. and McNamara, D.   2011. Automated Assessment of Paragraph Quality:   Introduction, Body, and Conclusion Paragraphs. Twenty-  Fourth International FLAIRS Conference (Mar. 2011).   [37] Roscoe, R.D., Snow, E.L., Allen, L.K. and McNamara, D.S.   2015. Automated detection of essay revising patterns:   applications for intelligent feedback in a writing tutor.   Cognition and Learning. 10, 1 (2015), 5979.   [38] Salager-Meyer, F. 1992. A text-type and move analysis   study of verb tense and modality distribution in medical   English abstracts. English for specific purposes. 11, 2   (1992), 93113.   [39] Simsek, D., Buckingham Shum, S., Sandor, A., De Liddo, A.   and Ferguson, R. 2013. XIP Dashboard: visual analytics   from automated rhetorical parsing of scientific   metadiscourse. (Leuven, Belgium, 2013).   [40] Simsek, D., Sandor, A., Shum, S.B., Ferguson, R., De Liddo,   A. and Whitelock, D. 2015. Correlations between automated   rhetorical analysis and tutors grades on student essays.   Proceedings of the Fifth International Conference on   Learning Analytics And Knowledge (2015), 355359.   [41] Southavilay, V., Yacef, K., Reimann, P. and Calvo, R.A.   2013. Analysis of Collaborative Writing Processes Using   Revision Maps and Probabilistic Topic Models. Proceedings   of the Third International Conference on Learning Analytics   and Knowledge (New York, NY, USA, 2013), 3847.   [42] Sun, G., Liu, X., Cong, G., Zhou, M., Xiong, Z., Lee, J. and   Lin, C.-Y. 2007. Detecting erroneous sentences using   automatically mined sequential patterns. Proceedings of the   45th Annual Meeting  of the Association for Computational   Linguistics (Prague, Czech Republic, 2007), 8188.   [43] Swales, J.M. 1990. Genre analysis: English in academic and   research settings. Cambridge University Press.   [44] Teufel, S. and Moens, M. 2002. Summarizing scientific   articles: experiments with relevance and rhetorical status.   Computational linguistics. 28, 4 (2002), 409445.   [45] Walton, D. 2012. Using argumentation schemes for   argument extraction: A bottom-up method. International   Journal of Cognitive Informatics and Natural Intelligence   (IJCINI). 6, 3 (2012), 3361.     1. INTRODUCTION  2. SEQUENCE AND PROCESS ANALYSES OF STUDENT WRITING  2.1 Text Cohesion  2.2 The Rhetorical Structure of a Text  2.3 Summary: Developing Analytic Approaches   3. SEQUENCE AND PROCESS MINING ON STUDENT DATA  4. APPROACH  4.1 Dataset: Annotation of Rhetorical Moves  4.2 Dataset Preparation Approaches  4.3 Analysis (Mining) Approaches  4.4 Illustrative Example: Cohort Analysis of Rhetorical Moves Dispersion   1.  5. CONCLUSIONS  1.  6. REFERENCES   "}
{"index":{"_id":"30"}}
{"datatype":"inproceedings","key":"Hoel:2017:IDP:3027385.3027414","author":"Hoel, Tore and Griffiths, Dai and Chen, Weiqin","title":"The Influence of Data Protection and Privacy Frameworks on the Design of Learning Analytics Systems","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"243--252","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027414","doi":"10.1145/3027385.3027414","acmid":"3027414","publisher":"ACM","address":"New York, NY, USA","keywords":"data protection, data protection by default, data protection by design, learning analytics, learning analytics process requirements, learning analytics systems design, personal information, privacy by design, privacy frameworks","Abstract":"Learning analytics open up a complex landscape of privacy and policy issues, which, in turn, influence how learning analytics systems and practices are designed. Research and development is governed by regulations for data storage and management, and by research ethics. Consequently, when moving solutions out the research labs implementers meet constraints defined in national laws and justified in privacy frameworks. This paper explores how the OECD, APEC and EU privacy frameworks seek to regulate data privacy, with significant implications for the discourse of learning, and ultimately, an impact on the design of tools, architectures and practices that now are on the drawing board. A detailed list of requirements for learning analytics systems is developed, based on the new legal requirements defined in the European General Data Protection Regulation, which from 2018 will be enforced as European law. The paper also gives an initial account of how the privacy discourse in Europe, Japan, South-Korea and China is developing and reflects upon the possible impact of the different privacy frameworks on the design of LA privacy solutions in these countries. This research contributes to knowledge of how concerns about privacy and data protection related to educational data can drive a discourse on new approaches to privacy engineering based on the principles of Privacy by Design. For the LAK community, this study represents the first attempt to conceptualise the issues of privacy and learning analytics in a cross-cultural context. The paper concludes with a plan to follow up this research on privacy policies and learning analytics systems development with a new international study.","pdf":"The Influence of Data Protection and Privacy Frameworks  on the Design of Learning Analytics Systems Tore Hoel   Oslo and Akershus University  PO Box 4 St. Olavs plass   NO-0130 Oslo  Norway   Tore.Hoel@hioa.no   Dai Griffiths  University of Bolton   Deane Road  Bolton BL3 5AB   UK  D.E.Griffiths@bolton.ac.uk      Weiqin Chen   University of Bergen,   PO Box 7807  NO-5020 Bergen   Norway  Weiqin.Chen@uib.no        ABSTRACT  Learning analytics open up a complex landscape of privacy and  policy issues, which, in turn, influence how learning analytics  systems and practices are designed. Research and development is  governed by regulations for data storage and management, and by  research ethics. Consequently, when moving solutions out the  research labs implementers meet constraints defined in national  laws and justified in privacy frameworks. This paper explores  how the OECD, APEC and EU privacy frameworks seek to  regulate data privacy, with significant implications for the  discourse of learning, and ultimately, an impact on the design of  tools, architectures and practices that now are on the drawing  board. A detailed list of requirements for learning analytics  systems is developed, based on the new legal requirements  defined in the European General Data Protection Regulation,  which from 2018 will be enforced as European law. The paper  also gives an initial account of how the privacy discourse in  Europe, Japan, South-Korea and China is developing and reflects  upon the possible impact of the different privacy frameworks on  the design of LA privacy solutions in these countries. This  research contributes to knowledge of how concerns about privacy  and data protection related to educational data can drive a  discourse on new approaches to privacy engineering based on the  principles of Privacy by Design. For the LAK community, this  study represents the first attempt to conceptualise the issues of  privacy and learning analytics in a cross-cultural context. The  paper concludes with a plan to follow up this research on privacy  policies and learning analytics systems development with a new  international study.   CCS Concepts   Security and privacyPrivacy protections  General and  referenceDesign  Security and privacySocial aspects of  security and privacy  Security and privacyPrivacy  protections  Applied computingE-learning   Keywords  Learning analytics; Privacy frameworks; Data protection; Data  protection by design; Data protection by default; Personal  information; Learning analytics systems design; Privacy by  design; Learning analytics process requirements.   1. INTRODUCTION  1.1 The Task Undertaken by this Paper  As learning analytics in schools, universities and the workplace  starts to scale up, concerns about privacy and data protection will  also inevitably increase. This was confirmed by the EU funded  Learning Analytics Community Exchange (LACE) which asked  in the title of a review paper if privacy was a show-stopper for the  field [13]. In the paper, examples are provided of Learning  Analytics (LA) projects that were stopped or red-flagged because  of privacy concerns. At the time that these problems emerged  (2013 - 2015), international standards groups were keeping  privacy issues out of scope in their work on specifying how  activity streams should be expressed, exchanged and stored [1,  23]. These events and processes are well known to the learning  analytics community, but, in parallel with this work, there are  emerging national and international policies on data protection  which are developing and generating requirements for LA which  have received less attention. One hopes that the field of LA  research has experienced enough set-backs, and has reflected on  them sufficiently, to realise that there is a need for a deeper and  more systematic treatment of ethical and data protection issues.  Assuming that this is the case, the next step is to ask what  implications an awareness of privacy issues could have for the  design of LA tools and architectures.   The panorama of legal and policy issues raised by new and  emerging LA technologies and practices, is immensely varied.  Moreover, LA takes place in a wide range of social contexts,  within which varying configurations of interest groups seek to  guide the development of LA.  Consequently, we are faced with a  landscape which is hard to understand, or even to survey. In order  to make a start on this task, there is a need for a comparative  analysis of the legal and policy environment in the different  regions of the world, together with an assessment of the varying  factors which come to bear on the regulatory framework in  different countries. The present paper will contribute to this  exploration by analysing emerging international privacy  frameworks, which are currently being turned into national laws  in various parts of the world. The commitments which these  frameworks and laws imply, in turn, determine the requirements  for the design of national and international LA systems and  architectures. We then present some initial, illustrative, case   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027414     studies on what role data protection regulations play in the  national discourse among LA experts in selected countries, to  discuss how privacy issues would influence the development and  application of LA in different national contexts.   1.2 The Complex Landscape of Learning  Analytics and Privacy Policy  The range of technical environments for LA is increasingly  complex, sometimes putting users in control of the management  of their data, and sometimes keeping them completely out for the  loop. This, and the range of data sources involved, enmeshes  learning analytics with multiple personal and societal issues in  ways that are not yet fully analysed. The situation is made still  more convoluted by the fact that the rise of learning analytics  is  not presented simply as a more effective way to carry out  educational activities, but also as a means to transform the context  in which the new methods are embedded  [13].    Early adoption of LA has predominantly occurred in the US,  where  it is playing an increasing role in determining how many  post-secondary education institutions (PSEIs) engage with their  students at multiple points in the student journey, as well as in the  design of teaching and learning content and delivery  [31]. What  is actually being done in PSEIs around the globe to apply student  data at an institutional level is hard to establish, as these  institutions are to some extent self-regulated, and adoption often  follows research initiatives (and research ethics policies) in  different university departments. While we wait for international  case studies to be published, we can observe that national  Ministries of Education (MoEs) have established  big data in  education  centres at selected universities, to do research on LA,  but also to develop a knowledge base for policy development.  Examples include SLATE (University of Bergen, Norway), the  Centre for Big Data on Technology-Mediated Education (Beijing  Normal University, China), and the Learning Analytics Center  (Kyushu University, Japan). In some Western countries LA is  diffused to schools from university research via publishers and  vendors, who adapt new applications using student data for  interactive learning resources, innovative apps for STEM  education, etc. However, there are examples of MoEs and school  agencies that have more ambitious plans for large-scale adoption,  e.g., the Republic of Korea wants to encourage large scale  application of LA within a couple of years (Cho, personal  communication, 24 September 2016).    When the Norwegian Centre for ICT in Education published its  first guide on learning analytics in 2015, it concluded that the  implementation of LA would probably be illegal unless a number  of principles of data protection were adhered to [6]. The principles  were summarised by the Centre as  lawfulness, purpose  limitation, data minimisation, data quality, storing and deletion,  right to know what information is stored, and information safety .  These principles are derived from The Personal Data Act of April  2000 [6], which in turn builds on the European Data Protection  Directive (Directive 95/46/EC). The text of the Act describes  these principles succinctly: The data controller shall ensure that  personal data are processed only if a) the data subject has  consented;  b) are used only for explicitly stated purposes that are  objectively justified by the activities of the controller, c) are not  used subsequently for purposes that are incompatible with the  original purpose of the collection, without the consent of the data  subject, d) are adequate, relevant and not excessive in relation to  the purpose of the processing, and e) are accurate and up-to-date,   and are not stored longer than is necessary for the purpose of the  processing  [7].   In the guide published on its website, the Norwegian school  agency asks  how will the school owner make sure that  information only are used for learning and not for other purposes,  for example to control pupils and teachers (..) What is the  boundary between information that are relevant for learning and  information that are not relevant, but nevertheless are of interest  for registration and analysis  [6] The text alludes to the Centres  limited trust in school owners being  able to maintain the most  important principle of data protection: The data subject should be  in control of and agree to how their own data are used  [6].   This initial response by a Northern European school agency to a  hot new topic may be at the most cautious end of the scale, but  legal constraints are something most authorities would consider  with great care. Therefore, the authors of this paper suggest that  the privacy frameworks that underpin national legal systems  should be explored to see what requirements can be extracted, and  the implications of these requirements for the design of technical  systems and practices serving the needs of the different  stakeholders of LA.   Before starting on the task of examining privacy frameworks it is  worth sounding a note of caution. The market for LA technologies  is global, and the drivers for the diffusion of LA practices are  international, such as standards, access to new data sources, new  sensor technologies, new pedagogical trends, etc. Privacy  frameworks are also international, and often developed by  organisations promoting international trade (OECD, APEC).  Nevertheless, agreed upon concepts like 'purpose specification',  'collection limitation', individual participation', etc. tend to get  very different interpretations when they are applied in national  and institutional policies, culture and professional practice.    We now turn to an analysis of some key privacy frameworks, and  their relationship to LA.   2. PRIVACY REQUIREMENTS FOR LA  SYSTEMS  Legal requirements are filtered through a set of national policies,  educational culture, infrastructure and organisational factors  before designers of LA systems can synthesise them as hard  requirements for applications. Figure 1 illustrates how, in the  authors experience, different factors contribute to MoEs' policies.  It makes a big difference if a privacy framework is turned into  national law (as the case with the European GDPR), compared to  the burying of privacy principles in agreements of trade and  economic cooperation. However, as many of the principles build  on the same ideas, policymakers who want to influence the  direction taken in the design of privacy and data protection for LA  could find arguments to support their proposals in these  agreements.   European countries, Korea and Japan are members of OECD,  while China, Japan, and Korea of the countries we have studied  are members of APEC, the Asia-Pacific Economic Cooperation.  OECD published its first on Guidelines on the Protection of  Privacy and Trans-border Flows of Personal Data in 1980 (revised  and published at OECD Privacy Framework in 2013 [29]). These  influenced the EU Data Protection Directive of 1995 (95/46/EC),  which now will be replaced by the EU GDPR, published in May  2016  [10], and designed to make sure peoples right to personal  data protection (...) remains effective in the digital age [11].  APEC published its Privacy Framework in 2005 [3]. APEC has     been working towards updating their framework in time to mark  the 10th anniversary of its adoption; however, a current activities  list at the APEC website informs that the organisation is still  working on the APEC Privacy Framework 2015, and that there  are ongoing activities to promote interoperability between the  APEC-EU Privacy Rules Systems [2].         Figure 1. Influence factors impacting national requirements  for LA tools and systems   The three frameworks are heavily influenced by each other, and  they share many of the same concepts, as shown in Table 1. (In  the table, we have listed the principles to show similarities, not to  reflect how their order in the framework documents.) All  frameworks try to strike a balance between protection of the  individual and free flow of trade. One might say that the APEC  framework leans more to the latter side, and the EU framework  more to the former. Whether this is reflected in national data  protection laws is beyond the scope of this study. However, in all  countries we have been studying, a discourse on privacy and  sharing of personal information for LA should find support in  globally shared concepts, including the central concepts of  purpose limitation, data minimisation and openness and  transparency.   Table 1. Privacy principles as defined in the privacy  frameworks of OECD, APEC and EU   OECD APEC EU GDPR    Preventing Harm Lawfulness,  Fairness and  Transparency   Collection  Limitation   Collection  Limitation   Data Minimisation   Purpose  Specification   Choice Purpose Limitation   Use Limitation Uses of Personal  Information   Storage Limitation   Data Quality Integrity of  Personal  Information   Integrity and  Confidentiality   Openness Notice    Individual  Participation   Access &  Correction   Accuracy   Accountability Accountability Accountability   Security Safeguards Security Safeguards      Data Protection by  Design and by  Default      A further comparison of the three frameworks gives the  impression that the GDPR is more updated in respect of meeting  the challenges of a digital world (rules regarding breach  notification, automated decision making and profiling, data  portability, etc.). GDPR is also alone in promoting the principle of  Data Protection by Design and by Default. It seems that the APEC  framework gives less rights to the individual and gives a higher  priority to the interests of the organisation.   2.1 Privacy Framework Requirements related  to LA Processes and Pedagogical  Requirements  In an exploration of the implications of the European data  protection regulations for learning analytics design Hoel and Chen  [16] used the LA process lifecycle model (Figure 2) of the  international standardisation organisation ISO/IEC JTC1/SC36 as  a template for discussing how GDPR requirement would influence  systems development. The conclusion was that GDPR had  specific requirements that would influence each process (possibly  with the exception of Visualisation).     Figure 2. LA processes defined in ISO/IEC 20748-1 [21]    Table 2 gives a summary of the findings of [15], where provisions  of the GDPR are mapped to each LA process (Column 2). Data  protection by design and default is an all-encompassing  requirement that influences all the LA subprocesses. In the present  paper, we have added pedagogical requirements derived from  mapping LA processes with GDPR requirements (Table 2,  Column 3). In a discourse about privacy for LA we would claim it  is important to frame legal requirements for system design in  terms of pedagogical aims in order to reach out to the educational  community. It will be much easier to move systems and tools  design forward if educational stakeholders see that their  pedagogical interests are met or affected.      Table 2. Summary of analysis of GDPR and pedagogical  requirements related to LA processes   LA  Processes   GDPR Requirements Pedagogical  Requirements   Learning  activity   Give information of  processing operation  and purpose   Explicit formulation of  the scope of LA  processes. Choice of  metrics that give  answers to the  pedagogical questions  that initiated the LA  process.   Data  collection   Affirmative action of  consent to data  collection   Support of learner  agency   Data storage  and  processing   Access to, and  rectification or erasure  of personal data.   Exercise the right to be  forgotten.   Pseudonymisation and  risk assessment   Support of learner  agency   Analysis Meaningful  information about the  logic involved.  Information of  profiling, e.g.,  predictive modeling   Support of learner  agency and  understanding of  learning context   Visualisation General requirements  about transparency and  communication   Selection of salient  issues for pedagogical  intervention   Feedback  actions   Information about the  significance and  envisaged  consequences of data  processing   Pedagogical  intervention, relating  actions to pedagogical  goals     The main pedagogical grounding of a LA process is centered on  the selection of which Learning Activities to analyse and  decisions on Feedback Actions. However, the other processes are  not pedagogically neutral. If Data Collection, Data Storing and  Processing, and Analysis are designed well, they could contribute  to building learner agency and a better understanding how data are  used in a modern society.   2.2 LA Process Requirements derived from  the GDPR  Table 2 gives a high level view of how LA processes would be  influenced by one privacy framework, the GDPR now being given  legal status as of 2018 in European countries. In order to develop  a more detailed list of design requirements for LA systems  coming out for privacy frameworks we have used the GDPR as a  starting point, in particular the overview of the GDPR developed  by UK Information Commissioner's Office (ico.org.uk) [24]. This  overview was published to help organisations understand their  responsibilities and what rights are given to the individual. Design  requirements for LA systems (see below) are developed by  transforming legal requirements into systems requirement using  the process life cycle model developed by ISO/IEC (Figure 2) as a   scaffold. (For simplicity, we will use the term learner for end- user of the LA system. In the list of design requirements for LA  systems, the numbers in parenthesis refer to the LA sub-process  described in Figure 2.)    Right to be informed  The learner will throughout the full cycle of the LA process (1-6)  be able to get information about a) what is the purpose of the LA  session for specific activities of learning (1); b) what data are  collected (2); c) how the data are stored  and processed (3); d)  what principles (e.g.,  predictive models, algorithms) are used for  analysing learning data (4); e) what visualisations are used to  render results (5); and f) what are the technical (as opposed to  human) LA feedback actions that are designed for the particular  LA process (6)  Right to access  The learner will throughout the full cycle of the LA process (1-6)  be able to access, i.e., read and download, a) personal information  (3), b) activity data (2) used for analysis (4), c) stored results of  analysis (3)  Right to rectification  The learner will at any time be able to enter into communication  with the data controller to launch claims for rectification of  personal information (1-3).  Right to erasure   The learner has at any time the opportunity to raise the wish to be  forgotten, which means deletion or removal of personal data when  there is no compelling reason for continued processing (1-3). In  an educational context, this could involve a number of actions,  depending on educational level (mandatory or voluntary  education) and contract agreements. These are some of possible  scenarios: a) LA is not a necessary pedagogical means: All  involvement with the LA process is terminated; b) LA is  necessary on aggregated data: Only anonymised data are collected  (and steps are taken to make sure that re-identification is not  possible); c) a time restriction for storage of data is agreed, and all  data are erased after completion of module, course, academic  semester, degree, etc.; d) LA is an integral part of the course  offering and the student is given the option to terminate the course  and have his/her data deleted.  Right to restrict processing  This is somewhat similar to the LA attributes described above  (Right to erasure), with the difference that the processing is put on  hold and the data kept for use in historical analysis, aggregated  analysis etc. (3).  The learner could also reserve herself from taking part in specific  learning analytics processing.  Right to data portability  Learners have access to their learning activity data, so that when  moving to another institution or another tool or LA system the  learner can take their data with them for reuse in the new setting  (3).  Right to object  There must be a service agreement that informs about the  learners rights to object to any aspect of the LA processes (1-6).  Right related to automated decision making and profiling     Individuals have the right not to be subject to a decision, which is  based solely on automated processing; and which produces a legal  effect or a similarly significant effect on the individual (4-6).  Learning analytics may entail automated decision making and  profiling of sorts, and these might be part of the contract between  the institution and the individual.  Learners must be able to a) obtain human intervention; b) express  their point of view; and c) obtain an explanation of the decision  and challenge it.  Accountability and governance  The institution must be able to demonstrate that they have systems  in place (policies and procedures) that uphold the protection of  personal information and minimise risk of breaches (1-6).  Breach notification  When systems are compromised in any way, learners should be  notified (3).  Transfer of data  Only EU relevant for European countries - the GDPR has  regulations about transfer of data outside the EU region (3).  Data Protection By Design And By Default  LA systems development should conform to the principle of Data  Protection By Design And By Default (1-6).  This exercise of constructing a detailed list of design requirements  for LA systems by mapping between provisions in the most recent  of the privacy frameworks and individual operations of a LA  process cycle raises a number of questions that could be asked to  different stakeholders around the world to get a picture of how  privacy is conceived in application of LA. This is work that lies in  the future. We will use this mapping of how legal and LA system  requirements points to a new design space for LA as a backdrop  for exploring how the discourse of these issues are held in  selected countries now planning educational interventions using  LA.   3. PRIVACY DISOURSE IN SELECTED  COUNTRIES  A 2015 survey of European citizens' attitudes to data protection  [34] concluded that only 15% felt they had complete control over  the information they provided online; one in three people (31%)  thought they had no control over it at all. Nine out of ten  Europeans expressed concern about mobile apps collecting their  data without their consent, and seven out of ten worried about the  potential use that companies may make of the information  disclosed.   This massive concern about data protection among ordinary  citizens in Europe is not reflected in the discourse of the  international LA research community. The LAK conference is the  principal forum of this community, and the place that one would  hope to find a response to these concerns, based on research  evidence. However, a search for mentions of the data protection  in the proceedings of LAK reveals that the term does not appear in  the proceedings of 2014 or 2015 [13], and only once in 2016 [8].  A similar lack of proposals for how data protection issues could  be handled in an educational context is observable in our brief  studies of privacy discourse in some of the countries that are now  considering policy on LA.    We have focused in this first phase of this research on European  and Asian countries; Europe because of the new Data Protection  Regulation, and Asia because of the presence of research   initiatives and establishment of organisations that take a national  responsibility to promote LA in countries like Japan, Korea and  China.   3.1 European Union / European Economic  Area  The EU/EEA includes more than 30 countries that vary a great  deal in terms of LA readiness and how privacy issues have been  discussed in education. The reasons we discuss the region as a  whole are twofold. Firstly, three years of community building by  the LACE project has provided a good overview of the discourse  on privacy. Secondly, in 2018 a new European data protection  reform will establish a uniform data protection law for all  EU/EEA countries, and this will influence the application of LA  in the region.   A LACE review paper on ethics and privacy [13] concluded that  learning analytics practice has shifted significantly from the  principles of informed consent of the participants, which have to  date been the bedrock of research ethics. Recent practices in  leveraging data are challenging the concepts of data minimization  (focused collection) and consent requirements. The review paper  takes as an example the requirements for educational institutions  set out in a UK Code of Practice developed by Jisc, a university  service provider [30]. Under the Code, institutions do not have to  ask students for permission to gather, hold and analyse their data.  Consent is reframed to refer to permission to take action on the  results of data analysis, a quite different matter from obtaining  permission to gather data. In taking this position Jisc does no  more than recognise current practice, and indeed the modest  proposal that institutions should normally obtain consent to take  action on the results of analytics is more than most educational  institutions in Europe currently do to obtain consent from their  students.   The Open University UK has been a trailblazer in developing  institutional policies on ethical use of student data for LA [35].  One of the principles states that The OU has a responsibility to  all stakeholders to use and extract meaning from student data for  the benefit of students where feasible. These guidelines from Jisc  and OUUK propose that there is an ethical duty on the institution  to gather the best data that it can about its learners, to ensure that  the service that it provides is as good as it can be. The implication  seems to be that if individuals were given the right to opt-out, then  this could be seen as unethical, because opting-out reduces the  efficacy of learning analytics which can improve the education of  others. This interpretation constitutes a radical reframing of the  rights and obligations of the individual and the collective that are  defined in the analysed privacy frameworks, with potentially  profound consequences.   The European LA community discourse on ethical use of student  data has not yet been influenced by another major debate on  privacy coming out of the revision of the European data protection  regulations. It seems inevitable that this will change, as in May  2016 a four year European Union revision process of the 1995  Data Protection Directive (95/46/EC) was concluded with the  publishing of the General Data Protection Regulation (GDPR).  Consequently the EU/EEA countries have until May 2018 to  transpose these regulations into their national law. According to a  factsheet from the European Commission (EC), the GDPR  will  ensure that you receive clear and understandable information  when your personal data is processed. Whenever your consent is  required, it will have to be given by means of a clear affirmative  action before a company can process your personal data. The new  rules will also strengthen individuals right to be forgotten, which     means that if you no longer want your personal data to be  processed, and there is no legitimate reason for a company to keep  it, the data shall be deleted  [9].   The  right to be forgotten  has become a topic of widespread  public debate online, but the importance of GDPR for the use of  data in education remains to be analysed. Hoel and Chen [16]  have argued that the regulations will influence development and  implementation of LA systems, and potentially strengthen the  pedagogical grounding of these systems. The core of the GDPR  relates to minimisation of data and use limitation. This restricts  data collection to specified purposes and prevents re-purposing. It  puts a bar on random collection of users digital footprints and  sharing (selling) them for other  not clearly declared  purposes.  This restriction to minimisation and specific use in turn will, one  may hope, lead to more focus on the core selling point, i.e.  pedagogic application of analytics.   3.2 Japan  Japan has chosen a bottom-up approach to application of  educational data for LA, with a number of ministries and agencies  launching projects that encourage industry to develop solutions.  There is still no public debate on privacy issues related to  educational big data, according to the president of the Japanese  Society for Learning Analytics (jasla.jp) Professor Yasuhisa  Tamura (personal communication, September 2016). However,  the various actors involved are monitoring international  development in order to understand the importance of current  trends, and there is a desire to  'import' guidelines and use cases  for learning analytics. MIC, the Ministry of Internal Affairs and  Communication, is leading a Smart School pilot project for  development of ICT in K-12 school. For 2017, 250 million yen is  allocated to develop a LA support system that records learning  histories and the results of learning lessons, provides  visualizations, improves the quality of teaching and student  guidance and class and school management.  Professor Hiroaki Ogata, Director of Learning Analytics Center at  Kyushu University, reports that it has been easier to introduce LA  in higher education than in K-12 in Japan. His university is  currently practicing LA at university level, and it is the  government's position that results from some universities will  apply to K-12 in the future (Ogata, personal communication,  October 2016).    One factor that will have an influence on how the landscape for  LA in Japan develops is that the different ministries take different  positions on how LA data could be used for commercial  development. The MEXT (Ministry of Education, Culture, Sports,  Science and Technology) is seen as rather conservative, being  reluctant to disclose educational data, even if it has been  anonymized. MIC and METI (Ministry of Economy, Trade and  Industry) are eager to give access to LA data for business use by  third parties. Professor Tamura sees that the different positions  make it difficult to reach national consensus on how to handle LA  data in Japan (Tamura, personal communication, September  2016).   3.3 Republic of Korea  In contrast to Japan, Korea will apply a top-down approach, at  least for K-12 education. KERIS (Korea Education and Research  Information Service) is the agency that is leading work on ICT in  education. The 2014 KERIS report on  Prospects for the  Application of Learning Analytics (available in Korean only)  [26] is the only official report on LA to date (Oct 2016). This  report does not discuss privacy and data protection issues as a   concern for development. However, KERIS has been active in  developing the new ISO/IEC framework standard on learning  analytics interoperability, where privacy policies play an  important part [25]. KERIS organised a LASI-ASIA event in  September 2016; and in panel discussions it became clear that  KERIS has very ambitious ideas of rolling out LA in schools as  soon as possible, and that technical development projects have  been established to support this initiative. However, it is not clear  to an international observer how policies for privacy and data  sharing will be handled. Vendors and tools developers have had  meetings with the government to discuss access to educational  data (Kya Ha Lee, personal communication, Sept 2015). As the  CEO of a small software company explained it to the authors, the  Ministry of Education is the most conservative, and the avoidance  of errors is a cornerstone of Korean culture. Therefore, vendors  try to be cautious and present boilerplate conditions for users to  accept by ticking a box  when signing up to services in order to be  in line with Korean privacy legislation (the Korean Government  runs a rich advisory service at www.privacy.go.kr).   3.4 China  Development of the Chinese educational system as a whole is  traditionally top-down, driven by national campaigns. However,  there is considerable room for experimentation and the testing of  new trends, including LA, providing it does not distract too much  from supporting national curricula. The establishment of LACE  China as a community exchange vehicle at Beijing Normal  University's Centre for Big Data on Technology-Mediated  Education in September 2016 has given insights into Chinese  research and discourse on LA. So far, it is these authors'  observation that Chinese researchers are now prioritising to find a  research focus for LA and to define what parts of the educational  system might benefit from LA. Issues of privacy and data  protection are recognised as important, but from a Chinese  perspective, other issues may be more pressing to discuss in order  to leverage the data currently available for analysis.   China does not have a data protection act or a data protection  regime. With growing awareness of the dangers of unprecedented  and often illegitimate online access to personal information the  need to speed up the legislative process is stated in the public  debate [36]. It is the examples of excessive collection of personal  data from online shopping, unauthorised disclosure of personal  information by governmental and commercial institutions, illegal  trade in personal information, etc. [4] that drive the expressed  needs for a Chinese personal information protection act. How data  sharing for LA will be conceptualised in this context remains to  be seen. Ongoing software development projects may give a hint,  as the LA dashboard application currently under development by  a substantial team at the Beijing Advanced Innovation Centre for  Future Education at Beijing Normal University.   This project will provide an integrated app and desktop  application for viewing data about students and teachers, which is  to be made available to both teachers and educational managers at  various levels. The authors asked the team what the teachers  thought about the application, and we were told that they did not  like it, because it made their work more open to management  control, but that this was not seen as a problem. This is only one  example, but it does suggest that the Chinese educational  authorities may feel able to override the concerns of interest  groups about the use of data, in order to promote the common  good as they conceive it. Interestingly this position is not far  removed from that taken by the Open Universitys policy,  discussed above.     4. BENEFICIARIES OF LEARNING  ANALYTICS  As indicated in the Korean case, the discourse on LA and privacy  will be coloured by stakeholder position, and by the identification  of those who are seen as the main beneficiaries of data-driven  analysis. LACE has suggested a classification [27] that looks upon  institutional administrators in relation to activities such as  marketing and recruitment, or efficiency and effectiveness  measures; individual learners to facilitate a greater understanding  of their progress and study behaviours; teachers and support staff  to inform interventions with individuals and groups; and academic  staff who might wish to adapt existing teaching materials or  develop new curricula.   It is not surprising that it is the institutional perspective that is  dominant in the cases we have reported. Institutions collect  information and ask themselves how they could make better use  of the data to promote their goals. LA is still in its infancy, and  learners, teachers and academic staff need to see tools and  solutions if they are to buy into the promises of analytics. Privacy  issues are used at a system or a national level to ward off change  (Japan, Korea); however, when individual actions are taken on the  results of data analysis (EU), questions about privacy and data  protection arise as a natural consequence. A general debate on big  data and data protection, partly spurred by introduction of new  legislation (EU) would add weight and give direction to this  discourse. Hoel and Chen [18] argue that in a European context  the GDPR with the new principles of Data Protection by Design  and Data Protection by Default will bring the focus of learning  analytics back to the learner and will serve as a lever for bringing  pedagogy into the discourse on LA.   Figure 2 indicates how the authors perceive the orientation  towards beneficiaries of the privacy frameworks and countries  that we have analysed, placing them on a continuum between  focus on the individual and focus on the organisation.     Figure 2. Orientation towards individual and organizational   beneficiaries of privacy frameworks and countries  Studies of cultures and organisations [14] are out of scope for this  paper; the classification of the countries we have studied is only a  first reflection on the scant discourse we have been able to trace  where concepts of privacy meet pedagogical and country-specific  cultures in an effort to define requirements for LA systems and  practices. Following extensive community exchange about LA  worldwide, the authors of this paper conclude that LA largely  remains on the drawing board, rather than on the commercial  shelves ready to be applied. In order to move towards design we  ask (also with the help of the model in Figure 2) how privacy  frameworks and legal constraints on access to data and data  sharing are influencing development of LA in countries such as  those that we have studied.   5. DISCUSSION  This paper is premised on the hypothesis that legal constraints,  defined in privacy frameworks and data protection acts, will have  an impact on LA tools and practices. We have argued that even if  the legal situation in this domain varies substantially, for example  between European countries and China, it is fruitful to solicit  specific design requirements (as reported in Section 2.2) through  systematic analysis of technical and organisational implications of  legal provisions in the most recent and advanced of the available  frameworks (i.e., the GDPR). With these requirements in mind,  we have turned to selected countries to see if the observed  discourse on LA application and privacy could shed some more  light on future development, keeping in mind that there is a global  market for both LA tools and practices. It goes without saying that  this is an initial exploration, which needs to be extended as  different policies on the use of educational data are developed  around the world.   5.1 Individual vs. organisational focus  It is interesting to consider whether requirements that are  grounded in the wish to protect the individual will have an  influence on LA system design in countries with a more  collectivistic or organisational focus (ref. Figure 2). Of course, the  answer to this question depends on the aims of LA in the different  national contexts, and also on whether individual beneficiaries are  considered in LA implementation work. In all the countries in this  study, we have seen an interest in supporting the learning process  per se, giving the individual learner a more adaptive learning  environment. Even if the legal backing of a student for having full  control of his or her data is completely different between a  European country and China, it is our observation that the  principles of fairness and transparency, accuracy, notice, and  accountability  and maybe also purpose limitation and collection  limitation  resonate well with the design criteria heralded by our  Asian colleagues. Also LA system designers in cultures that give  more value to organisational interests see that without the  confidence and trust of end-users, new tools will be repurposed or  circumvented if the user only sees them as part of a surveillance  apparatus.   5.2 Schools vs. Higher Education  When we consider the different sectors of the educational system  it appears that there are differences between K-12 and Higher  Education (HE) in their approaches to privacy. Schools may be  more susceptible to the influence of legal constraints than HE  [15], because of their responsibility for the minors under their  care, and because their work takes place under a social and  political spotlight. Development in HE is more research driven,  and strong role of research ethics rules in that environment may  delay the discussion of the ethical and data privacy implications of  full scale applications of LA outside the research context. Our  case studies from Japan and Korea show that innovative solutions  could still be stalled by the tug of war between parties that want  data to be open vs. those who want data to be confined to the  educational institutions that collect them.   5.3 Data Protection by Design and by Default   Both K-12 and HE institutions need incentives to build privacy  requirements into their systems and practices. The principles of  Data Protection by Design and by Default (DPbD&D) introduced  in the European GDPR could prove to be a vehicle for change  [18]. The principles are premised on Privacy by Design (PbD), a  term first coined by the Canadian information and privacy  commissioner of Ontario, Ann Cavoukian [5]. PbD can be seen as     an engineering and strategic management approach that commits  to selectively and sustainably minimize information systems  privacy risks through technical and governance controls [32].   With GDPR, from May 2018, this engineering and management  approach has the backing of the national European laws.   The way that DPbD&D could be used to change the design  discourse can be illustrated with an example mentioned by the  Korean vendor who contributed to our case study. She explained  that she adhered to the national privacy laws by requiring users to  tick a usage agreement form signing up to her company's service.  In this she is in line with the way that legal requirements have  been met by most learning technologies companies to date,  seeking the lowest bar to pass the threshold. With the introduction  of DPbD&D, developers of LA tools will not escape with just a  simple checkbox form; by default they will have to dig deeper and  open up each subprocess for a discussion related to data  protection. In doing so, it is our assumption that the discourse on  the need for strengthening privacy will focus more on the  individual learner as beneficiary of LA (Figure 2), and the  discourse will also have a pedagogical grounding (e.g., reasoning  about learner agency, see Section 2.2), highlighting benefits of  adaptive learning, etc.) [15].    5.4  The window of opportunity for design  This study has confirmed that we are at an important point in  time, when choices are being made about where to take LA as a  data-driven educational practice. Focusing on privacy issues could  be seen as throwing a spanner into the works, by raising  impractically complex issues. A stress on privacy issues may also  be perceived as being in opposition to the educational or cultural  values of a country. But a focus on privacy and data protection  also creates the opportunity to achieve the necessary leverage in  determining what questions LA should answer. The DPbD&D  principles raise relevant questions, but it is the educational  community that needs to provide the pedagogical scenarios that  make the design of LA possible. The window of opportunity is  tight: Will South Korea wait to launch a national LA solution for  K-12 until individualised privacy solutions are found, or will the  government build one data store for all Will the Japanese  industry come up with solutions that allow third party vendors to  analyse LA data Will European countries realise that the GDPR  has given them a tool to move the LA discourse away from only  covering technical issues such as the limits to anonymisation,  encryption algorithms, and data security mechanisms, and towards  supporting learner agency, teaching of learning of 21st century  skills, and a more active learner teacher dialogue In most  countries, legal requirements are seen as an abstruse topic, but we  propose that they could be used by the educational community as  a lever to bring pedagogy into the discourse on LA.    6. RELATED WORK AND RESEARCH  GAPS  Issues related to ethics and privacy are on the top of the list of  concerns that need to be addressed according to LA researchers  and practitioners [28, 22]. In a number of papers Hoel and Chen  [20, 17, 21] have also explored what technical solutions a privacy- driven design of LA might lead to. In [19] Hoel, Cho and Chen  researched how privacy and data protection requirements would  affect all processes of the LA cycle.    Spiekermann and Cranor [33] distinguished two approaches for  building privacy-friendly systems,  privacy-by-policy  and   privacy-by-architecture . The former approach focuses on the  implementation of the notice and choice principles of fair   communications, while the latter minimises the collection of  identifiable personal data and emphasising anonymisation and  client-side data storage and processing. It is argued that  notice  and choice are needed to implement privacy-by-policy only  where privacy-by-architecture cannot be implemented  [12]. In  this paper we have just started to unpack the differences between  these two approaches by analysing the differences between the  APEC and EU privacy frameworks and seeing how the discourse  on privacy issues are being conducted in European and Asian  countries. The EU changed the direction of data protection by  introducing privacy-by-architecture principles to the GDPR.  Whether these principles have resonance in the APEC countries,  where the tradition more is to define privacy by policy would be  an interesting topic for further research, which is needed to  understand how privacy approaches, national policies and  architectures form tools and practice development in a specific  domain as LA.    The momentum caused by the EU revision of the data protection  framework and other recent developments (e.g., the Edward  Snowdon case) has created an interest in defining privacy as an  integral part of the next wave in the technology revolution and  privacy engineering as new discipline [12]. How privacy  engineering will be conceptualised and applied in different parts  of the world is an interesting and new area of research, which this  paper identifies as a research gap of particular interest to the LA  community.   7. CONCLUSIONS AND FURTHER WORK  This paper makes a contribution to knowledge about how  concerns about privacy and data protection related to educational  data can drive a discourse on privacy engineering for LA.  International privacy frameworks developed by OECD, APEC  and the EU are identified as an impetus to solicit privacy  requirements concerning all parts of a LA process cycle. The role  that these requirements will in fact play in the design of LA tools  and practices around the world depends on a host of factors  discussed in this paper. As this paper is the first, to our  knowledge, that explores how legal frameworks might influence  LA design in different countries, its main contribution is to  identify this topic as an important research area. Such research has  the potential to give the LA community a better grasp of how  privacy, pedagogy and technical development interact, and what  the implications are for interoperability. An international scope is  essential in carrying out this work, as there is a global market for  learning technologies, and what is developed in one part of the  world is rapidly taken up in another.   The exploration of national discourse in this paper is the result of  talks with colleagues as part of a community building initiative in  September-October 2016. A community of US, Australian,  European, Korean, Japanese and Chinese researchers are now  ready to start a comparative study on LA and policy, international  aspirations, achievements and constraints. This new study will  conceptualise different privacy approaches and explore how  privacy-by-policy and privacy-by-architecture may impact the  beneficiaries of learning analytics.   8. REFERENCES  [1] ADL (Advanced Distributed Learning). 2015. xAPI   specification. Produced by the Experience API Working  Group in support of the Office of the Deputy Assistant  Secretary of Defense (Readiness) Advanced Distributed  Learning Initiative. Retrieved from  https://github.com/adlnet/xAPI- Spec/blob/master/xAPI.md.  Accessed: 2016-06-01.     [2] APEC. Undated. Asia-Pacific Economic Cooperation,  Electronic Commerce Steering Group, Current Activities.  Retrieved from http://www.apec.org/groups/committee-on- trade-and-investment/electronic-commerce-steering- group.aspx. Accessed: 2016-06-01.   [3] APEC. 2005. Privacy Framework. ISBN 981-05-4471-5  [4] Baidu Encyclopedia. Undated. Personal Information   Protection Act (In Chinese). Retrieved from  http://baike.baidu.com/subview/2046064/2046064.htm.  Accessed: 2016-06-01.   [5] Cavoukian, A. 2012. Privacy by Design: From Rhetoric to  Reality. Retrieved from  https://www.ipc.on.ca/images/Resources/PbDBook-From- Rhetoric-to-Reality.pdf. Accessed: 2015-02-13.   [6] Dahl, M. Undated. Lringsanalyse. Memorandum published  at the Norwegian Centre for ICT in Education website  http://iktsenteret.no/ressurser/notat-laeringsanalyse.  Accessed: 2016-06-01.   [7] Datatilsynet. Undated. Personal Data Act. Act of 14 April  2000 No. 31 relating to the processing of personal data  (Personal Data Act) Retrieved from  https://www.datatilsynet.no/English/Regulations/Personal- Data-Act-/.  Accessed: 2016-06-01.   [8] Drachsler, H. and Greller, W. 2016. Privacy and Learning  Analytics  its a DELICATE issue.  Proceedings of LAK  '16, April 20-24, 2016, Edinburgh, UK, 1 ACM 978-1-4503- 3417-4/15/03.   [9] European Commission. 2016. How does the data protection  reform strengthen citizens rights. Retrieved from  http://ec.europa.eu/justice/data- protection/document/factsheets_2016/factsheet_dp_reform_c itizens_rights_2016_en.pdf. Accessed: 2016-06-01.   [10] European Commission. 2016. REGULATION (EU)  2016/679 OF THE EUROPEAN PARLIAMENT AND OF  THE COUNCIL of 27 April 2016 on the protection of  natural persons with regard to the processing of personal data  and on the free movement of such data, and repealing  Directive 95/46/EC (General Data Protection Regulation).   [11] European Commission. 2015. Questions and Answers - Data  protection reform. Retrieved from  http://europa.eu/rapid/press-release_MEMO-15-6385_en.pdf.  Accessed: 2016-06-01.    [12] Finneran Dennedy, M., Fox, J., and Finneran, T. 2014. The  Privacy Engineers Manifesto: Getting from Policy to Code to  QA to Value (1st ed.). Apress, Berkely, CA, USA.  ISBN:978-1-4302-6355-5.   [13] Griffiths, D., Drachsler, H., Kickmeier-Rust, M., Steiner, C.,  Hoel, T., and Greller, W. 2016. Is Privacy a Show-stopper  for Learning Analytics A Review of Current Issues and  Solutions. Learning Analytics Review 6. Published by the  LACE project.  ISSN:2057-7494. Retrieved from  http://www.laceproject.eu/learning-analytics-  review/privacy-show-stopper. Accessed: 2016-06-01.   [14] Hofstede G., Hofstede G.J., and Minkov M. 2010. Cultures  and organizations: Software of the mind, 3rd Edition. USA:  McGraw-Hill Publishers.   [15] Hoel, T. and Chen, W. 2016. Implications of the European  data protection regulations for learning analytics design.   Proceedings of CollabTech 2016 and CRIWG 2016,  Kanazawa, Japan, September 14-16, 2016.   [16] Hoel, T. and Chen, W. 2016. Data Sharing for Learning  Analytics  designing conceptual artefacts and processes to  foster interoperability. In Chen, W. et al. (Eds.) (2016).  Proceedings of the 24th International Conference on  Computers in Education. India: Asia-Pacific Society for  Computers in Education.   [17] Hoel, T. and Chen, W. 2016. Privacy-driven design of  learning analytics applications: Exploring the design space of  solutions for data sharing and interoperability. Journal of  Learning Analytics, 139158. Retrieved from  http://doi.org/10.18608/jla.2016.31.9.    [18] Hoel, T. and Chen, W. 2016. The Principle of Data  Protection by Design and Default as a lever for bringing  Pedagogy into the Discourse on Learning Analytics.  Workshop paper in Chen, W. et al. (Eds.) (2016).  Proceedings of the 24th International Conference on  Computers in Education. India: Asia-Pacific Society for  Computers in Education.   [19] Hoel, T., Chen, W., and Cho, Yong-Sang. 2016. Privacy  Requirements for Learning Analytics  from Policies to  Technical Solutions. Paper presented at Workshop on Ethics  and Privacy for Learning Analytics, Monday, April 25th  2016 at the 6th International Conference on Learning  Analytics and Knowledge (LAK 16), Edinburgh, United  Kingdom    [20] Hoel, T. and Chen, W. 2015. Privacy in Learning Analytics   Implications for System Architecture  In Watanabe, T. and  Seta, K. (Eds.) Proceedings of the 11th International  Conference on Knowledge Management. ISBN 978-4- 9908620-0-8 Presented at ICKM 15 in Osaka, Japan, 4 - 6  November 2015.   [21] Hoel, T. and Chen, W. 2014. Learning Analytics  Interoperability looking for Low-Hanging Fruits. In Liu, C.- C. et al. (Eds.) Proceedings of the 22nd International  Conference on Computers in Education. Japan Asia-Pacific  Society for Computers in Education.   [22] Hoel, T., Mason, J., and Chen, W. 2015. Data Sharing for  Learning Analytics  Questioning the Risks and Benefits. In  Ogata, H. et al. (Eds.) Proceedings of the 23rd International  Conference on Computers in Education. China: Asia-Pacific  Society for Computers in Education.   [23] IMS Global. 2015. Caliper Analytics Background. Retrieved  from http://www.imsglobal.org/activity/caliperram.  Accessed: 2016-06-01.   [24] Information Commissioner's Office. Undated. Overview of  the General Data Protection Regulation (GDPR). Retrieved  from https://ico.org.uk/for-organisations/data-protection- reform/overview-of-the-gdpr/. Accessed: 2016-06-01.   [25] ISO/IEC 20748-1. 2016. Information technology  learning,  education, and training  Learning Analytics Interoperability   Part 1: Reference model.   [26] KERIS. 2014. Prospects for the Application of Learning  Analytics (in Korean). Retrieved from  http://www.keris.or.kr/board/pb_downloadNew.jspbbs_num =18451&ix=21990. Accessed: 2016-06-01.     [27] LACE. 2015. What are Learning Analytics Retrieved from  http://www.laceproject.eu/faqs/learning-analytics. Accessed  2016-06-01.   [28] Mason, J., Chen, W., and Hoel, T. 2016. Questions as data:  illuminating the potential of learning analytics through  questioning an emergent field. Research and Practice in  Technology Enhanced Learning, 114. Retrieved from  http://doi.org/10.1186/s41039-016-0037-1. Accessed: 2016- 06-01.   [29] OECD. 2013. Privacy Framework. Retrieved from  http://www.oecd.org/sti/ieconomy/oecd_privacy_framework. pdf. Accessed: 2016-06-01.   [30] Sclater, P. and Bailey, P. 2015. Code of practice for learning  analytics. Jisc. Retrieved from  https://www.jisc.ac.uk/guides/code-of-practice-for-learning- analytics. Accessed: 2016-06-01.   [31] Slade, S. 2016. Applications of Student Data in Higher  Education: Issues and Ethical Considerations. Ithaka S+R.  Retrieved from http://sr.ithaka.org/p=283891. Accessed:  2016-06-01.   [32] Spiekermann, S. 2012. The challenges of privacy by design.  Communications of the ACM, 55(7), 383. Retrieved from  http://doi.org/10.1145/2209249.2209263. Accessed: 2016- 06-01.   [33] Spiekermann, S., and Cranor, L. F. 2009. Engineering  Privacy. Software Engineering, IEEE Transactions on, 35(1),  6782. DOI:10.1109/TSE.2008.88.   [34] Special Eurobarometer 431. Undated. Data protection.  Retrieved from  http://ec.europa.eu/public_opinion/archives/eb_special_439_ 420_en.htm#431. Accessed: 2016-06-01.   [35] The Open University. 2014. Ethical use of Student Data for  Learning Analytics Policy FAQs. Retrieved from  http://www.open.ac.uk/students/charter/sites/www.open.ac.u k.students.charter/files/files/ec ms/web-content/ethical- student-data-faq.pdf. Accessed: 2016-02-01.   [36] Wei, L. 2015. Personal Information Protection Law under  Network Environment. In Chinese. Peking University Legal  Information Network. Retrieved from  http://article.chinalawinfo.com/ArticleFullText.aspxArticleI d=89816. Accessed 2016-06-01.           "}
{"index":{"_id":"31"}}
{"datatype":"inproceedings","key":"Haythornthwaite:2017:IPP:3027385.3027389","author":"Haythornthwaite, Caroline","title":"An Information Policy Perspective on Learning Analytics","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"253--256","numpages":"4","url":"http://doi.acm.org/10.1145/3027385.3027389","doi":"10.1145/3027385.3027389","acmid":"3027389","publisher":"ACM","address":"New York, NY, USA","keywords":"educational policy, information policy, learning analytics","Abstract":"Policy for learning analytics joins a stream of initiatives aimed at understanding the expanding world of information collection, storage, processing and dissemination that is being driven by computing technologies. This paper offers a information policy perspective on learning analytics, joining work by others on ethics and privacy in the management of learning analytics data [8], but extending to consider how issues play out across the information lifecycle and in the formation of policy. Drawing on principles from information policy both informs learning analytics and brings learning analytics into the information policy domain. The resulting combination can help inform policy development for educational institutions as they implement and manage learning analytics policy and practices. The paper begins with a brief summary of the information policy perspective, then addresses learning analytics with attention to various categories of consideration for policy development.","pdf":"An Information Policy Perspective on Learning Analytics  Caroline Haythornthwaite   Syracuse University  School of Information Studies   343 Hinds Hall  chaythor@syr.edu     ABSTRACT  Policy for learning analytics joins a stream of initiatives aimed at  understanding the expanding world of information collection,  storage, processing and dissemination that is being driven by  computing technologies.  This paper offers a information policy  perspective on learning analytics, joining work by others on ethics  and privacy in the management of learning analytics data [8], but  extending to consider how issues play out across the information  lifecycle and in the formation of policy. Drawing on principles  from information policy both informs learning analytics and  brings learning analytics into the information policy domain. The  resulting combination can help inform policy development for  educational institutions as they implement and manage learning  analytics policy and practices. The paper begins with a brief  summary of the information policy perspective, then addresses  learning analytics with attention to various categories of  consideration for policy development.   CCS Concepts   Social and professional topics~Computing / technology  policy 1   Keywords  Information policy; learning analytics; educational policy   1. INFORMATION POLICY  Information policy is a relatively new area of research and  practice. In its larger domain,  information policy is concerned  with public policy relating to information use and addresses legal,  ethical and moral positions and practices pertaining to information  from creation through use to disposition. Sandra Braman [3][4]  most comprehensively defines the area, addressing the impact on  government of the transformation to an information state, and  providing this definition:   Information policy is comprised of laws, regulations, and  doctrinal positions  and other decision making and practices  with society-wide constitutive effects  involving  information creation, processing, flows, access, and use.  ([4], p. 3)                                                                        Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada.   Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4870-6/17/03$15.00  DOI: http://dx.doi.org/10.1145/3027385.3027389   Of most interest here for application to learning analytics is the  articulation of the information activities where policy adheres.  The definition and distinction among these activities results from  a synthesis of the many, many conceptualizations of information  production chains that are used implicitly and explicitly across  disciplines ([4], p.3). Braman argues that [m]arking the  boundaries of the domain with information creation, processing,  flows, access, and use provides a synthetic and succinct heuristic  that meets important evaluative criteria that are valid,  comprehensive, theoretically sound, and translatable into the  multiple languages in which the audiences of information policy  speak ([4], p. 3).   These information activities are easily translatable for systems  analysis and learning analytics into the stages of the information  lifecycle. Although activities is no doubt a better term (given  that processes of information creation, use, etc. will overlap in  everyday practice), the lifecycle concept provides a procedural  approach to understanding where policy is needed in the  management of learning analytics data and information. And,  indeed, the information lifecycle has been used by others as a  framework for examining policy from an information perspective.  Pasek [18], for example, synthesizes discussion from several  papers in the information science area to tie information policy to  the information lifecycle phases of creation, production,  distribution, access, and use.    While intuitive and useful, an information lifecycle framework as  outlined by Pasek does lack some of the nuance of the Braman  categories. For example, by leaving out the concept of flows it is  possible to miss the idea that creation of an information resource  can be achieved by selecting from an ongoing stream (flow) of  information, e.g., by creating an archive of twitter posts, or by  curating an information flow to create a resources of selected,  topic specific postings. Similarly indexing is not explicitly  addressed; for twitter this may be the hashtag, forming a crowd-  based collaborative index to a stream of postings.   Both authors short lists also seem to give little prominence to the  disposition of information and data, where policies are needed to  address both retention and deletion of records. Attention to this  stage is well known in areas such as records management, and is  rapidly emerging in internet policy relating to issues around the  right to be forgotten [7].   Since learning analytics is an area that itself includes many,  many conceptualizations of information production chains, it is  an area that can benefit from engaging with information policy  frameworks. At the same time, learning analytics may be facing  issues not yet addressed in information policy, and examination of  learning analytic activities may inform information policy  development.  Writ broadly, information policy addresses issues relating to:  intellectual property, such as copyright; privacy of personal  information, particularly as related to government collection of  personal data; security of personal privacy as well as national     security in relation to intrusions into government information  systems; and access to information, including issues of literacy,  digital divide, and the role of libraries, archives and museums.    Well-known US government policies relating to information  policy include the Digital Millenium Copyright Act (DMCA),  Family Educational Rights and Protection Act (FERPA), Health  Insurance Portability and Accountability Act (HIPPA), Freedom  of Information Act (FOIA), with similar acts found in many other  countries. Also known are government requests for information,  e.g., in the US Patriot Act (now Freedom Act), and undisclosed  domestic information gathering (e.g., by the US National Security  Agency)[14], with public opinion divided between acceptance and  rejection of such activities [19].   Organizations such as the American Library Association, and  movements such as Freedom from Surveillance [1][2] and  Freedom to Read [3] work in the information policy domain  to  effect change and education relating to government monitoring,  censorship and privacy relating to information access (e.g., non- disclosure of library or video borrower records). The ubiquity of  records and personal information on the Internet has driven efforts  for the Right To Be Forgotten (RTBF), with European initiatives  in this area leading the way [7][11][21]. Open data, open  government data, and sunlight initiatives advocate for open access  to government information to increase access to resources and to  support government transparency [20].    These areas of attention provide a framework for examining  learning analytics policy. It is possible then to frame the  discussion based on the information activities outlined by Braman  and/or the information lifecycle. We can ask, for example, how  issues of information privacy are found in the activities of  creation, processing, flows, access, and use, or the stages of  creation, production, distribution, access, use, and disposition in  relation to learning data.    While it is beyond the scope of this short paper to address how  each information policy area relates to each information activity  or stage in the information lifecycle, what follows provides some  examples of how these information policy areas draw attention to  activities that relate to learning analytics practice and policy.  Important discussion in the information policy area address:     Protection of individual records, including legal  frameworks for privacy protection such as FERPA and  HIPPA; attention to the ramifications of the failure of  anonymization techniques to keep records anonymous;  and invasions of privacy whether for law enforcement  efforts or by breaches of secruity    Monitoring and dataveillance [5], including collection  of data without participant knowledge or consent;  general surveillance of individual actions in public  and/or online [15]; information discovery through  connecting databases and/or data mining techniques  [12][22]    Access to information, including literacy relating to  reading, computing and technologies, and the digital  divide; transparency in government or other data  collection, use and disposition activities    Redress mechanisms for correction, removal or  amendment of records    Ownership of information, including copyright, patents,  trademarking, etc.    The following sections discuss these areas in relation to learning  analytics.   2. PROTECTION OF DATA RECORDS  AND INDIVIDUAL PRIVACY  Protection of learner privacy opens up a wide array of questions  about data across the information lifecycle, from what data should  be collected, to how it is stored, used, and disposed of. Policies on  record retention are undoubtedly present in educational  institutions, relating to legal requirements and social practices of  record keeping. For learning analytics, the question of what data is  collected has to be revisited as analytics can  conceivably   collect more than just in-class records, and can connect to more  than educational data. Policies in this area may be more about  what data are not to be collected than about collection of data, and  about keeping databases separate rather than connecting them. For  example, what policies should apply to connecting student aid  data to student performance data Or connecting foreign versus  domestic status to student data    The issue of connecting databasesor joining them in the  database management senseis becoming a major issue in  information policy areas because the safe harbor of  anonymization as a means of protecting individual identity is no  longer providing the protection it should. This big data problem  arises from  widespread data collection, and a general trend to  information discovery by combining data across collections.  Recent studies reveal how little data it takes to reidentify  individuals. Following the release of anonymized data on movie  recommendations made in Netflix, researchers Arvind Narayanan  and Vitaly Shmatikov were able to reidentify 68 percent of the  users by knowing only when a user rated a movie and the ratings  on two movies; with data on when and six ratings 99 percent of  users could be reidentified  (cited in [17]); similarly, researchers  at MIT found that the dates and locations of four credit card  purchases allowed identification of 90 percent of users in a dataset  of 1.1 million records [12].    These are issues that change the direction of both policy and law,  and they are appearing through analytics first. Writing in 2010,  Ohm [17] notes that    Nearly every information privacy law or regulation grants a  get-out-of-jail-free card to those who anonymize their data   the public policy debate  centers almost entirely on  squabbles over magical phrases like personally identifiable  information (PII) or personal data.  Prior to these  [reidentification/ deanonymization] studies, nobody would  have classified ZIP code, birth date, sex, or movie ratings as  PII. ([17], p. 1705)   Thus, while most of the discussion here is of using information  policy as a roadmap to learning analytics policy, each area can  inform the other.   Joining databases is also an issue in relation to ideas of freedom to  read and keeping borrower records private [10]. Learning  analytics implementations float the idea of connecting individual  records about library use to the outcomes on courses. Freedom to  read would suggest that these databases should not be connected,  and that general library borrowing and reading practices should be  kept private, including in relation to the course in question. The  ethical issue that arises is whether students borrowing from a  university library are to be granted the same protections or  considerations as individuals borrowing from a public library.      3. DISCLOSURE OF INFORMATION  GATHERING AND USE  Perhaps the biggest challenge will be transparency, in identifying  and making evident the kinds of information collected and their  use  perhaps even the ability for students to opt out of their use.  Educational institutions, many of which are government  supported, may need to set policy that addresses openness in a  way that discloses what data are collected, how they are used, etc.,  and in a way that is accessible  technically, intellectually  to the  relevant constituencies. Thus, educational settings that adopt  learning analytics may also find themselves setting policies about  how and when to convey this information to relevant  constituencies.   Approaching learning analytics from an information policy  perspective calls for greater emphasis on disclosure, and this in  turn may change the emphasis of systems development. For  instance, one of the relevant constituencies is the student  population that supplies this data. Considering information use in  the context of openness can influence learning analytics  development by establishing a higher value to providing learning  data back to students than aggregating such data for institutional  use. Where this value enters design, it can set a different policy  and practice perspective for learning analytics, one that  differentiates if from academic analytic initiatives. (For more on  values in design, see [13].)   Disclosure also has other ramifications. In particular it can place a  bureaucratic burden on educational institutions to consider the  impact of each new data stream on existing policy. This could  have a chilling effect on innovation and spontaneity in learning  analytics systems development; and it is also likely to tip the  balance to testing of data collection options to identify those with  repeatable and useful outcomes for educational practice. Policy  about research and test environments are then needed to balance  the needs of development and production, including policy to  safeguard conditions for innovation, e.g., by designating and  maintaining separate exploratory data repositories, and by  establishing short rather than long-term retention policies about  test data.  Along with disclosure about data collection and use,  communication in clear language, and opportunities for  amendment of records, freedom from surveillance draws attention  to the need to address whether and for what data individuals may  withhold consent for data collection. Moreover, while the  emphasis is normally on surveillance, i.e., top down,  organizational monitoring, not all surveillance comes from above.  As monitoring technologies have become more prevalent, they are  now found in the hands of students, and thus policies about  sousveillance [16] need to be considered. There are already issues  arising around student filming of lectures, raising questions about  intellectual property: Who owns the lecture Who owns its  distribution rights Ubiquitous recording technologies may even  be co-opted into educational practice, e.g., requiring students to  record interviews, events, etc. as part of the class discussion. This  should immediately open up institituional discussion and policy  around ownership, privacy, ethics and academic behavior.    With every case of information gathering comes the issue of  ownership. Copyright protects original works of authorship (not  ideas, systems, or methods), but within that realm transcripts of  conversations may be included [9]. While university policies may  already lay out the parameters of ownership of materials  generated in the pursuit of education, new ground may need to be  broken to understand the copyright of online conversations in an   educational context, and perhaps even the patenting process  associated with ideas generated in online forums.   4. ACCESS TO RECORDS  Information policy developments, and Freedom of Information/  Right to Information legislation, have generated an expectation  that stored data records that are not routinely disclosed can be  made available by request. These generates a need to manage  requests for student access to their records. Learning analytics  systems may need to implement means of record production in  anticipation of such requests, and indeed such systems may be the  means for production of such records. Thus, policies about  implementation must grapple early with the eventuality of a  freedom of information request.    Similarly, requests for data may come from outside the institution,  for example in compliance with the Patriot Act/Freedom Act.  Learning analytics systems may hold data on individuals that is  not in the student record. Such data may be requested, and  become part of a record that by policy or law must be shared with  agencies outside the university. Thus, policy needs to consider the  way new data streams from learning analytics can potentially end  up in student records.    5. AMENDMENT AND REMOVAL OF  RECORDS  Mistakes, data corruption, ineffective data practices, and  individual requests can all lead to the need to amend and/or  remove data records.  Where learning analytics data and systems  are involved in creating and generating these records, policies  need to address not only whether changes can be made and under  what circumstances, but also how data changes will be  communicated to record holders.   New analytic techniques may also generate new data, which may  be retroactively attached to existing records. For example, later  analyses may generate predictive statistics on a students  likelihood of success, perhaps with the benevolent intention of  identifiying ways to increase success. Where does such data  belong in the student record If these data are generated based on  records of graduated students, should such a score be entered  retroactively into student records Where amendment might  include such new data points, policy needs to address whether  such data is retroactively attached to records or are added only  from an official start date; and where such data may appear  prejudicial in future review, policy may be needed to limi the  lifespan of the data, e.g., removing data at graduation or at another  well-considered point in time or student progress.    6. CONCLUSION  Information policy has emerged as a field of inquiry over the last  20 years, responding to the increasing presence, availability, use  and misuse of data and information streams. As a field, it aims to  affect national level policies relevant to the information activities  identified in many disciplines. While most examples here have  been of US initiatives and acts, information policies are in place in  most countries, and often stem from initiatives in  intergovernmental agencies. This paper advocates for bringing the  information policy knowledge into learning analytics policy as  well as exploring how learning analytics can add to information  policy.     7. REFERENCES  [1] American Library Association (nd). Vanishing liberties: The   rise of state surveillance in the Digital Age (video 27  minutes). Retrieved October 12, 2016 from:  https://chooseprivacyweek.org/   [2] American Library Association (nd). Choose Privacy Week.  chooseprivacyweek.org/.    [3] Braman, S. (2006). Change of state: Information, policy, and  power. Cambridge, MA: MIT Press.   [4] Braman, S. (2011). Defining information policy.  Journal of  Information Policy, 1, 1-5.   [5] Clarke, R. (1988). Information technology and dataveillance.  Communications of the ACM, 31(5), 498-512.  http://www.rogerclarke.com/DV/CACM88.html.   [6] De Montjoye, Y-A, Radaelli, L., Singh, V.K. & Pentland, A.  (2015). Unique in the shopping mall: On the reidentifiability  of credit card metadata. Science, 347(6221),  536-539. DOI:  10.1126/science.1256297   [7] European Commission (nd). Factsheet on the Right to be  forgotten ruling (C-131/12). http://ec.europa.eu/justice/data- protection/files/factsheets/factsheet_data_protection_en.pdf   [8] Ferguson, R. Hoel, T., Scheffel, M., & Drachsler, H. (2016).  Special section on ethics and privacy in learning analytics.  Journal of Learning Analytics, 3(1). http://learning- analytics.info/journals/index.php/JLA/issue/view/373/showT oc   [9] Fowler, M. (Friday, January 7, 2011). Who owns an  Interview. Retrieved Oct. 14, 2016 from  http://www.rightsofwriters.com/2011/01/who-owns- interview.html.   [10] Freedom to Read Foundation (nd). Retrieved from  http://www.ftrf.org/   [11] Garcia-Murillo, M. & MacInnes, I. (Nov.22, 2014). The right  to be forgotten: Its weaknesses and alternatives. Paper  presented at the Institute for Information Policy Workshop,  Washington, DC.  http://dx.doi.org/10.2139/ssrn.2529396    [12] Hardesty, L. (January 29, 2015). Privacy challenges:  Analysis: Its surprisingly easy to identify individuals from  credit-card metadata. Retrieved Oct. 14, 2016 from  http://news.mit.edu/2015/identify-from-credit-card-metadata- 0129.    [13] Knobel, C., & Bowker, G. C. (2011). Values in design.  Communications of the ACM, 54(7), 26-28.   [14] Lepore, J. (June 24, 2013). The prism: Privacy in the age of  publicity. The New Yorker.  http://www.newyorker.com/magazine/2013/06/24/the-prism   [15] Lyon, D. (2002). Surveillance society: Monitoring everyday  life. Buckingham, England: Open University Press.    [16] Mann, S., Nolan, J., & Wellman, B. (2002). Sousveillance:  Inventing and using wearable computing devices for data  collection in surveillance environments. Surveillance &  Society, 1, 331-355.  http://ojs.library.queensu.ca/index.php/surveillance-and- society/article/view/3344/3306   [17] Ohm, P. (2010). Broken promises of privacy: Responding to  the surprising failure of anonymization. UCLA Law Review,  57, 1701-1777.    [18] Pasek, J. E. (2015). Defining information policy: Relating  issues to the information cycle. New Review of Academic  Librarianship, 21(3), 286-303, DOI:  10.1080/13614533.2015.1009126   [19] Rainie, L. (Sept. 21, 2016). The state of privacy in post- Snowden America.   http://www.pewresearch.org/fact-tank/2016/09/21/the-state- of-privacy-in-america/    [20] The Annotated 8 Principles of Open Government Data (nd).  Retrieved from https://opengovdata.org/.   [21] Toobin, J. (Sept. 29, 2014). The solace of oblivion: In  Europe, the right to be forgotten trumps the Internet. The  New Yorker.   [22] Wall St. Journal (January 30, 2015) Your credit cards say a  lot more about you than you think  Retrieved October 12,  2016 from http://blogs.wsj.com/moneybeat/2015/01/30/your- credit-cards-say-a-lot-more-about-you-than-you-think/     "}
{"index":{"_id":"32"}}
{"datatype":"inproceedings","key":"Holstein:2017:ITT:3027385.3027451","author":"Holstein, Kenneth and McLaren, Bruce M. and Aleven, Vincent","title":"Intelligent Tutors As Teachers' Aides: Exploring Teacher Needs for Real-time Analytics in Blended Classrooms","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"257--266","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027451","doi":"10.1145/3027385.3027451","acmid":"3027451","publisher":"ACM","address":"New York, NY, USA","keywords":"adoption, blended learning, classrooms, intelligent tutoring systems, pedagogical decision-making, real-time analytics, teachers","Abstract":"Intelligent tutoring systems (ITSs) are commonly designed to enhance student learning. However, they are not typically designed to meet the needs of teachers who use them in their classrooms. ITSs generate a wealth of analytics about student learning and behavior, opening a rich design space for real-time teacher support tools such as dashboards. Whereas real-time dashboards for teachers have become popular with many learning technologies, we are not aware of projects that have designed dashboards for ITSs based on a broad investigation of teachers' needs. We conducted design interviews with ten middle school math teachers to explore their needs for on-the-spot support during blended class sessions, as a first step in a user-centered design process of a real-time dashboard. Based on multi-methods analyses of this interview data, we identify several opportunities for ITSs to better support teachers' needs, noting that the analytics commonly generated by existing teacher support tools do not strongly align with the analytics teachers expect to be most useful. We highlight key tensions and tradeoffs in the design of such real-time supports for teachers, as revealed by Speed Dating possible futures with teachers. This paper has implications for our ongoing co-design of a real-time dashboard for ITSs, as well as broader implications for the design of ITSs that can effectively collaborate with teachers in classroom settings.","pdf":"Intelligent Tutors as Teachers Aides: Exploring Teacher  Needs for Real-time Analytics in Blended Classrooms   Kenneth Holstein, Bruce M. McLaren, and Vincent Aleven  Human-Computer Interaction Institute   Carnegie Mellon University  Pittsburgh, PA 15213   {kjholste, bmclaren, aleven}@cs.cmu.edu  ABSTRACT  Intelligent tutoring systems (ITSs) are commonly designed to  enhance student learning. However, they are not typically  designed to meet the needs of teachers who use them in their  classrooms. ITSs generate a wealth of analytics about student  learning and behavior, opening a rich design space for real-time  teacher support tools such as dashboards. Whereas real-time  dashboards for teachers have become popular with many learning  technologies, we are not aware of projects that have designed  dashboards for ITSs based on a broad investigation of teachers  needs. We conducted design interviews with ten middle school  math teachers to explore their needs for on-the-spot support  during blended class sessions, as a first step in a user-centered  design process of a real-time dashboard. Based on multi-methods  analyses of this interview data, we identify several opportunities  for ITSs to better support teachers needs, noting that the analytics  commonly generated by existing teacher support tools do not  strongly align with the analytics teachers expect to be most useful.  We highlight key tensions and tradeoffs in the design of such real- time supports for teachers, as revealed by Speed Dating possible  futures with teachers. This paper has implications for our ongoing  co-design of a real-time dashboard for ITSs, as well as broader  implications for the design of ITSs that can effectively collaborate  with teachers in classroom settings.   CCS Concepts   Human-centered computing ~ User centered design      Applied computing ~ Computer-assisted instruction   Keywords  Intelligent tutoring systems, pedagogical decision-making, real- time analytics, blended learning, teachers, classrooms, adoption   1. INTRODUCTION  In recent years, there has been growing interest in teaching  analytics: the use of analytics to support teacher awareness,  reflection, and decision-making in both physical and virtual  classroom settings [2, 21, 22, 24]. In particular, there has been an   increasing interest in the design and development of systems that  can support teachers on-the-spot decision-making, by presenting  them with actionable analytics in real-time (e.g., [2, 13]). Some of  this work has focused specifically on supporting teacher  monitoring and decision-making in blended learning  environments, where students may work with adaptive learning  technologies at their own pace. A key advantage of such  classroom technologies is that they free the teacher to provide  more one-on-one support to students who may benefit from it the  most. However, they also present teachers with unique challenges,  as teachers are tasked with monitoring classrooms that may be  working on a broad range of divergent educational activities  simultaneously, and prioritizing help-giving across students, in the  face of limited time [2, 27].    We are working towards the design of a real-time dashboard for  teachers in K-12 classrooms who use adaptive educational  technologies [15, 19] as part of their instruction. In particular, we  are exploring how intelligent tutoring systems (ITSs) might be  better designed to meet the needs of teachers during blended class  sessions, and how a real-time dashboard might meet some of these  needs. ITSs are a type of adaptive educational technology that  provide students with detailed, step-by-step feedback during  complex problem-solving practice, while adapting instruction  based on continuously-updated models of students current state.  These student models may include moment-by-moment estimates  of student knowledge (e.g., [38]), whether a student seems stuck  on a given skill (e.g., [19, 32]) or engaged in their current activity  (e.g., [34]), whether a student is exhibiting particular  misconceptions (e.g., [3]), and so on.   Despite several meta-reviews indicating that ITSs can  significantly enhance student learning, compared with traditional  classroom instruction and other types of educational technologies,  these systems have thus far struggled to achieve high adoption [4,  15] (though see [20, 35, 36]). A recent systematic review of the  ITS literature, conducted by Nye, assessed the amount of attention  the field has paid to various potential barriers to broader ITS  adoption. In this work, Nye considered barriers at multiple levels  relevant to technology adoption and use (student, teacher, and  school), and highlighted the relative rarity of research on  monitoring and customization tools designed to help teachers  integrate ITSs into their pedagogy. Noting that systems with  higher adoption (e.g., Cognitive Tutor, ASSISTments, and  ALEKS) tend to have some form of either monitoring or  customization tools for teachers, Nye speculated that under- consideration of teachers needs in classrooms using ITSs may be  a significant factor affecting adoption and attrition [4]. Similarly,  other authors have speculated that a key barrier to adoption may  be that ITSs have not typically been designed with a focus on  teachers needs in blended classrooms [3, 9, 10, 26].     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada Copyright is  held by the owner/author(s). Publication rights licensed to ACM.   ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027451     Over a decade ago, Yacef proposed a reframing of intelligent  tutoring systems as intelligent teaching assistants (ITAs):  intelligent systems that are designed with the dual objectives of  helping human teachers teach and helping students learn (rather  than just the latter objective, as is typical of ITSs) [7]. Others have  proposed similar directions, from the lens of optimizing student  learning: ITSs might better facilitate student learning if these  systems could both help and leverage human teachers, when  teachers are present in the classroom [4, 9, 11, 28]. While there  has been some work on real-time teacher support tools for ITSs  since the vision of ITAs was introduced (e.g., [8, 25, 29]), the  choices of analytics presented within these tools often appear to  have been influenced more by the availability of data, rather than  by an analysis of teachers needs (c.f. [24]). Recent work has  explored the use of participatory design approaches to inform the  design of real-time teacher support tools for blended learning  environments (e.g. [30]). However, while this work focused on  understanding how teachers would use an existing prototype of a  real-time dashboard to inform their pedagogy, in the present work  we aim to understand teachers needs for real-time support tools  in classrooms using ITSs prior to designing any such tools.   The structure of this paper is as follows: to better understand how  current ITSs may meet some of teachers needs, while failing to  meet others, we first present a brief case study of technology-use  practices and breakdowns among a group of middle school math  teachers who, for five years, used a widely-used ITS as a core  component of their regular classroom instruction. We share  findings from semi-structured interviews with two of these  teachers, examining the evolution of their classroom work  practices over this five-year period, as well as the reasons behind  their decision to discontinue use of the software and its associated  curriculum. As a first step in our user-centered design of a real- time teachers dashboard, we then present a series of design  exercises that we conducted with teachers in order to better  understand their needs in classrooms that use ITSs. We discuss  the implications of our findings for the design of ITSs that can  facilitate synergistic interactions between K-12 teachers and their  students, via real-time analytics. In order to explore potential  barriers to adoption of such real-time supports, and also to detect  unexpected design opportunities, we then speed date a number  of imagined futures with teachers (presented as concept sketches  and storyboards) [5, 6]. Finally, we briefly discuss the broader  implications of these findings for the design of intelligent tutoring  systems that can effectively collaborate with human teachers, and  present directions for future work.   2. CASE STUDY: A FIVE YEAR  RELATIONSHIP BETWEEN TEACHERS  AND INTELLIGENT TUTORS  To gain a better sense of the ways current ITSs may meet some of  the needs of K-12 teachers but fail to meet others, we interviewed  two mathematics teachers (teachers 8 and 9 in Table 2) from a  school in the Pittsburgh area (school E in Table 1), who had used  an ITS in their classrooms for about five years. The interviews  were semi-structured, and incorporated a version of the Love  Letter and Breakup Letter design method, which uses  personification as a tool to probe participants original reasons for  adopting a technology (and continuing to use it for an extended  period) and their reasons for eventually breaking up with that  technology [6]. Our findings from these interviews are briefly  summarized below, charting these teachers journey from  adoption, to break-up, and then to the present.   According to the interviewed teachers, teachers at this school  originally pushed to adopt this ITS and its associated curriculum  as part of a larger teacher-led effort to move away from their  existing mathematics curriculum. These teachers felt that the  existing curriculum involved too much interleaving of topics, in  which you never really get fully though a topic the first time, or  the second time, or the third time. Instead, they wanted to  move to a curriculum that [teaches] a topic once, [making sure  that] they master it, and then allows students and teachers to  move on. They found this mastery learning approach to  mathematics instruction appealing largely because they felt it  represented the sort of deeper, more focused learning that students  would be required to do in high school and beyond. As such, they  decided to adopt the ITS  which implemented a mastery learning  approach to activity sequencing  in part to provide their students  with extra scaffolding during the transition from shallower and  less independent learning (in elementary school) to deeper and  more self-regulated learning (in high school).      Table 1. Demographic information for schools   School Region Free/Reduced Price Lunch   A Suburban 17.7%   B Urban n/a   C Suburban 23.4%   D Suburban 29.3%   E Rural 30.7%      The interviewed teachers noted that the first year of using the ITS  in their classrooms was a challenging adjustment period. Despite  the support materials that came with the ITS at that time,  including a curriculum with associated materials such as  textbooks, professional development, and a reporting system that  allowed teachers to track their students progress regularly, these  teachers initially struggled to decide how best to monitor and help  their students during lab sessions in which students used the ITS.  In particular, they had trouble determining how to assess students  fairly and accurately given the self-paced nature of adaptive  learning technologies. A major constraint teachers face (in most  US K-12 schools, at least) is that they need to provide students  with letter grades and to communicate their reasons for assigning  a particular grade to both students and their parents. During this  first year, these teachers often found that it was difficult to justify  their decisions to assign students grades based on their progress  within the ITS  particularly when communicating with these  students parents. Teachers grading decisions often involved a  considerable amount of subjectivity, as it was often unclear how  to balance between grading students based on the progress  theyve made in the software (i.e. how many units of the  curriculum a student has covered), how well students have  performed within those units (as shown in the software reports as  probabilities that a student has mastered each of a number of fine- grained skills), and how much growth students have shown  individually (i.e., change in students per-skill probabilities of  mastery over time).   After the first year of use, teachers began to hold meetings to  share reflections, insights, and strategies on how to most  effectively use the ITS in their classrooms. Through these     meetings, teachers at this school collectively developed common  work practices and grading procedures to help mitigate some of  the major challenges they had encountered over the previous year.  For example, these teachers developed a uniform grading scheme  by setting goals for where students should be (in terms of the  number of units covered) at regular checkpoints throughout each  semester. The teachers decided upon these goals by pooling their  recollections of the unit most students had reached in the previous  year, by certain checkpoints (e.g., the beginning of each month).  If a student was one or more units behind the goal unit, at a  certain checkpoint, the teacher would use a control panel in the  software to manually push that student forward to the goal unit  (and the student would receive no credit for any intervening  units). This was done to keep the whole class relatively  synchronized over the course of the year, and to manage the  complexity of assigning individual letter grades when different  students have covered different amounts of material. Over time,  teachers grading schemes became more nuanced, as teachers  would annotate printed versions of the ITS-generated reports with  their own observations, collected each day while monitoring their  classrooms. They would sometimes integrate these annotations  with the ITS-generated metrics to assign grades  allowing for  partial credit to be given based on their perception of the students  effort or students growth over time, rather than just the speed at  which they reached mastery.   According to the interviewed teachers, they (and other teachers at  the school) ultimately agreed that continued use of the ITS was  not worth the cost, for three primary reasons:   1. Challenges of curriculum alignment. Late in the five-year use  period, the school district began a shift to a new mathematics  curriculum, and the teachers needed to drop the curriculum that  came with the ITS. During this time, teachers increasingly found  that it was challenging to align the schools new mathematics  curriculum with the content and instructional design of the ITS.  Yet there was no convenient way for teachers to customize the  ITSs content to meet their changing needs. One teacher suggested  that the ability to make relatively minor customizations to the  ITSs problem interfaces (e.g., editing the way math problems  were represented, and altering the input format that the ITS would  accept from students) would have helped, but only if such  customizations could be made very quickly.   2. Semi-manual grading and monitoring systems were difficult  to maintain. Although the ITS generated detailed reports about  students progress and performance within the software (e.g.  probabilities that a student had mastered finely-defined skills in  the curriculum, and the number of hints a student had requested),  teachers noted that these reports did not provide them with  guidance about how to fairly and accurately assign students letter  grades based on the data. As such, the teachers felt the need to  develop their own grading system, which necessarily balanced  efforts to be fair and accurate with teachers time constraints.  Another key limitation these teachers highlighted was that it was  not always easy to identify students who were falling behind until  it was already too late for the student to catch up with the rest of  the class. That is, the most salient elements of the reports provided  by the ITS tended to be information about the past (e.g. that a  student had been overusing the ITSs hints, or that a student had  not yet mastered finely-defined skills in the curriculum). But these  reports typically did not provide predictive analytics that could  help teachers anticipate problems and proactively intervene. One  teacher noted that they would have liked to be able to see the  likelihood that a student who had fallen behind the class would   actually be able to catch up with the other students, if given  more time. Without this information, pushing a student forward  almost always seemed like the most reasonable decision.   3. Perceived susceptibility of these systems to student misuse.  Some of the teachers perceived that ITSs are particularly  susceptible to gaming or cheating (e.g. abusing the hints that  the ITS provides, or solving a math problem via a brute force  approach). These teachers believed that, since they had often been  unable to catch these behaviors in a timely manner, some of their  students had likely wasted a large amount of learning time.  Research supports these teachers intuitions to a degree: gaming  behaviors in ITSs have consistently been shown to have a  negative impact on student learning, overall (although not all  gaming behaviors are necessarily harmful, and only a relatively  small proportion of students has a tendency to game the system)  [37]. These teachers were also skeptical that a fully automated  mechanism could prevent students from gaming. One of the  interviewed teachers suggested that alerts about such misbehavior,  which are easily hidden in large computer labs, should be sent to  the teacher right away.   Reasons 1 and 2 correspond closely with two of the issues that  Nye highlighted as relatively under-considered in the ITS  literature  namely the lack of sufficient customization and  monitoring capabilities [4]. Each of these cases can be viewed as  an instance of the teacher adapting to the technology, rather than  the other way around [3, 26]. The length and difficulty of  teachers adjustment to the use of ITSs in their instruction may  also highlight a need for enhanced early support, in the form of  improved teacher training tools and peer support systems that  facilitate faster sharing of strategies and observations between  teachers (as teachers eventually felt the need to band together, but  did so only after significant struggle). Teachers practice of  pushing students forward when they do not achieve mastery  quickly enough represents an interesting case, as recent research  suggests that such teacher overrides of ITSs mastery learning  algorithms can be quite harmful to student learning over the  course of a school year [31]. This points both to a need for caution  in designing such customization and control options for teachers,  as well as a need to better understand the constraints and beliefs  that might lead teachers to make such decisions. Although  teachers were aware that the practice of pushing students forward  before they had mastered the skills in a given unit was counter to  the idea behind mastery learning, they continued to do so in order  to keep the class relatively synchronized and manage their own  orchestration load.   The interviewed teachers also noted that, since discontinuing use  of the ITS, they had not adopted any other learning technologies  in their classrooms. They emphasized that they had used the  system for many years because they believed the personalized,  detailed, and immediate feedback it provided to students was very  valuable for their learning. In fact, they strongly preferred using  ITSs to other educational technologies they had tried over the  years, for this reason. The primary obstacles to teachers  continued use of these systems did not lie in the perceived  effectiveness of ITSs, but rather in the difficulties that their use in  the classroom presented for teachers.   3. INVESTIGATING TEACHERS WANTS  AND NEEDS  3.1 Methods  To gain a better sense of teachers needs in blended classrooms  using ITSs, we conducted four types of design interviews     (generative card sorting exercises, semi-structured interviews,  directed storytelling, and Speed Dating sessions), with a total of  10 middle school teachers, from five schools in Pittsburgh and  surrounding areas. All of the participating teachers had previously  used at least one adaptive educational technology in their  classrooms, and nine out of ten teachers had previously used an  ITS at least once (though only teachers from schools D and E had  used ITSs as a regular component of their teaching).     Table 2. Demographic information for teachers   Teacher Gender  Teaching   experience  (years)   School   1 Male > 20 A   2 Female 2 B   3 Male > 20 C   4 Female > 10 C   5 Male 2 E   6 Male 25 E   7 Female 11 D   8 Male 16 D   9 Male 15 D   10 Female 19 D        3.2 Superpowers as a probe to investigate  teachers major perceived challenges  We wished to start by investigating teachers needs for real-time  support during blended class sessions, in a very broad sense. To  this end, we adopted a card generation and sorting approach [6,  14]. In separate sessions, we met with five teachers from two  schools (teachers 4, 7, 8, 9, and 10, from schools C and D) and  asked them, If you could have any superpowers you wanted, to  help you do your job, what would they be. We asked the  question this way to encourage teachers to talk freely about their  needs, and breakdowns in their current practices, without feeling  constrained to those for which they believed a technological  solution was possible. Although we initially asked this question in  a very broad sense, we gradually narrowed the questions focus to  superpowers that teachers would find useful specifically during  computer lab sessions in which students are interacting with an  ITS or other adaptive educational technology.   In each interview, we asked middle school teachers to write their  desired superpowers on index cards immediately upon generating  them (to reduce the chance that they would lose track of an idea as  the conversation progressed). In addition to identifying design  opportunities within the sets of cards teachers generated, we  wished to get a better sense of teachers priorities among  superpowers (and by proxy, the relative severity of the daily  challenges behind these superpower requests). To this end, once  a teacher had finished generating ideas for superpowers, they were  asked to sort them by subjective priority, from highest to lowest  [6, 14]. Throughout the card sorting process, teachers were   encouraged to generate new cards, if this process inspired new  ideas. Then, following this initial sorting, each teacher was  presented with cards generated by all previous teachers who had  participated, and was given the option to include any of these in  their hierarchy. If a teacher felt that one of these superpowers was  redundant with one of the superpowers they had generated, they  were encouraged to align these cards horizontally, to indicate a  tie. For any superpowers a teacher did not desire, the teacher was  asked to omit that card from the hierarchy. Figure 1 shows an  example of a hierarchy resulting from this iterative card  generation and sorting process. This teachers most desired  superpower was the ability to put the info directly into  [students] heads, without any need to interact with students.  Interestingly, no other teacher included this particular superpower.  One teacher defended this omission by suggesting that having  such a power would remove the joy from teaching. This teachers  second most desired superpower was omniscience, which the  teacher considered synonymous with the ability to see students  thought processes (a card generated by a previous teacher).    Figure 2 summarizes teachers aggregate preferences between  pairs of superpowers, within a pairwise comparison matrix. In this  figure, only superpowers that appeared in at least two teachers  hierarchies are shown. Overall, teachers tended to prefer seeing  students thought processes over most other superpowers.  Interestingly, most teachers ranked seeing thought process over  seeing misconceptions. Some teachers commented that if they     Figure 1. Hierarchy generated from a teachers superpower   card sort. The superpowers this teacher considered most  desirable are at the top of this hierarchy. Multiple cards   placed at the same level of the hierarchy represent ties (or  superpowers the teacher considered synonymous).        could really see and understand students step-by-step reasoning,  that would likely reveal students misconceptions and more   perhaps providing an explanation for this preference. It is also  worth noting that knowing whether students really know  something was not strongly favored by teachers overall, compared  with other superpowers, despite estimates of student knowledge  (e.g., in the form of probabilities that a student has mastered  particular skills) being one of the most central analytics presented  by common reporting systems for ITSs (e.g. [36]).   Across the hierarchies of superpowers that teachers generated,  some interesting regularities emerged. All five of the interviewed  teachers specified that they wanted the ability to:   See students thought processes.  Teachers wanted to be able to  see students thought processes  the chains of reasoning that led  them from one statement to the next  without having to ask  students to show their work (and without the need to  subsequently interpret students work and try to infer their  underlying thought processes). Some teachers explicitly  distinguished this from simply seeing estimates of students  mastery over certain skills (as was presented by adaptive learning  technologies with which they were familiar), noting that they  viewed seeing thought processes as more actionable than seeing  skill estimates. If teachers could follow students thought  processes in real-time, this would provide opportunities to re- route students at the moment when they took a wrong turn.  Know which students are truly stuck.  Teachers noted that  students often raise their hands during lab sessions when they  dont actually need help (and may simply be trying to avoid doing  their work). At the same time, teachers believed that many  students who actually need help almost never raise their hands.  Being able to see which students actually need the teachers help,  at any given moment, would enable the teacher to better prioritize  help across students, and fight the biggest fires first.  Know which students are almost there, and just need a  nudge to reach mastery.  Teachers noted that one of the most   fulfilling parts of their jobs is seeing students to the finish line:  working with students who are currently on the verge of  understanding a new concept, and helping them reach that  understanding more quickly. One teacher was initially conflicted  over whether to include this superpower in his hierarchy, noting  that these students would likely reach mastery even without the  teachers help, and that other students may need his help much  more. But this teacher ultimately decided to keep this superpower,  acknowledging that he wouldnt want to spend all of his time as a  teacher focusing on the students who are struggling the most.     In addition, four out of the five of the teachers interviewed wanted  to be able to:   Temporarily clone themselves (create Multiple Mes).  Teachers wanted the ability to provide one-on-one support to  many students simultaneously, rather than leaving instructional  personalization entirely up to the software. All of the interviewed  teachers highly value the instructional differentiation provided by  such software, but also acknowledge that such differentiation  makes it challenging for teachers to keep track of their students  current activities, let alone provide them with timely instruction.   Have eyes in the back of my head. Teachers noted that certain  students tend to take advantage of the challenges ITS lab sessions  pose for classroom monitoring, by misbehaving specifically when  the teachers back is turned. They shared stories of middle school  students switching to non-academic websites when they thought  the teacher was not watching, but immediately switching back to  the ITS interface when they knew they were in visual range. Thus,  much of these teachers energy during lab sessions is spent  patrolling the room and trying to make sure everyone is on-task.   Detect students misconceptions. Similar to teachers desire to  see students thought processes, their wish to immediately  diagnose students misconceptions was rooted in the actionability  of this information. While teachers viewed seeing students        Figure 2. Pairwise comparison matrix summarizing teachers preferences between superpowers. Each row shows a superpower   that appeared in at least two teachers hierarchies, and each column shows a different superpower against which it is being  compared (cells on the diagonal represent self-comparisons, and are blacked-out). Cell shade indicates the number of teachers   who ranked the row superpower higher than the column superpower, with darker shades indicating greater agreement  (minimum observed value is 0, and maximum is 4). Be able to engage students is highlighted in grey to indicate that this   superpower was not present in all five teachers card stacks (by the time a teacher first generated this card, no synonyms were  available among the cards generated by previous teachers).     thought processes as a means to correct particular student errors  and mold students thinking in desired directions, they viewed  detecting students misconceptions as an opportunity to eliminate  problematic beliefs that might hinder their future learning.  Know which students are making lots of careless errors.   Finally, teachers wanted to be able to automatically detect  whether students are actually putting in the effort required to  learn. Based on this information, they could decide whether it  would be most productive to spend their time on an instructional  intervention, or whether it would be more appropriate to first  determine how to motivate the student.   3.3 Eliciting and synthesizing teachers design  requirements for intelligent real-time supports  We conducted semi-structured interviews with 10 teachers from 5  schools (all teachers and schools identified in Tables 1 and 2), in  order to more directly investigate teachers needs for real-time  supports. In these interviews, we asked teachers to reflect on their  experiences using adaptive educational technologies such as ITSs  in the classroom, and to consider how these and similar systems  could be better designed for use in their classrooms. In particular,  we encouraged teachers to imagine that the ITS could  communicate with them in real-time, and that there were no limits  on what the system could measure about student learning.   A PhD student (the first author of this paper) and a research  assistant (an undergraduate design student from our institution)  then worked through transcriptions of approximately 5 hours of  video and audio recorded interviews, to analyze and synthesize  findings from the interview data, using two standard techniques  from Contextual Design: Interpretation Sessions and Affinity  Diagramming. Interpretation Sessions aim to help design teams  create a shared understanding of collected interview data by  extracting quotes representing key issues and insights from each  participants interview. Affinity Diagramming is a widely used  design method that aims to summarize patterns across the  interviewed population by iteratively clustering and organizing  interview quotes, based on content similarity, into a hierarchy of  increasingly abstract themes [16].    Both the extraction of quotes during Interpretation Sessions and  the hierarchical clustering process of Affinity Diagramming are   inherently subjective processes. However, bias can be reduced  during Interpretation Sessions by taking a conservative approach  to quote extraction (i.e. erring on the side of including a large  number of quotes, even when some of these may not seem  particularly interesting or important). In addition, Affinity  Diagramming involves a grounded, bottom-up approach to data  analysis: higher-level categories gradually emerge during the  clustering process via agreement across team members [18]. As  such, the process is designed to minimize the extent to which the  resulting summary is guided towards individuals preconceptions,  without entirely removing the influence of prior knowledge (e.g.,  knowledge of extracted quotes context).   We conducted several Interpretation Sessions of approximately 3  hours of transcribed interviews. The resulting 301 quotes were  then iteratively synthesized into higher-level categories, through  Affinity Diagramming sessions. Due to the large number of  extracted quotes, we opted to conduct these Affinity  Diagramming Sessions digitally, using Trello1: a web-based, drag- and-drop tool that allows users to easily organize information.   Following the Affinity Diagramming method, we first organized  quotes into level-1 categories  which were initially unnamed   based on perceived similarity in their content. We then  synthesized the quotes within each level-1 category, labeling each  category by a summary of its contents. Once we had labeled all  level-1 categories, we proceeded to group these into unnamed  level-2 categories, and then repeated the synthesis and labeling  process described above for these higher-level categories. Finally,  we repeated the grouping, synthesis, and labeling process for all  level-2 categories, producing a set of fairly abstract level-3  themes. The resulting Affinity Diagram (shown in Figure 3) had  40 level-1 categories (with between 1-2 to 20-27 quotes per  category), 10 level-2 categories, and 4 level-3 categories.    The most common high-level themes (level-3) reflected the  interviewed teachers common desires to maintain control of their  own classrooms, even when students are working with intelligent  tutoring systems, and to be an effective force in the classroom,  over and above what these technologies can offer. Both of these                                                                       1 Trello is freely available at https://trello.com/.     Figure 3. A partial view of our affinity diagram, showing teacher quotes within level-1 categories.        desires were often accompanied by comments about teachers  perceptions that technologists intend to replace their role with  educational technologies, instead of supporting their roles as  teachers. In addition, the level-3 themes revealed a desire to  receive analytics that can truly teach them something about either  their students learning or their own teaching  a theme which was  often accompanied by complaints about learning analytics  dashboards and reporting systems that either provide them with  information they are already likely to know, or provide them with  data they find useless for informing their own pedagogy. Finally,  one level-3 theme reflected teachers concerns that real-time  analytics in the classroom could, if not designed carefully, cause  more harm than good.   Within these high-level themes, the issues teachers raised broke  down into the following 10 mid-level categories:   Help me to intervene where, when, and with what Im most  needed.  Teachers wanted support from the ITS in deciding how  best to prioritize their time across multiple students who may  compete for their attention at once, when to help (or not help) a  given student, and how best to help individual students. Given  teachers limited time during lab sessions, recommendations about  how to help students might come as quick advice about effective  instructional strategies to use, or pointers to educational resources  (e.g. targeted remedial materials, available online) that may be  helpful to the student.   Im just one person: help ease my load.   Teachers emphasized  the usefulness of group activities and peer tutoring in reducing  their orchestration load in the classroom. They suggested that one  way an ITS could help them during a lab session would be to  recommend pairings or small groups of students that are likely to  be able to help one another (perhaps adaptively matched by the  ITS based on its knowledge of their current skills). This would  remove some of the responsibility of helping students from the  teachers shoulders, and also provide opportunities for the teacher  to work with a larger number of students (by meeting with groups  rather than individuals).   How can I know whether what Im doing is actually working  Teachers noted that they very rarely have opportunities for  immediate feedback on their own teaching. They often worry,  especially after seeing students test scores, that much of what  they have taught students over several weeks or months has had  no effect. Observing that ITSs can already track aspects of student  learning in real-time, they wanted ITSs to provide them with  timely feedback on the effectiveness their own help-giving during  lab sessions (e.g., one-on-one interactions with individual  students, or targeted mini-lectures provided to the whole class).  This would allow them to adjust their strategies on the fly.   Help me understand the why, not just the what.  Given  teachers active and time-constrained role during ITS lab sessions,  they wanted ITSs to provide them with summarized and directly  actionable information whenever possible. Teachers noted that  most reporting systems they had used in the past, for ITSs and  other educational technologies, tended to provide data about  students raw actions in the software. But a real-time monitoring  tool for use during ITS labs would need to provide diagnoses of  problems the teacher could act upon. For example, rather than  simply presenting teachers with the observation that a particular  student is making frequent errors in the software, it would be  highly valuable to also help the teacher diagnose whether this is  due to carelessness or genuine struggles with the material (and if   the latter, to also help the teacher diagnose what those struggles  may be).   But how do I judge whether my students are really doing well  Teachers wanted support from the ITS in determining what  constitutes good performance in an ITS (e.g., is a 70%  probability of mastery below or above average, for a particular  skill). To provide benchmarks for evaluating their students  performance, teachers suggested that ITSs might share descriptive  statistics about overall class performance between multiple  classrooms and schools.    Help me manage student motivation and engagement in my  classroom.  Teachers wanted ITSs to provide them with real-time  analytics about their students motivation and affective states in  the classroom, in addition to analytics about student learning and  performance. Notifications about student frustration while  working with the ITS, for example, could allow teachers to  intervene before the situation worsened.   What can you tell me about my students that I dont already  know  Teachers complained that reporting systems they had  used in the past tended to provide them with a lot of unsurprising  information about their students. They wanted ITSs to somehow  take into account their rich prior knowledge about their students  (e.g., [this student] is going to make slower progress, but thats  only because shes so deliberate). In doing so, ITSs could then  provide teachers mainly with notifications that are likely to  surprise them.   Make sure that the technology does not contribute to an  unhealthy classroom climate!  Teachers worried that real-time  learning analytics presented by an ITS could easily draw their  attention away from their classroom, thus defeating the purpose.  They emphasized that an effective classroom monitoring system  would need to be designed to keep teachers eyes and ears on the  classroom to the greatest extent possible.   Let me customize the technology to meet my needs.  Teachers  noted that ITSs often seem rigid and inflexible. In cases where the  instructional design of an ITS does not align with their own  pedagogy, teachers want to be able to quickly and easily adapt the  ITS to fit their needs.   Allow me to take control of the intelligent tutoring system.   Teachers also wanted to be able to go a step beyond customization  by actually taking control of the ITS on-demand. For some  teachers, this simply meant being able to pause all of their  students screens while giving a lecture in the midst of a lab  session, in order to ensure the software would not compete with  the teacher for their attention. For other teachers, this meant being  able to load a quiz problem on all students screens (in order to  quickly assess the whole class on a targeted set of skills).   4. SPEED DATING POSSIBLE FUTURES  In order to further probe and validate teachers needs for real-time  supports in ITS classrooms, we presented teachers with futuristic  classroom scenarios  inspired by teachers own ideas, as elicited  through the superpowers card sorting exercise and our semi- structured interviews. We adopted Speed Dating, a design  method for rapidly exploring new technology concepts, in which  participants are presented with a number of imagined futures in  quick succession (represented through technology usage  scenarios, in concept sketches and storyboards), while researchers  observe and explore their gut reactions to these futures [5, 6].  Speed Dating allows designers and researchers to probe the     boundaries of what participants find acceptable (which otherwise  often go undiscovered until after a prototype has been built and  tested, or even after a technology has been deployed), by  presenting them with imagined scenarios that are expected to  cross these boundaries. This method can also lead to the discovery  of unexpected design opportunities, when anticipated boundaries  are found not to exist, or when unexpected needs are revealed.  Importantly, Speed Dating can often reveal surprising results  about a user populations needs and desires, above and beyond  what methods for design requirements elicitation, like those  described above, can reveal.   We met again with five teachers from our previous interviews  (teachers 4, 7, 8, 9, and 10, from schools C and D), and presented  them with futuristic classroom scenarios  inspired by teachers  own ideas, as elicited through the superpowers card sorting  exercises and our semi-structured interviews. Teachers were  presented with eleven storyboards; each presenting a futuristic  scenario based upon a combination of teachers most commonly  requested superpowers and the 10 mid-level categories derived  from our affinity diagram. We next summarize some key findings  from these Speed Dating sessions below.   Contrary to teachers expressed desire for real-time support in  prioritizing their time across multiple students during a lab  session, teachers consistently rejected the concept of time  management systems that remind teachers not to spend too  much time with students who are doing well without the  teachers help (instead directing them towards other students who  may benefit more from assistance). One teacher reacted strongly,  stating, I dont need that to remind me its time to move on. I  know that. As an educator, you know when youve got other kids  to deal with. Although recent work suggests that, contrary to this  teachers assertion, educators intuitions about which students  need the most help (and when to help) may be limited [41], this  teachers comment reflects a key tension in the design of teacher   support tools. Teachers comments in response to this storyboard  suggested that such alert systems are undesirable both because  they threaten teachers autonomy in the classroom, and also  because they remove teachers ability to choose between two of  their primary desires during lab sessions (as indicated by their  requested superpowers): helping the students who are struggling  the most versus helping students who are almost there.   By contrast, teachers were highly receptive to technology designs  that presented them with information that could help them  prioritize their time among students, without attempting to  directly recommend certain actions. For example the panel on the  left in Figure 4 is from a storyboard showing an augmented reality  glasses based monitoring tool, through which an ITS can inform  the teacher that a given student may need their assistance (even if  the student is not necessarily aware that she/he needs help). One  teacher noted that such a tool would be particularly helpful  because there are always students who are shy and just dont  raise their hands [and some] raise their hands when they really  dont need help. A key reason teachers liked the concept of an  augmented reality glasses based dashboard was that, by displaying  analytics directly overtop their field of view, this technology  would not draw their attention away from the classroom. In fact,  teachers reactions to this and other storyboards (including a smart  watch based real-time dashboard) suggest that they may be quite  open to, and in fact strongly prefer wearable learning analytics  displays to handheld displays such as mobile phones and tablets.  Such wearable displays hold the potential to minimize teachers  cognitive load as they move throughout the classroom, while also  maintaining students privacy. In particular, teachers saw a head- mounted, augmented reality display as an opportunity to have  their own private smart classroom: one that only they could see.   5. DISCUSSION AND FUTURE WORK  In this paper we investigate how intelligent tutoring systems could  be better designed to meet teachers needs in K-12 blended        Figure 4.  Panels from two storyboards used in Speed Dating sessions.  Left: a teacher wears augmented reality glasses, which   help her identify which of her students most need her attention in class. When the intelligent tutoring system detects that a  student needs help from a human teacher, it raises the students hand for them  visible only within the teachers augmented  reality display. The ITS detects that a student on the far left of the image is browsing a non-academic webpage, and alerts the   teacher through her augmented reality glasses by displaying a Zzz over that students head.  Right: an extreme example of a  concept that we did not expect teachers would find acceptable. In this storyboard, the teacher can control a drone inside the   classroom, which collects additional data on students, and reminds students that the teacher is watching them. Each student is  equipped with various biosensors, and aspects of their physiological and affective states are displayed to the teacher in real-time.        classrooms, and we identify several design opportunities for real- time teacher support tools in ITS classrooms. Through semi- structured interviews with middle school teachers, we identified  opportunities for ITSs to better support teachers, for example, in  fairly and accurately assessing their students performance within  the software. In addition, these interviews suggested potentials for  predictive analytics to aid teachers in making challenging  decisions, such as whether to override ITSs built-in mastery  learning algorithms in order to keep slower-moving (perhaps  struggling) students in pace with the rest of the class.    Through card sorting exercises and design interviews, we  identified ITS design features and requirements for real-time  analytics that may help address some of teachers greatest  challenges in ITS classrooms. Importantly, these findings suggest  that the analytics most commonly generated by existing reporting  systems for ITSs do not align with the analytics that teachers  expect to be most useful and actionable.    By testing a number of alternative futuristic scenarios with  teachers, using the Speed Dating design method, we discovered  that K-12 teachers were highly receptive to the concept of  intelligent classroom monitoring tools that support them in  deciding how best to allocate their time and attention across  students during a lab session. However, they strongly disliked the  idea of such a system providing explicit, unsolicited  recommendations for action. We do not interpret these findings to  mean that teacher support tools should avoid making clear  recommendations for action. Indeed, given previous findings that  teachers sometimes make decisions that are suboptimal or even  harmful to students learning with ITSs (e.g., [31]), we suspect  that such directness could be important in guiding teachers  towards more effective interventions  perhaps especially in real- time usage scenarios, where teachers have scarce time to pore  over data visualizations and draw their own conclusions. Rather,  we believe these findings highlight a delicate tension between  technology designers desire to nudge teachers towards  instructionally effective patterns of behavior, on the one hand, and  the need to privilege teachers autonomy and rich prior  knowledge, on the other (paralleling recent findings in other  domains where intelligent systems are developed to support  human experts decision-making, such as healthcare [42]). It may  be, for example, that teachers would be more receptive to more  explicit and direct action recommendations if these were  presented only upon a teachers request, rather than in the form of  automated alerts. This question, and the broader question of how  teacher support tools can achieve an effective balance between  simply augmenting teachers awareness in the classroom [24, 40]  and more directly supporting their decision-making [22, 39]  remain interesting open questions for future design and  experimental work.   Our findings provide novel insights into teachers needs for real- time support tools in classrooms using intelligent tutoring  systems. In addition, to the best of our knowledge, this is the first  study that presents a broad exploration of K-12 teachers needs for  real-time learning analytics in ITS classrooms. These findings  may be useful for designers of adaptive educational technologies,  as well as designers of real-time monitoring tools such as  dashboards. We expect that many of our findings regarding  teachers needs for real-time analytics may generalize to a broader  class of educational software than ITSs.    The next phase of our project involves the use of these results to  inform the design of a real-time dashboard for K-12 teachers  using ITSs, as a first step towards designing intelligent tutoring   systems that can effectively collaborate with human teachers.  While the findings presented in this paper provide much direction  for design and highlight key teacher needs that a particular design  may meet or fail to meet, it is nonetheless a challenging design  problem to build a real-time dashboard that can both serve  teachers needs and ultimately enhance student learning in ITS  classrooms. Given our findings from Speed Dating, we plan to  explore the viability and affordances of wearable technologies  (such as smart watches or augmented reality glasses) as real-time  monitoring tools for K-12 teachers. Continuing our user-centered  design process, we will run simulated lab sessions in which  teachers experience using multiple alternative prototype designs  in a simulated classroom setting. In particular, we plan to use  these sessions to test and validate real-time analytics that are  designed to meet some of teachers requests for superpowers.   6. ACKNOWLEDGMENTS  This work was supported by NSF Award #1530726, and by the  Institute of Education Sciences, U.S. Department of Education,  through Grant R305B150008 to Carnegie Mellon University. The  opinions expressed are those of the authors and do not represent  the views of the Institute or the U.S. Department of Education. In  addition, we thank Jasper Tom, Franceska Xhakaj, Mary Beth  Kery, and all participating teachers and students.   7. REFERENCES  [1] Matuk, C., Gerard, L., Lim-Breitbart, J., & Linn, M. (2016).   Gathering requirements for teacher tools: Strategies for  empowering teachers through co-design. Journal of Science  Teacher Education, 27(1), 79-110.   [2] Martinez-Maldonado, R., Dimitriadis, Y., Kay, J., Yacef, K.,  & Edbauer, M. T. (2013). MTClassroom and MTDashboard:  supporting analysis of teacher attention in an orchestrated  multi-tabletop classroom. In Proc. CSCL2013, 119-128.   [3] Xhakaj, F., Aleven, V., & McLaren, B. M. (2016). How  Teachers Use Data to Help Students Learn: Contextual  Inquiry for the Design of a Dashboard. In European  Conference on Technology Enhanced Learning, 340-354.  Springer International Publishing.   [4] Nye, B. D. (2014). Barriers to ITS adoption: A systematic  mapping study. In ITS 2014, 583-590. Springer International  Publishing.   [5] Davidoff, S., Lee, M. K., Dey, A. K., & Zimmerman, J.  (2007). Rapidly exploring application design through Speed  Dating. In International Conference on Ubiquitous  Computing, 429-446. Springer Berlin Heidelberg.   [6] Hanington, B., & Martin, B. (2012). Universal methods of  design: 100 ways to research complex problems, develop  innovative ideas, and design effective solutions. Rockport  Publishers.   [7] Yacef, K. (2002). Intelligent teaching assistant systems.  In ICCE 2002, 136-140. IEEE.   [8] Feng, M., & Heffernan, N. T. (2006). Informing teachers live  about student learning: Reporting in the Assistment  system. Technology Instruction Cognition and Learning, 3,  1-8.   [9] Baker, R. S. (2016). Stupid tutoring systems, intelligent  humans. IJAIED, 26(2), 600-614.   [10] Tretiakov, A., Hong, H., & Patel, A. (2001). Human teacher  in intelligent tutoring system: a forgotten entity. In ICALT  2001, 227-230. IEEE     [11] Vivet, M. (1992). Uses of ITS: Which role for the teacher.  In New Directions for Intelligent Tutoring Systems, 171-180.  Springer Berlin Heidelberg.   [12] Chen, Z. (1991). From student model to teacher model:  Enriching our view of the impact of computers on society.  SIGCAS Computers and Society. 21(2-4), 46-48. ACM.   [13] Tissenbaum, M. & Matuk, C. (2016). Real-time visualization  of student activities to support classroom orchestration. In  ICLS 2016, 1120-1127.   [14] Cairns, P., & Cox, A. L. (Eds.). (2008). Research methods  for human-computer interaction (Vol. 12). New York (NY):  Cambridge University Press.   [15] Pinkwart, N. (2016). Another 25 years of AIED Challenges  and opportunities for intelligent educational technologies of  the future. IJAIED, 26(2), 771-783.         [16] Beyer, H., & Holtzblatt, K. (1997). Contextual design:  defining customer-centered systems. Elsevier.   [17] Evenson, S. (2006). Directed storytelling: Interpreting  experience for design. Design Studies: Theory and research  in graphic design, 231-240.   [18] Corbin, J., & Strauss, A. (2014). Basics of qualitative  research: Techniques and procedures for developing  grounded theory. Sage publications.   [19] Beck, J. E., & Gong, Y. (2013). Wheel-spinning: Students  who fail to master a skill. In AIED 2013, 431-440. Springer  Berlin Heidelberg.   [20] Razzaq, L., Feng, M., Nuzzo-Jones, G., Heffernan, N. T.,  Koedinger, K. R., Junker, B., Ritter, S., Knight, A., Mercado,  E., Turner, T., Upalekar, R., Walonoski, J., Macasek, M.,  Aniszczyk, C., Choksey, S., Livak, T., & Rasmussen, K.  (2005). The Assistment project: Blending assessment and  assisting. In AIED 2005, 555-562.   [21] McLaren, B.M., Scheuer, O., & Miktko, J.  (2010). Supporting collaborative learning and e-Discussions  using artificial intelligence techniques. IJAIED 20(1), 1-46.   [22] Vatrapu, R., Teplovs, C., Fujita, N., & Bull, S. (2011).  Towards visual analytics for teachers' dynamic diagnostic  pedagogical decision-making. In LAK 2011, 93-98. ACM.   [23] Prieto, L. P., Sharma, K., & Dillenbourg, P. (2015). Studying  teacher orchestration load in technology-enhanced  classrooms. In Design for Teaching and Learning in a  Networked World, 268-281. Springer International  Publishing.   [24] Rodriguez Triana, M. J., Prieto, L. P., Vozniuk, A., Shirvani  Boroujeni, M., Schwendimann, B. A., Holzer, A. C., &  Gillet, D. (in press). Monitoring, Awareness and Reflection  in Blended Technology Enhanced Learning: a Systematic  Review. IJTEL.   [25] Lesta, L., & Yacef, K. (2002). An intelligent teaching  assistant system for Logic. In ITS 2002, 421-431. Springer  Berlin Heidelberg.   [26] Dillenbourg, P., & Jermann, P. (2010). Technology for  classroom orchestration. In New Science of Learning, 525- 552. Springer New York.   [27] Prieto, L. P., Sharma, K., Dillenbourg, P., & Jess, M.  (2016). Teaching analytics: towards automatic extraction of   orchestration graphs using wearable sensors. In LAK 2016,  148-157. ACM.   [28] Segedy, J., Sulcer, B., & Biswas, G. (2010). Are ILEs ready  for the classroom Bringing teachers into the feedback loop.  In ITS 2010, 405-407. Springer Berlin Heidelberg.   [29] Miller, W. L., Baker, R. S., Labrum, M. J., Petsche, K., Liu,  Y. H., & Wagner, A. Z. (2015). Automated detection of  proactive remediation by teachers in Reasoning Mind  classrooms. In LAK 2015, 290-294. ACM.   [30] Matuk, C., Gerard, L., Lim-Breitbart, J., & Linn, M. C.  (2016). Teachers' reflections on the uses of real-time data in  their instruction. Poster session presented at AERA 2016,  Washington, DC, USA.   [31] Ritter, S., Yudelson, M., Fancsali, S. E., & Berman, S. R.  (2016, April). How Mastery Learning Works at Scale.  In L@S 2016, 71-79. ACM.   [32] Kser, T., Klingler, S., & Gross, M. (2016). When to stop:  towards universal instructional policies. In LAK 2016, 289- 298. ACM.   [33] Liu, R., Patel, R., & Koedinger, K. R. (2016). Modeling  common misconceptions in learning process data. In LAK  2016, 369-377. ACM.   [34] Baker, R.S.J.d, Gowda, S.M., Wixon, M., Kalka, J., Wagner,  A.Z., Salvi, A., Aleven, V., Kusbit, G., Ocumpaugh, J., &  Rossi, L. (2012). Towards Sensor-Free Affect Detection in  Cognitive Tutor Algebra. In EDM 2012, 126-133.   [35] Hardy, M. E. (2004). Use and evaluation of the ALEKS  interactive tutoring system. Journal of Computing Sciences  in Colleges, 19(4), 342-347.   [36] Ritter, S., Anderson, J. R., Koedinger, K. R., & Corbett, A.  (2007). Cognitive Tutor: Applied research in mathematics  education. Psychonomic Bulletin & Review, 14(2), 249-255.   [37] Baker, R.S.J.d., Corbett, A.T., Roll, I., Koedinger, K.R.,  Aleven, V., Cocea, M., Hershkovitz, A., de Carvalho,  A.M.J.B., Mitrovic, A., Mathews, M. (2013). Modeling and  Studying Gaming the System with Educational Data Mining.  In Azevedo, R., & Aleven, V. (Eds.) International Handbook  of Metacognition and Learning Technologies, 97-116. New  York, NY: Springer.   [38] Corbett, A. T., & Anderson, J. R. (1995). Knowledge tracing:  Modeling the acquisition of procedural knowledge. User  Modeling and User-Adapted Interaction, 4(4), 253-278.   [39] Borko, H., Roberts, S. A., & Shavelson, R. (2008). Teachers  decision making: From Alan J. Bishop to today. In Critical  Issues in Mathematics Education, 37-67. Springer US.   [40] Sherin, M., Jacobs, V., & Philipp, R. (Eds.).  (2011). Mathematics teacher noticing: Seeing through  teachers' eyes. Routledge.   [41] Holstein, K., McLaren, B.M., & Aleven, V. (in press).  SPACLE: Investigating learning across virtual and physical  spaces using spatial replays. In LAK 2017. ACM.   [42] Yang, Q., Zimmerman, J., Steinfeld, A., Carey, L., & Antaki,  J. F. (2016). Investigating the Heart Pump Implant Decision  Process: Opportunities for Decision Support Tools to Help.  In CHI 2016, 4477-4488. ACM.       "}
{"index":{"_id":"33"}}
{"datatype":"inproceedings","key":"Herodotou:2017:IPL:3027385.3027397","author":"Herodotou, Christothea and Rienties, Bart and Boroowa, Avinash and Zdrahal, Zdenek and Hlosta, Martin and Naydenova, Galina","title":"Implementing Predictive Learning Analytics on a Large Scale: The Teacher's Perspective","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"267--271","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027397","doi":"10.1145/3027385.3027397","acmid":"3027397","publisher":"ACM","address":"New York, NY, USA","keywords":"higher education, perceptions, predictive analytics, retention, teachers","Abstract":"In this paper, we describe a large-scale study about the use of predictive learning analytics data with 240 teachers in 10 modules at a distance learning higher education institution. The aim of the study was to illuminate teachers' uses and practices of predictive data, in particular identify how predictive data was used to support students at risk of not completing or failing a module. Data were collected from statistical analysis of 17,033 students' performance by the end of the intervention, teacher usage statistics, and five individual semi-structured interviews with teachers. Findings revealed that teachers endorse the use of predictive data to support their practice yet in diverse ways and raised the need for devising appropriate intervention strategies to support students at risk.Many introductory programming environments generate a large amount of log data, but making insights from these data accessible to instructors remains a challenge. This research demonstrates that student outcomes can be accurately predicted from student program states at various time points throughout the course, and integrates the resulting predictive models into an instructor dashboard. The effectiveness of the dashboard is evaluated by measuring how well the dashboard analytics correctly suggest that the instructor help students classified as most in need. Finally, we describe a method of matching low-performing students with high-performing peer tutors, and show that the inclusion of peer tutors not only increases the amount of help given, but the consistency of help availability as well.","pdf":"Implementing Predictive Learning Analytics on a Large  Scale: The Teacher's Perspective   Christothea Herodotou  The Open University   Walton Hall, Milton Keynes, UK  christothea.herodotou@open.a  c.uk  Zdenek Zdrahal    The Open University  Walton Hall, Milton Keynes, UK   zdenek.zdrahal@open.ac.uk  Bart Rienties  The Open University   Walton Hall, Milton Keynes, UK  bart.rienties@open.ac.uk     Martin Hlosta   The Open University  Walton Hall, Milton Keynes, UK  martin.hlosta@open.ac.uk  Avinash.Boroowa  The Open University   Walton Hall, Milton Keynes, UK  avinash.boroowa@open.ac.uk     Galina Naydenova  The Open University   Walton Hall, Milton Keynes, UK  galina.naydenova@open.ac.uk     ABSTRACT  In this paper, we describe a large-scale study about the use of  predictive learning analytics data with 240 teachers in 10 modules  at a distance learning higher education institution. The aim of the  study was to illuminate teachers' uses and practices of predictive  data, in particular identify how predictive data was used to  support students at risk of not completing or failing a module.  Data were collected from statistical analysis of 17,033 students'  performance by the end of the intervention, teacher usage  statistics, and five individual semi-structured interviews with  teachers. Findings revealed that teachers endorse the use of  predictive data to support their practice yet in diverse ways and  raised the need for devising appropriate intervention strategies to  support students at risk.     CCS Concepts   Applied computingEducationDDistance learning .   Keywords  Predictive analytics; teachers; perceptions; retention; higher  education.   1. INTRODUCTION  Large datasets, powerful analytics engines [1], and skillfully  designed visualisations of analytics results [2, 3] are becoming  increasingly available. Higher education institutions may be able  to use the experience of the past to create supportive models of  primary (and perhaps real-time) learning processes [4-6]. Several  institutions have already started to adopt predictive learning  analytics models to identify which students are going to pass a  course, and which are at risk [7-10].   By collecting longitudinal data from a range of sources the  accuracy in predicting learning performance is increasing. In one  of the first learning analytics studies, [11] found that some Virtual  Learning Environment (VLE) variables but not all (e.g., time  spent in the VLE) were useful predictors of student retention and   academic performance for 118 biology students. Similarly, in a  longitudinal comparison of 29 modules Joksimovi et al. [8]  found that time spent specifically on student-system interaction  was significantly predicting learning outcomes, while student- content interactions were negatively influencing performance. At  a distance learning higher education institution, engagement in the  VLE, in particular with assessment activities was found to  positively predict performance [9], while recent fine-grained  analyses using Bayesian modelling indicated that learning paths of  successful students are significantly different from failing  students [12, 13].   Predictive learning analytics data can be a powerful tool for  teachers. Yet, several researchers [7, 10, 14] indicate that most  institutions and teachers may not be ready to adopt predictive  learning analytics data. Indeed, recently several researchers have  reported mixed effects of providing predictive learning analytics  data and visualisations to teachers [15-17]. For example, in a  study amongst 11 modules [17] found that most teachers initially  struggled to understand the results of the various predictive  models. In addition, even when module teams were able to unpack  the underlying predictive learning analytics results, most teachers  found it difficult to identify specific actions or interventions.   To gain an evidence-based understanding of teachers' uses and  perceptions about the use of predictive analytics data and to  devise recommendations for future practice, we conducted a  large-scale study with 240 teachers facilitating 10 modules in a  distance learning higher education institution. We aimed to  answer the following research questions (RQs): a) How did  teachers use predictive analytics data b) What was the impact of  using predictive data on students' performance and retention, and  c) What factors may explain teachers' uses of predictive data   2. RETENTION AND PREDICTIVE DATA  Academic retention is a key concern of many institutions. In some  EU countries between 20% and 54% of students fail to complete  their degrees [18]. In distance education, the percentage of  students who fail to complete their degree is about 78% and in  MOOCs it is more than 93% [19]. Learning and institutional  factors are among the six main reasons significantly explaining  students' failure.   Given this picture, efforts are systematically made to design  predictive learning analytics models that will predict students at  risk in order to provide timely and appropriate interventions that  will support students and scaffold successful module or degree   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13-17, 2017, Vancouver, BC,  Canada  2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027397     completion. One such model is the OU Analyse (OUA) which  uses a range of advanced statistical and machine learning  approaches to predict students at-risk so that cost effective  interventions can be made [12, 13]. The aim is the early  identification of students who might fail to submit their next  teacher marked assessment (TMA). Predictions of students at risk  of not submitting their next TMA are constructed by machine  learning algorithms that make use of two types of data: a) static  data: demographics, such as age, gender, geographic region,  previous education, number of previous attempts on the module,  and b) fluid data: the students' interactions within the VLE hosting  a module.   The quality of predictions is measured by precision, recall and F- measure. Precision is the ratio between correctly identified at-risk  students and all students identified as at-risk, i.e. it says what is  the percentage of correctly identified students in the prediction.  Recall is the ratio between correctly identified at-risk students and  all students who are at-risk, i.e. the percentage of students who are  at-risk the prediction identified. F-measure is the harmonic mean  of precision and recall and its usually used to provide  performance of the classification model using one measure. For a  typical module, the precision of OUA predictions grows from  about 30% at the very beginning of the presentation to about 90%  at the end of the module presentation. Recall is stable at around  50%. The F-measure usually starts around 0.4 and increases to 0.7  at the end of the presentation. OUA predictions have been used in  this study to examine teachers' uses and perceptions about  predictive data.    3. METHODOLOGY  3.1 Sample  Participants were 240 teachers from 10 modules (Arts, Education,  Engineering, Law, Technology (x2), Maths, Health care (x2),  Social science). The ways in which OUA was introduced,  supported and endorsed by module teams and Faculties were  subtly different. The number of students enrolled in these modules  was 17,033 of which 4320 were supported by teachers who had  access to the OUA predictions. Seventy (N=70) teachers received  a weekly reminder (email) notifying them that the OUA  predictions were available through the dashboard (see 3.2) and  170 received them via email in excel sheets. Teachers with no  remote virtual private network (VPN) connection to access to  dashboard received predictions via excel sheets. In this early stage  of predictive analytics at the OU, teachers were volunteering to  participate, with obvious (self-selecting) biases [20, 21]. The     Figure 1. A section of the OUA dashboard showing the  students' possibility of submitting their next assignment   students' performance of teachers who had access to predictive  data was compared to students whose teachers did not have access  to this data.    3.2 Predictive data  The OUA dashboard provided teachers with information about  their students' performance (see Figure 1), including the average  performance of the whole cohort of students, the list of all  students and the predictions of their performance in the next  TMA, and information about an individual student such as  similarity between the selected student and his/her nearest  neighbours measured in terms of VLE activities and demographic  parameters.   3.3 Statistical analysis  The performance of the overall cohort of students was compared  to identify differences in formal withdrawal rates by the end of the  module and completion and pass rates by contrasting teachers  who had access to OUA or not. Also, the teachers logging in  activity in the OUA dashboard was examined to identify how  often and when teachers accessed the system.   3.4 Interviews  Five individual semi-structured interviews with teachers who  made use of OUA were conducted. Emails were sent to 10 module  team chairs participating in the study requesting for teachers  volunteers who could take part in the interviews. Volunteers were  also sought verbally during the catch up sessions about OUA that  were run by selected teachers who used the system in the past and  could provide guidance and support. This request reached a total  of 240 teachers who had access to OUA predictions. Teachers  were invited to individual semi-structured interviews expected to  last approximately 45 minutes. Interview participants were self- selected. Sample interview questions are: What are your first  impressions of using the OUA Did it help in any ways your  teaching practice Has the data from the OUA enabled you to gain  a better insight into your students What additional insight Did  you take action What action   4. RESULTS  4.1 Retention and OUA usage by teachers  The differences in students' formal withdrawal rates between the  two groups at the end of the module were compared. The chi- square analysis revealed statistically significant differences in  three of the participating modules yet with mixed outcomes. In the  case of the Technology (X (1, N = 2048) = 5.59, p <.05) and Arts  modules (X (1, N = 1783) = 6.65, p <.01), the OU Analyse  teacher groups had higher formal withdrawal rates, while in the  case of the Law module (X (1, N = 2066) = 5.61, p <.05)  significantly lower formal withdrawal rates were observed for the  OUA group.   The group comparison of students completing each of the  participating modules showed statistically significant differences  in only two modules, the Education (X (1, N = 3101) = 4.23, p  <.05) and the Law modules (X (1, N = 2060) = 5.91, p <.01). The  OUA teacher groups had a higher module completion rate  compared to the non-OUA groups in both modules. In terms of  pass rates, significant differences were observed in the Law  module only with the OUA group having a higher pass rate (X (1,  N = 2066) = 6.9, p <.01).   Seventy (N=70) teachers from Arts, Education, Engineering, Law,  Maths, Social sciences and Technology (x2), accessed the OUA  predictions through the dashboard. Reviewing the logging history     of the OUA dashboard, we identified that the majority of teachers  (N=60) logged into the system. Figure 2 shows the percentage of  teachers who accessed the dashboard per week per module. The  two modules with the highest frequencies were Technology  (2015) and Education, yet during the first weeks of the module  presentation with an average percentage around 80% (8 out of 10  teachers who had access to dashboard). Accessing the dashboard  was substantially lower in the rest of the modules including Law,  Maths, Social sciences, Technology (2016), Engineering (week 12  onwards), and Education (week 16 onwards) presenting rather  limited number of teachers accessing the dashboard. This  indicates that although teachers had access to predictive analytics  data, they did not access OUA predictions systematically. As  indicated in Figure 2, in some modules at least some of the  teachers logged in on a weekly basis, while in others there were  substantial engagement gaps over time.         Figure 2. Percentage of teachers accessing OUA dashboard  per week per module   4.2 Qualitative interviews with teachers  The semi-structured interviews with teachers were analyzed as  five individual case studies explaining uses, perceptions and  impact of predictive data on students' performance (See RQs).  Interviewee 1 (female): Interviewee 1 explained her teaching role  in a technology module as one involving marking of assignments,  offering feedback to students, helping their learning and  supporting students when they had inquiries. Her students were  described as being out of learning for a while. She was very keen  to use OUA to find out about students flagged as not being  engaged in the module. She explained about the use of OUA:  I  am very very on top of what each of the students is doing . The  value of OUA was found on the fact that it made her more aware  of what students were doing in between TMAs. As she explained:   The OUA made me more proactive in sending messages in  between the assignments .    OUA helped her to understand when students were not engaging.  However, being already aware of what students were doing she  did not find it that useful as at the beginning. Rather, OUA added  extra work and took her time from looking at other students not  signposted by the system but at the risk of dropping out. The  impact of OUA would have been greater if the system was more  sensitive in terms of informing her about what students are doing  in terms of literacy and numeracy, as these aspects are core in the  module she teaches. This could have influenced her teaching and  practice. She has the intention to try it again in the future, given  that VPN difficulties are resolved, there are changes in the type of  information ALs can input, and accounts for students that  suddenly disappear. She believes that OUA could be used to  inform accessibility studies and support students with disabilities.     Interviewee 2 (female): This participating teacher was teaching an  introductory module in engineering. Her students were mostly  new to university studies, which made the module quite difficult  for a lot of them. OUA helped her to gain insights about what was  happening between the TMAs, a period during which a lot of  students were going quiet. She gave the example of a student who  did well in the first two TMAs and then he was flagged as amber  and his activity in the VLE showed to be low. OUA highlighted  that there was a problem between TMAs. This resulted in  emailing the student, asking whether things were going well and  updating actions in the system, a feature she found quite useful.  She was already aware of students at risk from her interaction  with them. What OUA provided was valuable insights in between  the TMAs by flagging those at risk of not submitting the next  assignment. She referred those to Student Support Services. This  insight made her think of why students became less engaged  between the second and third assignment, or the third and fourth,  and motivated her to think of ways that might engage students  better, such as a semi-assignment that they would have to engage  with. Yet, it was hard for her to think what she could do to engage  students at risk beyond encouraging them. When she tried to  contact students, the majority of them did not reply to their emails  or answered her follow-up phone calls.     Two features of OUA dashboard were perceived as most useful:  the predictions which she sorted out based on the number of votes  (i.e., OUA provides data in form of votes on which of the four  engines flag a student at risk: 0 = no risk, 4 = all engines predict  student at risk). However, the system would be better if it gave her  information about what students were doing when, for example,  there is little activity (are they looking at forums the weekly  study planner); and the trends compared to the average and the  level of activity. She would be willing to use the system in the  future if changes were made to reduce workload, including the  integration of all information about students in one place (link  OUA with the OU teacher forum). Furthermore, it would be  useful if the system flagged up teachers' outstanding actions, such  as to follow up a student who did not reply to their teachers' email  or call, highlight students who dropped out so as teachers do not  check on them, add information from the automated marked  assignments as these could be good indicators of engagement, and  send email notifications to teachers flagging ups changes in  predictions and students' engagement.    Interview 3 (male): This teacher was teaching an engineering  module. He described his students as diverse in terms of previous  qualifications and age, some of whom have special needs and  struggle to balance learning and other life obligations. Overall,  most students coped well with the module from the start. He is a  teacher who likes to make use of all sorts of tools. OUA was  perceived to be a good tool to use, especially when someone gets  more experience with it and more in-depth understanding. He  found the VLE trends data particularly useful as he made use of it  to intervene when the learning activity tailed off. He characterized  the tool as a  proactive one  that complemented existing teaching  practices, such as emailing students, and participating in forums  by giving an indication of how much work students were doing.  He found it of most help after the submission of the first TMA as  students were much more settled then. Also, OUA informed him  of students who suddenly showed as not engaging, which  encouraged him to contact them. In addition, he looked at students  who were doing well and sent them emails to encourage them to  keep it up. As he explains  it is a good tool to use at both ends of  the spectrum . It would be useful for future module planning in  points where students dive in the module. Data from OUA could     inform the way he works with a given group. He intends to use it  again in the future.    Interviewee 4 (female): This teacher was teaching an  undergraduate module in mathematics in which some of the  students often needed motivational support. She perceived the use  of OUA very positively, as she said  I love it it's brilliant. It brings  together things I already do [...] it's an easy way to find  information without researching around such as in the forums and  look for students to see what they do when I have no contact with  them [...] if they do not answer emails or phones there is not much  I can do. OUA tells me whether they are engaged and gives me an  early indicator rather than waiting for the day they submit  like  Interviewee 3. She found the system particularly useful after the  submission of the first TMA. It highlighted to her two students  she was not aware of. When she gave them a call, she found out  what inhibited their study was lacking motivation and a regional  weather condition. Apart from the two students the system  correctly flagged to her, it generally confirmed her  suspicions  of  who might be at risk. She took action when students were flagged  as at risk; she called them saying that she was checking on their  progress. Most of the times, it was not easy to contact them as  they did not want to talk. She said that she trusted the predictions  from OUA, but not 100% as some of the flagged students she  knew that they were engaged but studied offline. She would use  insights from OUA to make changes to her module. She gave the  example of OUA flagging a lot of students before Christmas. An  assignment before or after Christmas might be a good change to  keep them motivated. For her, OUA is easy to use and she would  use it systematically in the future especially before the  assignments' submission.     Interviewee 5 (female): This participating teacher was teaching an  undergraduate module on arts. Her first impressions of using  OUA were that this was a useful tool, from the overall cohorts  point of view, and at the individual student level. However, OUA  does not take into account the non-online parts of the module, and  the fact that students might be engaged with printed materials, as  also indicated by Interviewee 4. As she said,  OUA flagged to me  what I should know anyway . She stressed the role of the teacher  and their interaction with students as affecting online engagement  and students at risk. For this purpose, she organised three optional  f-t-f meetings with students and two teacher groups. She enjoyed  having a look at OUA, though from time to time it showed  strange flaggings, different from her intuition. Her approach  was not to chase a student if everything else looks fine. She would  use OUA systematically if this was integrated in the teacher's  forums and across the university.   5. DISCUSSION  This large-scale study on the use of predictive learning analytics  data by 240 teachers at a distance learning higher education  institution has provided a mixed perspective on teachers uses and  practices in relation to the use of predictive data indicating a  variation in teachers degree and quality of engagement with  learning analytics and impact to students' performance and  retention. While many early studies of learning analytics [3, 22]  had high hopes of powerful visualisations of predictive analytics  to teachers, our study is the first to show in a large-scale  implementation of predictive analytics to teachers that many  teachers struggled with providing actions and support based upon  analytics data.    The comparison between groups of teachers having access to  OUA and groups of teachers with no access produced a rather   blurred picture in terms of the effectiveness of using predictive  learning analytics by teachers to support students at risk. In terms  of completion and pass rates, significant differences were  observed in only three of the modules indicating that OUA groups  had higher completion and pass rates. On some modules, an  increase in formal withdrawals was observed while in other this  was not the case. The lack of positive evidence of OUA might  also be explained by the teachers lack of adequate and systematic  engagement with OUA and actions taken to approach students at  risk. While we were unable to collect usage data amongst 170  teachers who received weekly predictions via email, most of the  70 teachers who were given access to OUA dashboards engaged  rather irregular and infrequent with predictive data. As a result,  one could question whether the lack of evidence for lowering  dropout in 9 out of 11 modules was due to the quality of OUA  predictions, or whether teachers were not sufficiently empowered  to intervene due to time-resource and organisational constraints  [15, 17].    In order to illuminate further teachers' uses and perceptions about  the use of predictive learning analytics data, five semi-structured  interviews with teachers-volunteers were conducted. Insights  revealed how the system was used and in what degree. Teachers  reported to make use of specific features of OUA only, including  the colour-coded system and the VLE activity. They were also  found to use the system rather rarely usually before the  submission of an assignment. Their use of the system was found  to relate to their understanding of how the system could be used in  their practices. In terms of the use of the risk indicators, teachers  reported that the predictions reaffirmed their suspicions about who  might be at risk. Yet, in line with van Leeuwen et al. [15]  in some  cases, the predictions provided additional insights a teacher might  not have identified without the support from the system.   The actual use of the system was found to relate to the general  teaching strategy a teacher has developed in relation to students  being at risk. For teachers who used to check on students and their  progress often, it systematised their practices and made it easier to  identify what students were doing at certain times. For others, it  influenced their practices positively by making them more  proactive in contacting students when needed and illuminated  students' activity in between the submission of TMAs and  especially for students that do not engage with e.g., forums or  online activities. It could be argued that, even though the  perceived usefulness of the system was subjectively defined,  teachers were not reluctant in using OUA; they found value in the  use of predictive learning analytics data either as complementing  their own existing teaching practices or empowering them to  becoming more proactive and engaging with students.    In line with Rienties et al. [20] a variation was observed in terms  of the types of intervention strategy developed to support students  at risk. Each teacher devised their own approach including making  a phone call, sending an email, or referring the student to support  services. Yet, it remained unclear which intervention strategy was  the most effective one in order to motivate and support students at  risk and potentially help them complete and pass their studies [9,  17, 20].    Overall, teachers expressed interest in using predictive learning  analytics data in the future to better support students at risk. In  addition, teachers were keen to use insights to improve the design  of the modules they are teaching, such as the inclusion of  additional activities when engagement between assignments tails  off. It could be advocated that the level of resistance[20]  towards the use of analytics to support teaching activities was     minimal as beliefs, feelings and future intentions to use the system  were relatively positive. Yet, it should be noted that teachers in  this study were self-selected and a large number of teachers  seemed to vote with their feet by not engaging with OUA at all, or  only close to the assessment deadlines. Previous studies have  highlighted that the time window of opportunity to effectively  support students-at-risk is relatively short [7, 10], ranging between  2-4 weeks. In other words, in addition to effective predictive  analytics tools a clear, supportive management and professional  development structure needs to be in place to empower teachers to  pro-actively help students flagged at risk [6].    Existing positive attitudes towards teaching innovations might  have had an impact on their perceptions. Also, established  teaching practices in relation to students at risk might have  aligned well with how systems like OUA could be used to support  students, thus leading to endorsing its use. To ascertain the  effectiveness of predictive data as a tool that enables teachers to  support students more proactively, we need to identify how, when  and what interventions to trigger to support students adequately.  For example, we should have a good understanding of whether we  must intervene with a student as soon as they are flagged up as at  risk of not submitting an assignment or alternatively wait for the  next set of predictions (a week after) before we take any action.    6. REFERENCES  [1] Tobarra, L., Robles-Gmez, A., Ros, S., Hernndez, R. and  Caminero, A. C. 2014. Analyzing the students behavior and  relevant topics in virtual learning communities. Computers in  Human Behavior, 31, 659-669.  [2] Ali, L., Hatala, M., Gaevi, D. and Jovanovi, J. 2012. A  qualitative evaluation of evolution of a learning analytics tool.  Computers & Education, 58, 1, 470-489.  [3] Verbert, K., Drachsler, H., Manouselis, N., Wolpers, M.,  Vuorikari, R. and Duval, E. 2011. Dataset-driven research for  improving recommender systems for learning. Proceedings of the  1st International Conference on Learning Analytics and  Knowledge (Alberta, Canada February 27 - March 01, 2011).  ACM: NY.   [4] Ferguson, R. and Buckingham Shum, S. 2012. Social learning  analytics: five approaches. Proc. 2nd International Conference on  Learning Analytics & Knowledge, (29 Apr-2 May, Vancouver,  BC). ACM Press: New York   [5] Papamitsiou, Z. and Economides, A. 2014. Learning Analytics  and Educational Data Mining in Practice: A Systematic Literature  Review of Empirical Evidence. Educational Technology &  Society, 17 (4), 4964.  [6] Mor, Y., Ferguson, R. and Wasson, B. 2015. Editorial:  Learning design, teacher inquiry into student learning and  learning analytics: A call for action. British Journal of  Educational Technology, 46 (2), 221-229.  [7] Tempelaar, D. T., Rienties, B. and Giesbers, B. 2015. In  search for the most informative data for feedback generation:  Learning Analytics in a data-rich context. Computers in Human  Behavior, 47 (2), 157-167.  [8] Joksimovi, S., Gaevi, D., Loughin, T. M., Kovanovi, V.  and Hatala, M. 2015. Learning at distance: Effects of interaction  traces on academic achievement. Computers & Education, 87(9),  204-217.   [9] Calvert, C. 2014. Developing a model and applications for  probabilities of student success: a case study of predictive   analytics. Open Learning: The Journal of Open, Distance and e- Learning, 29 (2), 160-173.  [10] Gasevic, D., Dawson, S., Rogers, T. and Gasevic, D. 2016  Learning analytics should not promote one size fits all: The  effects of instructional conditions in predicating learning success.  Internet and Higher Education, 28, 68-84.  [11] Macfadyen, L. P. and Dawson, S. 2010. Mining LMS data to  develop an early warning system for educators: A proof of  concept. Computers & Education, 54 (2), 588-599.  [12] Hlosta, M., Herrmannova, D., Zdrahal, Z. and Wolff, A.  2015. OU Analyse: analysing at-risk students at The Open  University. Learning Analytics Review, 1-16.  [13] Wolff, A., Zdrahal, Z., Herrmannova, D., Kuzilek, J. and  Hlosta, M. 2014. Developing predictive models for early detection  of at-risk students on distance learning modules,. In: Machine  Learning and Learning Analytics Workshop at The 4th  International Conference on Learning Analytics and Knowledge  (LAK14), 24-28 March 2014, Indianapolis, Indiana, USA.   [14] Greller, W. and Drachsler, H. 2012. Translating Learning  into Numbers: A Generic Framework for Learning Analytics.  Educational Technology & Society, 15 (3), 4257.   [15] van Leeuwen, A., Janssen, J., Erkens, G. and Brekelmans, M.  2014. Supporting teachers in guiding collaborating students:  Effects of learning analytics in CSCL. Computers & Education,  79 (10), 28-39.   [16] van Leeuwen, A., Janssen, J., Erkens, G. and Brekelmans, M.  2015. Teacher regulation of cognitive activities during student  collaboration: Effects of learning analytics. Computers &  Education, 90, 80-94.  [17] Rienties, B., Boroowa, A., Cross, S., Kubiak, C., Mayles, K.  and Murphy, S. 2016. Analytics4Action Evaluation Framework: a  review of evidence-based learning analytics interventions at Open  University UK. Journal of Interactive Media in Education, 1 (2),  1-12.   [18] Quinn, J. 2013. Drop-out and completion in higher education  in Europe among students from under-represented groups.  European Commission.    [19] Simpson, O. 2013. Supporting students for success in online  and distance education. Routledge, New York, NY.  [20] Rienties, B., Cross, S. and Zdrahal, Z. 2016. Implementing a  Learning Analytics Intervention and Evaluation Framework: what  works In: Kei Daniel, Ben and Butson, Russell eds. Big Data and  Learning Analytics in Higher Education: Current Theory and  Practice. Heidelberg: Springer.  [21] Torgerson, D. J. and Torgerson, C. 2008. Designing  randomised trials in health, education and the social sciences: an  introduction. Palgrave Macmillan, London.  [22] Dyckhoff, A. L., Zielke, D., Bltmann, M., Chatti, M. A. and  Schroeder, U. 2012. Design and Implementation of a Learning  Analytics Toolkit for Teachers. Journal of Educational  Technology & Society, 15 (3), 58-76.          "}
{"index":{"_id":"34"}}
{"datatype":"inproceedings","key":"Diana:2017:IDR:3027385.3027441","author":"Diana, Nicholas and Eagle, Michael and Stamper, John and Grover, Shuchi and Bienkowski, Marie and Basu, Satabdi","title":"An Instructor Dashboard for Real-time Analytics in Interactive Programming Assignments","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"272--279","numpages":"8","url":"http://doi.acm.org/10.1145/3027385.3027441","doi":"10.1145/3027385.3027441","acmid":"3027441","publisher":"ACM","address":"New York, NY, USA","keywords":"dashboards, introductory programming, learning analytics, machine learning, peer tutors","Abstract":"Many introductory programming environments generate a large amount of log data, but making insights from these data accessible to instructors remains a challenge. This research demonstrates that student outcomes can be accurately predicted from student program states at various time points throughout the course, and integrates the resulting predictive models into an instructor dashboard. The effectiveness of the dashboard is evaluated by measuring how well the dashboard analytics correctly suggest that the instructor help students classified as most in need. Finally, we describe a method of matching low-performing students with high-performing peer tutors, and show that the inclusion of peer tutors not only increases the amount of help given, but the consistency of help availability as well.","pdf":"An Instructor Dashboard for Real-Time Analytics in Interactive Programming Assignments  Nicholas Diana Carnegie Mellon University  5000 Forbes Avenue Pittsburgh, PA 15213 ndiana@cmu.edu  Michael Eagle Carnegie Mellon University  5000 Forbes Avenue Pittsburgh, PA 15213  meagle@cs.cmu.edu  John Stamper Carnegie Mellon University  5000 Forbes Avenue Pittsburgh, PA 15213 john@stamper.org  Shuchi Grover SRI International  333 Ravenswood Avenue Menlo Park, CA 94025  shuchi.grover@sri.com  Marie Bienkowski SRI International  333 Ravenswood Avenue Menlo Park, CA 94025  marie.bienkowski@sri.com  Satabdi Basu SRI International  333 Ravenswood Avenue Menlo Park, CA 94025  satabdi.basu@sri.com  ABSTRACT Many introductory programming environments generate a large amount of log data, but making insights from these data accessible to instructors remains a challenge. This research demonstrates that student outcomes can be accu- rately predicted from student program states at various time points throughout the course, and integrates the resulting predictive models into an instructor dashboard. The effec- tiveness of the dashboard is evaluated by measuring how well the dashboard analytics correctly suggest that the in- structor help students classified as most in need. Finally, we describe a method of matching low-performing students with high-performing peer tutors, and show that the inclu- sion of peer tutors not only increases the amount of help given, but the consistency of help availability as well.  CCS Concepts Applied computing  Education; Interactive learning environments;  Keywords Introductory Programming; Learning Analytics; Machine Learning; Dashboards; Peer Tutors  1. INTRODUCTION Recent advances in learning management systems and their  ability to collect and display information has been shown to aid student learning. The learning analytics embedded in dashboards can provide instructors with a wealth of infor- mation about their students, however much of the research in this area has been focused on online courses and next  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13-17, 2017, Vancouver, BC, Canada c 2017 ACM. ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027441  classdashboards rather than traditional, offline courses and real-time dashboards [16], [17], [10]. Furthermore, even in domains rich with data, such as introductory programming, there is often little to no infrastructure in place to make insights gleaned from these data available to instructors.  Unlike most other domains, computer science education is almost always (but not always) mediated by computers. A number of development environments used in computer science education collect log files of student actions. For example, systems such as BlueJ [7], CloudCoder [14] (used in more open ended programming environments), and Alice [18], which we use in this research, generate log data. De- spite this relative abundance of data, to date none of these systems integrate an instructor dashboard to take advantage of log use.  One successful use of analytics applied to student logs is providing students direct feedback in the form of hints or help messages. This forms the basis of many adaptive sys- tems such as intelligent tutors [9] that automatically create feedback [15]. Some research has explored automatically ap- plying these techniques in program representations [8], [12] as well.  Unfortunately, we cannot always count on the students who need help to ask for it. Student performance goals, in- structor attitudes, and classroom climate can result in dif- ferent patterns of help-seeking behavior [5]. For example, students concerned with social status tend to exhibit help- avoidance [13].  A potential alternative to relying on students to ask for help themselves is to train a model to predict when a student needs help and present this information to an instructor. For this to be possible, first the data need to be formatted such that at any point throughout the course the students progress can be represented. Second, it must be possible to then make accurate predictions of student outcomes from these data. Finally, these predictions can be combined with some assumptions about their use to evaluate how well they aid in choosing students who are most in need of help. The current paper uses a dataset collected in the aforementioned Alice introductory programming environment to explore the possibility of providing real-time insights derived from raw programming log data.    2. RELATED WORK The assessment used in the current study was originally  created by Werner and colleagues [18] as a way to measure computational thinking skills in middle school students. The assessment, referred to as the Fairy Assessment (because the characters used are fairies), consists of three tasks de- signed to test comprehension, design, and complex problem solving. The authors found that while scores were not cor- related with gender, age, and attendance, they did correlate with parent education, parent computer use, interest in tak- ing a computer science class, confidence with computers, and attitude toward computers [18]. The authors also found that content knowledge of the programming environment (Alice) measured at post-survey was positively correlated with scores on the Fairy Assessment, which they argue is evidence of construct validity.  Two key features of theWerner [18] dataset are the human- graded rubric scores generated for each student and the col- lection of log data. The researchers graded each of the three tasks along a series of task metrics. Those task metrics are totaled to produce the Task Total, and then the Task Totals are aggregated to give the Aggregated Total. The re- searchers also utilized a seldom used logging feature present in Alice to capture student actions at each step. The rubric scores served as the basis for their various correlational anal- yses, but analysis of the log data was largely left for future work.  We revisit the Fairy Assessment dataset to explore what insights can be gained from combining the low-level log data with the human graded rubric scores, and how those data- driven insights can be made accessible to instructors in real- time. We hypothesized that, by using a supervised machine learning algorithm, we will be able to accurately predict Task Totals and Aggregated Totals. We then integrated these predictive models into a real-time instructor dash- board. We evaluated our dashboard by simulating how a teacher might use it to identify students who need help, and measuring how accurately our model identifies those stu- dents. Finally, to increase the number of students who were able to receive help, we generated a network graph of the student data to test a method of peer tutor matching.  3. METHOD AND MATERIALS Our experiment consisted generally of three stages. First,  we converted the raw log data into a series of code-states. Next, we trained a series of predictive models to predict various student grades. Finally, we integrated these predic- tive models into an instructor dashboard, and estimated the usefulness of the dashboard using a classroom replay.  3.1 Data The data were collected by Werner and her colleagues [18]  as part of a two year project exploring the impact of game design and programming on the development of computer science skills. The students were asked to complete an as- sessment task called the Fairy Assessment, in which stu- dents are required to fix several errors in a malfunctioning program. A key feature of this dataset is the way in which it was graded. Each students program was hand-graded by two experimenters along a 24 point rubric. These grades serve as the ground truth that we can use to both train and evaluate our models. We used a subset of the original data (N=227), excluding students who worked on the assessment  more than 5 minutes longer than the 30 minutes allotted or with missing, ambiguous, or incorrect grade or log data.  Figure 1: Visual representation of the conversion process from log files to cumulative code-states.  The raw log data generated by Alice are simply a sequen- tial list of software actions in a text log file, and do not accurately capture the structure of the final program. To make the log data more amenable to analysis, we imple- mented a two-step data transformation. The first step is simply reformatting the mostly unreadable, raw list of log entries into a readable JSON format. This step was not sim- ply for aesthetics; it allowed us to visually inspect the log data and make meaning from it, which helped us identify some important characteristics. Two key characteristics are the temporal and structural relationships between log en- tries. A single user action in Alice may result in multiple log file entries, and determining where one action ends and another begins is difficult for both humans and computers. Similarly, most log entries contain information about where this entry happens in Alices internal data structure, but the exact structural relationship is often difficult to determine due to the limited detail present in the logging system.  To empirically define these temporal and structural rela- tionships more precisely, we created a small, locally-hosted Python server to continually monitor the log file of an active instance of Alice. Each time we performed some single ac- tion inside Alice, the server would detect a change in the log file, reformat the new data, and output the list of log entries associated with that single user action. That list could then be condensed into a single, meaningful entry. The result of this exploration is a principled method for transforming complex, sequential log data into a meaningful and succinct data structure that mirrors the internal data structure of Alice. We refer to the resulting data structure as a code- state.  Representing the log data as code-states also allowed us to shift our focus from the students product (i.e., the fi- nal program) to the students process (i.e., each student ac- tion). Generating the students set of actions is done using the same data transformation; we simply limit the amount of data to transform. For example, to generate the students first code-state, we only transform the log entries that cor- respond to the students first action. Code-states are cu- mulative, so to generate the second code-state we transform the log entries that correspond to the students first and sec- ond actions, and so on. We generated a code-state for each action, for each student.    3.2 Building Predictive Models The human-graded scores allowed us to train a supervised  machine learning algorithm. First, we tokenized the final code-states of each student to generate a vocabulary of 707 tokens. We then counted the number of times each token occurs in each state, and used these token counts as features for our model. We used this vocabulary created from the final states to generate a matrix of token counts for all other codes-states. This ensured that the training data (i.e., final states) and the testing data (i.e., states prior to final states) used the same set of features.  Each reported value is the average of a 10 fold Shuffle- Split Cross-Validation. For each fold, we chose a random, classroom-sized sample of students (n=30) to use in the test- ing set. The remaining 197 students were assigned to the training set. We then fit a ridge regression model on the final states of every user in the training set. Because we were interested in how the model performs over time, we generated 30 time points (1 per minute) at which to test the predictive ability of the model. At each time point, we selected only the most recent code-state for each student in the testing set, and used the fitted model to predict Task Totals and Aggregated Totals for each student. We then compared these predicted scores to the known scores to pro- duce the Root Mean Square Error (RMSE) for that time point. The python package scikit-learn was used for both cross-validation and ridge regression [11].  0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   1   1 3 5 7 9 11 13 15 17 19 21 23 25 27 29   St an  da rd  iz ed   R M  SE    Time (minutes)   Task1_Total   Task2_Total   Task3_Total   Aggregated_Total   Figure 2: RMSEs of task and aggregated total pre- dictions over time. Each model seems to stabilize at approximately 10 minutes into the course.  3.3 Instructor Dashboard  3.3.1 Classroom Replay In order to evaluate the potential useful benefits of our  predictive models in a typical lab-based classroom environ- ment, we used the corpus of previously collected log data to create a classroom replay. Each student is assumed to start the assignment at the same time, and as the course progresses, their data are streamed into the live dashboard. The instructor can then monitor what the students pre- dicted task and aggregated totals are at any point in the course, and how they change over time.  3.3.2 Dashboard Components Figure 3 highlights the important components of the dash-  board. First, the Timeline (indicated by letter A. on the fig- ure) displays how much time has past since the start of the class. Users can either drag the slider to a specific time-point or run the simulation automatically by choosing a playback speed. Figure 7 shows the dashboard progressed to 12 min- utes into the class.  Below the Timeline is the Class Summary (letter B. on the figure). The Fairy Assessment consists of three distinct tasks. This component utilizes the predictions of task met- rics to estimate the proportion of students who are currently or have already worked on each task. If the model gener- ates a prediction of greater than 50% for a particular metric, then we guess that that student is or has worked on the task that corresponds to that metric.  Below the Class Summary is a visual representation of the classroom (indicated by letter C. on the figure). Here each circle represents a student in the class. In the screenshot shown, the color of each student corresponds to their pre- dicted Aggregated Total, but the coloring can be changed to correspond to evaluation measures such as model accu- racy and true positive rate by selecting one of the buttons listed above the students. These evaluation measures, as well as others in the dashboard, are displayed in gray text to indicate that these features are only available because the software has access to the true scores for comparison. An icon displayed within a students circle indicates the student has been classified as belonging to one of three states. First, a caution sign icon indicates a student who has the lowest predicted score. Second, a clock icon indicates a student that has been idle for at least five minutes. Finally, a graduation cap icon indicates a student that has been idle for at least five minutes, but who also has a high predicted Aggregated Total (above a 93% or 28 out of 30 points). We classify these students as having finished the assessment. These icons can be seen in use in Figure 7.  Selecting a student will provide more detailed information about that student in the right hand panel (letter D. in the figure). This panel shows the predicted total score for the selected student and the actual total score for comparison. Also shown are student specific model evaluation measures and the selected students current and former code-states.  3.3.3 Evaluating the Instructor Dashboard We estimated the potential value of the instructor dash-  board by replaying classroom data and providing some as- sumptions about how the instructor might use the dash- board. First, we assume that the instructor always wishes to help a student that needs help (i.e., a student who would do poorly without help). Second, we assume that the in- structor helps each student for five minutes. This number is fairly arbitrary and merely dictates the number of students helped in the 30 minute class period. Third, once a student is helped, we exclude that student from the pool of possible students who could receive help. Finally, we assume that the students most in need of help are the students whose fi- nal grades (i.e., Aggregated Totals) are the lowest. Provided these assumptions, we can estimate how well our model can aid this instructor in identifying the students she wishes to help.    Figure 3: Various components of the instructor dashboard. A. Timeline - Classroom replay controls, B. Class Summary - General estimates of student progress, C. Classroom - Visual representation of students, D. Selected Student - Student specific predictions and model evaluation measures as well as current and former code-states  We evaluate how well our model is selecting the correct students, the Help Index (HI), at time t as:  HIt = X  |At Bt|  X (1)  Where X is the highest number of points possible, At is the lowest true score at time t, and Bt is the lowest predicted score at time t.  3.4 Peer Tutor Matching While a measure like Help Index can aid in directing in-  structors to students who need assistance, it does little to address the primary resource limitation: instructor time. Even assuming we find a perfect model, if the instructor spends 5 minutes helping each student, only 6 students can possibly be helped in a 30 minute class period. Furthermore, the instructors time does not scale with the size of the class, making this limitation especially troubling for large classes.  To increase the percentage of students who are able to receive help, we propose utilizing high performing students as peer tutors. A basic (and typical) approach to picking peer tutors consists of simply choosing a small group high performing students. Each one of these students is gener- ally assigned to a low performing student randomly, with the two students sharing nothing except for the fact that they are both students. In a fairly open-ended environment like Alice, multiple solutions can be equally correct without sharing any similar features. Therefore, randomly match- ing a student with a tutor who has a different approach to the problem at best is inefficient and at worst may result in the tutor suggesting the student start over. Having access to student log data allows us to test a peer tutor matching method that is more precise than random assignment.  Figure 4: Interaction Network for the Fairy Assess- ment task.    We used a network representation of student work to mea- sure student approach similarity. Interaction networks rep- resent student interactions with the Alice environment as a complex network; vertices represent snapshots of the en- vironment and edges represent the transitions that occur when students edit the Alice code. Eagle et al., expanded on the theoretical framework of interaction networks, explor- ing their structure and the processes that generate them [2]. Hint Factory from Stamper et al., uses an interac- tion network created from previous student data to train a Markov Decision Process (MDP) of student problem-solving approaches to serve as a domain model for automatic hint generation [15]. Hint factory has been applied across do- mains [3, 4, 6], and been shown to increase student retention in tutors [15].  The network was constructed using igraph [1], a free graph- ing library for network analysis. Each node of the graph represents a code state. Each edge represents a transition from one code-state to another. The network was popu- lated by looping over each users code states, linking them together sequentially with state transitions. If a code state identically matched another code state already represented as a node in the graph, that code state was not added, and a state transition would be drawn from the already present node to the users next code state. A visualization of this network is shown in Figure 4.  At each time point t, we select the students in the class whose predicted final score is in the bottom 25% of all stu- dents. This represents the pool of low-performing students who we operationally define as needing assistance. From this pool, we remove students who either have already been helped or are currently being helped. Then we try to as- sign the remaining students tutors. This is done by select- ing students from the class whose predicted final score is in the top 25% (though these thresholds are arbitrary and can be adjusted). These high-performing students make up our pool of potential tutors. For each unhelped low- performing student, we use the network graph to search for a node that is the most recent common ancestor to both the low-performing student and one of our high-performing potential tutors. These nodes not only represent a common- ground that both students have passed through, but also a potentially crucial decision-point in the task. In other words, from this shared point, one student goes on to do well, while the other goes on to do poorly. By matching low-performing students to tutors using these common ancestor nodes, we are 1) giving those students an opportunity to take a differ- ent path, and 2) reducing the probability that the tutor will simply ask the student to start over  saving not only time, but the value of the work the student has already done.  4. RESULTS  4.1 Student Performance Predictions We were able to accurately predict the scores for all three  tasks in the Fairy Assessment. Task 1 produced the best model (RMSE=0.384), followed by Task 2 (RMSE=0.500), and Task 3 (RMSE=0.556). Our model predicting the aggre- gated total score performed the best overall (RMSE=0.367).  To examine how the model changes over time, we gener- ated a new model for every minute of the 30 minute course. Results from this analysis can be seen in Figure 2. As ex- pected we see the models generally do worse at the beginning  Figure 5: A selected low-performing student (black dotted outline) and a suggested peer tutor (red dot- ted outline).  0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   1   1 3 5 7 9 11 13 15 17 19 21 23 25 27 29   St an  da rd  iz ed   M et  ric s   (L TS  , L PS  , H I,   R M  SE )   Time (minutes)   LTS  LPS  HI  RMSE   Figure 6: Help Index, RMSE, Lowest Predicted Score (LPS), and Lowest True Score (LTS) over time. Note that the Help Index (HI) does well over- all, but decreases slightly over time despite a de- creasing RMSE as well. This may be explained by an increasing divergence between the Lowest True Score and the Lowest Predicted Score.  of the class period when data is scarce. However, the models seem to stabilize at around the 10 minute mark. Interest- ingly, we see a second pronounced dip in the Task 3 model around 16-18 minutes into the course. This may be due to several factors (e.g., diminished student activity), but may indicate the point at which most students begin working on Task 3. It is important to remember that these tasks are cumulative, so we might expect to see these temporal mark- ers. The aggregated total model follows a similar, though less pronounced, pattern.  4.2 Predicting Help Index Figure 6 shows Help Index over time. On average, the  model is fairly accurate at choosing the student with the lowest total score (average HI = 0.875). However, the HI also trends down over time. This may be due to a number of reasons. One possible explanation is that as low-performing students are helped (and consequentially excluded from the pool of students who can receive help), the lowest true score inches upwards. The model may be better at distinguish- ing no points at all (a 0%) from a small number of points, than it is at distinguishing a small number of points from a slightly higher small number of points. Another possible    Figure 7: The instructor dashboard progressed to approximately halfway through the course. Note that the Class Summary now shows that the majority of students are working on the third task. The Classroom view shows less students in need of help, and more students that are idle or finished. Finally, the Selected Student pane now shows many more previous code-states than in Figure 3.  explanation is that, over time, the model has a more difficult time guessing the lowest scoring student as the code-states become more and more complex. Evidence of this can be seen in Figure 6 where the lowest predicted score (LPS in the figure) seems to trend upwards sooner than the lowest true score (LTS in the figure).  4.3 Peer Tutor Impact In addition to evaluating how well our model can iden-  tify low-performing students, we were also interested in in- creasing the number of low-performing students that could be helped at any given timepoint. To this end, we imple- mented a peer tutor matching system that uses a network graph of all student code-states to match low-performing students with high-performing students who share a com- mon ancestor code-state. If multiple potential tutors are found, we chose the tutor whose common ancestor is the shortest distance away from the students current code-state. The average distance from a low-performing students cur- rent code-state to the shared common ancestor code-state was 30.73 steps (SD=13.81).  Figure 8 shows the percentage of students classified as low-performing, high-performing, or tutors over time. The percentage of students identified as low-performing is very high at the beginning of the class period. This is most likely due to the scarcity of data at that time. As the students code-states become more complex (and more distinguish- able), we see a sharp drop in low-performing students and a steady increase in high-performing students. Interestingly, though the number of high-performing students continues to rise over the interval between 5 and 23 minutes, the num-  ber of those students who are selected to be tutors does not follow the same trajectory.  Figure 9 shows the percentage of low-performing students helped over time by the instructor, the peer tutors, and over- all. We see that, while peer tutors contribute to the number of students helped, the instructor contributes more. By the end of the class period, the instructor had helped 20.81% more students than the peer tutors.  While peer tutors may not be as effective as the instruc- tor at helping a large percentage of low-performing students, they may offer another benefit: availability. Our imposed 5 minutes of help assumption can be seen clearly (as ex- pected) in Figure 9s blue, Helped by Instructor line, but also is evident, to a lesser extent, in the other two lines as well. However, evidence of our 5 minute assumption is least prominent in the Helped by Tutors line, suggesting that different students are becoming available as tutors as previously selected tutors are still working with their stu- dents. The impact of this improved availability of help can be most clearly seen in the steady increase of the Total % Helped line. Without peer tutors, we would see stretches of time where only one student is helped, leaving other low- performing students waiting. Peer tutors provide a way to supplement the more efficient, less constant instructor help with a more steady stream of availability.  5. DISCUSSION The results of the classroom replay evaluation are promis-  ing. Our grade prediction model starts off fairly accurate and increases in accuracy until leveling off after about 10    Figure 8: The percentage of the class classified as low-performing students, high-performing stu- dents, or tutors. Note: tutors are a subset of high- performing students.  Figure 9: Percentage of low-performing students who have been helped (or are receiving help) over time. The dashed blue line represents the percent- age of low-performing students helped by the in- structor. The dashed orange line represents the per- centage of low-performing students helped by peer tutors. The solid black line is the total percentage of low-performing students helped.  minutes. Our predictive model was also successful at ac- curately identifying students who are predicted to have low scores. The Help Index metric shows that the dashboard can consistently identify students who are the most in need of assistance. Finally, we were able to increase the percentage of students receiving help and the consistency with which students received help by matching low-performing students with high-performing peer tutors who have similar program states.  The intervention strategy we used in this evaluation, while simple, succeeds in demonstrating that we can identify students who are most in danger of failing the assignment, and that we can identify these students relatively early. In a real classroom, instructors using the dashboard will likely have interruptions from help-seeking students, and other real-world events that could result in selecting a different student for one-on-one intervention. Additionally, an expert instructor may not need the grade prediction portion of the dashboard, however it might still prove useful for any teach- ing assistants available. In addition to the grade predictions, the dashboard also provides a high level view of the current progress of the entire classroom including which tasks stu- dents are currently working on and how many students are sitting idle. These insights would not be possible otherwise.  The current Alice environment does not support this type of real-time logging, however the work we have presented here provides a good preliminary look into the potential benefits of implementing such as system. It is important to explore interventions, such as this dashboard, thoroughly before placing them into a classroom environment, and the classroom replay presented here is one way of doing that. The results of our study provide evidence that the imple- mentation of real-time logging could have an impact in a real classroom.  6. CONCLUSIONS In this paper, we demonstrate that task and aggregated  totals from an introductory programming assessment can be predicted by training a supervised machine learning al- gorithm on human-graded rubric scores. These predictions were integrated into an instructor dashboard. Finally, the ability of this dashboard to successfully identify the students who might most benefit from help was evaluated by simu- lating an instructors interaction with the dashboard. These results suggest that, given an appropriate representation of the students program state coupled with a rich set of train- ing data, a machine learning model can accurately predict student scores. These predictions have a multitude of ap- plications. This paper explored identifying low-scoring stu- dents, but these predictions may also be useful in evaluating peer-grading or identifying students who have completed the assessment early.  7. FUTURE WORK One potential way to increase the number of students  helped is by clustering similar low-performing students to- gether. Future work will focus on identifying clusters of students who may benefit from the same intervention.  Another potential way to increase the number of students helped is to provide intelligent non-human help. We hope to utilize the accuracy of our predictive models to implement automatically generated feedback for the students.    8. ACKNOWLEDGMENTS This research was supported by the National Science Foun-  dation (NSF grant award number 1522990).  9. REFERENCES [1] G. Csardi and T. Nepusz. The igraph software package  for complex network research. InterJournal, Complex Systems, 1695(5):19, 2006.  [2] M. Eagle, D. Hicks, B. Peddycord, III, and T. Barnes. Exploring networks of problem-solving interactions. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, LAK 15, 2130, New York, NY, USA, 2015. ACM.  [3] M. Eagle, M. W. Johnson, T. Barnes, and A. K. Boyce. Exploring player behavior with visual analytics. In FDG, 2013.  [4] D. Fossati, B. Di Eugenio, S. Ohlsson, C. W. Brown, L. Chen, and D. G. Cosejo. I learn from you, you learn from me: How to make ilist learn from students. In AIED, 491498, 2009.  [5] J. M. Furner and A. Gonzalez-DeHass. How do students mastery and performance goals relate to math anxiety. Eurasia Journal of Mathematics, Science & Technology Education, 7(4):227242, 2011.  [6] A. Hicks, B. Peddycord III, and T. Barnes. Building games to learn from their players: Generating hints in a serious game. In Intelligent Tutoring Systems, 312 317. Springer, 2014.  [7] M. C. Jadud. A first look at novice compilation behaviour using bluej. Computer Science Education, 15(1):2540, 2005.  [8] W. Jin, T. Barnes, J. Stamper, M. J. Eagle, M. W. Johnson, and L. Lehmann. Program representation for automatic hint generation for a data-driven novice programming tutor. In Intelligent Tutoring Systems, 304309. Springer, 2012.  [9] K. R. Koedinger, J. R. Anderson, W. H. Hadley, M. A. Mark, et al. Intelligent tutoring goes to school in the big city. International Journal of Artificial Intelligence in Education (IJAIED), 8:3043, 1997.  [10] M. Lovett, O. Meyer, and C. Thille. Jime-the open learning initiative: Measuring the effectiveness of the oli statistics course in accelerating student learning. Journal of Interactive Media in Education, 2008(1):118, 2008.  [11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011.  [12] K. Rivers and K. R. Koedinger. Automatic generation of programming feedback: A data-driven approach. In The First Workshop on AI-supported Education for Computer Science (AIEDCS 2013), pages 5059, 2013.  [13] A. M. Ryan, L. Hicks, and C. Midgley. Social goals, academic goals, and avoiding seeking help in the classroom. The Journal of Early Adolescence, 17(2):152171, 1997.  [14] J. Spacco, D. Fossati, J. Stamper, and K. Rivers. Towards improving programming habits to create  better computer science course outcomes. In Proceedings of the 18th ACM conference on Innovation and technology in computer science education, 243248. ACM, 2013.  [15] J. Stamper, M. Eagle, T. Barnes, and M. Croy. Experimental evaluation of automatic hint generation for a logic tutor. International Journal of Artificial Intelligence in Education (IJAIED), 22(1):318, 2013.  [16] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. L. Santos. Learning analytics dashboard applications. American Behavioral Scientist, 57(10):15001509, 2013.  [17] K. Verbert, S. Govaerts, E. Duval, J. L. Santos, F. Van Assche, G. Parra, and J. Klerkx. Learning dashboards: an overview and future research opportunities. Personal and Ubiquitous Computing, 18(6):14991514, 2014.  [18] L. Werner, J. Denner, and S. Campe. The Fairy Performance Assessment : Measuring Computational Thinking in Middle School. Proceedings of the 43rd ACM Technical Symposium on Computer Science Education - SIGCSE 12, 215220, 2012.    "}
{"index":{"_id":"35"}}
{"datatype":"inproceedings","key":"Fu:2017:RLA:3027385.3027407","author":"Fu, Xinyu and Shimada, Atsushi and Ogata, Hiroaki and Taniguchi, Yuta and Suehiro, Daiki","title":"Real-time Learning Analytics for C Programming Language Courses","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"280--288","numpages":"9","url":"http://doi.acm.org/10.1145/3027385.3027407","doi":"10.1145/3027385.3027407","acmid":"3027407","publisher":"ACM","address":"New York, NY, USA","keywords":"C programming, information visualization, learning analytics, learning dashboard, programming education","Abstract":"Many universities choose the C programming language (C) as the first one they teach their students, early on in their program. However, students often consider programming courses difficult, and these courses often have among the highest dropout rates of computer science courses offered. It is therefore critical to provide more effective instruction to help students understand the syntax of C and prevent them losing interest in programming. In addition, homework and paper-based exams are still the main assessment methods in the majority of classrooms. It is difficult for teachers to grasp students' learning situation due to the large amount of evaluation work. To facilitate teaching and learning of C, in this article we propose a system---LAPLE (Learning Analytics in Programming Language Education)---that provides a learning dashboard to capture the behavior of students in the classroom and identify the different difficulties faced by different students looking at different knowledge. With LAPLE, teachers may better grasp students' learning situation in real time and better improve educational materials using analysis results. For their part, novice undergraduate programmers may use LAPLE to locate syntax errors in C and get recommendations from educational materials on how to fix them. BibTeX | EndNote | ACM Re","pdf":"Real-time Learning Analytics for C Programming  Language Courses      Xinyu Fu  Graduate School of Information   Science and Electrical Engineering  Kyushu University   Japan  fxy0207@gmail.com   Atsushi Shimada  Faculty of Arts and Science   Kyushu University  Japan   atsushi@artsci.kyushu-u.ac.jp   Hiroaki Ogata  Faculty of Arts and Science   Kyushu University  Japan   ogata@artsci.kyushu-u.ac.jp    Yuta Taniguchi  Faculty of Arts and Science   Kyushu University  Japan   taniguchi@artsci.kyushu-u.ac.jp   Daiki Suehiro  Faculty of Arts and Science   Kyushu University  Japan   suehiro@artsci.kyushu-u.ac.jp         ABSTRACT  Many universities choose the C programming language (C) as the  first one they teach their students, early on in their program.  However, students often consider programming courses difficult,  and these courses often have among the highest dropout rates of  computer science courses offered. It is therefore critical to provide  more effective instruction to help students understand the syntax of  C and prevent them losing interest in programming. In addition,  homework and paper-based exams are still the main assessment  methods in the majority of classrooms. It is difficult for teachers to  grasp students learning situation due to the large amount of  evaluation work. To facilitate teaching and learning of C, in this  article we propose a systemLAPLE (Learning Analytics in  Programming Language Education)that provides a learning  dashboard to capture the behavior of students in the classroom and  identify the different difficulties faced by different students looking  at different knowledge. With LAPLE, teachers may better grasp  students learning situation in real time and better improve  educational materials using analysis results. For their part, novice  undergraduate programmers may use LAPLE to locate syntax  errors in C and get recommendations from educational materials on  how to fix them.     CCS Concepts   Social and professional topics   Professional topics   Computing education   Computing education programs   Computer science education  CS1.   Keywords  C programming; programming education; learning analytics;  information visualization; learning dashboard   1. INTRODUCTION  Programming is a very useful skill and many universities choose C  as the first programming language to teach to students. However,  novice programmers typically do not understand Cs syntax very  well, and frequently make simple errors, such as typographical  errors or careless use of syntax. Though these errors are simple,  novices typically find their identification and resolution difficult.  That is, they may struggle to locate the cause of the errors, or may  find the nature of the error obscure [6]. In addition, as teachers are  mostly competent programmers, it is easy to omit to discuss simple  errors while explaining course content. It is, therefore, necessary to  devote some attention to how best to address these issues to  facilitate the teaching and learning of C.    Previous research on teaching and learning of programming is  reviewed and discussed in [16, 19]. Those studies reviewed some  research on examined novices and discussed some different  teaching methods and indicated that students considered  programming courses difficult. Most traditional programming  instruction focuses on syntax and logic and is delivered through  lectures in the classroom [10, 20], but technical tools and  visualizations are simply learning aids and materials. Teachers  must thoroughly design their instructional approach to the issues in  the course, and how the aiding materials are incorporated into  education [1]. To effectively facilitate teaching and learning of C,  it is important to identify the different difficulties faced by different  students with different background grappling with different target  knowledge, in a way that reflects their (different) behaviors in the  classroom.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03 $15.00   DOI: http://dx.doi.org/10.1145/3027385.3027407     In our last study [6], we collected compiling logs from novice  students learning programming and classified error types on their  basis. This revealed that a large number of students didnt  understand the basic concepts of the syntax very well, and some  didnt know how to apply conceptual knowledge even after sitting  in on related lectures. Another very interesting finding is that when  some students received compiling errors, they simply tried to  compile the program again and again, without any modification.    The main purpose of this article is to present a tool to help identify  the weaknesses of novice programmers in order to improve  teaching materials supporting C education in the classroom. The  tool we propose, LAPLE (Learning Analytics in Programming  Language Education), may be used by novice undergraduate  programmers in the class. On the one hand, LAPLE can be used to  locate syntax errors in C and get recommendations to address them  derived from educational materials. On the other hand, teachers  may be able to better grasp students learning situation using  LAPLE. We plan to analyze programming error logs and class  material reading logs, reflecting the learning by doing mode  (learningpracticingreflection; figure 1) adopted at Kyushu  University, our institution, and on the basis of the analysis to  discuss key findings and their implications for programming  education.    The remainder of this text is structured as follows: Section 2  provides a review of relevant programming and computing  education literature. Section 3 provides a description of our own  empirical study. Section 4 presents the results gathered by our  learning dashboard. Finally, we discuss our findings implications,  and sketch out scope for further research.         2. RELATED RESEARCH  2.1 Research on Programming Education  Many studies have been done with the intention of gathering  knowledge to support people learning programming. Extant  research predominantly examines cooperative programming and  self-education systems [10, 12, 15 and 19]. The relationship  between students cooperative programming behavior and their  learning performance was investigated for cooperative learning of  ASP.NET [10]. The results revealed that using WPASC (Web- based Programming Assisted System for Cooperation), students  who are active in programming will conduct more activity.  However, more work is still needed to increase learning motivation  for students who have poor learning performance. An agent support  system [12] for C provides the opportunity for students to share  knowledge and conduct error resolution through agent software, as  another type of cooperative programming. The results indicated   that the agent system is useful for programming, but also that  novices need more support on programming, for example, similar  examples. In another study, HTML and CSS syntax errors were  analyzed in a web-development course [15]. The authors examined  students use of the JavaScript programming language and the open  HTML editor system to analyze what difficulties these novices  experienced in learning HTML and CSS syntax. The results  revealed that syntax errors were made by almost all the students  and usually persisted for a very long time in students programming  efforts.    Certain systems have been suggested and provided to help make  programing and debugging easier for students [3, 9]. However,  these systems are much more useful for students who already have  some programming knowledge; how to effectively support  programming novices to avoid, correct, and understand syntax  errors in the classroom is also very important.    Various methods have been suggested to support programming  education in a way that reflects students needs. These include  assessment of learning standards in programming courses by  benchmarking a set of exam questions [21] and studying debugging  behavior in order to better understand and address students  debugging errors [11]. Monitoring students programing efforts in  detail is very important for teachers in programming education,  who are then able to determine when students need guidance and  what kind [7]. Teachers should keep in mind that around any  principle or concept being taught orbit the following modalities:  what actually gets taught; what we think is getting taught; what  we feel wed like to teach; what would actually make a difference  [2].    However, with so much research done to understand novices  programming behavior and make programming easier for them,  research on supporting teaching and learning at the beginning in the  classroom or on improving education materials is still limited. How  to help students understand basic conceptual and technical  knowledge faster and better is really important.   2.2 Supporting Education using Dashboard  Applications  A dashboard is a visual display of the most important information  needed to achieve one or more objectives, consolidated and  arranged on a single screen so the information can be monitored at  a glance [5]. In recent years, several dashboard applications have  been developed to support learning and/or teaching [4, 8, 17 and  22]. Learning process are made easier and more comfortable by  using such educational data. Advanced data analysis can provide  immediate feedback on students engagement and performance in  educational activities, and on that basis guide students to  appropriate learning materials and help teachers discover where  students weaknesses are [4, 17]. A semantic visual analytics tool  for programming courses (EduAnalysis) is designed to provide  teachers with immediate feedback on students exams, allowing  them to produce more balanced exams [8]. Some learning analytics  dashboards and data visualizations are available in Moodle, a  common learning management system, for analyzing the  relationship between motivation and students success on exams;  these have had positive, enthusiastic feedback [13-14, 17]. Here,  we propose a learning dashboard system that may facilitate students  understanding of C as well as supporting teachers. Further, we aim  to analyze the correspondence between error logs and BookLooper  reading logs and apply the results to improve educational materials.   Figure 1. Structure of practice learning in course.              3. DATA ANALYSIS AND  VISUALIZATION  3.1 Environment and Data Source  Kyushu University employs a single-platform learning system  (Mitsuba, M2B) constructed using various tools: Moodle; Mahara,  an e-portfolio system; and BookLooper, an e-book system [13].  Our present research mainly relies to Moodle and BookLooper. As  LAPLE is a plug-in to Moodle, all teachers and students may use it  in class. Teaching/learning materials are uploaded to BookLooper,  which students can use; we are then able to gather reading logs from  BookLoopers server.   The C programming language course at our university is organized  into 90-minute sessions. It begins with about 20 minutes on  imparting Cs knowledge by teachers using the teaching materials  in BookLooper. In the remaining course time, students need to  finish 6 assignments (2 difficult, 2 average, and 2 simple). The  teaching approach used is one of learning by doing (figure 1); if  students make errors or encounter problems that they cannot solve  during practice time, they can ask the TA (teaching assistants) or  the teacher for help, or can investigate themselves using  BookLooper. Using this approach, we can collect and review  students learning log information in real time in the classroom.         Figure 2 illustrates the construction of our LAPLE system. In the  initial lecture, the teacher and students log in to Moodle, and the  teacher introduces the materials on BookLooper which is already  linked into Moodle. Then, the students work on the exercises, as  described. To help students work out their compiling programs, our  university provides students with a common server they can  connect to through terminal software TERA-TERM using their  student ID and password. Students are not required to install any  programming software; the compiled software GCC (GNU  Compiler Collection) is already installed on the server. Students  can access a workspace by using their account. We gather logs from  the server through SFTP (SSH File Transfer Protocol) in real time,  analyze the content, and provide feedback to users. From October  2014 to June 2016, we collected 989,560 compiling error messages  from 1975 students. The detailed data are shown in table 1.   To better assess novices syntax errors, we categorized error  messages in terms of the course schedule (appendix table 1).  Students need to log in to BookLooper during classes to download  educational materials and consult them to acquire needed  knowledge. We are able to gather BookLooper reading logs from   BookLoopers server. For basic information on BookLooper  reading logs, refer to [23].     Table 1. Collected data   Semester Student Error logs Source code courses   2014 2nd 164 11,581 17,642 2   2015 1st 745 607,076 153,808 17   2015 2nd 404 265,602 104,943 6   2016 1st 662 105,301 19,073 20   All 1,975 989,560 295,466 45      3.2 System Design and Methods  The advance of internet-capable consumer technologies has led to  many learning support systems being made available online for  people learning programming [10, 12, 15 and 19]. However, most  students still acquire programming knowledge from traditional  classroom education, and teachers usually determine whether or not  students have acquired knowledge through the reports of  homework and exams. On the one hand, cheating remains a big  issue; on the other, it is a lot of work for teachers to check reports  evaluate the knowledge evidenced therein. Further, without  accurate feedback, making meaningful teaching materials is very  time consuming.    To support teaching and make the learning process easier, we  propose the web system LAPLE for students and teachers. Users  may use their account to browse the analyzed results. Base on the  traditional C programming learning path, we proposed a new  design with LAPLE for supporting C programming education  (figure 3). The detailed functions are described in the following  subsections.         3.2.1 How does LAPLE support teaching  The LAPLE system provides real-time feedback. Real-time  analysis allows teachers to grasp student programmers learning  styles and problems [18], with proven benefits for the students, who  can get targeted guidance as faster than otherwise. We set LAPLE  to collect and analyze logs every 5 minutes to yield real-time  visualization feedback.   Figure 2. The construction of the LAPLE system.            Figure 3. System design.              Table 2. Method for grouping students   Level Evaluation Method Evaluation of internal order in the same level   A Complete at least 4 programs (all 6 programs), and no remaining errors.   X = Time of Programming / programs  The X is smaller, the student is more excellent.  (Some students can finish  a lot of programs in a short time.)   B Complete at least 2 programs (all 6 programs), and no remaining errors.   X = Time of Programming / programs  The X is smaller, the student is more excellent.  (Some students can finish  a lot of programs in a short time.)   C  Complete at least 2 programs (all 6  programs), finally, there are still some  errors.   X = Time of Programming / programs * remaining errors  The X is bigger, the student needs more help. (Some students need more  time to finish programs and cannot fix the problems in programs.)   D  Complete less than 2 programs (all 6  programs), but try to program more than 20  minutes.   X = Time of Programming / (programs + 1) + remaining errors  The X is bigger, the student is work harder. This part of students needs  more help about simple knowledge in programming.   E  Complete less than 2 programs (all 6  programs), and the time of trying to  programming is less than 20 minutes.   X = Time of Programming / (programs + 1) + remaining errors  The X is bigger, the student is work harder. This part of students do not  study hard, and enthusiasm is very low.   3.2.1.1 Discover students weaknesses and critical  knowledge points for teaching  We collected and analyzed students compiling logs with the  intention of identifying high-frequency errors and those which may  need more time to rectify. Besides allowing teachers to give  students timely, targeted feedback, the results can help conveyed  effectively and modify class materials accordingly.   Analysis was critical during the early stages of the systems  preparation. Since at the start of the analysis we had already  collected 11,581 different error messages (62,812 by May 2016), it  was of course impractical to try to explain or provide resolutions  for each one individually. Instead, to understand the syntax errors  that novices commonly make and on this basis to effectively  improve outcomes in C programming education, we classified  these errors into 26 types; these initial findings have already been  discussed in previous work [6]. As more and more students take the  course and compile their programs, error messages are updated and  new ones may appear. At present, we have identified 36 (see  appendix table 2).   3.2.1.2 Support teachers to grasp students learning  situation in real time  By grouping the students into different knowledge levels, we detect  which students are in trouble so as to provide support immediately.    We group the students by the time they spend on programming,  how many compiling errors they fix, and how many exercises they  finish in class. We set 5 knowledge levels on the basis outlined in  table 2. Students in levels A and B can be considered outstanding  students; teachers do not need to pay close attention to supporting  them fix their programs, but can compose and provide them with  more complex exercises to help them develop and produce the best  work they can. Level A students complete more exercises with no  compiling errors left, and can be considered excellent students.  Level B students have good performance in programming, and  although they cant finish all the exercises, they are able to modify  the errors that occur. Students in levels C, D, and E are the ones  that need more help and the ones we especially want to identify.   Students who try to compile two or more programs but do not  address all errors are classified into level C; those who work on  only one program and try to compile it for more than 20 minutes,  into level D. Students in levels C and D are the students who have  troubles with programming but are still trying hard to get it.  These students can be considered active students. Those who work  on only one program for less than 20 minutes before giving up are  classified into level E, these students also can be considered as  inactive students. Teachers and TAs should provide one-on-one  instruction to students in levels C, D, and E.   3.2.2 How does LAPLE support learning  LAPLE provides students an opportunity to engage in more  effective reflection on their learning process.    1.     Relevant learning materials and ones own previous related  programming experience in previous assignments can be  referred to when one meets with troubles during compiling.    2. Common errors among students can be extracted and  converted into new exercises.    3. Similar examples can be provided to help students notice and  understand errors more clearly.    Work on precisely how to provide optimal reflection opportunities  to students is still in process. A potential scenario is laid out in  figure 4. With LAPLE, when an error occurs, the system will  analyze the most visited pages of BookLooper to detect the relevant  materials. Here we present a very simple example involving  knowledge of variables, which are usually taught at the beginning  of the course. Novice programmers are usually not clear on the  difference between programming language and natural language  when they begin to learn programming. Errors of type 4 are often  made by these students, who often take a long time to determine the  cause of their error and experience flagging enthusiasm as a result.  To avoid this situation, LAPLE shows students the most-visited  pages at that time among their peers working on the same problems,  helping them locate needed knowledge quickly. One program  example is provided at left in figure 4. There are errors in line 4     here, colored in red. Our analysis results showed that the most  visited page at the time was page 7 of topic 2.         As most-visited pages have shown themselves to be useful for  addressing errors, LAPLE can then recommend these pages to  students and link to them on students homepage as resources.  Identifying the most visited pages can also help teachers discover  which parts of material students are having trouble with, what  should be explained more clearly in class, or whether they need to  improve the content of some material.   4. RESULTS OF DATA VISUALIZATION  We provide LAPLE as a Moodle plugin so that all the students and  teachers who attend the course are able to get feedback from our  analysis. The matter of supporting reflection on learning is still in  process. In this section, we will mainly discuss the lessons that can  be learned from the error data and the feedback provided to teachers  and TAs to support teaching through the learning dashboard (seen  in figure 5, details can be seen in figures 6-11). The dashboard  contains two main parts: the left part indicates the error distribution  among students, and the right part indicates students learning  situation. The details are described in the following subsections.         4.1 Learning Dashboard Data on Error Types  The error data are mainly used to discover students weaknesses  and critical knowledge points for helping them acquire the material.   The bar chart is used to show the error distribution by course topic,  based on the log data collected so far. Teachers may be able to  provide more targeted explanations of C programming topics with  which students have difficulty based on the results. For example,  figure 6 clearly illustrates that errors of types 27, 26, 11, 20, 23, 4,   24, 8, 5, 28, 10, and 9 are most encountered by students in topic 4.  Among these, type 26, 20 and 10 are mostly caused by missing  semicolons, while type 23 is caused by mistyping of the standard  library of C. From our experience, these kinds of errors do not  decrease as the course progresses; teachers should, therefore,  emphasize these error types and how to address them throughout  the course. Types 8 and 9 involve incorrect use of mathematical  functions in relation to content in topic 4; the high frequency rates  thus indicate that teachers should spend more time on explaining  mathematical functions, especially pow functions, during class  coverage of this topic. Further, these results are useful for  improving teaching materials, so that different knowledge points  can be explained more concretely.         The bar chart is suggested to use at the stage of introducing  knowledge, we also provide a heatmap chart to show the  (changing) distribution of error types in real time. We present an  example in figure 7, in which logs collected on June 17, 2016, from  14:50 to 16:20 are presented. The following charts also use these  logs. Darker color indicates that more errors of that type occurred  at that time, giving the teacher or TA chance to react expeditiously.         Further, as the results remain visible until the next class, teachers  may summarily explain simple errors made on the previous classs  topic. Additionally, the results can be referred to later on to  optimize C education materials.   4.2 Learning Dashboard for Grasping  Students Learning Situation  The charts shown in the figures help teachers grasp students  learning situation in real time. The intention is to identify students  as outstanding, active, or struggling. The outstanding  students are those who can correctly complete most exercises by  themselves; active students are those who keep trying and   Figure 4. Scenario supporting students learning.            Figure 5. Learning dashboard of LAPLE.            Figure 6. Error distribution for topic 4.            Figure 7. Sample error distribution.              modifying their code although they meet many compiling errors;  and inactive students are those who are frustrated by errors and give  up or do not spend enough time on programming. It is possible to  provide targeted guidance to students through this grouping.   Figure 8 use a heat-map chart to indicate the students activity. We  collect logs every 5 minutes and update the analyzed results to  Moodle so that teachers can get feedback almost in real time. The  numbers on the chart show how many times students try to compile  programs (with larger numbers in darker color). With this heat-map  chart, we can easily detect the activity and inactivity of students.  For example, the student highlighted in green on the image is active,  since he keeps trying to compile his program over the whole course  of the class. In contrast, the student in blue only tries to compile 4  times within one 10-minute period, across the whole class; this  student is inactive.  In traditional evaluation in the C course,  students reports of these exercises are usually a big part of their  course mark. Some students do not really try to engage with  programming, and instead submit reports copied from other  students. We suggest that teachers can conduct a fairer evaluation  of the reports with reference to the heat-map chart.         Figure 8 indicates all students activity levels; however, it shows  only activity, not which students get outstanding results, and it is  also not easy to see what kind of help a given student needs. The  line-point chart in figure 9, developed using the method described  in section 3.2 above, allows grouping by academic achievement,  with levels A and B (as described above) classified as outstanding,  C and D as active, and E as inactive. In this chart, red circles mean  that at that timestamp the student still has compiling errors that need  to be fixed, and blue means no errors left. The size of the point  shows the number of times trying to compile; once the new program  begins to compile, the point will become small again. The lines help  us see the progress of individual programs; when the line turns up,  it means a new program is beginning to compile. These results are  also updated every 5 minutes, so that teachers can see which  students need help immediately (making which students are in  levels C, D, and E the most important information). The teacher can   easily locate the student by name on the chart and provide  appropriate support.         Further some teachers will want to know not only the compiling  situation of the students but also which type of program is really  accusing difficulties. In LAPLE, teachers can get detailed  information on one student, as shown in figure 10. We flag the  different difficulty grades of exercises into four colors in  descending order of difficulty: blue, orange, green and red. A-grade  is colored by blue, which is the highest difficulty level. B-grade is  colored by orange, which is the second-highest. C-grade is green,  which is the part of most easily. Before doing this research, we  named the exercises in the teaching materials so that we could   Figure 8. Students activity in class.          Figure 9. Situation of different students upon trying to   compile.            distinguish the programs from students compiling logs. Some  students named their programs as a rule, however some did not.  To analyze all the logs that we collected, we add a red part which  shows those without the right rule, like 2-a-1.c. From figure 10,  teachers are able to see which level of exercises students are trying  to do, how many times the student tries to compile, how long the  student works on the program, and how many errors the student  makes at different stages.         Finally, another bar char is used to show the time spent on programs  of each level, as seen in figure 11. All together, these charts support  teachers to understand the state of their students learning and help  them notice students weaknesses and adjust teaching materials  accordingly. Thus, with LAPLE, teaching and learning C is made  easier and more comfortable.         5. CONCLUSIONS  In this article, we proposed a C education support system LAPLEdiscussed how to support teachers to more effectively  educate students in their C programming classes with the real-time  learning dashboard. Our work makes the following three main  contributions. First, the error type visualization, which was  analyzed in our previous research [6], allows us to characterize  students weaknesses in their understanding of C material. To our  knowledge, our study is the first to offer analysis of novices coding  errors in a C course, and use the results to suggest ways to optimize  education materials. Second, we use real-time analysis to support  immediate feedback, so that teachers are able to provide effective  and timely explanation when they notice the students who are in  trouble. Further, we suggested combining programming logs and e- book reading logs to allow automatic recommendation of relevant  material. These approaches have in common that knowledge  gleaned from students is used to benefit the students. We plan to  finish this part of work soon, so that LAPLE can be applied as soon  as possible to support students learning.    However, at least one limitations of the study should be reflected.  Given that we currently assemble error logs on the basis of syntax  error messages obtained from the compiler, the causes of the errors  we receive are not known exactly. A new error message analysis  model will need to be created that allows us to obtain more precise  error causes and, accordingly, better support learning.   The LAPLE system will be deployed in C programming courses  from November 2016 (autumn semester) to verify its usefulness.   Finally, as students in C programming courses are a mixture of  computing majors and others, we wish to investigate potential  differences in rates of error types between majors and adapt course  teaching accordingly. To do so, we intend to implement an online  system for all novices studying C programming in order to collect  and examine more error messages and error types. It is our hope  that the LAPLE system may in that way serve a greater number of  learners, and be useful in more complex programs.      6. APPENDIX TABLE     Appendix Table 1. Weekly overview of the course schedule   Week Topics Assessments   1 Introduce to C Language 3 Exercises of Using Printf Statement   2 Variables 6 Exercises of Variables   3  Functions 6 Exercises of Using Scanf Statement and Operator Symbol like +,   -, *, ++   4 Mathematical Functions 6 Exercises of Mathematical Functions   5 Decision Making Structures If-else 6 Exercises of If-else   6 Multiple If-else and Switch-case 6 Exercises of Multiple If-else and Switch-case   7 For Loop 6 Exercises of For Loop   8 Array 6 Exercises of Array   9 Multi-dimensional Array 6 Exercises of Multi-dimensional Array   10 Multiple For Loop 6 Exercises of Multiple For Loop   11 While--Do-while Loop 6 Exercises of While--Do-while Loop   Figure 10. A students detailed learning.            Figure 11. Program time distribution among students.              12 String Functions 6 Exercises of String Functions   13 User-defined Functions 6 Exercises of User-defined Functions   14 File I/O 6 Exercises of File I/O   15 Programming Practice 1 Exercises of Final Test of Semester     Appendix Table 2. Error types   Type  number Error Description   Type  number Error  Description   Type 1 Unmatched data type Type 19 Missing semicolon before return  Type 2 Re-declaration of variables Type 20 Missing semicolon  Type 3 Mismatch of { (particularly, { after main) Type 21 , used after variables, not .   Type 4 Undeclared variables (particularly, mismatched  symbols and mistyping of symbols)   Type 22 Missing semicolon or comma   Type 5 Syntax errors (invalid operand or invalid suffix) Type 23 Mistyping of standard library  Type 6 Unmatched variable type for array Type 24 Full-width characters are used  Type 7 Mistakes on array declaration Type 25 Misuse of switch statement (with or without use of   break)  Type 8 Misuse of pow function Type 26 Undeclared variables, or ; or } is missing in the   previous row  Type 9 Misuse of mathematical functions Type 27  Miss input symbol such as ,,<=,or the mismatch   of []symbol  Type 10 Missing punctuation (e.g. = or , or ; or asm   or __attribute__)semicolons were most  frequently missing   Type 28 Errors in definition of variables or the boundary of  the main function is wrong   Type 11 Other error type Type 29 Unmatched symbols such as (), {} ,[]  Type 12 Statement is out of main class Type 30 Variables should define before the  for  loop  Type 13 Missing } at the end of code Type 31  (),{}symbols are not matched in if-else   statement  Type 14 Mismatch of {} Type 32 Parameters' type is incorrect in the function  Type 15 Mismatch of quote marks, or mismatch of <> Type 33 Array size should be defined as a constant  Type 16 Mismatch of {}: missing semicolons or comma   before {  Type 34 Miss input variable's type   Type 17 Re-declaration of variable type Type 35 Misuse of do-while statement  Type 18 Two main classes in one program Type 36 Variable's type in return statement is unmatched   desired variables     7. ACKNOWLEDGMENTS  This research work was supported by the Grant-in-Aid for  Scientific Research No. 26560122 and No. 16H06304 from the  Ministry of Education, Science, Sports, and Culture of Japan and  by Research and Development on Fundamental and Utilization  Technologies for Social Big Data (178A03), Commissioned  Research of the National Institute of Information and  Communications Technology (NICT), Japan.   8. REFERENCES  [1] Ala-Mutka, K., 2004. Problems in Learning and Teaching   Programming - a literature study for developing  visualizations in the Codewitz-Minerva project. Tampere  University of Technology.  https://www.cs.tut.fi/~edge/literature_study.pdf   [2] Burton, P., 1998. Kinds of language, kinds of learning. ACM  SIGPLAN Notices, 33, 53-61.   [3] Chang, K.-E., Chiao, B.-C., Chen, S.-W., & Hsiao, R.-S.,  2000. A Programming Learning System for Beginners-A  Completion Strategy Approach. IEEE Transactions on  Education. (May, 2000) Vol. 43, 211-220.   [4] Corrin, L., & Barba, P. d., 2015. How do students interpret  feedback delivered via dashboards Learning Analytics &  Knowledge Conference. (Mar 16-20, 2015), Poughkeepsie,  NY, USA. DOI=  http://dx.doi.org/10.1145/2723576.2723662.   [5] Few, S., & Edge, P., 2007. Dashboard Confusion Revisited.  Visual Business Intelligence Newsletter.(March, 2007)   [6] Fu, X., Yin, C., Shimada, A., & Ogata, H., 2015. Error Log  Analysis in C Programming Language Courses. The 23nd     International Conference on Computers in Education (ICCE  2015). (Nov.30-Dec.4, 2015). Hangzhou, China. 641-650.   [7] Helminen, J., Ihantola, P., Karavirta, V., & Malmi, L., 2012.  How Do Students Solve Parsons Programming Problems   An Analysis of Interaction Traces. ICER '12 Proceedings of  the ninth annual international conference on International  computing education research. (Sep. 9-11, 2012) Auckland,  New Zealand. 119-126.   [8] Hsiao, I-H., Pandhalkudi, S. K., & Lin, Y.-L., 2016.  Semantic Visual Analytics for Todays Programming  Courses. Learning Analytics & Knowledge Conference.  (April 25-29, 2016), Edinburgh, United Kingdom. DOI=  http://dx.doi.org/10.1145/2883851.2883915.   [9] Hu, Y.-J., & Chao, P.-Y., 2015. A simulation-based learning  environment for learning debugging. Proceedings of the  23nd International Conference on Computers in Education.  (ICCE 2015). (Nov.30-Dec.4, 2015). Hangzhou, China.310- 312.   [10] Hwang, W.-Y., Shadiev, R., Wang, C.-Y., & Huang, Z.-H.,  2012. A pilot study of cooperative programming learning  behavior and its relationship with students learning  performance. Computers & Education, 1267-1281.   [11] Lewis, C. M., 2012. The Importance of Students Attention  to Program State: A Case Study of Debugging Behavior.  ICER '12 Proceedings of the ninth annual international  conference on International computing education research.  (Sep. 9-11, 2012). Auckland, New Zealand. 127-134.   [12] Nagao, K., Ishii, N., 2003. Evaluation of Learning Support  System for Agent-Based C Programming.  Knowledge-Based  Intelligent Information and Engineering Systems Lecture  Notes in Computer Science, Vol. 2774, 540-546.   [13] Ogata, H., Yin, C., OI, M., Okubo, F., Shimada, A., Kojima,  K., & Yamada, M., 2015. E-Book - based Learning Analytics  in University Education. The 23rd International Conference  on Computers in Education (ICCE2015). (Nov.30-Dec.4,  2015). Hangzhou, China. 401-406.   [14] Okubo, F., Shimada, A., Yin, C., & Ogata, H., 2016.  Visualization and Prediction of Learning Activities by Using  Discrete Graphs. The 23rd International Conference on  Computers in Education (ICCE2015). (Nov.30-Dec.4, 2015).  Hangzhou, China. 739-744.   [15] Park, T. H., Dorn, B., & Forte, A., 2015.  An analysis of  HTML and CSS syntax errors in a web development course.  ACM Trans. Compute. Educ. Vol. 15, No. 1, 4:1-4:21. DOI=  http://dx.doi.org/10.1145/2700514.   [16] Perkins, D. N., Hanconck, C., Hobbs, R., Martin, F., &  Simmons, R. 1989. Conditions of learning in novice  programmers. In Soloway & Spohrer: Studying the Novice  Programmer, pp. 261-279.   [17] Podgorelec, V., & Kuhar, S., 2011.Taking Advantage of  Education Data: Advanced Data Analysis and Reporting in  Virtual Learning Environments. Electronics and Electrical  Engineering Elektronika IR Elektrotechnika. (No. 8, 2011).  DOI = http://dx.doi.org/10.5755/j01.eee.114.8.708.   [18] Rao, S., & Kumar, V., 2008. A theory-centric real-time  assessment of programming. In proceedings of the Eighth  IEEE International Conference on Advanced Learning  Technologies (ICALT), Cantabria, Spain, July, pp. 139143.   [19] Robins, A., Rountree, J., & Rountree, N., 2003. Learning and  Teaching Programming: A Review and Discussion.  Computer Science Education, Vol. 13, No. 2, 137-172. DOI=  http://dx.doi.org/10.1076/csed.13.2.137.14200.   [20] Sharan, S. 1980. Cooperative learning in small groups: recent  methods and effects on achievement, attitudes, and ethnic  relations. Review of Educational Research, 50(2), 241-271.   [21] Sheard, J., Simon, Dermoudy, J., Souza, D. D., Hu, M., &  Parsons, D., 2014. Benchmarking a set of exam questions for  introductory programming. Proceeding of the Sixteenth  Australasian Computing Education Conference (ACE2014),  Auckland, New Zealand. Vol. 148. 113-121.   [22] Verbert, K., Duval, E., Klerkx, J., Govaerts, S., & Santos,  J.L., 2013. Learning Analytics Dashboard Applications.  American Behavioral Scientist. (Feb. 28, 2013)  DOI:10.1177/0002764213479363.   [23] Yin, C., Okubo, F., Shimada, A., OI, M., Hirokawa, S., &  Ogata, H., 2015. Identifying and Analyzing the Learning  Behaviors of Students using e-Books. The 23rd International  Conference on Computers in Education (ICCE2015).  (Nov.30-Dec.4, 2015). Hangzhou, China.118-120.             1. INTRODUCTION  2. RELATED RESEARCH  2.1 Research on Programming Education  2.2 Supporting Education using Dashboard Applications   3. DATA ANALYSIS AND VISUALIZATION  3.1 Environment and Data Source  3.2 System Design and Methods  3.2.1 How does LAPLE support teaching  3.2.1.1 Discover students weaknesses and critical knowledge points for teaching  3.2.1.2 Support teachers to grasp students learning situation in real time   3.2.2 How does LAPLE support learning    4. RESULTS OF DATA VISUALIZATION  4.1 Learning Dashboard Data on Error Types  4.2 Learning Dashboard for Grasping Students Learning Situation   5. CONCLUSIONS  6. APPENDIX TABLE  7. ACKNOWLEDGMENTS  8. REFERENCES   "}
{"index":{"_id":"36"}}
{"datatype":"inproceedings","key":"Scheffel:2017:WWY:3027385.3027428","author":"Scheffel, Maren and Drachsler, Hendrik and Kreijns, Karel and de Kraker, Joop and Specht, Marcus","title":"Widget, Widget As You Lead, I Am Performing Well Indeed!: Using Results from an Exploratory Offline Study to Inform an Empirical Online Study About a Learning Analytics Widget in a Collaborative Learning Environment","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"289--298","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027428","doi":"10.1145/3027385.3027428","acmid":"3027428","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, statistical analysis, tool evaluation","Abstract":"The collaborative learning processes of students in online learning environments can be supported by providing learning analytics-based visualisations that foster awareness and reflection about an individual's as well as the team's behaviour and their learning and collaboration processes. For this empirical study we implemented an activity widget into the online learning environment of a live five-months Master course and investigated the predictive power of the widget indicators towards the students' grades and compared the results to those from an exploratory study with data collected in previous runs of the same course where the widget had not been in use. Together with information gathered from a quantitative as well as a qualitative evaluation of the activity widget during the course, the findings of this current study show that there are indeed predictive relations between the widget indicators and the grades, especially those regarding responsiveness, and indicate that some of the observed differences in the last run could be attributed to the implemented activity widget.The pathways and learning outcomes of university students are the culmination of numerous experiences inside and outside of the classroom, with faculty and with other students, in both formal and casual settings. These interactions are guided by the general education requirements of the university and by the learning goals of the student. The only official record and representation of each student's education is captured by their academic transcript: typically a list of courses described by name and number, grades recorded on an A-F scale and summarized by GPA, degrees awarded, and honors received. This limited approach reflects the technological affordances of a 20th century industrial age. In recent years, scholars have begun to imagine a transcript of the future, perhaps combining a richer record of the student experience along with a portfolio of authentic products of student work. In this paper, we concentrate on first, and develop analytic methods for improving measures of both classroom performance and intellectual breadth. In each case, this is done by placing elements of individual transcripts in context using information about their peers. We frame the study by addressing basic questions. Were the courses taken by the student difficult on average? Did the individual stand out from their peers? Were the courses representative of a broad intellectual experience, or did the student delve into detail in the chosen field of study? And with whom did they take courses?","pdf":"Widget, widget as you lead, I am performing well indeed! Using results from an exploratory offline study to inform  an empirical online study about a learning analytics widget in a collaborative learning environment  Maren Scheffel Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, NL  maren.scheffel@ou.nl  Hendrik Drachsler Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, NL  hendrik.drachsler@ou.nl  Karel Kreijns Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, NL  karel.kreijns@ou.nl Joop de Kraker Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, NL  joop.dekraker@ou.nl  Marcus Specht Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, NL  marcus.specht@ou.nl  ABSTRACT The collaborative learning processes of students in online learning environments can be supported by providing learn- ing analytics-based visualisations that foster awareness and reflection about an individuals as well as the teams be- haviour and their learning and collaboration processes. For this empirical study we implemented an activity widget into the online learning environment of a live five-months Master course and investigated the predictive power of the widget indicators towards the students grades and compared the re- sults to those from an exploratory study with data collected in previous runs of the same course where the widget had not been in use. Together with information gathered from a quantitative as well as a qualitative evaluation of the ac- tivity widget during the course, the findings of this current study show that there are indeed predictive relations be- tween the widget indicators and the grades, especially those regarding responsiveness, and indicate that some of the ob- served differences in the last run could be attributed to the implemented activity widget.  CCS Concepts Applied computing  Collaborative learning; E- learning; Human-centered computing User stud- ies; Visualization systems and tools; General and reference  Evaluation;  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027428  Keywords learning analytics; statistical analysis; tool evaluation  1. INTRODUCTION One way to support the collaborative learning processes  of student teams in virtual learning environments is to pro- vide explicit information to the students about the activities of the group members and to stimulate awareness, reflec- tion and social interaction [11]. Although using behavioural data automatically collected from the learning environment is not to be seen as a one-to-one replacement for using sub- jective data collected via questionnaires or interviews [8], making use of learning analytics based on interaction data does have the advantage of being non-disruptive and cov- ering the whole student population of a course. A learn- ing analytics widget in a computer-supported collaborative learning environment can thus provide feedback [9, 14] to students as well as teachers by visualising the students ac- tivities within the virtual learning environment in order to facilitate awareness and reflection[15].  Endsley [5, 6] describes being aware of ones own situa- tion as a three level process: (i) perceiving the elements in the current situation, (ii) comprehending the situation, and (iii) projecting what a future status could look like. Once awareness of the situation is established, a user can reflect on it in relation to his behaviour [19] and can subsequently adapt or even change his behaviour if necessary. Accord- ing to McAlpine & Weston [13] reflection is to be seen as a mechanism that can improve teaching and thus maximise learning and not as an end in itself. Reflection processes and behavioural change are, however, not only influenced by awareness [2]. Whenever someone engages in self-regulated learning, they bring their own knowledge, beliefs and skills into the process [23]. Additionally, emotions, the social envi- ronment as well as ones own behaviour play a role [25]. The way in which someone acts and reacts in a given situation thus depends on the different ways they have constructed their current knowledge [24].    The relevance of these aspects has been emphasised by Verbert et al. [22] in their process model for learning ana- lytics applications that consists of four stages: awareness, re- flection, sensemaking, and impact. As the discussion about the effect of learning analytics and the need for empirical studies has increased [20, 7], a number of recent studies have investigated the impact of learning analytics dashboards on different aspects, e.g. individual goal attainment and mo- tivation. Lonn et al. [12] investigated whether the motiva- tion of students in a summer bridge program, i.e. students among the at-risk population in postsecondary education, was affected by the use of learning analytics. Their findings suggest that being exposed to a learning analytics applica- tion displaying their academic performance can negatively predict the change of mastery orientation, i.e. it decreases, and can thus affect a students interpretation of their data and their success. The authors stress that student goal per- ception and formative performance thus need to be carefully considered when designing learning analytics interventions.  Beheshitha et al. [1] also examined the effect of learn- ing analytics visualisations. Their experiment took place in a blended course setting where each student was randomly assigned to one of three available visualisations. The results revealed that the visualisations had different, i.e. positive or negative, effects on the quality and quantity of forum posts by the students that depended on the students achievement goal orientation. These authors stress that it is important to consider individual differences such as achievement goal orientation in the design process of learning analytics visuali- sations. A third study by Khan and Pardo [10] showed that students use learning analytics differently, i.e. depending on their information need or the learning activity or phase. All three of these studies clearly emphasise that for learn- ing analytics visualisations to have a positive effect, they need to be embedded into the instructional design and that the students personal preferences, e.g. goal attainment or motivation, need to be considered.  In order to add further results to the collection of empirical data studies, we have designed a learning analytics widget called activity widget and implemented it into the learning environment of the European Virtual Seminar (EVS), an online course where geographically dispersed students work together on different topics in small teams. Based on data automatically collected in the EVS platform, the activity widget is made up of several radar and bar charts. The aim is to make students aware of the platform activity of their team in relation to their own activity level. Apart from making students aware, the activity widget also aims to foster reflection about how their behaviour can influence their position in the team and their course outcome.  1.1 Exploratory Offline Study In a previous exploratory data study [17] (referred to as  exploratory study throughout this paper) we investigated the predictive power of several indicators of the activity wid- get towards the students grades by instantiating these in- dicators with data from the four previous runs (2011-2012, 2012-2013, 2013-2014 and 2014-2015) of the European Vir- tual Seminar on Sustainable Development (EVS). That is, although the activity widget had not been used in those years, we analysed the log data from these years to explore what the widget indicator scores would have been if the widget had been used in those years. We tested whether  the students activity scores of the previous runs correlated with the tutor gradings and whether they validly reflected them. We did so for the whole run of the courses as well as for individual months.  More specifically, in the exploratory study we wanted to know (1) whether the widget indicator scores correlated with the tutor gradings of individual students at all, (2) whether the scores of some widget indicators were better predictors for the students individual grades and (3) whether certain points in time produced indicator scores that are better grade predictors than others. We hypothesised that signifi- cant positive correlations existed between the widget indica- tors and the grades, that the widget indicator presence (see explanation below) was a better predictor than the other ones and that the widget indicator scores produced in the second half of the course were better predictors towards the grades than those in the first half of the course.  The results of the correlation analysis and the structural equation modelling (SEM) of the exploratory study showed that most of the indicators indeed significantly and posi- tively correlated with the grades and that they can be used as predictors. The scores of the presence indicator, how- ever, did not turn out to be better predictors for the grades, neither for the whole run nor for the individual months. Instead, the responsiveness indicator achieved the best re- sults. Looking at the individual months, the analysis showed that the months in the first half of the course yielded better correlation and SEM results than those in the second half. This unexpected outcome was due to an unforeseen large usage of communication tools outside of the courses learn- ing environment. For detailed results and their discussion please refer to [17].  1.2 Approach Keeping these results in mind, we implemented the acti-  vity widget into the learning environment of EVS and made it available to students and tutors in the 2015-2016 run of the course. In this current study (referred to as online study throughout this paper) we investigated whether using the ac- tivity widget live in a run of the course yielded similar or dif- ferent correlations between the widget indicator scores and the grades and whether the regression analyses performed in SEM showed approximately the same path-coefficients when compared to the exploratory study. The same set of analy- ses as used in the exploratory study was therefore applied to the data from the 2015-2016 run. The research questions that guided the correlation and regression analyses in our online study are:  RQ-A1: With the activity widget in use, do widget indica- tor scores again correlate significantly and positively with the tutors gradings of individual students  RQ-A2: With the activity widget in use, are the scores of the responsiveness indicator again better predictors for the students individual grades than those of the others  RQ-A3: With the activity widget in use, are the widget indicator scores produced in the first half of the course again better predictors than those produced in the sec- ond half  As the activity widget aims at making students aware of their own activities relative to those of their fellow students    as well as fostering reflection about how their behaviour in- fluences their position within the team and the teams col- laboration processes, we were interested in the users ex- perience with the widget during the 2015-2016 run. We therefore evaluated the activity widget using the Evalua- tion Framework for Learning Analytics (EFLA) question- naire twice: the first evaluation was conducted in the middle of the course and the second one at the end. Using EFLA allowed us to take the students as well as the tutors points of view into account and to compare the two user groups with one another. The research questions that guided the widget evaluation are:  RQ-B1: Is there a difference in widget evaluation results between the mid-course questionnaire and the end- course questionnaire  RQ-B2: Is there a difference in widget evaluation results between students and tutors  The next section describes the course, the activity widget and the evaluation questionnaire in more detail and also elaborates on the method of analysis. After that, we present the results of our online study followed by a discussion and the conclusions.  2. METHOD  2.1 Participants and Materials  2.1.1 The EVS Course Coordinated by the Open University of the Netherlands,  the European Virtual Seminar on Sustainable Development (EVS) is a web-based Master course jointly offered by ap- proximately ten different universities in Europe each year, some of which are campus universities while others are dis- tance education institutions. An extensive description of EVS1 and its aims is provided in [3].  EVS runs for five months (November 1 till April 1) every year. During that time students work together on sustain- ability issues in teams of four to seven, with about six to nine teams every year. Ages range between 20 and 25 years for the students from the regular universities and between 30 and 50 years for those from the distance universities. Ev- ery team is coached by a tutor and guided by an expert on the teams topic.  The students final grade for the course can range from 0 to 10 and is comprised of several components: 50% are based on the grade for a teams research report which is given by the expert; 20% are based on the grade for a teams collaboration process which is given by the tutor; 30% are based on the grade for the individual students contribution which is also given by the tutor. This last grade is called the individual-overall grade (T4) and is divided into three subgrades: T1 planning & progress, T2 contribution to team and T3 support. These four grades evaluating an individual students contribution are the ones used in our analyses. Table 1 explains the different aspects covered by these grades.  Since the run of 2011-2012 EVS has been using an Elgg- based2 platform which automatically collects and generates  1http://www.ou.nl/evs 2https://elgg.org/  data on the students activities on the platform. This data forms the input to our awareness widget.  Table 1: Tutor-based grades for students in EVS  grade aspects covered by grade  T1 planning & progress  planning a realistic own workload dealing with deadlines and agreements flexibility in making appointments/agreements/planning ability to change roles and responsibilities  T2 contribution to team dealing with feedback from the group taking initiative, helping the group to progress productivity and quality of contributions  T3 support being supportive (offering support and help others) encourage the learning of the other members giving feedback / reviewing contributions of others  T4 individual-overall overall grade (average of the three sub-grades)  Table 2: Calculation of the 5 widget indicator scores  widget indicator calculation of the widget indicator scores  W1 initiative # of posts (discussion, blog, files, pages) W2 responsiveness # of comments to posts (discussion, blog, files, pages) W3 presence # of page views (on EVS platform) W4 connectedness # of contacts made W5 productivity (W1 initiative + W2 responsiveness) / W3 presence  Figure 1: Cumulative student view of the widget.  Figure 2: Periodic student view of the widget.    Table 3: Criteria and items of the learner and the teacher section of the Evaluation Framework for Learning Analytics.  Learners Teachers  Data D1: I know what data is being collected. I know what data is being collected. D2: I have access to my data. I have acces to my students data. D3: I understand the presented results. I understand the presented results.  Awareness A1: I am aware of my current learning status. I am aware of my learners current learning status. A2: I comprehend my current learning status. I comprehend my learners current learning status. A3: I can project my future learning status. I can project my learners future learning status.  Reflection R1: I reflect on my learning activities. I reflect on my teaching activities. R2: I reflect on alternative learning activities. I reflect on alternative teaching activities. R3: I know when to change my behaviour. I know when to change my behaviour.  Impact I1: I can detect whether I am falling behind. I can detect whether my students are falling behind. I2: I study more efficiently. My students learn more efficiently. I3: I study more effectively. My students learn more effectively.  2.1.2 The Activity Widget We developed the widget as an Elgg environment plug-  in. It can be downloaded under the GNU GPL version 2 [21]. The widget is meant to make students aware of their activities on the platform in relation to those of their team members and to then reflect on this information. It also allows the tutors to become aware of the different activity levels of the students in their team. There are five indica- tors representing different types of activities on the platform: W1 initiative, W2 responsiveness, W3 presence, W4 con- nectedness and W5 productivity. Table 2 explains how the scores of the different widget indicators are calculated.  There are two different views available in the activity wid- get: one showing the widget indicator scores for the whole run of EVS (see Figure 1) and one showing them per month (see Figure 2). The widget indicator scores are automati- cally calculated from the data recorded in the EVS platform and are scaled from 0 to 10. The team member with the highest activity gets a score of 10 for that widget indicator and the scores of the other team members are then scaled in relation to that. In both views, the team average scores are shown in blue while the current users scores are shown in orange.  As showing a students widget indicator scores to the other team members is a privacy sensitive issue, we followed the process suggested by Drachsler and Grellers DELICATE checklist [4] and created a manual explaining the widgets intentions and functionalities. It was distributed to all EVS users making clear what data is collected, how it is visualised and how they can protect their privacy. Implemented within the widget is a Reciprocal Privacy Model (RPM) that allows students to decide whether their team members can see their widget indicator scores or not. Those students that share their data get to see the data from those who also decided to share theirs. Those students that do not want to share their data do not get to see their team members data. The team average is visible to all students all the time.  2.1.3 The Evaluation Framework for Learning Ana- lytics  The added value of providing learning analytics to stu- dents and teachers has clearly been recognised in many edu-  cational institutions. While new widgets and dashboards are continuously being developed and implemented, their evalu- ation has not been standardised yet. We thus developed the Evaluation Framework for Learning Analytics (EFLA)3 that can be used to evaluate learning analytics tools according to several aspects.  The first version of the EFLA was developed with experts from the learning analytics community using a group con- cept mapping study [18]. It consisted of five criteria (Ob- jectives, Learning Support, Learning Measures and Out- put, Data Aspects and Organisational Aspects) with four items each. In a follow-up study [16], this first version of the EFLA was evaluated by a small group of learning analytics experts. Based on the results of this evaluation combined with a revisit of the original group concept mapping data as well as a thorough look at related literature, a second version of the EFLA was developed. This version is split in two parts, one for learners and one for teachers, that both consist of four criteria (Data Aspects, Awareness, Reflec- tion and Impact) with three items each. Table 3 shows the twelve items of the learner as well as the teacher part of the framework. This version was turned into an applicable tool, i.e. a questionnaire for students and teachers, and then used to evaluate the activity widget in EVS.  2.2 Procedure  2.2.1 Correlation and Regression Analyses As in our exploratory study, we used the scores of the wid-  get indicators W1 initiative, W2 responsiveness and W3 presence4 for our analysis. The other two widget indicators W4 connectedness and W5 productivity were excluded again for the same reasons as in the previous study (see [17]).  We first conducted a t-test to see whether the difference between the widget indicator scores from the online study and those from the exploratory study were significant or not. Then, the scores of the three widget indicators (W1, W2, W3) were correlated with the students four individual  3http://www.laceproject.eu/evaluation-framework-for-la/ 4For the EVS run of 2011-2012 the W3 presence scores were unfortunately not available.    grades given by the tutors (T1, T2, T3, T4) using Spear- mans rank correlation. The ranking corrects for differences in scales and units as well as for differences in grading style of the tutors.  We also applied structural equation modelling in order to determine predictive relations between the widget indica- tors and the grades. Although the data follows a Poisson distribution because the widget indicators consist of count variables, we could assume a normal distribution because most count variable data had a nearly normal distribution and a mean value far enough from 05. We were thus able to do the regression analysis.  Spearmans rank correlations and the t-test were calcu- lated using IBMs SPSS Statistics 23 while the regression analyses were performed in Mplus 7. All calculations were done for the entire length of the run as well as for the indi- vidual months.  2.2.2 Widget Evaluation At the beginning of the course in the fall of 2015, all EVS  users received a course manual that included information about the activity widget, i.e. its intentions and function- alities. Two weeks into the course a discussion thread was opened in EVS offering students the opportunity to ask ques- tions about the widget and to comment on it. The discussion thread was kept open and active throughout the courses runtime.  In order to apply the EFLA to the activity widget in EVS, it was turned into a questionnaire. Using online forms, we created a section for each criterion and its three indicators. Every indicator could be rated on a scale from 0 to 6. At the end of the questionnaire, open ended comment boxes were provided for each section asking the users whether they had any comments about this section. Two separate question- naires were created: one for the students and one for the tutors of EVS.  About halfway through the course, on January 12, 2016, students as well as tutors were sent an invitation to partici- pate in the evaluation of the widget by answering the EFLA questionnaire. They were given ten days to answer. Shortly before the end of the course, on March 18, 2016, students and tutors were invited to participate in a second evaluation round of the widget by answering the EFLA questionnaire again. They were given a week to answer.  3. RESULTS  3.1 Correlation and Regression Analyses Looking at the average number of actions per student du-  ring the different months gives us a first impression of the students behaviour of the online study in comparison to the data from the exploratory study. Figure 3 shows a stu- dents average number of initiative and responsiveness posts as well as the presence counts per month for the four years of the exploratory study (2011/12, 2012/13, 2013/14, 2014/15) and the year of the online study (2015-2016) where the acti- vity widget was in use. While the number of initiative posts clearly varies a lot between the years, the number of respon- siveness posts and presence counts are much closer together.  5The mean should be > 10 to be far enough from 0 according to www.umass.edu/wsp/resources/poisson/ and www.umass.edu/wsp/resources/poisson/poisson1.html and www.umass.edu/wsp/resources/poisson/poisson2.html.  Figure 3: A students average number of actions for the three widget indicators per month for five different years.  The most striking difference between the years is that the course run with the widget (2015-2016) has the fewest ini- tiative posts (highly significant, P < 0.000) and the most responsiveness posts (marginally significant, P = 0.053).  The regression analyses were done in two sets: one had T1, T2 and T3 as the dependent variable while the other had T4 as the dependent variable due to T4 being a combi- nation of the other grades. All Root Mean Squared Errors of Approximation and all Standardised Root Mean Squared Residuals were equal to 0.0 while all Tucker-Lewis Indices and all Comparative Fit Indices were equal to 1.0 except for the CFI of the analysis in month3 between the three    Table 4: Spearman correlation coefficients for individual grades (tutor-based) and widget indicator scores (widget-based) based on the individual months from the online study in 2015-2016, n=33.  W1 i n i t i a t i v e W2 r e s p o n s i v e n e s s W3 p r e s e n c e m1 m2 m3 m4 m5 m1 m2 m3 m4 m5 m1 m2 m3 m4 m5  T1 Corr. .336 .154 .188 .200 .297 .421* .130 .116 .571** .580** .221 .024 .119 .453** .407*  Sig. .056 .391 .295 .265 .094 .015 .470 .522 .001 .000 .217 .895 .511 .008 .019  T2 Corr. .354* .118 .231 .299 .290 .374* .101 .274 .599** .641** .103 -.120 .018 .393* .365*  Sig. .043 .512 .195 .091 .102 .032 .577 .123 .000 .000 .569 .507 .921 .024 .037  T3 Corr. .305 .036 .124 .371* .362* .331 .039 .149 .641** .656** .045 -.142 -.013 .481** .443**  Sig. .084 .844 .491 .034 .039 .060 .830 .407 .000 .000 .805 .431 .942 .005 .010  T4 Corr. .372* .098 .174 .306 .342 .378* .064 .212 .609** .669** .146 -.082 .072 .458** .424*  Sig. .033 .586 .333 .083 .051 .030 .723 .237 .000 .000 .416 .650 .689 .007 .014  **. significant at the 0.01 level (2-tailed). *. significant at the 0.05 level (2-tailed).  Table 5: Standardised path coefficients () for the individual grades (tutor-based) and the widget indicator scores (widget-based) based on the individual months from the online study in 2015-2016, n=33.  m o n t h 1 m o n t h 2 m o n t h 3 m o n t h 4 m o n t h 5 W1 W2 W3 W1 W2 W3 W1 W2 W3 W1 W2 W3 W1 W2 W3  T1  .228 .499** -.232 .313 .217 -.335 -.036 -.029 .231 -.314 .388* .418 -.225 .411* .284  Sig. .208 .004 .208 .128 .253 .110 .909 .879 .441 .177 .028 .074 .389 .046 .346  T2  .257 .480** -.272 .352 .290 -.524** .110 .150 -.016 -.055 .513** .125 -.219 .622** .129  Sig. .157 .007 .140 .072 .105 .007 .726 .421 .957 .811 .002 .591 .367 .001 .645  T3  .213 .476** -.267 .160 .206 -.332 -.113 .045 .208 .037 .436** .185 -.117 .498** .213  Sig. .253 .009 .157 .456 .290 .122 .724 .814 .491 .870 .008 .417 .634 .008 .451  T4  .237 .504** -.261 .288 .246 -.407* -.017 .054 .154 -.123 .462** .261 -.194 .526** .220  Sig. .189 .004 .155 .161 .191 .048 .957 .777 .611 .589 .005 .257 .432 .005 .439  **. significant at the 0.01 level (2-tailed). *. significant at the 0.05 level (2-tailed).  Table 6: Spearman correlation coefficients and stan- dardised path coefficients () for individual grades (tutor-based) and widget indicator scores (widget- based) based on the entire length of the run from the online study in 2015-2016, n=33.  correlations coefficients standardised path coefficients W1 W2 W3 W1 W2 W3  T1 Corr. .234 .508** .281  .190 .366 -.091  Sig. .189 .003 .113 Sig. .452 .063 .702  T2 Corr. .285 .518** .168  .299 .500** -.323  Sig. .108 .002 .351 Sig. .200 .005 .142  T3 Corr. .266 .512** .231  .214 .404* -.148  Sig. .135 .002 .197 Sig. .389 .036 .530  T4 Corr. .285 .527** .238  .238 .438* -.185  Sig. .108 .002 .183 Sig. .326 .019 .420  **. significant at the 0.01 level (2-tailed). *. significant at the 0.05 level (2-tailed).  indicators and grade T4 which was equal to 0.0. In the exploratory study, all grade-indicator combinations  except the one between T1 planning & progress/W3 pres- ence yielded significant and positive correlations when mea- suring the students activity over the entire length of the run. In the online study, however, W2 responsiveness is the only widget indicator that positively and significantly  correlates with the four grades (see Table 66). All grade-W2 correlations are significant at the 0.01 level and higher than .500. That is, there are less significant correlations in the online study than in the exploratory study but those that are significant are stronger.  When calculating the correlations for the online study per month instead of the whole run, the results are again quite different from those in the exploratory study. In the ex- ploratory study the scores of the indicators W1 initiative and W2 responsiveness correlated significantly with all four grades in months 1, 2, 3 and 4 with W2 also significantly correlating with the grades T2, T3 and T4 in month5. The indicator W3 presence had the smallest number of signifi- cant correlations with the different grades that were rather low. The strongest correlations were obtained between W2 and all grades in month2. Looking at the individual month, the correlation results from the online study with the live activity widget here also look quite different (see Table 4). Overall there are now less significant correlations and hardly any in month1 or month2. The strongest correlation coeffi- cients (ranging from .571 to .669) are received between the W2 responsiveness indicator and the four different grades in month4 and month5. All of them are significant at the  6Due to lack of space we only show the online study re- sults (for the correlation as well as the regression analyses). Please refer to [17] for detailed results of the exploratory study.    Figure 4: Average scores of the twelve ELFA items on the left and the four criteria on the right for students and tutors for both rounds.  0.01 level. Additionally, the previously low scoring W3 pres- ence indicator now obtains high and significant correlations with all four grades in month4 and month5.  Conducting the structural equation modelling over the en- tire length of the run in the exploratory study showed that all three widget indicator scores could be used as predictors for all four grades except the T1 planning & progress / W3 presence combination. The W2 responsiveness was the strongest and most significant predictor. In our current online study, there are only three predictive relations (see Table 6), i.e. the W2 responsiveness indicator is a predic- tor for the grades T2 contribution to the team, T3 support and T4 individual-overall. None of the other indicators can be used as predictors.  Comparing the regression analysis results for the individ- ual months from the exploratory study with the online study again reveals a number of differences. Previously the W2 responsiveness indicator was a predictor for all grades in all months with month1 and especially month2 providing the strongest predictive relations. The W1 initiative indi- cator received a predictive relation with all four grades in month1 and month3 while the W3 presence indicator was negatively predictive for the T1 planning & progress grade only. In the online study, however, the W2 responsiveness indicator can only be used as a predictor in month1, month4 and month5 with the latter one holding the strongest pre- dictive relations (see Table 5). The W1 initiative indicator is in no predictive relation with any of the grades in any of the months. The widget indicator scores of W3 presence, though, are in a significant negative predictive relation with the grades T2 contribution to the team and T4 individual- overall in month2.  3.2 Widget Evaluation In order to gauge how the learners and tutors of EVS  evaluate their experience with the activity widget, we asked them to fill out the Evaluation Framework for Learning An- alytics (EFLA) questionnaire. As we distributed the ques- tionnaire twice during the course, we are able to compare not only the two user types with one another but also any changes in the users perception of the activity widget over time. Figure 4 shows the average scores of the twelve ques-  tionnaire items as well as the combined criteria for both user types and both rounds.  On average students as well as tutors rated awareness and reflection items higher than the items of the data and im- pact criteria. Also, while the students on average rated the activity widget more positively in the middle of the course, tutors gave more positive ratings at the end of the course.  Conducting a t-test for the four criteria allowed us to see whether the differences between the two user types or be- tween the two rounds were significant or not. Table 8 shows the mean, standard deviation and standard error mean for the answers given by students and tutors in rounds 1 and 2. We conducted t-tests for four different settings. First, we compared the answers from the students to those from the tutors in round 1 and round 2. We then compared the answers from round 1 to those from round 2 for each user group. Table 7 shows the Levenes test as well as the t-test results for the four different settings.  There are two cases where the differences in ratings are significant. The first one is the rating of the awareness criterion when comparing students and tutors in round 1: t(28) = 2.158, p = .040. The second one is the rating of the reflection criterion when comparing round 1 and round 2 of the students: t(47) = 2.110, p = .040. None of the other t-tests obtained significant results at the .05 or even the .01 level. In two cases the equality of variance could not be as- sumed due to the results of the Levenes test. Both of those cases involved the ratings for the reflection criterion from the tutors in round 1, which are rather low, but did not yield significant t-test results. If the equality of variance had been assumed for those cases, however, the difference in ratings between students and tutors for the reflection criterion in phase 1 would have been highly significant (0.006).  From the open ended questions at the end of each EFLA questionnaire we were able to gather some qualitative feed- back about the students and tutors impression of the acti- vity widget. Generally most students liked the idea behind the dashboard and appreciated to see their platform acti- vities being set in relation to those of their team members. Many students, however, mentioned several issues they were concerned about: The activity widget was not able to reflect activities outside the platform nor did it take the quality of    Table 7: Results of the Levenes tests and the t-tests for four different settings  Round1: students vs tutors Round2: students vs tutors Students: round1 vs round2 Tutors: Round1 vs Round2 Levenes test t - t e s t Levenes test t - t e s t Levenes test t - t e s t Levenes test t - t e s t  F Sig. t df Sig. F Sig. t df Sig. F Sig. t df Sig. F Sig. t df Sig.  D .209 .651 1.555 28 .131 1.006 .324 -.952 29 .349 .132 .718 1.311 47 .196 .024 .879 -1.078 10 .306 A 2.903 .099 2.158 28 .040 .129 .722 .468 29 .643 2.998 .090 1.350 47 .183 .009 .924 -.376 10 .714 R 4.555 .042 2.236 5.994 .067 2.514 .124 -.273 29 .787 2.889 .096 2.110 47 .040 7.401 .022 -1.327 6.988 .226 I 1.357 .254 .435 28 .667 1.114 .300 -.891 29 .380 1.639 .207 .860 47 .394 1.209 .297 -.572 10 .580  Table 8: Statistics of the EFLA results for students and tutors for both round  S t u d e n t s T u t o r s round n Mean Std.Dev. St.Er. n Mean St.Dev. St.Er.  D 1 24 12.21 3.659 .747 6 9.33 5.502 2.246 2 25 10.84 3.648 .730 6 12.50 4.637 1.893  A 1 24 13.83 3.002 .613 6 10.17 6.014 2.455 2 25 12.32 4.634 .927 6 11.33 4.633 1.892  R 1 24 12.58 3.296 .673 6 7.50 5.320 2.172 2 25 10.12 4.720 .944 6 10.67 2.422 .989  I 1 24 9.00 3.901 .796 6 8.17 5.345 2.182 2 25 8.00 4.223 .845 6 9.67 3.559 1.453  the posts into account. Some students complained that they noticed people posting irrelevant things in order to achieve higher scores. Some students, though, were made aware that they indeed did less than their team mates and thus partic- ipated more in the group.  The tutors also expressed their appreciation of the activity widget in the open ended comments and generally liked hav- ing the widget as a reference. For most of them, the activity widget confirmed their own impression about their students throughout the course. With regard to the widget fostering reflection about their own tutoring style it was mentioned that such support would be especially useful in those cases where the student groups do not work together well as the tutors could then use the widget to detect such issues early on. One concern the tutors also had was that the activity widget only reflects actions within the EVS platform and that any work the students do with other online tools is not included.  4. DISCUSSION When student activity is calculated over the whole run of  the course, the Spearman correlation results show that in the online study the scores of the W2 responsiveness in- dicator correlate significantly and positively with the four grades. Research question A1 can thus be answered with a yes. However, while in the exploratory study the scores of all three widget indicators correlated significantly and posi- tively with at least three if not all four of the grades, in the online study only W2 responsiveness did. But although there are now less correlations that are significant, those that are significant are very strong. Regarding the increase of strength of the Spearman correlation results in our online study, similar results are achieved when calculating the acti- vity for the individual months instead of over the whole run of the course. In comparison to the exploratory study, there are also less correlations that are significant in the individual months but those that are significant are very strong.  Going into the online study, we had expected something  like this to happen. Of the three indicators analysed, W2 re- sponsiveness, i.e. commenting on the posts, pages or files of others, is the one that best represents team interaction and collaboration. With the activity widget in use during the course, we expected it to foster the students awareness and reflection about their position within the team and the team as a whole and to thus facilitate collaboration processes.  The standardised path coefficients from the structural equa- tion modelling (see Tables 5 and 6) show that there are indeed significant predictive relations between some of the widget indicator scores and the grades. As with the correla- tion coefficients, only the W2 responsiveness scores receive significant results when looking at the entire length of the run. Slightly more diverse results become apparent when looking at the standardised path coefficients for the differ- ent months, e.g. in month2 the score of the W3 presence indicator can also be seen as a predictor for some of the grades. However, the best and by far the most frequent predictor for all four grades are the scores from the W2 re- sponsiveness indicator. Research question A2 can thus also be answered with a yes. Since the W2 responsiveness in- dicator scores, although surprisingly at the time, had been by far the best predictor in the exploratory study and since we had anticipated an increase of team interactions due to the widget triggering awareness and reflection processes, we had expected this indicator to be the best overall predictor in the online study as well.  What surprised us, however, was that in the online study none of the predictive relations involved the W1 initiative indicator. As presented earlier, the 2015-2016 run had a significantly lower number of initiative posts per student. When looking into the log data from this year it became apparent that the number of posted files (which is by far the major contributor to the initiative score) was lower during the year of the online study. This may be explained by an increased use of external tools already from early on in the course which cannot be logged and was thus not included in the calculation of the widget indicator scores.  During the exploratory study this use of external tools, especially during the second half of the course where the different group reports needed to be written, turned out to be the most likely explanation for the widget indicators of the first half of the course to be better predictors than those of the second half. As the students in the online study also made use of such external tools, we expected the same to be true for the 2015-2016 run even though the widget was now in use. However, research question A3 has to be answered with a no as the strongest correlations and best predictive relations are now more likely to happen towards the end of the course.  This shift to the last few months now being the main source for widget scores with predictive power is already in-    dicated by the correlation results: all grade / W2 as well as all grade / W3 combinations in month4 and month5 are significantly and positively correlated with correlation co- efficients ranging from .365 to .669. Month2 and month3 do not show any significant correlations in the online study whereas they did so for many grade / widget indicator com- binations in the exploratory study. With regard to predic- tive relations, while none of the widget indicator scores from month3 can be used as a predictor for any of the grades, there are two predictors in month2: The scores of the W3 presence indicator are in a significant negative predictive relation with the grades T2 contribution to team and T4 individual-overall. With regard to the best predictor, the results of the regression analysis in the online study confirm the afore mentioned shift and show that for three grades the best predictors are the scores of the W2 responsive- ness indicator in month5, except the grade T1 planning & progress that is best predicted by the W2 responsiveness indicator score in month1.  In the previous years, the widget indicator scores in the last months were poor predictors of the grades, which we attributed to the students mostly using non-logged external tools in this period. In the online study, the widget indica- tor scores in the last months were the best predictors of the grades. We can think of two probable causes of this shift. First, students that were initially less active may have been stimulated by widget feedback to become more active, re- sulting in better grades. Second, students, aware that their activities with the external tools were not captured by the activity widget, posted more frequently on the EVS plat- form as they wanted the widget to reflect their being active in the course.  Another surprising observation for us was the students neglect of the privacy protection option through the recip- rocal privacy model. None of the students disabled this func- tionality to mask their data from their team. This could be due to the nature of the collaborative learning process that requires to be aware of the status of other students. In fact, we received generally positive responses from the students about the activity widget and that it indeed supported their team awareness processes as well as added a fun factor to the online learning environment.  The results of the formal evaluation of the activity wid- get using the EFLA questionnaire show that the answer to research question B1 is yes, but for the students reflection criterion only as their reflection ratings in round 2 were sig- nificantly lower than those in round 1. In all other cases, neither for the students nor the tutors was there a signifi- cant difference in evaluation results between the two rounds. From what we were able to gather from the open ended ques- tions as well as the discussion thread, this difference was most likely due to the students feeling less accurately repre- sented the more the course progressed as the activities of the external tools was not reflected in the widget scores. When comparing the evaluation results from the two user groups with one another, the only significant difference is that of the awareness criterion in round 1. Here, students have rated the awareness items significantly higher than the tutors did. In all other cases, neither in round 1 nor in round 2 was there a significant difference in evaluation results between the two user groups. Research question B2 can therefore be answered with yes, but for the awareness criterion of phase 1 only. This is most likely due to the generally positive  reception of the activity widget by students already at the beginning of the course while tutors used and thus appreci- ated the widget more towards the end of the course when they saw their personal impressions about the students con- firmed. Except for those two cases, students and tutors thus evaluated the activity widget in a very similar way.  Combining the EFLA results with the comments gathered via the open ended questions allows us to conclude that both students and teachers generally liked and appreciated the activity widget and felt supported in their awareness and reflection processes. Both user groups, however, had issues with the widgets data access (D2) as well as its support of more efficient (I2) and more effective (I3) learning. Ad- ditionally, both user groups found it problematic that the activity from external tools could not be included in the widget. Students would also like to see not only the quan- tity but also the quality of their discussion posts to be taken into account as they otherwise fear that too many irrelevant message are posted to increase the widget indicator score.  We had already identified the risk of students playing the system during our exploratory study and had thus provided a detailed user manual at the beginning of the 2015-2016 course explaining the activity widgets aim and function- alities. This, although being an important step, however, does not seem to have been enough. As emphasised in other studies [12, 1, 10] learning analytics visualisations need to be tightly embedded into a courses instructional design, es- pecially if they are to be used by the students themselves. For the next run of EVS we will therefore carefully take the gathered results into account in order to improve the activity widget as well as the instructional design and to enhance the user experience.  There are several limitations of our study. Due to the change in student population, the students behaviour in the five different runs cannot be set into a one-to-one relation. Their previous experience with and usage of online learning platforms as well as external communication and collabora- tion tools influences the cohorts actions. The same applies to the tutors. Although many of them have been tutors for EVS for a number of years, their experience and interac- tions with their student groups also changes from year to year. Related to this aspect of change in student popula- tion, student and tutor behaviour as well as external tools is another aspect that has to be kept in mind when looking at the results of our online study: although a number of our observations can be explained as effects of the activity wid- get being in use, there is no proof that this is the case. Only after observing and analysing further years of the EVS will we be able to attribute differences between the years that did not have the widget and those that did clearly to the use of the widget.  5. CONCLUSIONS This paper presented an empirical study conducted with  data collected during the five months of a live Master course where students work collaboratively in virtual teams. We implemented a learning analytics-based activity widget to foster awareness and reflection among the team members into the courses online learning platform and examined the predictive power of the widget indicators towards the stu- dents grades of this course in comparison to the data from previous years where the widget had not been in use. Our re- sults indicate that the widget indicator responsiveness, i.e.    the number of response posts made on the courses platform, is a significant positive predictor towards the grades. In the years without the widget, the students behaviour of the first few months of the course held more predictive power, whereas in the year where the widget was implemented into the platform, the last few months of the course had a higher predictive potential. This, in combination with the results from a quantitative as well as qualitative evaluation of the activity widget during the course, suggests that the differ- ences between the years could be explained by the use of the widget and its effective fostering of awareness and reflection. More investigations are needed in order to provide further evidence that can substantiate this hypothesis and confirm the effectiveness of the widget. We will therefore continue to deploy the activity widget in future editions of the course.  6. ACKNOWLEDGMENTS This work was partly funded by the LACE project (GA  No. 619424) under the FP7 programme of the European Commission.  7. REFERENCES [1] S. Beheshitha, M. Hatala, D. Gasevic, and  S. Joksimovic. The role of achievement goal orientations when studying effect of learning analytics visualizations. In Proc. of the 6th Int. Conf. on Learning Analytics & Knowledge, LAK 16, pages 5463, New York, NY, USA, 2016. ACM.  [2] D. Butler and P. Winne. Feedback and self-regulated learning: a theoretical synthesis. Review of Educational Research, 65(3):245281, 1995.  [3] J. de Kraker and R. Corvers. European Virtual Seminar on Sustainable Development: international, multi-disciplinary learning in an online social network. E-learning and Education for Sustainability, Series Environmental Education, Communication and Sustainability, 35:117136, 2014.  [4] H. Drachsler and W. Greller. Privacy and Analytics - its a DELICATE issue. A Checklist to establish trusted Learning Analytics. In Proc. of the 6th Int. Conf. on Learning Analytics & Knowledge, LAK 16, pages 8998, New York, NY, USA, 2016. ACM.  [5] M. R. Endsley. Toward a Theory of Situation Awareness in Dynamic Systems. Human Factors, 37:3264, 1995.  [6] M. R. Endsley. Theoretical underpinnings of situation awareness: a critical review. In M. R. Endsley and D. J. Garland, editors, Situation Awareness Analysis and Measurement, pages 328. Lawrence Erlbaum Associates, Mahwah, NJ, USA, 2000.  [7] R. Ferguson and D. Clow. Learning Analytics Community Exchange: Evidence Hub. In Proc. of the 6th Int. Conf. on Learning Analytics & Knowledge, LAK 16, pages 520521, New York, NY, USA, 2016. ACM.  [8] M. Fishbein and I. Ajzen. Predicting and Changing Behavior: The reasoned action approach. Psychology Press, New York, NY, USA, 2010.  [9] J. Hattie and H. Timperley. The Power of Feedback. Review of Educational Research, 77(1):81112, 2007.  [10] I. Khan and A. Pardo. Data2U: scalable real time student feedback in acive learning environments. In  Proc. of the 6th Int. Conf. on Learning Analytics & Knowledge, LAK 16, pages 249253, New York, NY, USA, 2016. ACM.  [11] P. Kirschner, K. Kreijns, C. Phielix, and J. Fransen. Awareness of cognitive and social behaviour in a CSCL environment. Journal of Computer Assisted Learning, 31(1):5977, 2015.  [12] S. Lonn, S. Aguilar, and S. Teasley. Investigating student motivation in the context of a learning analytics intervention during a summer bridge program. Computers in Human Behavior, 47:9097, 2015.  [13] L. McAlpine and C. Weston. Reflection: Issues related to improving professors teaching and students learning. Instructional Science, 28(5):363385, 2000.  [14] E. H. Mory. Feedback Research Revisited. In D. H. Jonassen, editor, Handbook of Research on Educational Communications and Technology, pages 745783. Lawrence Erlbaum Associates, Mahwah, NJ, US, 2004.  [15] C. Phielix, F. Prins, P. Kirschner, G. Erkens, and J. Jaspers. Groups awareness of social and cognitive performance in a CSCL environment: Effects of a peer feedback reflection tool. Computers in Human Behavior, 27:10871102, 2011.  [16] M. Scheffel, H. Drachlser, and M. Specht. Developing an evaluation framework of quality indicators for learning analytics. In Proc. of the 5th Int. Conf. on Learning Analytics & Knowledge, LAK 15, pages 1620, New York, NY, USA, 2015. ACM.  [17] M. Scheffel, H. Drachsler, J. de Kraker, K. Kreijns, A. Slootmaker, and M. Specht. Widget, widget on the wall, am I performing well at all IEEE Transactions on Learning Technologies, PP(99):11, 2016.  [18] M. Scheffel, H. Drachsler, S. Stoyanov, and M. Specht. Quality Indicators for Learning Analytics. Educational Technology & Society, 17(4):117132, 2014.  [19] D. Schon. The reflective practitioner: How professionals think in action. Temple Smith, London, UK, 1983.  [20] G. Siemens, S. Dawson, and G. Lynch. Improving the Quality and Productivity of the Higher Education Sector - Policy and Strategy for Systems-Level Deployment of Learning Analytics. SoLAR report for the Australian Government Office for Learning and Teaching, December 2013.  [21] A. Slootmaker, M. Scheffel, K. Kreijns, J. De Kraker, and H. Drachsler. Performance dashboard to support awareness and reflection in elgg communities (version 1.15) [software], 2015.  [22] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. Santos. Learning Analytics Dashboard Applications. American Behavioral Scientist, 57(10):15001509, 2013.  [23] P. H. Winne. Inherent Details in Self-Regulated Learning. Educational Psychologist, 30(4):173187, 1995.  [24] P. H. Winne. How Software Technologies Can Improve Research on Learning and Bolster School Reform. Educational Psychologist, 41(1):517, 2006.  [25] B. J. Zimmerman. Self-Regulation Involves More Than Metacognition: A Social Cognitive Perspective. Educational Psychologist, 30(4):217221, 1995.    "}
{"index":{"_id":"37"}}
{"datatype":"inproceedings","key":"Koester:2017:BTF:3027385.3027418","author":"Koester, Benjamin P. and Fogel, James and Murdock,III, William and Grom, Galina and McKay, Timothy A.","title":"Building a Transcript of the Future","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"299--308","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027418","doi":"10.1145/3027385.3027418","acmid":"3027418","publisher":"ACM","address":"New York, NY, USA","Abstract":"The pathways and learning outcomes of university students are the culmination of numerous experiences inside and outside of the classroom, with faculty and with other students, in both formal and casual settings. These interactions are guided by the general education requirements of the university and by the learning goals of the student. The only official record and representation of each student's education is captured by their academic transcript: typically a list of courses described by name and number, grades recorded on an A-F scale and summarized by GPA, degrees awarded, and honors received. This limited approach reflects the technological affordances of a 20th century industrial age. In recent years, scholars have begun to imagine a transcript of the future, perhaps combining a richer record of the student experience along with a portfolio of authentic products of student work. In this paper, we concentrate on first, and develop analytic methods for improving measures of both classroom performance and intellectual breadth. In each case, this is done by placing elements of individual transcripts in context using information about their peers. We frame the study by addressing basic questions. Were the courses taken by the student difficult on average? Did the individual stand out from their peers? Were the courses representative of a broad intellectual experience, or did the student delve into detail in the chosen field of study? And with whom did they take courses?","pdf":"Building a Transcript of the Future  Benjamin P. Koester University of Michigan  Deptartment of Physics 450 Church St.  Ann Arbor, MI 48109 bkoester@umich.edu  James Fogel University of Michigan  Department of Economics 611 Church St.  Ann Arbor, MI 48109 jsfog@umich.edu  William Murdock III Harvard University  Department of Economics 1805 Cambridge Street Cambridge, MA 02138  wmurdock@g.harvard.edu Galina Grom  University of Michigan Deptartment of Physics  450 Church St. Ann Arbor, MI 48109 grom@umich.edu  Timothy A. McKay University of Michigan  Deptartment of Physics 450 Church St.  Ann Arbor, MI 48109 tamckay@umich.edu  ABSTRACT The pathways and learning outcomes of university students are the culmination of numerous experiences inside and out- side of the classroom, with faculty and with other students, in both formal and casual settings. These interactions are guided by the general education requirements of the uni- versity and by the learning goals of the student. The only official record and representation of each students education is captured by their academic transcript: typically a list of courses described by name and number, grades recorded on an A-F scale and summarized by GPA, degrees awarded, and honors received. This limited approach reflects the techno- logical affordances of a 20th century industrial age. In recent years, scholars have begun to imagine a transcript of the fu- ture, perhaps combining a richer record of the student expe- rience along with a portfolio of authentic products of student work. In this paper, we concentrate on first, and develop analytic methods for improving measures of both classroom performance and intellectual breadth. In each case, this is done by placing elements of individual transcripts in context using information about their peers. We frame the study by addressing basic questions. Were the courses taken by the student difficult on average Did the individual stand out from their peers Were the courses representative of a broad intellectual experience, or did the student delve into detail in the chosen field of study And with whom did they take courses  CCS Concepts Applied computing  Computer-assisted instruc- tion; General and reference  General conference pro- ceedings; Networks Network economics; Information  Copyright is held by theowner/author(s). LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 ACM. ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3027418  systems  Data analytics;  1. INTRODUCTION An academic transcript summarizes the career of a stu-  dent. University registrars record and store the grades, de- grees, courses, and other credentials that students seek when they enroll [10]. Students may then choose to share them with institutions where they continue their studies, or with potential employers. As such, the transcript serves first and foremost as an official validation that a student did indeed participate in academic activities at the university (took classes, earned a degree). It also forms a learning profile of the student; a representation of the education they have received. The courses, grades received in those courses, a degree, and some indication of the area of study provide our only record of the way each student met the educational goals of the institution (e.g. [19]). Institutions use their collection of transcripts to better understand the progress of students through their campuses. Individuals charged with evaluating students - typically graduate or professional school admissions teams and potential employers - learn to read the genre of the transcript with care, attempting to glean deeper information about academic performance, in- tellectual breadth, disciplinary depth, and effort. Often this interpretation of the transcript becomes baroque, creating a narrative from crumbs of information. Students, too, re- act to the transcript, attending closely to things which we record (like GPA) while discounting those we dont (intel- lectual breadth or effort).  Clearly the traditional transcript paints an incomplete picture of the student experience both in and out of the classroom ([2]). Hence, many have advocated for an e- portfolio [18, 20] combining information about courses and co-curricular activities [9] with content produced by the stu- dent, all with the official stamp that accompanies a tran- script. For the student, a portfolio that grows over an aca- demic career can engage the individual in reflection on pro- gression toward future goals. The process of metacognition is known to enhance learning [4].  As we work toward a rich transcript of the future, we be-  rodkin Typewritten Text This work is licensed under a Creative Commons  Attribution International 4.0 License.  https://creativecommons.org/licenses/by/4.0/ https://creativecommons.org/licenses/by/4.0/   gin by asking basic questions. Absent appropriate context, many elements of the traditional transcript are difficult to interpret. For instance, were the courses taken difficult In what areas, and to what degree, did this individual stand out from their peers Were the courses taken representa- tive of a relatively broad intellectual experience, and how deeply did this student delve into detail in the chosen field of study With whom did they take courses - were they iso- lated with a homogenous group, or did they interact with a diverse cohort Even today, advances in the availability and processing of information allow each transcript to be placed in context, becoming a richer, more informative portrait of a student. Relatively simple calculations, based entirely on current administrative data, can provide substantially re- fined measures of relative student performance, along with more insight into the nature of the courses taken and diver- sity of classmates.  1.1 Use of Grades in the Transcript Letter grades are the common currency of academic suc-  cess in higher education [23]. While their apparent similar- ity suggests that all As are equal, challenges to their ef- fective exchange emerge at many levels. Differences among disciplines in content, evaluative style, grading practice, and strength of students make grades awarded in different classes difficult to compare [12, 14]. Reward systems in higher edu- cation use grades as incentives in ways which further distort their utility for both students and faculty [1] and periodic attempts to regularize grades across courses and disciplines have found little success (e.g. [22]). Some authors have been sufficiently troubled to declare the use of grades uneth- ical:inconsistent grading policies render the competitions for academic awards unfair, deprive people of positions they merit, and leave people, students, institutions, and societies less well off than they would otherwise be. [14].  Insofar as grades continue to be an integral part of the transcript, derivatives of them will continue to be used as summary statistics. The grade point average (GPA), typi- cally a credit-hour weighted-average, is the most widespread. On its face, GPA cannot be a measure of how much a stu- dent has learned: it does not grow when a student takes more courses. Indeed, it may fall as a student learns more. Imag- ine the dismay of the 4.0 GPA student receiving their first B+ in a challenging junior level class. Despite learning more - perhaps a lot more - their GPA has fallen [16]. Instead, GPA serves the same dual purpose present in all grades. It is considered both as a summary of a students mastery of course material and a tool for ranking and locating each stu- dent among their peers. Both purposes are undermined by inconsistency of practice: mean grades given out in courses at the University of Michigan vary by more than 25% across the disciplines. These variations provide false signals, sug- gesting to all students particular talent in courses with high average grades and a lack of ability in courses with low aver- age grades. Ranking too is harmed. Just as in Meyers time [17], a student may obtain the same high GPA through ex- ceptional performance in courses giving low average grades or average performance in classes giving high average grades. Perhaps the simplest paliative for grading dispersion is to re- port a mean course grade alongside each students grade on a transcript. This approach was adopted, for example, at Dartmouth in 1994 [6], but it does nothing to alter GPA or  affect its primary uses.  In the outside world, GPA is also used in several ways. In some contexts, it is seen as a measure of tenacity. Those who achieve a high GPA have demonstrated a sustained ability to meet whatever requirements have been placed on them. In other contexts, GPA is used as a predictor of future per- formance. Those with high GPAs are expected to perform well in subsequent endeavors. Is GPA adequate for these purposes Perhaps. Employers, graduate and professional admissions committees, and those administering University honors of various kinds have used GPA for a century, and found value throughout [13]. But there are real reasons for concern, and several alternatives to GPA have been con- sidered, most of which acknowledge the considerable effort expended to assign grades, but suggest that the information is best applied in a relative sense [14].  1.2 Alternatives to GPA Practical alternatives to the GPA begin with grades as  reported by instructors, then utilize information contained in the distribution of grades to refine measures of ranking [25, 15, 5, 11, 3]. Larkey and colleagues [15] were among the first to use basic linear models in an effort to extract mea- sures of student achievement. Caulkins et al. [5] presented several methods that recompute GPA after normalizing stu- dent grade distribution according to other students in the same course. In essence, each of these takes existing grad- ing data as-is and calculates a metric that is less sensitive to the peculiarities of grading practices among courses and more indicative of student performance. Perhaps the most sophisticated of these efforts, a Bayesian latent trait formu- lation of a summary statistic to replace GPA-based measures [12], was considered for use by the faculty at Duke, but ulti- mately rejected [8]. So far, none of these schemes has entered widespread use. In what follows we consider several possible replacements for GPA.  1.3 Courses and Classmates The transcript also provides insight into the breadth and  depth of each students course of study. Readers of a tran- script place the list of courses a student takes into context using a combination of their experience and imagination. Institutions might provide much more precise insight, clari- fying how students met their graduation requirements. In- stitutions might also place the intellectual breadth and disci- plinary depth of courses taken by a student in local context, comparing each students transcript to those of their peers, both within their discipline and across the institution. In an important sense, the list of courses taken by a student pro- vides a measure of their success or failure to meet the goals of a liberal education, regardless of peformance. It also en- codes information about the students creativity in meeting course distribution requirements, whether that means expo- sure to many new topics or detailed study in a few.  Institutions which have access to the transcripts of all their students can use this information to add important context to what they present for each student. They can refine their measures of student performance, perhaps aug- menting or replacing GPA. They can also place the course- taking and enrollment patterns in context, along with the diversity of individuals with which the student took classes.    A student taking courses that expose them to peers from many different majors, backgrounds, and identities likely has a much different experience from one who primarily inter- acts with students like them in course of study, background, or identity.  Advances in the use of information technology in edu- cation have revitalized the use of data in evidence-based advising and decision making [3]. A new field of Learning Analytics has emerged, working to use data about learners and their environments to understand and optimize learn- ing [24]. Perhaps the time has come to seriously reconsider the ways in which we represent a students performance in college. In this work, we explore a few small steps toward the transcript of the future. Keeping in mind the intent of the transcript - to accurately summarize and represent a students education  we explore several alternatives to the GPA and their sensitivities, as well measures of intellectual breadth and diversity of experience.  2. DATA The University of Michigan DataWarehouse contains grades,  student admissions information, and demography back to Fall 1974. In this work, we are chiefly concerned with grades, which are complete back to at least Fall 1998. Thus, we consider student course data from Fall 1998 through Win- ter 2014 on the Ann Arbor Campus. This data set in- cludes 4,384,169 grades for 192,987 students, who received grades in 15,571 courses distributed among 318 subjects. We only consider grades for students that took a course for credit and were still enrolled at the end of term. We ig- nore withdrawals, incompletes, audits, and pass/fail evalu- ated courses. For students that took a course multiple times, we consider only the last attempt, mirroring the U-Ms stan- dard practice used to compute a GPA. This gives a grand total of 132,223 course-terms over 16 years.  Where we are concerned only with student grades, our student sample is quite liberal. Students who transfer into and out of the University and across its many colleges are retained. Students who arrived on campus before Fall 1998, or who had not yet completed their degrees in Winter 2014 are also included. Some statistics require a students com- plete undergraduate record for calculation. These include measures of academic performance at graduation and the diversity of subjects and classmates with whom the student interacted. For these statistics, we further limit our data set to include only students admitted as freshmen since Fall 2005, who took at least 30 courses on campus, and gradu- ated.  At Michigan, majors of students are only finalized upon graduation; they are not expected to declare an intended major before matriculation to campus, and usually declare an intent only later in their sophomore year. As at most uni- versities, instructors award letter grades which are then con- verted to grade points: an A = 4.0, A- = 3.7, B+ = 3.3, ..., E = 0. We note that the grade points given for plus grades to Business School undergraduates are unusual. A+ = 4.4, B+= 3.4, and so forth. In this analysis, these are normalized to the grade points given by the rest of the University. We also only consider undergraduate courses at the University of Michigan, which are numbered between 100 and 499. 100  and 200 level courses are mostly introductory lectures, dis- cussion, laboratories, and seminars, and they contain among their ranks the highest enrollment courses on campus. By contrast, 300 and 400 level courses are typically directed at majors and have smaller enrollments formatted as small lec- tures, laboratories, and research for credit. Course types and size vary dramatically, ranging in enrollment from 1 to more than 2000 in a term and in structure from apprenticeship to massive lecture. Course content spans the intellectual range of the university, and both instructional style and grading practice vary with discipline.  2.1 Placing Student Performance in Context A typical transcript reports a single students list of courses  taken and grades received without context. Little infor- mation is provided about what the courses were like, who took and taught them, how they were taught, evaluated, or graded. Lacking this context, the transcript is difficult to in- terpret. Given that registrars possess information adequate to provide substantial context, we argue that it should be put to use.  Consider the list of the courses a student took and the grades they received. Since courses are listed only by name and number, the reader must guess at content studied, work done, nature of the instructor or classmates, evaluative style, or grading practice. If we consider instead the context avail- able from the full collection of student transcripts, substan- tially more information is available. It becomes possible to compare this students performance to others in this class, even to relevant subsets of others (e.g. those who continued on to complete the same major). We can learn more about who takes this class; what they take before or concurrent with it, what they continue on to study, and what degrees they ultimately receive. We can see whether students in this class typically receive grades higher or lower than they get in other courses. Analysis of the full network of courses and student grades allows us to knit together performance com- parisons even between students who were never coenrolled in a class. In this context, grades and courses taken may be viewed through a different lens. Of course the quality of these relative performance measures depends on how deeply each student is embedded in the campus network of coen- rollment. As a result, we begin with some descriptive explo- ration of how many classmates each student in our sample has encountered directly.  Undergraduate degree-seeking students need 120 credits to graduate, which typically requires taking between 30 and 50 courses. Usually, before declaring a major, they take an array of introductory and prerequisite courses which include classes both small (writing courses and seminars) and large (lecture classes with enrollments from 50 to 2000). Most direct comparisons among coenrolled students take place in these large lecture courses. To give some sense for the num- bers of classmates encountered by each student during their career, Figure 1 shows the number of total classmates for sets of students that, at any point in their career, took one of a selection of introductory courses at Michigan. These courses are part of the Psychology, English, Chemistry, and Statis- tics Majors, but are taken by large numbers of non-majors as well. Over their careers, the students in our sample ac- cumulate anywhere from a dozen or so classmates to nearly    25,000, with typical values ranging from 7,500 to 12,500. Grades handed out in these head-to-head comparisons form the basis of the grading system and the grade point averages that characterize students performance over their career.  Number of classmates  N um  be r o  f S tu  de nt  s  0 5000 10000 15000 20000 25000  0 20  0 40  0 60  0 80  0  PYSCH111 CHEM210 STATS350 ENGL225  Figure 1: Histogram of the total number of unique classmates a student had during their academic ca- reer, given that they took one of four selected large courses at the University of Michigan.  Patterns of course-taking also create clusters of students who take a series of large introductory courses together, al- lowing us to repeatedly compare the performance of these classmates at large nodes in the student network. Figure 2 shows the course correlation matrix for the 40 University of Michigan courses with the largest average enrollments. It is no surprise that courses cluster within subject (e.g. CHEM, PHYSICS), or that there is a general clustering of science courses. There are also courses (e.g. PSYCH 111) with little association to core science courses, and in general no par- ticularly strong correlation with any other high enrollment courses. This is in part due to the diversity of careers of our students, but also to a fundamentally different distribu- tion of course enrollments: science majors take several core high-enrollment classes together with other science majors. Humanities majors experience much more flexible require- ments, and hence are less strongly coupled to either science students or even one another, especially outside their major. As a result, more caution is warranted in the comparison of weakly-coupled students than for those who are richly con- nected.  Finally, the assignment of grades varies broadly among academic departments of the University. Figure 3 answers the question what kinds of grades do students receive in the courses where most grades come from To construct this figure, we first identify all departments which teach at least 10 courses with average enrollments of at least 50 stu- dents. For those departments and their biggest courses, we  C H  E M  12 6  C H  E M  13 0  C H  E M  12 5  M AT  H 11  6 E  N G  R 10  0 E  N G  R 10  1 M  AT H  21 6  M AT  H 21  5 P  H Y  S IC  S 24  0 P  H Y  S IC  S 24  1 P  H Y  S IC  S 14  0 P  H Y  S IC  S 14  1 B  IO LO  G Y  16 2  P H  Y S  IC S  12 6  P H  Y S  IC S  12 5  P H  Y S  IC S  12 7  B IO  LO G  Y 30  5 B  IO LO  G Y  22 5  C H  E M  21 0  C H  E M  21 1  C H  E M  21 5  C H  E M  21 6  FR E  N C  H 23  1 FR  E N  C H  23 2  C O  M M  10 2  C O  M M  10 1  E N  G LI  S H  12 4  P O  LS C  I1 60  P O  LS C  I1 11  E N  G LI  S H  22 3  S TA  TS 10  0 N  TH R  B IO  16 1  M AT  H 11  5 E  C O  N 40  1 E  C O  N 40  2 M  K T3  00 AC  C 27  1 E  C O  N 10  2 E  C O  N 10  1 S  TA TS  25 0  U C  28 0  B IO  LO G  Y 17  1 B  IO LO  G Y  17 3  O M  E N  S TD  22 0  N TH  R C  U L1  01 S  TA TS  35 0  P S  Y C  H 25  0 P  S Y  C H  27 0  P S  Y C  H 24  0 S  PA N  IS H  23 1  S PA  N IS  H 23  2 S  O C  10 0  E N  G LI  S H  22 5  M AT  H 10  5 E  N G  LI S  H 12  5 P  S Y  C H  11 1  CHEM126 CHEM130 CHEM125 MATH116 ENGR100 ENGR101 MATH216 MATH215 PHYSICS240 PHYSICS241 PHYSICS140 PHYSICS141 BIOLOGY162 PHYSICS126 PHYSICS125 PHYSICS127 BIOLOGY305 BIOLOGY225 CHEM210 CHEM211 CHEM215 CHEM216 FRENCH231 FRENCH232 COMM102 COMM101 ENGLISH124 POLSCI160 POLSCI111 ENGLISH223 STATS100 ANTHRBIO16 MATH115 ECON401 ECON402 MKT300 ACC271 ECON102 ECON101 STATS250 UC280 BIOLOGY171 BIOLOGY173 WOMENSTD2 ANTHRCUL1 STATS350 PSYCH250 PSYCH270 PSYCH240 SPANISH231 SPANISH232 SOC100 ENGLISH225 MATH105 ENGLISH125 PSYCH111  CourseCorrelation Matrix  0.2 0 0.2 0.4 0.6 0.8 1 Value  0 40  0 80  0  Color Key and Histogram  C ou  nt  Figure 2: The distribution of course taking. For the top 40 courses by total enrollment, the Pear- son correlation coefficiencts between taking pairs of courses at the University of Michigan. Lighter shades symbolize higher correlation, and hierachical complete linkage clustering dendrograms arrange courses into groups that indicate similar patterns of course-taking.  record the total enrollment and mean grade, aligning the courses from highest to lowest average enrollment and indi- cating grade in a color scale ranging from red to white as mean grades vary from 2.65 to 3.85 on a four point scale. Grade trends depend strongly on department, level, and di- vision. This reconfirms earlier work of [7], which concludes that the use of grades and grading depends strongly on aca- demic discipline. Interestingly, this was less true in the time of [17], when diversity in the application of new grading standards apparently had more to do with individual fac- ulty than disciplines. Today at Michigan, eight of the ten lowest grading departments are in science, engineering, and math, along with Economics and the Romance Languages. The highest grading departments are more mixed, with hu- manities departments making up five of the top ten highest, joined by others like Biomedical Engineering and the School of Education.  2.2 Performance Measures Enriched by Con- text  The general system of grades described above has long been in place and is not likely to disappear soon. Despite its drawbacks, considerable effort is expended in the deter- mination of grades, so that for most courses grades contain useful information both about characteristics of the course and the relative performance of the students who are en- rolled. Given signs that grading standards vary significantly across courses and departments, we are concerned that stu- dents are punished for taking difficult, low graded courses or incentivized to pursue easy courses which tend to as- sign higher grades. Our challenge then is to leverage exist- ing grades to better understand both grading practices in courses and student performance. To this end, we consider four different metrics of average student performance: tra- ditional GPA, two previously suggested standardizations of the GPA, and a fixed effects model which we introduce for the first time here.    Mathematics Department  Earth and Environmental Sciences  Program In Computer Science  Economics Department  Biology Department  Romance Languages Department  Electrical Engr & Computer Sci  Statistics Department  IndustrialOperations Engr Dep  Chemistry Department  Mech Eng & Applied Mech Dept  Physics Department  College of Architecture & Urban Planning  Chemical Engineering Department  Materials Science & Engineering  Philosophy Department  School of Business Administration  Aerospace Engineering  Sociology Department  Communication Studies  Sch Of Nat Resources & Environ  College Of Pharmacy  Astronomy Department  Civil & Environmental Engr  Molecular, Cellular, and Developmental Biology  History Department  School Of Kinesiology  History Of Art Department  Department of AfroAmerican and African Studies  Anthropology Department  Screen Arts and Cultures  Studies In Religion  Political Science Department  Engineering Undergraduate Educ  English Language & Literature Dept  Program in the Environment  Classical Studies Department  School Of Nursing  Psychology Department  School Of Art And Design  Asian Languages And Cultures  Women's Studies Department  Department of Linguistics  American Culture Program  Germanic Languages & Lit Dept  Near Eastern Studies Department  Office of International Programs  Biomedical Engineering  School of Music, Theatre and Dance  School Of Education  LS&A First Year Seminars  Mathematics Department103 105 115 116 156 215 216 217 417 425  Earth and Environmental Sciences100 102 103 105 106 107 110 111 113 222  Program In Computer Science100 181 183 198 270 280 303 370 380 482  Economics Department101 102 310 340 395 398 401 402 404 435  Biology Department118 162 171 172 173 207 225 226 305 310  Romance Languages Department101 102 103 103 231 231 232 232 275 276  Electrical Engr & Computer Sci183 203 215 270 280 281 314 370 482 496  Statistics Department100 250 350 401 402 408 412 425 426 470  IndustrialOperations Engr Dep201 202 265 310 316 333 334 366 373 425  Chemistry Department125 126 130 210 211 215 216 230 241 260  Mech Eng & Applied Mech Dept211 235 240 250 350 360 382 395 450 495  Physics Department125 126 127 128 135 136 140 141 240 241  College of Architecture & Urban Planning312 313 314 315 316 317 322 323 326 425  Chemical Engineering Department230 330 341 342 343 344 360 460 466 487  Materials Science & Engineering220 242 250 330 350 360 412 420 480 489  Philosophy Department180 181 196 201 202 232 303 355 359 361  School of Business Administration271 272 300 300 300 300 301 312 350 471  Aerospace Engineering215 225 245 285 305 315 325 335 345 405  Sociology Department100 101 102 210 303 305 310 344 345 368  Communication Studies101 102 111 211 351 361 371 381 439 458  Sch Of Nat Resources & Environ100 210 239 256 301 306 337 375 418 438  College Of Pharmacy409 410 411 412 431 432 434 462 485 486  Astronomy Department101 102 103 104 106 111 112 115 127 142  Civil & Environmental Engr211 212 260 303 325 351 360 402 421 431  Molecular, Cellular, and Developmental Biology300 306 310 400 418 422 427 428 429 436  History Department110 160 161 201 218 241 266 318 322 396  School Of Kinesiology101 101 110 111 111 230 241 320 330 340  History Of Art Department101 102 112 212 222 250 251 271 272 394  Department of AfroAmerican and African Studies103 111 111 340 358 450 451 458 490 495  Anthropology Department101 161 272 285 298 330 344 364 365 368  Screen Arts and Cultures200 236 236 272 290 350 360 366 366 370  Studies In Religion122 201 202 230 280 296 310 312 381 481  Political Science Department101 111 140 160 300 314 353 389 489 496  Engineering Undergraduate Educ100 101 103 110 151 195 280 390 455 490  English Language & Literature Dept124 125 223 225 239 240 313 317 325 367  Program in the Environment102 105 110 111 139 201 211 232 302 360  Classical Studies Department101 101 102 191 192 222 231 232 372 385  School Of Nursing122 210 245 252 254 354 356 358 454 456  Psychology Department111 120 230 240 250 270 280 303 370 401  School Of Art And Design100 110 120 121 130 150 151 220 231 300  Asian Languages And Cultures101 102 125 126 201 202 220 225 226 230  Women's Studies Department220 240 253 270 295 300 324 375 400 483  Department of Linguistics102 111 200 209 210 211 272 315 370 375  American Culture Program100 201 204 205 206 209 240 301 374 399  Germanic Languages & Lit Dept101 102 221 231 232 243 322 325 326 386  Near Eastern Studies Department100 101 101 102 102 122 201 202 281 331  Office of International Programs230 240 350 351 354 363 368 453 459 468  Biomedical Engineering211 221 231 321 331 418 419 450 458 499  School of Music, Theatre and Dance139 139 140 149 344 345 346 347 348 349  School Of Education118 304 310 362 391 392 401 402 406 490  LS&A First Year Seminars104 105 106 107 150 151 254 256 270 280  Figure 3: Grades at the University of Michigan. For Departments offering at least 10 courses with typical enrollments of at least 50 students, the course cat- alog number and mean grade (red to white = 2.65- 3.85 grade points) are shown. From left to right, the top 10 courses are ranked by enrollment, while from top to bottom, Departments are ordered by their enrollment-weighted mean grades, and color-coded according to division: red for Natural Science and Engineering, blue for Social Sciences, and green for Arts and Humanities   Grade Point Average (GPA) is the traditional weighted- average of course grades, where the weights are credit- hours. Courses are otherwise assumed to be the same and no information about course grade distributions is included.   Grade Points Above Replacement 1 (GPAR1) is a modification of the prescription given in [5]: student grades are compared to the course mean. The orig- inal formulation given by Caulkins is modified here - GPAR1 represents a students career performance and, like GPA, is a credit hour weighted average.   Grade Points Above Replacement 2 (GPAR2) like GPAR1, corrects student grades to the course mean, but then standardizes this difference with the standard deviation of the course. Again, we modify the Caulkins formulation such that a students final GPAR2 is a credit-hour weighted average of GPAR2s in all courses.   Student Fixed-Effects (SFE) models every grade given to a student as a linear combination of student and course fixed effects, estimated across the full ar- ray of student-course-term records. Student is grade in course c, term t is written as the sum of a course and term-invariant student component StudentFEi, a student-invariant course-term component ClassFEct, and an idiosyncratic error term, ict.  Gradeict = StudentFEi + ClassFEct + ict  The coefficients in this model may be estimated with by ordinary least-squares techniques.  Our use of the two Grade Points Above Replacement (GPAR) statistics is loosely inspired by Major League Baseballs Wins Above Replacement statistics, which computeHow many more games did your team win with you as a player than they would have with a plausible replacement player Here, this question is reformed: How much higher were your grades than those which would have been received by a plausible replacement student In this case, the plausible replacement is the average student in every class you took. Unlike GPA, the two GPAR statistics leverage basic infor- mation about the grade distribution in each class. Both take into account the mean grade awarded in the class, ascribing this to the instructor rather than the students, and refer- encing every students grade to this local average. GPAR2 also takes the dispersion in grades into account. The intent here is further account for varying dispersion in grades, but this method may backfire, ascribing extraordinary discrimi- natory power to a course in which almost everyone receives the same grade. Both GPAR statistics, like GPA, fail to ac- count for the possibility that students in some courses may be, on average, much more successful students than those in other courses. Our final model jointly estimates student and course effects as parameters in a matrix that couples stu- dents to one another through courses taken together. The matrix still contains the full information about the grade distribution in every class, and a solution that returns co- efficients encoding the student and course effects should in principle be a more accurate measure of student ability in college courses, at least if the assumptions underlying the fixed effects model are valid. This method does account for the possibility that students taking one course are on aver- age substantially stronger than those taking another.  2.3 Intellectual Depth and Breadth Collective analysis of course taking patterns for all stu-  dents can support a variety of measures of intellectual diver- sity. The distribution of individuals among a set of groups may be described by a class of diversity indices, that take the general form (e.g. [21]):  qD = 1  q1 R  1 pip q1 i  =  ( R 1  pqi  ) 1 q1  (1)    This is the inverse of the weighted generalized mean. R is the number of unique groups, and pi is the proportion of members in group i. Setting q = 2 makes this inverse of the weighted arithmetic mean, and q = 1 is the weighted geo- metric mean; it reduces to the exponential of the Shannon entropy. When we set q = 0, this is the harmonic mean, which just reduces to the total number of groups, R. As q increases, the weight given to the most abundant groups increases.  The variety of subjects of the courses found on a tran- script are a proxy for the intellectual breadth and depth of the content that a student was exposed to over his or her career. Using the same sample built for the GPA statistics, we compute a subject diversity index with q = 2 for each individual. Here, each subject is considered a group. In much the same way as GPA summarizes grades, the statis- tic 2D reduces a complex aspect of the transcript to a single number. As with the simple GPA, this statistic makes no explicit account of the student-student network in which an individual is embedded. While a student may have taken a broad range of courses, he or she could have potentially taken them all with students from the same major. Indeed the opportunity to interact with individuals from different intellectual backgrounds is a stated goal of many institu- tions. This compels the construction of measures that cap- ture the diversity of classmates, that is, people with whom the student had the opportunity to interact. We use gradu- ating major to classify each of a students classmates, which we call major diversity, wherein we set q = 2. This final statistic involves the computation of a large student-student course coenrollment network (e.g. Figure 1), which is the subject of forthcoming paper. To make computation more manageable, we restrict this network and consider only the course coenrollments of students that entered in the College Literature, Science, and Arts (LSA) since Fall 2005 with any other student at the University.  3. RESULTS  3.1 Comparing Single Metric Performance Mea- sures: GPA, GPAR1/2, SFE  The heterogeneity in grades is evident. These grades de- pend on an interplay of the term the course was taken, the subject, the strength of the students in the class, and the in- structor, among other things. GPA is a ubiquitous statistics of a career, agnostic to everything about courses and stu- dents aside from the grades assigned. Given that we know mean grades vary by 25% among departments, this cannot be a perfect estimator of student success. GPAR1 attempts to correct for variations in average course grades by com- paring student grades to the course means; it is a first-order correction to GPA. Courses with unusually high or low mean grades should skew this grade-based metric less. GPAR2 of- fers a second-order correction to this effect: deviation from the mean is measured in units of standard deviation, under the assumption that large deviations where the grade distri- bution was narrow implicitly contain more information than those where the distribution of grade was broad.  In none of these first three metrics is there an explicit ac- count of the strength of the other students in the course. In simpler terms, a grade received in a course with above  or below average students should be interpreted differently than one with average students. This information is put to use in the fixed-effects model. In Figure 4  200, 000 final GPAs and SFEs are plotted. Histograms show the distri- bution of students in each dimension and individual points show means for courses offered in some departments. SFE, GPAR1, and GPA are all in units of grade points. Increasing GPA generally tracks with increasing SFE but with consid- erable scatter. That the centroid of the distribution in SFE does not fall on 0 reflects the strong negative skew of the SFE distribution; the departmental mean SFEs more closely match the centroid of the SFE distribution, as they cluster around zero. The GPA distribution is strongly peaked, due in part to the truncation of the GPA scale at 4.0; SFE expe- riences no such truncation and is spread more broadly, which reflects this measures better ability to distinguish individ- ual students from one another. A student in the centroid has a GPA of  3.5 and SFE of  0.2. At nearly fixed GPA, departments range from those with higher SFE (Math) to those with lower SFE (English, Psychology). At fixed SFE, GPAs in Math are considerably lower than those in Organi- zational Studies. While the stucture of the two-dimensional distribution is smooth, its composition is rich.  Figures 5 and 6 show similar comparisons. The GPA and GPAR1 track each other well, with considerably less scatter at the centroid than SFE vs. GPA; GPA1 and GPA are more similar in construction than SFE, and hopefully this implies that SFE carries more information. GPAR1 vs. SFE has a scatter somewhere in between the first two figures. In this comparison, the two metrics track each other, and naively one expects symmetry about the line of equality in the distributions. Instead a tilt is apparent: in the lower left quadrant, GPAR1 > SFE, and the in the upper right quadrant, GPAR1 < SFE.  C U  M G  PA  SFE 1.0 0.5 0.0 0.5 1.0  2. 0  2. 5  3. 0  3. 5  4. 0                1  1. Biomedical Engineering          2  2. Chemistry Department                      3  3. Department of AfroAmerican and African Studies        4  4. Earth and Environmental Sciences     5  5. Electrical Engr & Computer Sci  6  6. English Language & Literature Dept                  7  7. Mathematics Department      8  8. Molecular, Cellular, and Developmental Biology        9 9. Organizational Studies    10  10. Physics Department      11  11. Psychology Department                            Figure 4: GPA vs. SFE for  200, 000 undergraduate majors at the University of Michigan. Histograms represent total numbers of students in each GPA or SFE range. Points show the means of these quan- tities for different departments, with a selected few departments labeled.  3.2 Student and Course Effects The sorting of students by some measure of achievement  into different subjects has been commented on many times    GPAAR vs. GPA  GPA  G PA  A R   50    100    150    200    250    300    350    400   2.0 2.5 3.0 3.5 4.0  1 .0  0 .5  0. 0  0. 5  1. 0               1  1. Biomedical Engineering         2  2. Chemistry Department                      3  3. Department of AfroAmerican and African Studies        4  4. Earth and Environmental Sciences     5  5. Electrical Engr & Computer Sci  6  6. English Language & Literature Dept               7  7. Mathematics Department      8  8. Molecular, Cellular, and Developmental Biology        9  9. Organizational Studies    10  10. Physics Department     11  11. Psychology Department                        Figure 5: GPAR1 vs. GPA for  200, 000 undergrad- uate majors at the University of Michigan. Contours represent the density of students in each range, and labeled dots show the means of these quantities for various majors.  GPAAR vs. SFE  Student FE  G PA  A R   50     100    150    200    250    300    350   1.0 0.5 0.0 0.5 1.0  1 .0  0 .5  0. 0  0. 5  1. 0               1  1. Biomedical Engineering         2  2. Chemistry Department                      3  3. Department of AfroAmerican and African Studies        4  4. Earth and Environmental Sciences     5  5. Electrical Engr & Computer Sci  6  6. English Language & Literature Dept              7  7. Mathematics Department      8  8. Molecular, Cellular, and Developmental Biology        9  9. Organizational Studies    10  10. Physics Department     11  11. Psychology Department                        Figure 6: GPAR1 vs. SFE for  200, 000 undergradu- ate majors at the University of Michigan. Contours represent the density of students in each range, and labeled dots show the means of these quantities for various majors.  over the years. The situation with the SFE is no different (Figure 7) in its assessment of the typical students in dif- ferent subjects. For the top courses in total enrollment, the lowest course effect courses have students with higher av- erage SFEs. These top courses are a mix of science (23), humanities (16), social science (13).  Introductory science classes cluster in the low mean course fixed-effect, high SFE quadrant of the plot: CHEM 210,215 (Organic Chem I & II); BIOLOGY 171 (Organismal and Population Biology); PHYSICS 140,240 (Mechanics,Electricity and Magnetism); MATH 115,116,215,216 (Calc I-IV) as well as ECON 101,102,401 (Micro, Macro, Intermediate MacroE- con) and Accounting 271. Intro science labs (BIOLOGY 173, CHEM 211, CHEM 216, PHYS 141, PHYS 241) also contain these high SFE students, but have higher mean course effects as already noted in [1]. In the low SFE, high                                                                                                     2.6 2.8 3.0 3.2 3.4 3.6 3.8  0 .3  0 .2  0 .1  0. 0  0. 1  0. 2  0. 3  StudentCourse FE Space,   Mean Course FE  M ea  n  S  tu de  nt  F  E                                                                       ACC271  ANTHRBIO161  ANTHRCUL101  BIOLOGY162  BIOLOGY171  BIOLOGY173  BIOLOGY225  CHEM125  CHEM126  CHEM130  CHEM210  CHEM211  CHEM215 CHEM216  COMM101 COMM102  ECON101  ECON102  ECON401 ECON402  ENGLISH124  ENGLISH125  ENGLISH223  ENGLISH225  ENGR100 ENGR101  FRENCH231  FRENCH232  MATH115  MATH116  MATH215  MATH216  MKT300  PHYSICS125  PHYSICS126  PHYSICS127PHYSICS140  PHYSICS141PHYSICS240  PHYSICS241  POLSCI111  POLSCI160 PSYCH111  PSYCH240 PSYCH250PSYCH270  SOC100 SPANISH231  SPANISH232  STATS100  STATS250  STATS350  UC280  WOMENSTD220  Figure 7: The correlation between student effect and course effect for high enrollment courses at the Uni- versity of Michigan. For each course, the student and course effects averaged over all terms are plot- ted for Natural Sciences (red), Humanities (black), and Social Sciences (blue).  course effect quadrant are ENGLISH 125, 223, 225, WOM- ENSTD 220. In general, the course-effects are not unex- pected given the grading patterns in Figure 3. However, we caution against interpretation of the SFE as an indepen- dent, intrinsic quality of a student, or the course effect as a measure of difficulty and rigor of a course. Student-course interaction is one source of confusion in this picture. Com- paring CHEM 210 and CHEM 211, a naive expectation is that the students taking this lecture/lab combination are identical. They have very similar SFE, but receive quite different grades.  3.3 Subject and Major Indices Figure 8 shows the subject diversity (the diversity of the  subjects one studied) and majors diversity (the diversity of majors of ones classmates) indices for all LSA students. The mean of the index, its standard error, and the number of students is given.  In Table 1 the indices are listed for a select set of majors Several majors  especially Physics and Chem  sit below the mean subject diversity, while Business Administration stands out as particularly diverse; the sheer number of these students with high subject diversity pushes the overall dis- tribution higher. Physics BS and Chem BSChem stand out as high in major diversity, while Psych BA and Business Administration sit on the low end of the distribution.  4. DISCUSSION Our purpose in this paper is to consider ways to use the  full collection of student transcripts to add context to each. We have suggested new ways to estimate performance and an initial method for measuring the intellectual diversity of a students experience. We conclude by considering a few practical questions which would emerge. We examine the re-ranking of students within and among departments that would occur as a consequence of alternative performance metrics, and interpret the observed variation in intellectual    LSA: Major Index  D (q=2)  Fr eq  ue nc  y  0 10 20 30 40  0 10  00 20  00 30  00 40  00 50  00  19.3 +/ 0.0334  N = 37127  ULSA: Subject Index  D (q=2)  Fr eq  ue nc  y  0 5 10 15  0 10  00 20  00 30  00 40  00 50  00 6.48 +/ 0.017  N = 37127  Figure 8: Subject and Major Indices for Graduating LSA majors. The mean and its standard error are given for each distribution, as well as the number of students.  diversity that emerges college-wide.  4.1 Intra-Department Performance For three departments in LSA (Figure 9) we consider  graduates of those departments in Winter 2012. Each ap- pears with an anonymous ID in both the left column (final cumulative GPA, left axis) and right column (SFE, right axis). A single student is connected between the columns by a line. Crossing of lines indicates reranking within the de- partment. Low Spearman rank correlations (bottom) indi- cates a greater amount of re-ranking. Within a department drastic re-ranking rarely occurs, with most of the shuffling happening among close neighbors. At least one exception to this trend is student 248591 in Philosophy (middle panel), who fell from the top third of the class in GPA to nearly dead last in SFE. The transcript reveals that this individual earned only about half of their credits at Michigan, all in two years. Most of the courses were 300 and 400 level with relatively high mean grades (3.1  g  3.7). This student  Table 1: Subject and Major Indices by Major. Di- versity indices are tabulated for University of Michi- gan for selected majors that comprise extremes of of the distributions.  MAJOR <Dsub > <Dmajor > N Psychology BA 4.7+/-0.028 16.4+/-0.088 3169 Psychology BS 5.74+/-0.087 19.8+/-0.27 352 English BA 4.21+/-0.034 17.4+/-0.14 1979 Economics BS 5.68+/-0.083 19.2+/-0.25 452 Bus. Admin. BBA 12.6+/-0.066 13.9+/-0.091 3370 Physics BS 3.63+/-0.096 23.8+/-0.68 124 Mathematics BS 5.42+/-0.069 20.9+/-0.26 663 Chemistry BSChem 3.5+/-0.074 25.3+/-0.49 161  earned a relatively high GPA by receiving below average grades in courses which awarded high mean grades.  4.2 GPA Error Gross re-ranking within a department is in part a con-  sequence of unusual transcripts, and we hypothesize that local reranking is mostly noise. Ultimately, GPA and simi- lar measures have an error associated with them that is part systematic and part statistical. The systematic component includes things like a students major, which courses a stu- dent took, and when they were taken, while the statistical component is driven by how well-sampled is the students career; the latter should approach zero as the number of courses taken goes to infinity.Bootstrap resampling provides a simple insight into the magnitude of statistical uncertainty in GPA. For each student, we use bootstrap resampling of courses (N = 100) to compute a bootstrap mean GPA and error. The median standard error on the cumulative GPA for graduates is 0.058. Higher GPAs necessitate lower stan- dard errors. This means that in practice, GPAs which differ by less than 0.05 grade points are statistically indistinguish- able. This reality is never acknowledged by our system of awards, which attends carefully to the meaningless third dec- imal place in GPA.  4.3 College Honors GPA forms the basis for traditional University or College  Honors. Students are ranked by GPA and selected accord- ingly. One concern with this system is that if students accu- mulate most of their GPA in departments that assign high grades, honors will be biased. Figure 3 suggests that in the highest enrollment classes, which often reach 400-level, there are grading trends among and within departments.  In U-Ms College of Literature, Science, and the Arts, aca- demic honors (called distinction) is awarded on the basis of GPA. If this ranking were done by other means, award of these honors would go to different students. Figure 9 hints at the ways distinction might change if we ranked by SFE instead of GPA. The color of the lines indicates how students in departments are re-ranked when LSA graduates are ranked by SFE instead of GPA: students in Physics gen- erally receive higher rankings, those in History lower, and those in Philosophy a mix. The alternative ranking scheme does indeed reshuffle the assignment of distinction. At one extreme of the reordering is English, in which the number of normal, high, and highest distinction students goes from 30, 9,and 9 to 20, 8, and 0 students. At the other end is Mathematics which goes from 13, 10, and 4 to 22, 11, and 15.        Physics Department, Winter 2012  rho = 0.9062  217742 217742  316280  316280  487134  487134  505761  505761  207491  207491  347246  347246237003  237003  460477  460477  69834  69834  390234  390234  175546  175546  243349  243349  327914  327914  52614 52614  331413  331413  427851  427851  416432  416432  355764 35576418154 18154  13521  13521  132622  132622  60450 60450 441094  441094  165399 165399 499486  499486  228271  228271  108590  108590  10898  10898  201154 201154  46130  46130  208747 208747 6123  6123  250547 25054717444  17444  169386 169386 34690  34690  140935  140935  354201  354201  461904  461904  286374  286374  21390  21390  500176 500176  383730  383730  435322  435322  446656 446656  483424  483424  514995  514995  380123  380123  265019  265019  5219  5219  425721  425721  334496  334496  201248 201248 33668  33668  438587  438587  396993  396993  200198  200198  524887  524887  271377  271377  382057  382057  181066  181066  199984  199984 2.  39 3.  39 3.  89  0 .8  52 0.  14 8  0. 64  8  CUMGPA SFE      Philosophy Department, Winter 2012  rho = 0.9062  217742 217742  316280  316280  487134  487134  505761  505761  207491  207491  347246  347246237003  237003  460477  460477  69834  69834  390234  390234  175546  175546  243349  243349  327914  327914  52614 52614  331413  331413  427851  427851  416432  416432  355764 35576418154 18154  13521  13521  132622  132622  60450 60450 441094  441094  165399 165399 499486  499486  228271  228271  108590  108590  10898  10898  201154 201154  46130  46130  208747 208747 6123  6123  250547 25054717444  17444  169386 169386 34690  34690  140935  140935  354201  354201  461904  461904  286374  286374  21390  21390  500176 500176  383730  383730  435322  435322  446656 446656  483424  483424  514995  514995  380123  380123  265019  265019  5219  5219  425721  425721  334496  334496  201248 201248 33668  33668  438587  438587  396993  396993  200198  200198  524887  524887  271377  271377  382057  382057  181066  181066  199984  199984  2. 39  3. 39  3. 89  0 .8  52 0.  14 8  0. 64  8  CUMGPA SFE      History Department,1870  rho = 0.9418  151919  151919  500300  500300133394 133394 227393  227393  169327  169327  39496 39496  236846  236846  520671  520671  330419  330419  451707  451707  270906  270906  63042  63042  171806 171806  142607  142607  520477  520477 514458 514458  515554  515554  336087 336087  3886  3886  166854  166854  170876  170876  240878 240878  466677  466677  465613  465613  93697  93697  345529  345529  483113 483113506926 506926 93091  93091  400402 400402128135 128135  345344  345344  135054  135054  337754  337754  168079  168079  166749  166749  58418  58418  5502 5502  168018  168018 442662  442662  84074  84074  443100 443100 65971  65971  175841  175841  287562  287562  154973 15497399802  99802  406716 406716  481283 481283  123810  123810  284927  284927  398857  398857  228804  228804  372274 372274  178669 178669  487483 487483184986  184986  437390  437390  175704 175704251052 251052 198004  198004  82555 82555  266619 266619213948  213948  216042  216042  467459  467459  3008 3008289742 289742  284361  284361  3900 3900 302409  302409  74434  74434  480960  480960  186846  186846  400417  400417  424250  424250  279361  279361  17456  17456  102767  102767  5219  5219  298437  298437  42408 42408454861  454861  396993 396993  77465  77465  456535  456535  53972  53972  2. 39  3. 39  3. 89  0 .9  02 2  0. 09  78 0.  59 78  CUMGPA SFE  Figure 9: The Re-ranking of Students by Department and College: Physics, Philosopy, and History Winter 2012. The final GPA (left) and SFE (right) for students that graduated with an undergraduate degree in Winter 2012 is represented. Anonymized ID numbers match particular students to the text. Lines connect a student between the two columns, and aid in tracking re-ranking within a department. Blue lines indicate that a student was ranked higher in the College of LSA using SFE as a ranking criteria than GPA. Red lines indicate that they fared worse under an SFE ranking. The Spearman rank correlation for intra-department GPA and SFE ranks is given at the bottom as well: lower rank correlations indicate a greater degree of intra-departmental reranking.  4.4 Subject and Major Diversity As with grading, degree pathways and requirements vary  between departments in LSA, and certainly in the University beyond. Graduation requirements exist in part to encour- age intellectual diversity in study, and this places a con- straint on how much diversity (or lack thereof) is present in a transcript. For instance, Business Administration BBA students have high subject diversity, but upon closer inspec- tion, it turns out that this is one of the few undergraduate programs for which multiple subjects exist within a school: subjects of FIN (finance), STRATEGY, MKT (marketing), ACC(accounting) and several others are all exclusive to the business school. This is in contrast to, for instance, math- ematics where all courses are designated MATH. For this reason, it may be that a better measure of subject diversity only depends on the Department that owns a course. Until then, the subject indices may be best considered only within departments, not across.  4.4.1 Major Trends Individually, the lowest subject diversity indices come from  students graduating with degrees in Dance or Art and De- sign, likely after a transfer from LSA. In fact this describes the bottom 10 in the list. The lowest subject index was 1.40, for a student that took over 50 courses in DANCE, the remaining 11 coming from ENGLISH, WOMENSTD, and an array of singles in other subjects. Interestingly, this students major diversity index was 23.3, or about 4 points above the mean, which indicates that a broad array  of students from other majors were classmates with this indi- vidual.Business Administration students were the highest in subject diversity, with one individual at 24.4. However, 20 of this students courses come from 13 subjects that are owned by the School of Business. For these situations, the sub- ject diversity becomes a measure intra-department breadth. This same students major diversity index = 18.46, which is below the mean University-wide.  4.4.2 Outlier Careers Within a department, these measures are more standard-  ized, and comparable. Overall, subject and major diversity show a mild ( = 0.14) anti-correlation with cumulative GPA, and have no correlation with SFE (not shown) - our di- versity indices provide new, nearly orthogonal information. What is it telling us As an example (for its large num- bers of students) we consider Psych BA graduates. For the 10 highest SFE students, the subject indices range from 3.4 - 7.4. The two students at these boundaries had major in- dices of 19.1 and 19.2. The student with lower subject index took 37 courses from a total of 11 subjects: 18 PSYCH, 15 SW (Social Work), 7 WOMENSTD and an array of others. The higher index is comprised of 31 courses in 19 different subjects: 10 in PSYCH, and the others spread across the academic spectrum. Neither double-majored. At the other end of the Psych BA SFE spectrum (SFE < -0.75), students range from 4.8 - 8.8 on the subject diversity indices, and 16.7-24.8 on the major indices. The individual at 4.8 took 33 courses in 15 subjects: 14 in PSYCH, 3 in FRENCH and the others spread across other subjects with 1 or 2 instances.    This is in contrast to the 19 subjects taken by the 8.8 stu- dent across 37 courses: 10 in PSYCH, 4 in ASIANLANG, 4 POLSCI, and 3 in COMM. These two students were, re- spectively, 19.1 and 21.1 in the major diversity index.  4.5 Conclusions This paper explores ways in which existing student tran-  script information might be placed in context using straight- forward techniques. Rather than advocating for any of these measures in detail, we prefer to promote the idea of enriched transcripts, and to encourage the community to consider other ways in which we might use existing information to better represent the experience of students on college cam- puses.  5. ACKNOWLEDGMENTS This work has been supported by the NSF WIDER grant  DUE-1347697 for the REBUILD project, by NSF TUES grant DUE-1245127, and by the University of Michigan Provosts Learning Analytics Task Force through the Learning Ana- lytics Fellows Program. We thank Kar Epker for prelim- inary work which inspired the diversity analysis. We also thank the U-M Registrar Paul Robinson, the Office of the Registrar, U-M CIO Laura Patterson, and all the staff at the U-M Information Technology Services division for both maintaining and supporting access to this remarkable data set. Finally, we acknowledge the important contributions of former U-M Provost Phil Hanlon and current U-M Provost Martha Pollack. Their strong advocacy of appropriate re- search using student record data has made learning analytics at Michigan possible. This research has been determined ex- empt from human subjects control under exemption #1 of the 45 CFR 46.101.(b) by the U-M Institutional Research Board (HUM00079609).  6. REFERENCES [1] A. C. Achen and P. N. Courant. What are grades  made of The journal of economic perspectives: a journal of the American Economic Association, 23(3):77, 2009.  [2] A. W. Astin. What matters in college: Four critical years revisited. Jossey-Bass, 1993.  [3] M. A. Bailey, J. S. Rosenthal, and A. H. Yoon. Grades and incentives: assessing competing grade point average measures and postgraduate outcomes. Studies in Higher Education, 41(9):15481562, 2016.  [4] J. D. e. Bransford, A. L. e. Brown, and R. R. e. Cocking. How people learn: Brain, mind, experience, and school. National Academy Press, 1999.  [5] J. Caulkins, P. Larkey, and J. Wei. Adjusting gpa to reflect course difficulty, 1996. Working Paper, Carnegie Mellon University, The Heinz School of Public Policy and Management, retreived on Jan 17, 2017 from http://repository.cmu.edu/heinzworks/42/.  [6] Dartmouth Office of the Registrar. Median Grades for Undergraduate Courses, 2015. Retreived on Jan 19, 2017 from http://www.dartmouth.edu/ reg/transcript/medians/.  [7] R. D. Goldman, D. E. Schmidt, B. N. Hewitt, and R. Fisher. Grading practices in different major fields. American Educational Research Journal, 11(4):343357, 1974.  [8] B. Gose. Duke rejects plan to alter calculation of grade-point averages. The Chronicle of Higher Education, March 1997.  [9] J. Gutowski. Co-curricular transcripts: Documenting holistic higher education. The Bulletin, 34(5), 2006. Retrieved January 19, 2017 from http://www.acui.org/publications /bulletin/article.aspxissue=306id=1900.  [10] J. Hope. Support campuswide educational goals with transcript enhancements. The Successful Registrar, 16(7):15, 1 Sept. 2016.  [11] V. E. Johnson. An alternative to traditional gpa for evaluating student performance. Statistical Science, pages 251269, 1997.  [12] V. E. Johnson. Grade inflation: A crisis in college education. Springer Science & Business Media, 2006.  [13] E. R. Julian. Validity of the medical college admission test for predicting medical school performance. Academic Medicine, 80(10):910917, 2005.  [14] C. Knapp. Assessing grading. Public Affairs Quarterly, 21(3):275294, 2007.  [15] P. D. Larkey and J. P. Caulkins. Incentives to fail. H. John Heinz III School of Public Policy and Management, Department of Social and Decision Sciences, Carnegie Mellon University, 1992.  [16] J. Lorkowski, O. Kosheleva, and V. Kreinovich. How to modify grade point average (gpa) to make it more adequate. In International Mathematical Forum, volume 9, pages 13631367, 2014.  [17] M. Meyer. The grading of students. Science, 28(712):243250, 1908.  [18] R. Miller and W. Morgaine. The benefits of e-portfolios for students and faculty in their own words. Peer Review, 11(1):8, 2009.  [19] A. of American Colleges and Universities. The leap vision for learning: Outcomes, practices, impact, and employers views, 2011. Retreived Jan 19, 2017 from https://www.aacu.org/publications- research/publications/leap-vision-learning-outcomes- practices-impact-and-employers.  [20] A. of American Colleges Universities. E-portfolios. Retrieved on Jan 19, 2017 from https://www.aacu.org/eportfolios.  [21] S. E. Page. Diversity and Complexity. Primers in Complex Systems. Princeton University Press, 2010.  [22] Princeton Office of the Dean of the College. Grading at Princeton, 2015. Retreived on Jan. 19, 2017 from http://odoc.princeton.edu/faculty-staff/grading- princeton.  [23] J. Schinske and K. Tanner. Teaching more by grading less (or differently). CBE-Life Sciences Education, 13(2):159166, 2014.  [24] G. Siemens and P. Long. Penetrating the fog: Analytics in learning and education. EDUCAUSE review, 46(5):30, 2011.  [25] J. W. Young. Adjusting the cumulative gpa using item response theory. Journal of Educational Measurement, pages 175186, 1990.      "}
{"index":{"_id":"38"}}
{"datatype":"inproceedings","key":"Bodily:2017:TIS:3027385.3027403","author":"Bodily, Robert and Verbert, Katrien","title":"Trends and Issues in Student-facing Learning Analytics Reporting Systems Research","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"309--318","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027403","doi":"10.1145/3027385.3027403","acmid":"3027403","publisher":"ACM","address":"New York, NY, USA","keywords":"educational recommender systems, learning analytics, learning analytics dashboards, literature review, student-facing systems","Abstract":"We conducted a literature review on systems that track learning analytics data (e.g., resource use, time spent, assessment data, etc.) and provide a report back to students in the form of visualizations, feedback, or recommendations. This review included a rigorous article search process; 945 articles were identified in the initial search. After filtering out articles that did not meet the inclusion criteria, 94 articles were included in the final analysis. Articles were coded on five categories chosen based on previous work done in this area: functionality, data sources, design analysis, perceived effects, and actual effects. The purpose of this review is to identify trends in the current student-facing learning analytics reporting system literature and provide recommendations for learning analytics researchers and practitioners for future work.","pdf":"Trends and Issues in Student-Facing Learning Analytics  Reporting Systems Research      Robert Bodily  Brigham Young University  bodilyrobert@gmail.com   Katrien Verbert  University of Leuven   katrien.verbert@cs.kuleuven.be  ABSTRACT  We conducted a literature review on systems that track learning   analytics data (e.g., resource use, time spent, assessment data,   etc.) and provide a report back to students in the form of   visualizations, feedback, or recommendations. This review   included a rigorous article search process; 945 articles were   identified in the initial search. After filtering out articles that did   not meet the inclusion criteria, 94 articles were included in the   final analysis. Articles were coded on five categories chosen   based on previous work done in this area: functionality, data   sources, design analysis, perceived effects, and actual effects. The   purpose of this review is to identify trends in the current student-  facing learning analytics reporting system literature and provide   recommendations for learning analytics researchers and   practitioners for future work.   CCS Concepts   Information systems ~ Decision Support Systems     Human centered computing ~ Visualization    Information systems ~ Data Mining    Information systems ~ Web Mining   Keywords  Learning analytics; learning analytics dashboards; educational   recommender systems; student-facing systems; literature review   1. INTRODUCTION  As online learning continues to grow, it becomes increasingly   important to identify design and teaching strategies to improve   student success in online and technology mediated environments   [1]. Learning analytics (LA) is commonly defined as the   measurement, collection, analysis and reporting of data about   learners and their contexts, for purposes of understanding and   optimizing learning and the environments in which it occurs, and   could be used to help improve student success in online   environments [2]. Within the LA process, there are a number of   stages that have been identified: select, capture, predict, use,   refine, and report [3]. This article focuses on the reporting stage of   the LA process. Learning analytics dashboards, educational   recommender systems, intelligent tutoring systems, and automated   feedback systems are commonly used in the reporting stage to   close the feedback loop and provide information to stakeholders   that can be easily understood in a short period of time.    There have been previous literature reviews conducted in this area   (see [4], [5], [6], and [7]) which focus on learning analytics   dashboards for all stakeholders (e.g. administrators, instructors,   students). In order to enable student autonomy and compare   student-facing reporting systems across disciplines, we focus   exclusively on student-facing systems (collecting student data and   reporting the data back to students) that report data back in a   learning analytics dashboard, educational recommender system,   educational data mining system, intelligent tutoring system, or   automated feedback system.   This review has implications in the learning analytics community   because student-facing reporting systems close the feedback loop   and in best case scenarios, give students real-time access to their   data to increase student awareness, reflection, and achievement.    This review identifies research trends and issues related to   designing, developing, and evaluating student-facing reporting   systems. Based on the analysis from this review, we provide   recommendations to (1) aid researchers in conducting more   rigorous research in this area, and (2) enable practitioners to   increase the impact of their systems on student success.   2. PREVIOUS LITERATURE REVIEWS  This review builds on four literature reviews conducted within the   past four years.    Verbert, et al. [2013] selected interesting dashboard articles and   provided a framework for coding various types of systems [4].   Their framework included what types of data were tracked, which   stakeholder the dashboard was intended for, and whether the   system was evaluated or not. This article did not have a systematic   search of the literature so it is hard to make comprehensive   statements about learning dashboards from this article alone.   However, this article is an excellent example as the first review   article on learning analytics dashboards.   Verbert, et al. [2014] built on the previous review by including a   few additional systems not included in the previous review. The   authors also expanded the article categorization framework   discussed in Verbert et al. [2013]. The expanded framework   included what kind of technology was used to track the data,   additional evaluation categories, and the presentation medium   (tablet, cell phone, computer, etc). This study was a good follow-  up to Verbert et al. [2013], but in order to generalize across   learning dashboards, a comprehensive literature review is still   needed [5].   Yoo et al. [2015] used the Verbert et al. [2014] review to find   articles about learning analytics dashboards that conducted system   evaluations. They excluded articles if they did not conduct an   evaluation and ended up with 10 articles in their final analysis.   The purpose of this article was to find learning analytics   dashboard articles that conducted evaluations in order to develop   an evaluation framework. Yoo et al. [2015] provided an   evaluation framework at the end of their article to guide future   Permission to make digital or hard copies of all or part of this work for personal   or classroom use is granted without fee provided that copies are not made or   distributed for profit or commercial advantage and that copies bear this notice   and the full citation on the first page. Copyrights for components of this work  owned by others than ACM must be honored. Abstracting with credit is   permitted. To copy otherwise, or republish, to post on servers or to redistribute   to lists, requires prior specific permission and/or a fee. Request permissions   from Permissions@acm.org.   LAK '17, March 13-17, 2017, Vancouver, BC, Canada     2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00    DOI: http://dx.doi.org/10.1145/3027385.3027403     evaluations of dashboard systems. Our literature review categories   have included elements from this evaluation framework [6].    Schwendimann, et al. [2016] conducted the first systematic search   in the literature for learning analytics dashboard articles. Their   final analysis included 53 articles. They analyzed all types of   learning analytics dashboards, including administrator-,   counselor-, instructor-, and student-facing systems. Some of their   findings include that most dashboard systems are developed   predominantly for instructors, and mainly exist in higher   education [7]. In addition, most articles do not report on research   experiments to determine effects on students.   The majority of analytics systems focus on providing teacher- or   administrator-facing views in their systems [7]. These can be   beneficial in helping teachers or administrators accomplish their   goals, however, these approaches generally increase teacher   control and decrease student autonomy. Ryan and Deci [2000]   suggest that from a self-determination theory perspective, students   will have greater intrinsic motivation to succeed in their   coursework when they have greater autonomy [8]. Student-facing   reporting systems enable, rather than inhibit, student autonomy,   and could increase student motivation in ways that teacher or   administrator systems could not.   Additionally, many articles use different terminology and are   presented in different venues (e.g., automated feedback systems,   educational recommender systems, intelligent tutoring systems, or   educational data mining systems). However, the goal of these   student-facing systems is the same: to provide some kind of   feedback to learners to improve teaching and learning. Because   each of these systems has a common purpose, we wanted to   review all systems trying to accomplish the same goal to better   compare the strengths and weaknesses of each type of system.   In order to enable student autonomy and compare student-facing   reporting systems across disciplines, we build on the research that   has been previously conducted by reviewing student-facing   learning analytics reporting systems.   Our review builds on the previous reviews in the following ways:   (1) we use the evaluation framework proposed by Yoo et al.   [2015] in the creation of the categories for this literature review,   (2) we use the categorization frameworks from Verbert et al.   [2013] and Verbert et al. [2014] as part of our literature review   categories, (3) we build on the work of Schwendimann et al.   [2016] by enlarging the search criteria from learning analytics   dashboards to all learning analytics reporting systems, and (4) we   narrow our scope by focusing exclusively on student-facing   reporting systems. Instead of focusing on the tool (learning   analytics dashboards) we focus on the stakeholder (students) in   order to provide practical suggestions for all student-facing   learning analytics reporting systems (see Figure 1).   The research questions that will be addressed in this review   include the following:   1. What types of features do student-facing learning analytics  reporting systems have   2. What are the different kinds of data collected in these  systems   3. How are the designs of these systems analyzed and  reported on   4. What are the perceptions of students about these systems  5. What is the effect of these systems on student behavior,   student skills, and student achievement   Figure 1. An illustration of the focus of this literature review.   3. METHODS  Learning analytics is a multidisciplinary field situated between   education and computer science. Because of this, searches were   conducted in both education databases and computer science   databases. Specifically, searches were conducted in the following   databases or conference proceedings: ERIC (main education   database), Learning Analytics and Knowledge conference   proceedings (main conference in this research field), Educational   Data Mining conference proceedings (main conference in this   research field), IEEE Xplore (main computer science database),   Computers and Applied Sciences (main computer science   database), and ACM database (main computer science database).   The exact search keywords can be found in Appendix A.   In order to ensure that we did not miss important articles due to   missed keywords, we searched for literature reviews in similar   areas (educational data mining, educational recommender   systems, learning analytics dashboards) and included articles   found in the following literature reviews: Drachsler et al. [2015],   Romero and Ventura [2010], Verbert et al. [2013], Verbert et al.   [2014], and Schwendimann et al. [2016].    As another check to make sure we did not miss important articles,   we took the keywords from the titles of the articles that we had   already found (e.g., awareness, dashboard, feedback, etc.) and   conducted targeted Google Scholar searches. Articles that met our   inclusion criteria that we found in these Google Scholar searches   were included in our analysis. These searches can be seen in   Appendix A.   After removing duplicates, the initial search yielded 945 articles.   3.1 Inclusion Criteria  For an article to be included in our final analysis, the system   described in the article had to (1) collect learning analytics data   (e.g., resource use, time spent) and (2) provide a report of this data   to students. Because we wanted to focus our review on student-  facing learning analytics reporting systems, learning analytics   data was defined as resource use, time spent data, social media   activity, or additional unobtrusive data collected. Notice the   system could not simply report assessment data. To be included,   the system had to provide some kind of feedback, reporting,   recommendation, or visualization directly to students. A student is   defined as someone attending a course in a higher education   context.      One researcher read through the titles and abstracts of the articles   to determine if it was possible to exclude articles after only   reading those sections of the articles. If it was certain the article   would not be included, it was excluded from the analysis. If the   researcher was unsure, the article was skimmed, specifically   focusing on the methods and results to understand whether the   system described in the article was collecting the right kind of   data and reporting it directly to students. After this process of   removing articles that did not meet our inclusion criteria, the final   analysis included 94 articles.   3.2 Coding Categories  Each of the articles in our final analysis was coded based on the   categories functionality, data sources, design analysis, perceived   effects, and actual effects. These categories were synthesized from   the categorization and evaluation frameworks identified in   previous literature reviews. Each of these categories was then   separated into sub-categories which will be discussed in the   results and discussion sections below. None of the sub-categories   were mutually exclusive, meaning an article could be coded as   having included every sub-category of every category. One   researcher coded all of the articles. A second researcher double-  coded 20% of the articles to ensure a rigorous and consistent   article coding process. The two coders achieved an 86%   agreement rate.   4. RESULTS AND DISCUSSION  4.1 Functionality  The functionality category describes the features of the system   reported on in each article of our analysis. This category includes   the following sub-categories: purpose of the system, data mining,   visualizations, class comparison, recommendation, feedback, and   interactivity.   4.1.1 Purpose of the system  The purpose of the system sub-category describes what the   authors indicated as the purpose of the system in their article. The   purpose of each article was extracted and then coded using an   open coding approach to identify common themes across articles.   Table 1 summarizes the results of the analysis.   Table 1. Summary of purposes of articles in this review.   Category Name # of articles % of articles   Awareness or reflection 35 37   Recommend resources 27 29   Improve retention or   engagement  18 19   Increase online social   behavior  7 7   Recommend courses 3 3   Other 4 4      Most instructor systems focus on improving engagement or   retention by helping instructors identify struggling students so   they can intervene (e.g., OLI Dashboard [9], Moodle Dashboard   [10], Student Inspector [11]). It is interesting that awareness or   reflection is the primary purpose of student-facing systems. Only   19% of the articles in this analysis had the purpose of improving   retention or engagement. Why are there not more student-facing   systems with the purpose of increasing student engagement or   student achievement Other common purposes for student-facing   reporting systems included recommending resources, increasing   online social behavior, or recommending courses.    There are various purposes of student-facing reporting systems, as   shown in Table 1. These purposes largely depend on the problems   the system is trying to solve. We advise researchers and   practitioners to be explicit in identifying the purpose of their   system so research findings and implications for practice can be   generalizable within a concrete problem domain.    4.1.2 Data Mining  Articles were coded in the data mining category if there was some   kind of statistical analysis (beyond descriptive statistics) that   happened between data collection and data reporting. We   acknowledge that not all of the methods in these articles coded   with this category can be called data mining, but we use the name   data mining for our category name for simplicity. Just under half   (N=46) of the articles in this analysis were coded in the data   mining category. The other half of the articles used simple text   feedback, descriptive statistics reporting, or simple visualizations   of what happened.    Data mining as a methodology was more common in the   educational recommender system and educational data mining   literature while visualizations and dashboards were more common   in the learning analytics literature. This is interesting because   visualizations/dashboards provide information on what has   happened or provides context for why something happened, and   data mining or recommender systems provide information on   what to do because of what has happened. There were only a few   systems (N=16) that included both a visualization and   recommendation component. More systems should consider   bridging the gap between these fields by including both what has   happened as well as what to do because of what has happened.   4.1.3 Visualizations  The visualizations sub-category is defined as any type of visual   used to display data. For example, showing a picture of a smiley   face if students are doing well and showing a picture of a frowny   face if students are doing poorly would count in this category.   Another example would be showing a complicated visual   dashboard website that students could visit to see their activity   compared to the class. Table 2 shows how often common   visualization types were used.   Table 2. Common visualization types   Visualization Type # of Articles   Bar chart 25   Line chart 19   Table 15   Network graph 10   Scatterplot 10   Donut graph 5   Radar chart 4   Pie chart 3   Timeline 3   Word cloud 3   Stoplight 2   Other 21      Most visualizations in dashboard articles were basic visualization   types, such as bar charts, line charts, or tables. While these can be     helpful, more research is needed on additional visualization types   and how they compare to bar charts, lines charts, and tables. In the   visualization type analysis, the Other category included   visualizations that were only mentioned in one article and provide   examples of additional visualizations that merit further research.   These include the following: learning path visualization (with   squares and arrows), box and whisker plot, tree map, explanatory   decision tree, parallel coordinates graph, editable planning and   reflection tool, plant metaphor visual, and tree metaphor.   4.1.4 Class Comparison  An article was coded in the class comparison category if it   included a system that allowed students to compare their data with   another students data. This could be comparing student grades to   the A students in the class or comparing students based on   social media posting frequency. There were 35 articles that   included some type of class comparison functionality. An issue   that still needs to be addressed is the effect of a class comparison   tool on student motivation. If a students achievement is above the   class average, do they become complacent in their coursework If   a students achievement is below the average, do they become   discouraged in their coursework This is an issue that has yet to   be addressed in the literature and merits additional research.    Another issue that can be addressed in future research is the effect   of different class comparisons on students. For example, a student   may want to compare their activity to the A students in the   class, the B students in the class, the top 10% of the class,   students (anonymized) that are most similar to them in activity, or   historical students that are similar to them in activity.   4.1.5 Recommendation  The recommendations category is defined as any article that   included a system that provided a recommendation to a student. A   recommendation is defined as telling or suggesting the user to do   something based on what has happened. Just under half (N=43) of   the articles included a recommendations component. The   recommendations category was most similar to the data mining   category. Out of the 45 articles with a data mining component, 35   of them also had a recommendations component. This shows that   the systems using data mining are the ones providing   recommendations or suggestions to let the student know what to   do based on what has happened.   Many of these recommender systems are not transparent in the   recommendations that they provide. In other words, the system   does not tell the student why they are receiving a specific   recommendation. Additional work should examine the effect   between transparent recommendations and more traditional black-  box recommendations on student motivation to use the system or   follow recommendations. This is important because if students   know why they receive a particular recommendation, it could   increase their trust in the system along with the likelihood of them   following feedback provided by the system.   4.1.6 Feedback  Feedback in this context is defined strictly as text feedback   because we have another category for visualization feedback.   There were only 17 articles with a feedback component (18%).   Text feedback was descriptive in nature, telling students what   happened in the past or how they were doing in the course up to   that point. Text feedback is used frequently for just-in-time   feedback, but is not used as frequently for unit-level or concept-  level data reports.   4.1.7 Interactivity  An article was coded in the interactivity section if the reporting   system gave the student the opportunity to click around to explore   their activity data. Examples of interactivity include providing   additional content as links, allowing the user to filter their data   based on type of activity or grade, or giving students a simple and   advanced view based on their preferences. There were 29 articles   that discussed systems that were interactive in some way. An   interesting further line of research should investigate whether   interactive visualizations or recommendations change student   behavior with the reporting system. How do students use the   interactive features Do these interactive features increase student   achievement more than systems without those features   4.2 Data Sources  The data sources category has sub-categories to describe the types   of data collected in reporting systems. The sub-categories include   resource use, time spent, social interaction data, other sensor data,   assessment data, and manually reported data. Resource use is   counting the number of times students accessed materials in the   course or performed course actions. Time spent data is tracking   how long students accessed materials or performed actions in the   course. Social interaction data is tracking student use or posts in   blog, wiki, discussion board, or messaging systems. Other sensor   data was collected from sensors such as face recognition, mouse   tracking, or biometric sensors. Manually reported data asked the   students to provide answers to surveys or track their own time and   input it into the system. The number of articles that tracked each   data type is presented below in Table 3.   Table 3. Article counts for each data source   Subcategory Name # of Articles   Resource use 71   Assessment data 34   Social interaction 33   Time spent 29   Other sensor data 7   Manually reported data 5      The majority of systems in our analysis collected resource use   data. Then assessment data, social interaction data, and time spent   were all collected about one-third of the time. Future research   should investigate what additional information could be useful to   include in a student-facing reporting system. Potential data   sources include biometric sensor data (heart rate, EEG, skin   conductance), mouse tracking, GPS location, university access   card swipes, library use, sports facilities use, fit bit tracker steps,   social media use not related to school, or internet use not related   to school. There are very few systems integrating multiple data   sources together into a student-facing reporting system, so   research should focus on the impact of adding additional data   sources to these systems.   4.3 Design Analysis  The design analysis category describes the effort that went into   the reporting system design. What goal is the system trying to   achieve How did the authors identify this goal How did the   authors attempt to achieve the goal Did they evaluate how well   they achieved the goal To address these questions, the sub-  categories of the design analysis category include needs   assessment, information selection, visual design, and usability   testing.     4.3.1 Needs Assessment  A needs assessment is a common step in many design models.   The purpose of a needs assessment is to determine the needs of   the stakeholder for which you are designing something. In this   case, the stakeholder is the student, and the design is the reporting   system. This was not common in the articles we analyzed, and   only six articles included a report of a needs assessment.    The solutions to the student needs identified by these six articles   included alerting a student if something goes wrong in the course,   showing students how they use their time, facilitating group   communication for group projects, supporting student motivation   in engaging with their course, providing relevant learning material   when it is needed, showing students what is important to study,   and increasing awareness of tools and resources available to   students.   Needs assessments are critical to make sure a designed system is   fulfilling stakeholder goals [12]. More researchers and   practitioners should adopt this approach when designing a   student-facing reporting system.   4.3.2 Information Selection  Select is one of the stages in the learning analytics process [3],   however it is not discussed very much in the learning analytics   reporting system literature. Information selection is defined as   including justification for why data was included in system   reports. There were three good examples of articles that had   justification for the information selection stage. Ott et al. [2015]   conducted a literature review to provide justification for the   variables included in their reporting system [13]. Feild [2015]   used exploratory analysis to identify which variables to include in   their reporting system [14]. Iandoli et al. [2014] used a theoretical   framework to guide their information selection process [15]. The   majority of the articles in our analysis did not include any   justification for the data included in their reporting system. It   seems most research is including the data that is easily accessible   and not many people are going out of their way to include   additional data sources. A justification for why data was included   in reporting systems is key for other researchers and practitioners   in the early stages of designing a student-facing reporting system   to help them determine what data sources they will include in   their system.   Future research should also investigate the benefits or drawbacks   to using certain kinds of data in a reporting system. Do students   respond better to certain kinds of data over other kinds If so,   why The collection of certain types of data requires expensive   system architecture. Are there data sources that are not useful to   students and are not worth collecting   4.3.3 Visual Design  An article was coded in the visual design category if the article   had justification for why the data was visualized or reported in the   way it was reported. There were 12 articles that included a visual   design component. Most of the authors of reporting system   articles have likely considered why they are visualizing or   reporting data in the ways they have chosen, however, many of   them are not reporting it in their reporting system articles. We   advise learning analytics researchers to include justifications of   design choices in the reporting of their work, as they are key to   guide the selection of good visualizations.   Additional research should try to identify the affordances and   constraints of each type of visualization to better illustrate when   certain types of visualizations should be used and when certain   visualizations should be avoided.   4.3.4 Usability Testing  An article is included in usability testing if the authors conducted   and reported on a usability test of their reporting system. This   usability test is more in depth than simply asking students if the   system was user-friendly. If the system only asked about student   perceptions of the system, it will be included in the student   perceptions usability category, discussed below. There were 10   articles that included some sort of usability test. A few examples   of effective usability tests that were conducted include (1) asking   students to answer questions about a demo view of the system to   see if they can navigate and understand the system, (2) conducting   a think-aloud-protocol with the students to understand how   students are thinking about the system as they interact with it, (3)   using the validated System Usability Scale (SUS) [16] to get one   number describing the usability of the system (that can be   compared to other systems using that scale), or (4) bringing in a   system usability expert to professionally evaluate the usability of   the reporting system.   There were more articles that conducted a randomized control   trial (RCT) to determine the effect of the reporting system on   student achievement (discussed in 4.5 Actual Effects) than articles   that conducted a usability test on their system. This is problematic   because it is hard to trust the results of an RCT if the authors did   not control for system usability by making sure it would not affect   students as they interacted with the reporting system. As research   in this field becomes more mature, authors should be sure to   include usability test reports on their system so we can make   generalizable conclusions about RCT results.   4.4 Student Perceptions  The student perceptions category included sub-categories for how   students perceived the learning analytics reporting system. Sub-  categories include usability, usefulness, and perceptions on the   effect the system had on the student.   4.4.1 Student Perceptions of Usability  An article was included in the student perceptions of usability   category if the authors asked the students about the usability of   the system and reported it in their article. This section is different   from section 4.3.4 in that this section deals with student   perceptions of usability while section 4.3.4 deals with other forms   of system usability tests. There were 32 articles that were coded in   the student perceptions of usability category. There were three   times as many articles that asked students about the usability of   the system instead of conducting a more rigorous usability test.   System usability can affect how students perceive and use a   reporting system, so in order to better understand how students   use these systems, more rigorous usability tests should be   conducted. In future research, authors should consider conducting   a more rigorous usability test instead of simply asking students if   their system was easy to use.    4.4.2 Usefulness  The sub-category usefulness is defined as asking students if they   thought the system was useful or if students were satisfied with   the system. There were 34 articles coded in this sub-category,   which is about the same as the usability perception category.   There were 25 articles that included both perceived usability and   perceived usefulness, so these questions were usually asked   together. Student perceptions of usefulness were generally   positive regardless of the system they were using, however this   information is not very helpful in helping us understand anything   about student use with the system or the effect the system has on   students. Instead of asking about system usefulness, we     recommend to consider asking about perceived effect on student   behavior, student skills, or student achievement.   4.4.3 Student Perceptions of System Effects  This category is concerned with identifying articles that included   a discussion of perceived system effects on student behavior or   student achievement. There were 16 articles that asked about   perceived behavior change, 2 articles that asked about perceived   achievement change, and 15 articles that asked about perceived   student skills. Student skills are defined as metacognitive   strategies or self-regulated learning strategies. Because of the low   number of articles in this sub-category, future research should ask   students what effect they believe the reporting system had on   them and what feature of the reporting system led to that effect.   Research should also consider why there are so many more   perceived behavior and skills change articles when compared with   perceived achievement. Is it difficult to trust student perceptions   of achievement changes Are RCTs preferred to investigate the   effect of a reporting system on student achievement more than for   student behavior or skills Additional research topics related to   these questions are discussed in the actual effects section.   4.5 Actual Effects  The actual effects category is different than the student   perceptions of system effects category because the actual effects   had to include some sort of research experiment to try to   determine the effect of the reporting system on student behavior,   skills, or achievement. There were 15 articles that looked at   student behavior, 14 articles that looked at student achievement,   and 2 articles that looked at student skills. The articles reviewing   student achievement have been summarized and are included in   Appendix B. The articles coded in the actual effects category, on   average, used small sample sizes, descriptive statistics, and did   not have very many significant results. There were a few articles   with large sample-sizes that conducted randomized control-trials   [17, 18], but these were rare. More research is needed on the   actual effects of these reporting systems on student behavior,   student achievement and student skills.    Very few articles are using RCTs to investigate student skill   change in this context. This may be because it is easier to ask   students about their perceived awareness, motivation, or self-  regulation change than to find a validated scale to use in a pre-   and post-survey research design. Future research should not only   use RCTs to investigate student achievement changes, but should   also give validated scales to students before and after the class to   see if student skills are changing as well.   Most of the methodologies examining experimental effects used   RCTs or descriptive statistics. Because reporting systems are   helpful to students and we want all of our students to have access   to these systems, it is hard to argue that RCTs are the best   research methodology because not all students will have access to   the tool. Quasi-experimental methods should be considered in the   future to give all students access to the reporting systems during   the course instead of a random selection of students.   5. LIMITATIONS  One challenge we faced in conducting this literature review was   the lack of a common vocabulary across fields talking about   learning analytics reporting systems. For example, an intelligent   tutoring system that tracks resource use is similar to an   educational recommender system that also tracks resource use and   provides recommendations in real-time to the student. However,   these systems are from two different but related research fields.   This also applies to learning analytics dashboards and automated   feedback systems. Then, there were also systems that did not use   any of these keywords in their manuscripts and called their system   a visualization system or gave their system a specific name (e.g.,   ECoach, StepUp!, itree, etc). To mitigate this issue, we added in   Google Scholar searches based on popular title words and we   conducted related literature review searches to find articles we   might not have found using keyword search criteria.   Another limitation we faced in conducting this literature review is   the potential for bias in the article coding process because we used   human coders. To address this issue, two coders coded 20% of the   articles and the codes were compared to find their percent   agreement. The coders had an 86% agreement.   This review presents the state of the art in the student-facing   learning analytics reporting literature, however because we   restricted our search to research articles and conference   presentations, many systems that are not reported on in research   will not be included in our review. We feel justified in only   selecting conference presentations and research articles because,   in general, the best student-facing reporting systems will have   research conducted on them to determine their efficacy.   6. RECOMMENDATIONS FOR PRACTICE  The recommendations for practice included in this section are   based on the extensive analysis of articles included in this review.   For practitioners and researchers thinking about or starting to   implement a student-facing learning analytics reporting system,   use the questions in Table 4 as guiding points to make sure you   are addressing the items needed to make a tool that will benefit   students the most.    Table 4. Questions to guide in implementing reporting systems   Question Category  % of   Articles   What is the intended goal of the   system  Intended Goal 100   What visual techniques will best   represent your data  Visualizations 13   What types of data support your   goal   Information   Selection  15   What do students need Does   this need align with your goal   Needs   Assessment  6   Is the system easy and intuitive   to use  Usability Test 11   Why are you using the visual   techniques or recommendations   you have chosen   Visual Design 13   How do students perceive the   reporting system   Student   Perceptions  17   What is the effect on student   behavior/achievement  Actual Effects 18   How are students using the   system How often Why  Student Use 13      Most of the articles did not report on the categories in Table 4,   however, the authors of these articles were probably thinking   about these questions informally. It is important to document the   answers to these questions in final research manuscripts and   conference presentations to increase the rigor of the learning   analytics reporting systems field. Eventually, there are going to be   enough articles published on the effects of these systems on   student achievement and behavior to start to make inferences   about the types of design, data, visualization, or functionality that   best help students succeed. However, these generalizations cannot     be made if the research articles written about these systems were   not explicit in addressing the questions in Table 4.   If you are an administrator or instructor thinking about adopting   an educational technology system that includes learning analytics   tracking and student-facing reporting systems, you should   consider the questions in Table 4 during the selection process. The   creators of many systems have not conducted rigorous research on   their student-facing systems, so they may over-promise on the   results of these systems.   7. CONCLUSION  Student-facing learning analytics reporting systems is an   emerging area of research and practice. In this review, we   conducted a systematic search of the literature in education   databases, computer science databases, Google Scholar, and   related literature review articles. We only included articles that   tracked student learning analytics data and then reported that data   directly back to students. Our final analysis consisted of 94   articles. We coded the articles using a closed coding approach into   categories synthesized from Verbert et al. [2013], Verbert et al.   [2014], and Yoo et al. [2015]. The categories were functionality,   data sources, design analysis, perceived effects, and actual effects.   There were two types of systems that emerged from this analysis.   First, there were systems that used data mining to analyze the data   and then provided recommendations to students. Second, there   were systems that used descriptive statistics and then provided   data visualizations in the form of a dashboard for students. Only a   few systems conducted a data mining analysis and then used   visualization to report the results. This may be because of the   differences between the educational recommender and learning   analytics dashboard fields. Similarly, intelligent tutoring systems   and automated feedback systems use their own methods to try to   achieve a similar purpose. Researchers and practitioners should   consider interdisciplinary efforts across these fields to bring   expertise together in order to accomplish their goals.   This systematic literature review on student-facing learning   analytics reporting systems was the first to examine student use of   reporting systems across multiple articles. Student use is   important in experimental research because the way students use a   tool will determine the effect it has on them. None of the studies   included in the student use category broke down student use by   demographic, learner characteristics, or student achievement   levels. In order to better personalize recommendations and   dashboards to students, we need to put more emphasis on   understanding student use of these systems.    The previous literature reviews identified in this review ([3], [4],   [5], & [6]) provided categorization frameworks for dashboards   after they had already been designed and developed. However,   there is a lot of work that goes into designing and developing a   dashboard that is rarely discussed in the literature. This review has   enumerated a number of practices to increase the rigor of   designing and developing a student-facing reporting system:   needs assessment, information selection, visual design, and   usability testing. A needs assessment ensures that the system   being developed will actually accomplish the goal, the   information selection process determines the information needed   to accomplish the goal, the visual design stage establishes how the   information will be provided to students, whether in a dashboard,   feedback system, recommender system, or text feedback, and the   usability test phase assesses the user-friendliness and usefulness   of the system. These practices will greatly enhance the rigor of the   design and development process in student-facing learning   analytics reporting systems research.   8. ACKNOWLEDGEMENTS  Part of this work has been supported by the Research Foundation   Flanders (FWO), grant agreement no. G0C9515N, and the KU   Leuven Research Council, grant agreement no. STG/14/019.   9. REFERENCES  [1] Allen, I. E., and Seaman, J. 2014. Tracking Online Education   in the United States, 145. Retrieved from   http://www.onlinelearningsurvey.com/reports/gradechange.p  df   [2] Siemens, G. 2010. In Proceedings of 1st International  Conference on Learning Analytics and Knowledge 2011.   Retrieved March 30, 2016 from   https://tekri.athabascau.ca/analytics/   [3] Elias, T. 2011. Learning Analytics: The Definitions, the  Processes, and the Potential. DOI=10.1.1.456.7092.   [4] Verbert, K., Duval, E., Klerkx, J., Govaerts, S., and Santos, J.  L. 2013. Learning Analytics Dashboard Applications.   American Behavioral Scientist, 110.   DOI=http://doi.org/10.1177/0002764213479363   [5] Verbert, K., Govaerts, S., Duval, E., Santos, J. L., Van  Assche, F., Parra, G., and Klerkx, J. 2014. Learning   dashboards: An overview and future research opportunities.   Personal and Ubiquitous Computing, 18(6), 14991514.   DOI=http://doi.org/10.1007/s00779-013-0751-2   [6] Yoo, Y., Lee, H., Jo, I., & Park, Y. 2015. Educational  Dashboards for Smart Learning: Review of Case Studies.   Emerging Issues in Smart Learning, 145155.   DOI=http://doi.org/10.1007/978-3-662-44188-6   [7] Schwendimann, B. A., Boroujeni, M. S., Holzer, A., Gillet,  D., and Dillenbourg, P. 2016. Understanding learning at a   glance: An overview of learning dashboard studies. In   Proceedings of 6th International Conference on Learning   Analytics and Knowledge. (pp. 34).   [8] Ryan, R. M., and Deci, E. L. 2000. Self-determination theory  and the facilitation of intrinsic motivation, social   development, and well-being. American psychologist, 55(1),   68.   [9] Dollar A., and Steif P. S. 2012. Web-based statics course  with learning dashboard for instructors. In: Uskov V (ed)   Proceedings of computers and advanced technology in   education (June 2527, 2012, Napoli, Italy). CATE 2012.   [10] Podgorelec V., and Kuhar S. 2011 Taking advantage of  education data: advanced data analysis and reporting in   virtual learning environments. Electron Electr Eng   114(8):111116.   [11] Scheuer O., and Zinn C. 2007. How did the e-learning  session go The student inspector. In: Luckin R et al. (eds)   Proceedings of the 2007 conference on artificial intelligence   in education: building technology rich learning contexts that   work. IOS Press, Amster- dam, pp 487494.   [12] Altschuld, J. W., and Kumar, D. D. 2009. Needs assessment:  An overview. Sage Publications.   [13] Ott, C., Robins, A., Haden, P., and Shephard, K. 2015.  Illustrating performance indicators and course characteristics   to support students self-regulated learning in CS1. Computer   Science Education, 25(2), 174198.   DOI=http://doi.org/10.1080/08993408.2015.1033129   http://www.onlinelearningsurvey.com/reports/gradechange.pdf http://www.onlinelearningsurvey.com/reports/gradechange.pdf   [14] Feild, J. 2015. Improving Student Performance Using Nudge  Analytics. In Proceeding of the 8th International Conference   on Educational Data Mining (pp. 464467).   [15] Iandoli, L., Quinto, I., De Liddo, A., and Buckingham Shum,  S. 2014. Socially augmented argumentation tools: Rationale,   design and evaluation of a debate dashboard. International   Journal of Human Computer Studies, 72(3), 298319.   DOI=http://doi.org/10.1016/j.ijhcs.2013.08.006   [16] Brooke, J. 1996. SUS-A quick and dirty usability  scale. Usability evaluation in industry, 189(194), 4-7.   [17] Dodge, B., Whitmer, J., and Frazee, J. P. 2015. Improving  undergraduate student achievement in large blended courses   through data-driven interventions. In Proceedings of the Fifth   International Conference on Learning Analytics And   Knowledge - LAK 15 (pp. 412413).   DOI=http://doi.org/10.1145/2723576.2723657.   [18] Janssen, J., Tattersall, C., Waterink, W., van den Berg, B.,  van Es, R., Bolman, C., and Koper, R. 2007. Self-organising   navigational support in lifelong learning: How predecessors   can lead the way. Computers and Education, 49(3), 781793.   DOI=http://doi.org/10.1016/j.compedu.2005.11.022   [19] Drachsler, H., Verbert, K., Santos, O. C., and Manouselis, N.  2015. Panorama of Recommender Systems to Support   Learning. In Recommender Systems Handbook (pp. 421  451). DOI=http://doi.org/10.1007/978-1-4899-7637-6.   [20] Romero, C., and Ventura, S. 2010. Educational Data Mining:  A Review of the State of the Art. IEEE Transactions on   Systems, Man, and Cybernetics, Part C (Applications and   Reviews), 40(6), 601618.   DOI=http://doi.org/10.1109/TSMCC.2010.2053532   [21] Grann, J., and Bushway, D. 2014. Competency Map:  Visualizing Student Learning to Promote Student Success, in   Proceedings of the Fourth International Conference on   Learning Analytics And Knowledge - LAK 14, pp. 168172.   [22] Arnold, K. E., Hall, Y., Street, S. G., Lafayette, W., and  Pistilli, M. D. 2012. Course Signals at Purdue: Using   Learning Analytics to Increase Student Success, in LAK 12,   no. May, pp. 25.   [23] Park, Y., and Jo, I. 2015. Development of the Learning  Analytics Dashboard to Support Students Learning   Performance. J. Univers. Comput. Sci., vol. 21, no. 1, pp.   110133.   [24] Kim, J., Jo, I.-H., and Park, Y. 2015. Effect of learning  analytics dashboard: Analyzing the relations among   dashboard utilization, satisfaction, and learning achievement.   Asia Pacific Educ. Rev.   [25] Denley, T. 2014. How predictive analytics and choice  achitecture can improve student success. Res. Pract. Assess.,   vol. 9, no. 2, pp. 6169.   [26] Ott, C., Robins, A., Haden, P., and Shephard, K. 2015.  Illustrating performance indicators and course characteristics   to support students self-regulated learning in CS1. Comput.   Sci. Educ., 25(2) pp. 174198.   [27] Dodge, B., Whitmer, J., and Frazee, J. P. 2015. Improving  undergraduate student achievement in large blended courses   through data-driven interventions in Proceedings of the Fifth   International Conference on Learning Analytics And   Knowledge - LAK 15, 2015, pp. 412413.   [28] Chen, G. D., Chang, C. K., and Wang, C. Y. 2008.  Ubiquitous learning website: Scaffold learners by mobile   devices with information-aware techniques. Comput. Educ.,   50(1), pp. 7790.   [29] Saul, C., and Wuttke, H. D. 2014. Turning learners into  effective better learners: The use of the askMe! System for   learning analytics in CEUR Workshop Proceedings, vol.   1181, pp. 5760.   [30] Beheshitha, S. S., Hatala, M., Gaevi, D., and Joksimovi,  S. 2016. The Role of Achievement Goal Orientations When   Studying Effect of Learning Analytics Visualizations. Learn.   Anal. Knowl.  LAK 16.   [31] Huang, Y. M., Huang, T. C., Te Wang, K., and Hwang, W.  Y. 2009. A Markov-based recommendation model for   exploring the transfer of learning on the Web. Educ. Technol.   Soc., 12(2), pp. 144162.   [32] Vesin, B., Klanja-Milievi, A., Ivanovi, M., and Budimac,  Z. 2013. Applying recommender systems and adaptive   hypermedia for e-learning personalization. Comput.   Informatics, 32(3), pp. 629659.   [33] Santos, O. C., Boticario, J. G., and Perez-Marin, D. 2014.  Extending web-based educational systems with personalised   support through User Centred Designed recommendations   along the e-learning life cycle. Sci. Comput. Program., 88,   pp. 92109.   [34] Wang, F.-H. 2008. Content Recommendation Based on  Education-Contextualized Browsing Events for Web-based   Personalized Learning. Educ. Technol. Soc. 11(4), pp. 94  112.      10. APPENDIX A     Table 5. The search criteria used in this literature review.   Source Search Term or Topic  Article   Count   ERIC   (student OR students) AND ( data driven decision making  OR  resource use  OR   analytics OR  student interaction OR clickstream OR online activity OR  data   mining ) AND (dashboard OR visualization OR visual OR recommendation OR   recommendations OR recommender)   193   LAK & EDM Proceedings  dashboard OR visualization OR visual OR recommendation OR recommender OR   feedback  24   http://doi.org/10.1145/2723576.2723657 http://doi.org/10.1007/978-1-4899-7637-6   IEEE Xplore   (student OR students) AND (.QT.data driven decision making.QT. OR .QT.resource   use.QT. OR analytics OR .QT.student interaction.QT. OR clickstream OR .QT.online   activity.QT. OR .QT.data mining.QT.) AND (dashboard OR visualization OR visual   OR recommendation OR recommendations OR recommender)   260   Computers and Applied   Sciences   (student OR students) AND ( data driven decision making  OR  resource use  OR   analytics OR  student interaction  OR clickstream OR  online activity  OR  data   mining ) AND (dashboard OR visualization OR visual OR recommendation OR   recommendations OR recommender)   102   ACM database   (student OR students) AND ( data driven decision making  OR  resource use  OR   analytics OR  student interaction  OR clickstream OR  online activity  OR  data   mining ) AND (dashboard OR visualization OR visual OR recommendation OR   recommendations OR recommender)   172   Google Scholar: search 1 intitle: feedback system  AND intitle: learning  66   Google Scholar: search 2 intitle: learning analytics  AND intitle: feedback  9   Google Scholar: search 3 intitle: learning dashboard  OR intitle: learning analytics dashboard  14   Google Scholar: search 4 intitle: dashboard  AND intitle: feedback  8   Google Scholar: search 5 intitle: learning analytics  AND (intitle: reflection  OR intitle: reflect ) 7   Google Scholar: search 6 intitle: learning analytics  AND intitle: awareness  6   Google Scholar: search 7  intitle: data mining  AND (intitle: recommendations  OR intitle: recommendation    OR intitle: recommend ) AND intitle: learning   17   [19] Literature review on educational recommender systems 37   [20] Literature review on educational data mining 30   [4], [5], and [7] Literature reviews on learning analytics dashboards 20      11. Appendix B.     Table 6. Article summaries included in the experimental effects achievement change category.   Citation Sample Size Context Result   [21] Not listed   Mean difference testing was used to determine   whether students that used the competency map   had higher levels of performance than students   that did not   Students that used the competency map had slightly   higher achievement rates, however, this was not   statistically significant   [22] about 8,000   By comparing student achievement before and   after course signals, descriptive statistics were   used to determine the effect on student   achievement.   Classes with course signals (compared with the same   course before course signals) saw increased A's and   B's and decreased C's, D's, and E's.    [23]  36 treatment,   37 control   A randomized control trial research design was   used to determine the effect of the LAPA   dashboard on student achievement. Mean   difference testing was used to determine if there   was a significant difference between groups.   Although the treatment group had slightly higher   achievement rates than the control group, there were   no significant differences between the treatment and   control group regarding their achievement rates.   [24]  72 treatment,   79 control   A randomized control trial research design was   used to determine the effect of the learning   dashboard on student achievement. Mean   difference testing was used to determine if there   was a significant difference between groups.   The treatment group (received access to dashboard)   had significant higher achievement rates on the final   exam than the control group.   [25]  about 50,000   students   Descriptive statistics were used to compare   students in Degree Compass courses to those not   Compared with previous students that did not use   Degree Compass, students that used Degree     enrolled in Degree Compass courses. Compass received more passing grades (A, B, or C),   especially if the student belonged to an at risk   population. The prediction algorithm accuracy was   90%.   [26] 512 students   T-tests were used to determine if there was a   significant achievement difference between   previous semesters without the infographic and   later semesters with the infographic. Assessments   did not change between years and course   curriculum stayed the same.   There was no significant difference after the   introduction of the class infographic on student   achievement.   [27]   442   treatment,   440 control   T-tests to compare treatment and control groups   of a randomized control trial were used to   determine the effect of trigger events   (recommendation emails) on student achievement.   There was no significant difference between   treatment and control groups in terms of   achievement. However, in one course there was a   significant treatment effect on pell eligible students.   This effect was not seen in the other course included   in this study.   [28]  27 treatment,   27 control   T-tests were used to compare treatment and   control groups to determine the effect of the   ubiquitous learning website as well as the device   used (cell phone, laptop, PDA) on student   achievement and learning goal achievement.   Use of the ubiquitous learning website had   significant effects on testing results, task-  accomplished rate, and learning-goal-achieved rate   (Chen et al., 2008, p. 90).   [29]  about 80   students   Comparisons were made between students that   used the askMe! system and the students that did   not use the system.   The average grade of students that used the system   was higher than those that did not. In addition, the   failure rate was four times lower for those that used   the system when compared with those that did not.   [30]  about 100   students   Controlling for achievement goal orientation,   what effect do learning analytics visualizations   have on the quality of student social media posts   A linear mixed-effects analysis was conducted.   The frequency and quality of student posts were   affected positively and negatively, depending on the   visualization.   [31]  57 treatment,   56 control   A Markov chain model and an entropy-based   approach were used to see if the recommender   system could provide helpful learning paths to   students.   Learners in the treatment group performed   significantly better than the control group on the   evaluation system task.   [32]  35 treatment,    35 control   T-test were used for mean difference testing to   determine whether Protus, an adaptive and   personalized recommendation engine, had an   effect on student learning.   Student learning efficiency was improved, but no   analyses were conducted to determine change in   grade based on treatment effect.   [33] 173 students  T-tests were used to compare treatment and   control groups to determine the effect of   recommendations on student achievement   There were no significant differences between the   treatment and control groups in terms of learning   achievement   [34]  40 treatment,   40 control   A t-test was used to determine the effect of   content recommendations on student exam score.   The treatment group performed equivalently to the   control group on the pre-test, and then the treatment   group had significantly higher scores than the   control group on the post-test.        "}
{"index":{"_id":"39"}}
{"datatype":"inproceedings","key":"Hsiao:2017:URR:3027385.3027415","author":"Hsiao, I-Han and Huang, Po-Kai and Murphy, Hannah","title":"Uncovering Reviewing and Reflecting Behaviors from Paper-based Formal Assessment","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"319--328","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027415","doi":"10.1145/3027385.3027415","acmid":"3027415","publisher":"ACM","address":"New York, NY, USA","keywords":"blended instruction classes, computing education, cross LAK, feedback, orchestration technology, programming learning, reflection","Abstract":"In this paper, we study students' learning effectiveness through their use of a homegrown educational technology, Web Programming Grading Assistant (WPGA), which facilitates grading and feedback delivery of paper-based assessments. We designed a classroom study and collected data from a lower-division blended-instruction computer science class. We tracked and modeled students' reviewing and reflecting behaviors from WPGA. The results show that students demonstrated an effort and desire to review assessments regardless of if they were graded for academic performance or for attendance. Hardworking students achieved higher exam scores on average and were found to review their exams and the correct questions frequently. Additionally, student cohorts exhibited similar initial reviewing patterns, but different in-depth reviewing and reflecting strategies. Ultimately, this work contributes to the aggregation of multidimensional learning analytics across the physical and cybersphere.","pdf":"Uncovering Reviewing and Reflecting Behaviors From  Paper-based Formal Assessment   I-Han Hsiao  School of Computing, Informatics &   Decision Systems Engineering,  Arizona State University,   Tempe, AZ, USA  Sharon.Hsiao@asu.edu   Po-Kai Huang  School of Computing, Informatics &   Decision Systems Engineering,  Arizona State University,   Tempe, AZ, USA  phuang24@asu.edu   Hannah Murphy  W.P. Carey School of Business,   Arizona State University,  Tempe, AZ, USA   hmurphy2@asu.edu       ABSTRACT  In this paper, we study students' learning effectiveness through  their use of a homegrown educational technology, Web  Programming Grading Assistant (WPGA), which facilitates  grading and feedback delivery of paper-based assessments. We  designed a classroom study and collected data from a lower- division blended-instruction computer science class. We tracked  and modeled students reviewing and reflecting behaviors from  WPGA. The results show that students demonstrated an effort and  desire to review assessments regardless of if they were graded for  academic performance or for attendance. Hardworking students  achieved higher exam scores on average and were found to review  their exams and the correct questions frequently. Additionally,  student cohorts exhibited similar initial reviewing patterns, but  different in-depth reviewing and reflecting strategies. Ultimately,  this work contributes to the aggregation of multidimensional  learning analytics across the physical and cybersphere.   CCS Concepts   Education Computer-assisted instruction    Interactive  learning environment  E-Learning  Learning management  system.    Keywords  Feedback; Reflection; Programming Learning; Computing  Education; Cross LAK; Orchestration Technology; Blended  Instruction Classes.   1. INTRODUCTION  We have begun to see more and more orchestration technologies  (i.e. smart classrooms etc.) that focus on integrating and modeling  physical learning activities while making use of advanced learning  analytics. Some examples include Clickers [1] and multi-touch  tabletops [2] etc. Even with all of these new technologies, most  data sources of students performance are collected from  computer-assisted formative assessments or retrieved from  learning management systems. Data integration is less focused on  bridging physical and cyber learning spaces. In the inaugural  International Workshop on Learning Analytics Across Physical  and Digital Spaces in conjunction with the 6th International  Conference of Learning Analytics and Knowledge [3], learning   analytics researchers and learning scientists gathered to discuss  blended learning scenarios and associated overarching concerns in  data integration and learning analytics coordination across space.  In this work, we further investigate the integration of learning  analytics between the physical and digital environment by  deploying a ubiquitous learning technology and then tracing  students learning activities across space.    In todays blended instruction classrooms, paper-based exams are  still one of the most popular formal assessment methods. Paper  exams allow the teacher a reasonable high degree of flexibility in  making the exam and proctoring it (i.e. any text editing software  can support making paper-based exams; on the contrary, online  assessments may require instructors to learn new content  authoring tools, which are typically domain & application  dependent); additionally, paper-based exams minimize the  potential for academic dishonesty, which is high when exams are  online. Several issues surface as class sizes grow. For instance,  grading a large amount of paper exams is difficult.  There are  usually many inconsistencies in the grading (among and within  the graders) [4]; there are difficulties in providing feedback (hand  written feedback is time consuming; delivering graded paper  exams back to students can be challenging etc.).  Therefore,  graders end up providing only limited feedback on tests; as a  result, students may end up focusing mostly on their final scores,  among several other issues [5]. From the literature, we have  learned that feedback is one of the most effective methods to  enhance students learning [6]. There has been a range of studies  discussing the impact of feedback types and feedback timing on  learning. Even so, there are more important questions about the  ubiquitous learning environment that we should be asking: Do  students care about their returned exams at all Do they focus  only on their final score or do they put in the effort to review their  returned test When they do review, how does it associate with  their learning When using traditional paper-based exams, we can  only hope that students spend time reviewing and self-regulating  their learning.    Our research team designs a new educational technology to  facilitate grading paper-based assessment items, providing  feedback and delivering graded results to students via an online  platform. We hypothesize that providing a digital channel, which  allows students to access their physical assessments, will promote  reviewing and reflecting, and in turn positively impact students  learning. Thus, in this work, we focus on investigating students  reviewing and reflecting behaviors. We aim to answer the  following research questions: How do reviewing-and-reflecting  behaviors reveal to practitioners how they should overcome  contextual constraints How do researchers design better, more   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee. Request permissions  from Permissions@acm.org.  LAK17, March 1317, 2017, Vancouver, BC, Canada.   2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00.  DOI: http://dx.doi.org/10.1145/3027385.3027415     pedagogically sound learning analytics solutions for blended  instruction classes  In the rest of the paper, we present the theoretical background that  supports our educational technology design and research  methodology. We then describe the research platform and  classroom study design. Finally, we present evaluation results and  discuss the educational implications.    2. THEORETICAL BACKGROUND   2.1 Feedback on Learning  There are an abundance of factors that affect educational  achievement and some are more influential than others. Hattie and  Timperley [6] explores in The Power of Feedback specifically the  effects of positive vs. negative feedback. Interestingly, positive  feedback is not always positive for students growth and  achievement. For instance, praise for task performance is mostly  ineffective and extrinsic rewards can even be negative in that they  undermine peoples taking responsibility for motivating or  regulating themselves. In fact, in recent educational data mining  literature, we also found that critical rather than confirmatory  feedback is the most beneficial for learning regardless of whether  feedback was chosen or assigned [7].   In [6], researchers also discussed the effects of timing of  feedback, stressing the importance of promptness. Another study,  PeerStudio [8] also explores the timing of feedback and reaches a  similar conclusion: fast feedback is very important. The article  reports that there really is no significant difference between slow  feedback and no feedback at all. Additionally, studies also reveal  that the availability of immediate self-corrective feedback greatly  elevates performance on examinations [9]; feedback with  individual scores for components of an assignment are more  effective than feedback with a totally summed up score [8].    Ultimately, these studies suggest that immediate feedback that is  task-related that provides individual scores for components rather  than a totally summed up score (more specific) is the most  beneficial for increasing students academic achievement.  Additionally, in recent learning analytics literature, we found that  formative assessment data has higher predictive power and  aggregated data sources are key to getting timely and predictive  feedback [10]. Therefore, in this work, we aim to streamline  feedback delivery across space, capture students behavior of how  they attend to feedback, and understand how students behavior  impacts their learning.   2.2 Reflection on Learning  Metacognitive awareness and self-regulation are essential to  successful learning [11-13]. Thus, in addition to formally  assessing students knowledge, the assessments should also  encourage or allow students to reflect on their own learning and  reasoning processes. Successful learners monitor their own  memory, comprehension, and performance to evaluate their  progress, and then use this information to adapt their current  strategies and behavior e.g., [11, 12]. Learners who do not engage  in metacognitive monitoring and reflection may fail to detect  knowledge gaps (i.e., they do not know what they do not know)  and may overestimate how well they will perform on subsequent  assessments. Ertmer and Newby [14] posed an interesting  distinction between two types of reflection: reflection in action  and reflection on action. The former type of reflection occurs  during study and practice as students track their immediate  performance. The latter occurs at a higher level as students   evaluate their performance and strategies over time. In this work,  we focus on the latter type.  We observe how students reflect after  being assessed.     Metacognition and self-regulation are not easy, and many learners  need support to engage in these processes reliably e.g. [15],  particularly in large lectures where students may easily fall into  passive learning habits [16]. However, feedback and explicit  prompts can be effective in stimulating or jump-starting self- regulatory activities. In our proposed technology, students will  receive formative and summative feedback in electronic form  after quizzes and exams are graded. More importantly, students  will be prompted to react on the graded items. Specifically,  students will also be invited to express their thought processes on  the graded items.   2.3 Technology Support in Feedback  Generation and Delivery  Automated assessment is one of the most popular methods in  scaling feedback generation. It also guarantees a fast turnaround  time. Such techniques have already been widely deployed in many  assessment types and are especially pertinent in STEM subjects,  such as programming assessments, physics exercises, math  exercises etc. For instance, exemplar systems are WEB-CAT [17]  and ASSYST [18], among many. The common approach is to  apply pattern-matching techniques that verify students answers  by comparing them with the correct answers. Most of these  systems are web-based evaluation tools. Unfortunately, in our  domain of interest, programming learning, automatic  programming evaluation emphasizes only the concrete aspects of  an answer. It does not take into account the flavor of the answer  (i.e. whether the student seems to have been on the right track or  if their logic/reasoning was somewhat correct). As a result,  programming instructors frequently examine the program quality  and issue feedback personally. The assessment approach is  particularly applicable for paper-based evaluation, which leads to  our central research question: how to integrate feedback analytics  across space. A few relevant early innovations have attempted to  process paper exams and hand written code. One such example is  the tablet grading system [19]. It uses tabletop scanners to  digitalize the exam papers and provides a central grading interface  on the tablet to assist in mass programming grading. It reports a  few benefits of digitizing paper exams (i.e. some default feedback  can be kept on the digital pages; a students identity can be kept  anonymous, preventing potential grader bias that may have  occurred if the grader recognized a students name).   Other adjacent related works also attempt to address the issue of  scaling up feedback production by creating parameterized  exercises, peer production etc. WebAssign  and QuizJET [21] are  two example programs that utilize  parameterized exercises to  create a sizeable collection of questions to facilitate automatic  programming evaluation; PeerGrader [22] and PeerWise [23] are  examples that utilize student cohorts to leverage mass production.  Overall, the field of automatic programming evaluation is less  focused on grading paper-based programming problems.  Therefore, there is less support for personalization in this area.  Our goal is to study students learning effectiveness through the  use of a new feedback delivery tool that bridges the physical and  digital learning environment.       3. RESEARCH PLATFORM: WEB  PROGRAMMING GRADING ASSISTANT   We develop a web-based system to facilitate grading paper-based  exams online. We name it Web Programming Grading Assistant  (WPGA). WPGA connects paper-based assessments to the  cybersphere and ensures teachers the flexibility to continue using  paper exams without having to learn new content authoring tools.  WPGA has three key features:    (1) Documenting paper-based assessments; (2) Augmented  grading and feedback-giving; (3) Reflective feedback delivery.   3.1  Streamlining the Documentation Process  of Paper-based Assessments  We utilize quick response codes (QR-codes) to associate each  assessment hard copy with an individual learner. We then use an  automatic document feeder to scan the students written exams.  All the scanned documents will be imported to WPGA as images.  The streamlining documentation process not only digitizes  physical content, but also establishes learner and learning content  links.   3.2 Augmented Grading & Feedback-giving  Interfaces  From the instructors perspective, management functionalities are  implemented to leverage grading efficiency (interface omitted).  The documents are digitized and labeled with learners  information and learning content (question number, question text,  exam number etc.).  This makes it easy for the instructor to  partition an exam into sections, and thus assign graders to target  specific question sets. Multiple graders are able to grade distinct  sections of the same exam simultaneously. WPGA is designed to  enhance grading efficiency at the macro-level, by greatly reducing  the time required for communication and physical content  delivery between instructors and graders.    From the graders perspective, the sorting feature permits a grader  to easily grade the same question on different students exams all  at once (Figure 1). This resolves the challenge of having to flip  through hundreds of pages when grading stacks of paper exams. It  also increases grading coherence because each grader focuses on  grading a certain question [4].    Figure 2 shows the grading interface, where the grading scheme is  bundled with feedback into interactive buttons (upper right  corner). Each question is awarded full marks in the default setting,  with all feedback buttons being blue. Upon each click, the grades  can be automatically calculated based on instructors pre- configured grading schemes, and the feedback button will turn red  (partial understanding) or grey (missing this concept).  Additionally, free form feedback can be provided in the text area.  According to our previous studies [4, 24], graders prefer to type in  comments rather than physically write them on paper. They are  comfortable using the technology to grade. Most importantly,  graders are able to copy and paste previously used comments for  similar mistakes.  Having to reuse earlier comments is a common  scenario.      Figure 1. An exam sorted by questions.   3.3 Reflective Feedback Delivery: Student  Interfaces  We purposefully design a reflective feedback delivery interface to  allow students to receive not only the numeric values of their  assessment grades & graders feedback, but to also allow them to  reflect and monitor their performances, through note-taking,  bookmarking, and explicitly acknowledging their understanding.  We implement three forms of reflection prompts: (a) a star  bookmark to note the importance of or the need to reference a  question in the future; (b) a checked box to express I know how  to solve it now; and (c) a free form text area allowing the student  to type in elaborated notes. Such prompts to reflect or explain  [25] can encourage students to (a) reflect on the accuracy of their  responses, and (b) reflect on the reasoning processes that led to  their answer (i.e., reflection in action). We consider the collection  of bookmarks, checkboxes, and notes as an externalization of  what s/he knows, and thus s/he might become more  metacognitively aware of her/his own subject matter knowledge  [26].     Figure 2. Grading interface: grading scheme is tied to  feedback buttons; free form commenting area; correct   solution details.  There are two levels on which to view the student's interface:  Exam level overview (Figure 3) & Question level detail view  (Figure 4). Exam level overview provides the summary of the  quiz or exam, which displays the grade summary and the  snapshots of each exam page. Graded items are color-coded to  facilitate navigation: green shows full marks, red indicates zero  marks, and yellow indicates partial credit. To select a page to  review, one can click on the snapshot and enter the Question level  detail view. In this level, one can see the scores awarded for the  question, the grading scheme, the grading feedback, and the  correct solution. In addition, one can take notes to reflect on the  particular problem-solving assessment item or also bookmark the  question for future reference. We log all the students behavior in     the feedback delivery interface.  These behaviors include when  they log on to check their exams and what they review. Finally,  we provide a checkbox that students can use to mark a question as  understood or not understood (Figure 4  bottom right  corner).     Figure 3. Exam level overview: overall marks and the number   of correct/incorrect question will be shown.   4.  METHODOLOGY  In this section, we describe the methods we used to research  students behavior in investigating feedback analytics.    4.1 Study Design & Data Collection  To investigate the impact of feedback on students learning, we  use WPGA to capture students behavior. We ask: What do  students do after exam grades are published Do students review  the exams at all When they do attempt to review, do they spend a  good amount of time studying or do they perform only a shallow  review What content are they reviewing Do students only look  at the questions that they made mistakes on or do they conduct a  comprehensive review We conduct a classroom study in an   undergraduate Object-Oriented Programming & Data Structure  course, offered by the Computer Science program at Arizona State  University. The course is taught in a traditional blended  instruction format, with face-to-face lectures, supported by online  assignment submissions and in-class paper-based quizzes and  exams. There are two instructors among three sessions who teach  the same course. All sessions are supported by WPGA, which  serves as the formal assessment delivery portal. To control the  potential variables that may or may not affect the effectiveness of  WPGA, this paper focuses on the sessions taught by the same  instructor, which consists of 232 students in total. One session  consists of a smaller amount of students than the other (39 & 193  respectively). Essentially, both sessions are identical in terms of  course material and course conduct. The only difference is that the  sessions are offered on different days and hours. The smaller class   has three class meeting times (Monday, Wednesday, Friday); the  larger one has two (Tuesday and Thursday)    WPGA was just officially launched at the beginning of this Fall  semester. Currently, there are 6 computing courses using WPGA,  including Introduction to Programming, Data Structure, and  Algorithms. There are 35 active graders, including 3 instructors  and 32 student graders. At the moment of writing, there are 32  quizzes and exams administered collectively from these 6 courses,  serving 1198 of student users. WPGA records 12 distinct  operations, including exam and question clicks, filter clicks,  bookmarks, notes, see the correct answer clicks etc.    Figure 4. Question level detail view: summative and formative feedback will be displayed; students reflection can be  submitted and logged via this interface.     4.2 Modeling Sequential Review Strategy  using Hidden Markov Model  The Hidden Markov Model (HMM) is a popular method for  modeling sequential data. Previous studies have already shown its  effectiveness in modeling user information search processes [27,  28] and student learning processes [29]. In this study, we employ  the HMM to model students hidden tactics in reviewing an exam,  and refer to the use of each activity (e.g. view question, keep  notes) as the generated tactics by the hidden states. The hidden  tactics can be explained as the strategy used during the review  period.   The model consists of a sequence of review and reflect activities  from T1 to TM, and each activity is one of those predefined  actions: TS = {E, C, I, F and R} (Table 1). HMM assumes that we  also have a sequence of hidden states, from H1 to HM, and each  activity is generated by a corresponding hidden state, but different  activities can be generated by the same hidden state with different  probabilities. A HMM model has several parameters: the number  of hidden states HS, the start probability of each state , the  transition probabilities among any two hidden states Aij and the  emission probability from each state to each action bij. By only  defining the HS and , a Baum-Welch algorithm [30] can be used  to learn the emission and transition probabilities.  In order to investigate further on the impact of reviewing  behaviors on students learning, we look into their actions on the  returned assessments via WPGA. We categorize all students  activities based on their interactions with WPGAs interface; there  are two categories (review and reflect) of behavior, with a total of  five actions (review exams (E), review correct questions (C),  review incorrect questions (I), apply advanced filtering to review  content (F)), and finally, reflect on learning content (R) by  keeping notes, bookmarking, or marking the item as learned  (Table 1). Specifically, we hypothesize that repeated reviewing  actions should lead to learning. To verify our hypothesis, we first  conduct unsupervised clustering on mined students performances  and sequential patterns. Secondly, we apply the Hidden Markov  Model to uncover behavior transitional tactics.   Table 1. WPGA reviewing and reflection activities   Behavior Action Description   Review   Exam (E) Click on Exam tab to examine each  individual quiz/exam; Overall marks  are shown.   Correct  Question (C)   Click on a single question to examine  question & answer details; Question  marks, graders/instructors  feedback; Question is color coded in  green   Incorrect  Question (I)   Click on a single question to examine  question & answer details; Question  marks, graders/instructors  feedback, and reflection prompts are  shown; Question is color coded in  yellow or red   Filter (F) Click on any advanced filters to  select targeted set of questions, i.e.  show only bookmarked questions,  not yet reflected questions, show  both.   Reflect Reflect (R)  Keep notes on the question  reviewing interfaces; Bookmark the  question for future review; Tick a  checkbox to acknowledge ones  understanding on a question.    5. EVALUATION  5.1 Descriptive Data Results   There are several observations based on the descriptive data, they  are reported in the following subsections.    5.1.1 Students review quizzes regardless of if they  count towards academic performances or not   We found that when quizzes are only counted as attendance  toward their academic performance (quiz 1-2 & 4-6), averagely,  34.9% of the students review them at least once (Table 2).  Although the review rate is lower than the rate for quizzes that do  count towards academic performance (quiz3: 47.6%, exam1:  80.9%), it demonstrates students effort to learn the subject.  Note  that previously, we were unable to track how little or how  frequently students reviewed past quizzes or exams once graded  and redistributed. Presumably, students would review the returned  assessment items. Now, we know for certain that quizzes are  reviewed by 34.9% of students on average regardless of if the quiz  accounts for academic performance or simply for attendance.  Other questions arise: Do these attendance quizzes help in  learning at all How does focused reviewing behavior of formal  assessments impact learning To answer these questions, we must  look more deeply at how students review and what content they  review (in section 5.2).    Table 2. Average WPGA view rate (%) per assessment item.  Quiz3 & Exam1 scores count towards the final course grade;   the other quizzes are purely for attendance.   Avg  View  (%)   quiz1 quiz2 quiz3 quiz4 quiz5 exam1 quiz6   57.5 36.6 47.6 36.9 24.7 80.9 18.8   5.1.2 Hardworking reviewers perform better than  sluggish reviewers.    Based on WPGA average view distribution, we notice that  students generally pay more attention when the assessments are  directly attributed to course performance, such as quiz3 and  exam1 (Table 2). It raises the question: Who is reviewing the  assessments and how do they perform We do a binary split on  student cohorts into two groups based on whether they viewed the  graded exam1 or not. We hypothesize that the students that  viewed their graded exams are hardworking; they should have  higher exam marks, and vice versa. Therefore, we label the group  that viewed exam1 at least once as the Hardworking group; the  other group is labeled as the Sluggish group, because they failed  to view their graded assessments. We found that the Hardworking  group indeed achieved higher average exam scores (M=91.64,  SD=6.68), while the Sluggish group scored 83.54 on average, with  a large standard deviation, 16.46 (Table 3). The Hardworking  group shows more coherent performance (smaller standard  deviation) and the Sluggish group illustrates the opposite.  Additionally, we look into how often and for how long  hardworking students reviewed their exams on WPGA. We found  that hardworking students viewed exam1 9.26 times on average  (SD=7.76).   Each visit was approximately 373.74 (SD=150.20)     seconds (about 6.22.5 minutes; we consider a students first  action to the last action on WPGA, thus, it eliminates those  students who log on but do nothing). The amount of effort  demonstrates the behavior of hardworking students. The next  question is: What do hardworking students do specifically when  reviewing on WPGA that contributes to their learning We will  discuss students reviewing behavior shortly in Section 5.2.   Table 3. WPGA users performances   Hardworking Sluggish   MSD 91.646.68 83.5416.46   5.2 Behavior versus performance clusters   5.2.1 General behavioral patterns based on WPGA  usage  Based on all the labeled students behavioral actions, there is a  range of 1 to 73 actions on one exam for a total of 188 out of 232  students (Note that these 188 students were previously labeled as  the Hardworking group, who used WPGA to review the exam at  least once). We first sort the exam scores by median, splitting  them into Top-half (M=96.28, SD=2.59, the third bar) and Bottom- half (M=83.43, SD=7.45, the fourth bar) (Figure 5). We find that  Top-half falls into the letter grade A range; Bottom-half belongs  to the letter grade B range (we consequently refer to both halves  as A students and B students). There is a significant difference in  grades between the two groups (p<.01).      Figure 5. The blue bars represent students who use WPGA   and indicate what their assessment outcomes are. The orange  bar represents the lazy students and shows that they never   access WPGA nor view their exams.  We then look into their corresponding reviewing and reflecting  behaviors. Figure 6 shows the behavior frequency distribution for  both performance groups. We found that no matter which  performance group a student is in, they attend to their exam and  correct questions frequently (3 times on an exam per person and  6.5 times on correct questions). In fact, B students review correct  questions at least 1 more time on average per person than A  students do. It again demonstrates the amount of effort that  hardworking students invest in reviewing. The frequencies of  filtering questions, reflecting on questions, and reviewing  incorrect questions are lower than the frequencies of reviewing  the correct questions and reviewing the overall exam score.  Therefore, we plot the data using a different scale (Figure 6 -  right) in order to visualize the contrasts between different  performance groups. We found that B students review incorrect  questions significantly more than A students. This is not  surprising, as B students made more mistakes on their exams, and   therefore have more incorrect questions that they are able to  review. We also found that B students applied significantly more  advanced filters when reviewing than A students did.  However,  they only reflected on their learning as frequently as A students.  Such results indicate that B students might demand more support  while reviewing; additionally, these students may not be reflecting  enough on what they learned during their review.      Figure 6. Average behavior frequency (y-axis) for A & B   students.   5.2.2 Overall sequential behavioral patterns   In order to understand the reviewers effectiveness, we investigate  their strategies by clustering all students reviewing sequences.  We performed k-means clustering analysis. According to elbow  criterion [31], we found 6 distinct sequential pattern clusters  (Figure 7), where the x-axis represents the length of the sequence,  and each bar indicates each sequential pattern observation. Cluster  1 shows that 28.19% of the students conducted minimum review  activities. That means either there was nothing worth reviewing or  they did merely a shallow review, only looking at their overall  exam score or the incorrect questions. Cluster 2~3 show low to  medium review activities from a majority of the students (31.38%  and 29.25% respectively).  This cohort managed to conduct a  more comprehensive review by mixing up diverse reviewing  events, such as examining correct and incorrect questions,  reflecting on their learning, and applying filters to perform a  concentrated review. Finally, Cluster 4~6 conducted long review  sequences with various review activities and multiple reflection  episodes; however, this accounts for only 11.7% of the students.  Cluster statistics and attributes are summarized in Table 4. The  clusters reveal overall sequential behavioral patterns: reflection  generally happened in higher reviewing activities clusters. The  results suggest that there might be a positive correlation between  reviewing and reflecting, which is affirmation for practices that  encourage students to review their assessments more frequently.   Table 4. Summary of sequential pattern clusters   Cluster Attributes   1 Minimum review activities, no reflection activities (n=52)   2 Low to medium review activities, low reflection activities (n=59)   3 Low review activities, low reflection activities (n=55)   4 Medium review activities, low review incorrect questions and filters, high reflection activities (n=16)   5 Medium to high review activities, high reflection activities (n=3)  6 High review activities, low reflection activities (n=3)   0.00   5.00   10.00   15.00   20.00   Exam CorrectQ   0.00   1.00   2.00   3.00   A  B     5.3 Mapping the Reviewing Sequences to  Hidden Markov Model   To dig deeper into the learning effectiveness of reviewing and  reflecting behaviors, we use A & B student groups of sequential  actions data to construct statistical models to uncover the  probability of internal action transitional structures. The first step  in using the Hidden Markov Model (HMM) is to determine the  number of hidden states. A complex model with a large number of  states will increase the sequence likelihood because there are  more parameters that can be used to describe the model more  precisely. However, there is a high risk of over-fitting. A simple  model is less likely to over-fit the given dataset, but it may not be  able to uncover the natural features of the dataset. How to  determine the number of hidden states is still an open issue.  It is a  model selection problem in parameter learning of the HMM. In  our model selection, we use Akaike Information Criterion (AIC)  [30] to determine the optimal number of states. Based on the  models best performance by AIC, we choose HS=4 and HS=5 for  A and B student groups accordingly (Figure 8).     Figure 8. Choosing number of hidden states using AIC   Based on the state transitions for both performance groups  actions, we found the following interesting results:    5.3.1 Both A & B students perform on overview first,  detail on demand.  The hidden states are considered to be students reviewing  strategies. For instance, HS1 from both A & B students refers to  the reflection state, where students perform reflection activities.   This usually involves reviewing either incorrect questions or  correct questions. Another example, illustrated by the B students  HS4, reveals that students tend to focus on incorrect questions  when reviewing their exams. According to the prior probabilities  (start probabilities), the highest probabilities for both A & B  students are HS4 & HS3 (0.88 and 0.78 respectively) (Table 5). It  means the review sessions are likely to happen from these states.  Both states begin with reviewing the Exam, which are 0.95 & 1  with no other first order transitions (Table 6). The emission  probability of each hidden state to review/reflect states is shown  in Table 7 & 8, in which the probabilities under 0.05 were  removed for better presentation of the results. Such results  illustrate that most students attend to summative feedback first, by  examining their overall exam marks.      Table 5.  The prior probability of each hidden state ()  Student group HS1 HS2 HS3 HS4 HS5   A 0.01 0.05 0.045 0.88 -  B 0.03 0.08 0.78 0.02 0.08      Figure 7. WPGA usage in sequential behavioral pattern clusters: reflection  generally happened in higher reviewing activity clusters.     Table 6. The hidden states of reviewing and reflecting  behavioral actions (bij)   Hidden States E C I F R   A-students   HS1 0 0 0.520 0 0.443  HS2 0 0.999 0 0 0  HS3 0 1 0 0 0  HS4 0.950 0 0 0 0   B-students   HS1 0 0.486 0 0 0.514   HS2 0 1 0 0 0   HS3 1 0 0 0 0   HS4 0.301 0 0.650 0 0   HS5 0 0 0 1 0     Table 7. A students transition probability among the hidden   states (Aij)   A HS1 HS2 HS3 HS4   HS1 0.694 0.158 - 0.147   HS2 - 0.416 0.554 -   HS3 - 0.654 - 0.323   HS4 - 0.147 0.286 0.550     Table 8. B students transition probability among the hidden   states (Aij)  B HS1 HS2 HS3 HS4 HS5   HS1 0.721 0.068 0.151 - 0.060   HS2 - 0.850 0.129 - -   HS3 - 0.503 0.446 - -   HS4 - 0.070 - 0.930 -   HS5 - - 0.253 - 0.723   5.3.2 A students review & reflect strategically: they  strive to get the wrong right.  Recall the B students HS4 (previously discussed in 5.3.1) in  which students reviewed exams and targeted incorrect questions.   From this information, we found that there are no internal  transitions from HS4 ! HS1 or HS1! HS4. It suggests that B  students did examine their mistakes, but never disclosed their  thoughts after reviewing them. On the contrary, A students  typically made fewer mistakes (higher grades imply fewer errors),  but did reflect after reviewing (HS2!HS1 and HS4!HS1). From  figure 9 orange shades & Figure 10 grey color shades are the  identified reviewing incorrect questions states (red bars indicating  reviewing incorrect questions), where A students managed to  reflect on the incorrect questions (actions I & R coexist in Figure  9 orange color shades state), but B students failed to do so (when  incorrect questions were reviewed, there were no corresponding  reflection activities; Figure 10 grey color shades state).    5.3.3 B students review persistently, but fail to  engage in deeper reflection.  We now examine the B students transition probability results  (Table 8).  Each diagonal cell is the highest in each row (except   HS3), which suggests a very interesting phenomenon in the  reviewing process: the same types of actions tend to be applied  closely with each other. This indicates a clear consistency among  reviewing episodes. One of the biggest benefits of applying such a  review strategy is that the coherent reviewing procedure may  help reviewers reduce the cognitive load caused by switching  between different types of actions. For instance, a student may  keep on browsing graded items during review. However, in order  to raise students metacognition, we argue that students should  switch between reviewing and reflecting. They should not  unmindfully review without deeper reasoning or thinking.    Moreover, based on Figure 10, orange-color shades is the  identified reflection state for B students. It shows that B students  tended to make remarks on the correct questions. Such findings  suggest that B students may still consider the correct questions  challenging, and therefore, spend time reviewing them. Based on  reflection activities (keeping notes, bookmarking, and  acknowledging how to solve a problem), we also find that B  students mainly just keep bookmarks instead of engaging in  deeper reflection of assessment items. These discoveries are  important learning analytics for teachers to be able to remind  students to engage in deeper reflection. Meanwhile, they open up  opportunities to build intelligent models in the system to alert  such behaviors when a predictive sequential behavioral model  detects them.   6. CONCLUSIONS & DISCUSSIONS  6.1 Summary & Discussions  The goal of this project was to study students' learning  effectiveness through their use of Web Programming Grading  Assistant (WPGA), a homegrown innovative educational  technology that facilitates grading and feedback delivery of paper- based assessments. We designed a classroom study and collected  data from a lower-division blended-instruction computer science  class. We tracked and modeled students reviewing and reflecting  behaviors from WPGA. From the data gathered in the study, we  were able to mine students' behavior in response to feedback  received through WPGA. We reached many interesting  conclusions.   First, our students demonstrated an effort and desire to review  assessments regardless of if they were graded for academic  performance or for attendance. Second, hardworking students  achieved higher exam scores on average and were found to review  their exams and the correct questions frequently on WPGA. Third,  we found that the majority of students engaged in minimum and  low to medium review activities, while only a small percentage of  students conducted long review sequences and various  review/reflection activities. Lastly, A students and B students all  exhibited similar initial reviewing patterns, but different in-depth  reviewing and reflecting strategies. All students initially paid  attention to their exam scores. After reviewing their overall exam  scores, A students reflected on past mistakes, while B students  tended to repeat the same reviewing procedure and focus on  reviewing correct questions, and ultimately failed to engage in  deeper reflection.     Classroom orchestration is a field in transition, which defines  how a teacher manages multilayered activities in real time and in  a multi-constraints context [32]. The challenge of managing a  large size blended instruction class is evident. WPGA is designed  to support bridging the gap between the physical and cyber sphere  by aggregating multidimensional learning analytics. It prevents     teachers from being fearful that students may not be putting in  enough effort. It reinforces the feedback loop by amplifying  review-and-reflect opportunities. In conclusion, WPGA  successfully captures reviewing and reflecting learning behaviors.  The results indicate that students with higher performance  outcomes tend to engage in similar learning strategies, as do  students with lower performance outcomes.  Using the learning  analytics empowers instructors to better advise students as to how  they should improve their learning processes. Consequently,  students will ultimately be more effective and successful in their  studies.    6.2 Limitations and Future Work  Despite active usage in WPGA and many promising results found,  due to the late deployment of WPGA, only 5 weeks worth of  behavior data were captured at the point of writing. Programming  learning is inherently cumulative in nature. We need to conduct  more exhaustive analyses, such as human computer interaction   aspects of WPGA impacts on reflection quality. Additionally, one  of the foremost objectives of this project is to integrate multiple  sources of learning analytics across space. In this paper, we  focused on examining reviewing and reflecting behaviors via  seamless connection between physical and cyber spaces. In the  future, we need to consolidate the understanding of cross-spaces  by integrating more comprehensive assessment analytics, such as  assignments. Finally, we need to conduct more robust model  validations to be able to further predict students academic  performances.   7. REFERENCES  [1] Trees, A.R. and M.H. Jackson, (2007) The learning   environment in clicker classrooms: student processes of  learning and involvement in large university level courses  using student response systems. Learning, Media and  Technology. 32(1): p. 21-40.   Figure 9. A students review and reflect transition probability diagram   Figure 10. B students review and reflect transition probability diagram     [2] Martinez-Maldonado, R., Dimitriadis, Y., Martinez-Mons,  A., Kay, J., & Yacef, K., (2013) Capturing and analyzing  verbal and physical collaborative learning interactions at an  enriched interactive tabletop. International Journal of  Computer-Supported Collaborative Learning, 8(4): p. 455- 485.   [3] R. Martinez-Maldonado, D. Suthers, N. R. Aljohani, D.  Hernandez-Leo, K. Kitto, A. Pardo, S. Charleer, and H.  Ogata. (2016) Cross-LAK: learning analytics across physical  and digital spaces. in Proceedings of the Sixth International  Conference on Learning Analytics & Knowledge.    [4] Hsiao, I. H. (2016). Mobile Grading Paper-Based  Programming Exams: Automatic Semantic Partial Credit  Assignment Approach. In European Conference on  Technology Enhanced Learning (pp. 110-123). Springer  International Publishing.   [5] Ambrose, S. A., Bridges, M. W., DiPietro, M., Lovett, M. C.,  & Norman, M. K., (2010) How learning works: Seven  research-based principles for smart teaching. John Wiley &  Sons.   [6] Hattie, J. and H. Timperley, (2007) The power of feedback.  Review of educational research. 77(1): p. 81-112.   [7] Cutumisu, M. and D.L. Schwartz. (2016) Choosing versus  Receiving Feedback: The Impact of Feedback Valence on  Learning in an Assessment Game. in The 9th International  Conference on Educational Data Mining. (pp. 341-346).    [8] Kulkarni, C. E., Bernstein, M. S., & Klemmer, S. R. (2015).  PeerStudio: rapid peer feedback emphasizes revision and  improves performance. In Proceedings of the Second (2015)  ACM Conference on Learning@ Scale (pp. 75-84). ACM.   [9] Dihoff, R. E., Brosvic, G. M., Epstein, M. L., & Cook, M. J.,  (2004) Provision of feedback during preparation for  academic testing: Learning is enhanced by immediate but not  delayed feedback. The Psychological Record. 54(2): p. 207.   [10] Tempelaar, D.T., B. Rienties, and B. Giesbers, (2015) In  search for the most informative data for feedback generation:  Learning Analytics in a data-rich context. Computers in  Human Behavior. 47: p. 157-167.   [11] Butler, D.L. and P.H. Winne, (1995) Feedback and self- regulated learning: A theoretical synthesis. Review of  educational research. 65(3): p. 245-281.   [12] Bjork, R.A., J. Dunlosky, and N. Kornell, (2013) Self- regulated learning: Beliefs, techniques, and illusions. Annual  review of psychology. 64: p. 417-444.   [13] Zimmerman, B.J. and D.H. Schunk, (2012) Self-regulated  learning and academic achievement: Theory, research, and  practice. Springer Science & Business Media.   [14] Ertmer, P.A. and T.J. Newby, (1996) The expert learner:  Strategic, self-regulated, and reflective. Instructional science.  24(1): p. 1-24.   [15] de Bruin, A.B. & T. van Gog, (2012) Improving self- monitoring and self-regulation: From cognitive psychology  to the classroom. Learning and Instruction. 22(4): p. 245- 252.   [16] Freeman, S., Eddy, S. L., McDonough, M., Smith, M. K.,  Okoroafor, N., Jordt, H., & Wenderoth, M. P., (2014) Active  learning increases student performance in science,   engineering, and mathematics. Proceedings of the National  Academy of Sciences. 111(23): p. 8410-8415.   [17] Edwards, S. H., & Perez-Quinones, M. A. (2008). Web- CAT: automatically grading programming assignments. In  ACM SIGCSE Bulletin (Vol. 40, No. 3, pp. 328-328). ACM.   [18] Jackson, D., & Usher, M. (1997). Grading student programs  using ASSYST. In ACM SIGCSE Bulletin (Vol. 29, No. 1,  pp. 335-339). ACM.   [19] Bloomfield, A., & Groves, J. F. (2008). A tablet-based paper  exam grading system. In ACM SIGCSE Bulletin (Vol. 40,  No. 3, pp. 83-87). ACM.   [20] Titus, A.P., L.W. Martin, and R.J. Beichner, (1998) Web- based testing in physics education: Methods and  opportunities. Computers in Physics. 12(2): p. 117-123.   [21] Hsiao, I.-H., S. Sosnovsky, and P. Brusilovsky, (2010)  Guiding students to the right questions: adaptive navigation  support in an E-Learning system for Java programming.  Journal of Computer Assisted Learning. 26(4): p. 270-283.   [22] Gehringer, E. F. (2001). Electronic peer review and peer  grading in computer-science courses. ACM SIGCSE Bulletin,  33(1), 139-143.   [23] Denny, P., Luxton-Reilly, A., & Hamer, J. (2008, June).  Student use of the PeerWise system. In ACM SIGCSE  Bulletin (Vol. 40, No. 3, pp. 73-77). ACM.   [24]  Hsiao, I.-H., S.K.P. Govindarajan, and Y.-L. Lin. (2016)  Semantic Visual Analytics for Todays Programming  Classrooms. in The 6th international Learning Analytics &  Knowledge Conference. Edinburgh, UK: ACM.   [25] Chi, M.T., (2000) Self-explaining expository texts: The dual  processes of generating inferences and repairing mental  models. Advances in instructional psychology. 5: p. 161-238.   [26] Roscoe, R.D. and M.T. Chi, (2007) Understanding tutor  learning: Knowledge-building and knowledge-telling in peer  tutors explanations and questions. Review of Educational  Research. 77(4): p. 534-574.   [27] Han, S., Z. Yue, and D. He. (2013) Automatic detection of  search tactic in individual information seeking: A hidden  Markov model approach. in iConference. arXiv preprint  arXiv:1304.1924.   [28] Lu, Y. and I.-H. Hsiao. (2016) Seeking Programming-related  Information from Large Scaled Discussion Forums, Help or  Harm The 9th International Conference on Educational  Data Mining, EDM. NCSU. p.442-447.   [29] Piech, C., Sahami, M., Koller, D., Cooper, S., & Blikstein,  P., (2012) Modeling how students learn to program, in  Proceedings of the 43rd ACM technical symposium on  Computer Science Education, ACM: Raleigh, North  Carolina, USA. p. 153-160.   [30] Baum, L. E., Petrie, T., Soules, G., & Weiss, N., (1970) A  maximization technique occurring in the statistical analysis  of probabilistic functions of Markov chains. The annals of  mathematical statistics. 41(1): p. 164-171.   [31] Ketchen, D.J. and C.L. Shook, (1996) The application of  cluster analysis in strategic management research: an  analysis and critique. Strategic management journal. 17(6): p.  441-458.   [32] Dillenbourg, P., (2013) Design for classroom orchestration.  Computers & Education. 69: p. 485-492.     "}
{"index":{"_id":"40"}}
{"datatype":"inproceedings","key":"Quigley:2017:SMU:3027385.3027420","author":"Quigley, David and Ostwald, Jonathan and Sumner, Tamara","title":"Scientific Modeling: Using Learning Analytics to Examine Student Practices and Classroom Variation","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"329--338","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027420","doi":"10.1145/3027385.3027420","acmid":"3027420","publisher":"ACM","address":"New York, NY, USA","keywords":"classification, collaborative modeling, scientific modeling, teacher differences","Abstract":"Modeling has a strong focus in current science learning frameworks as a critical skill for students to learn. However, understanding students' scientific models and their modeling practices at scale is a difficult task that has not been taken up by the research literature. The complex variables involved in classroom learning, such as teacher differences, increase the difficulty of understanding this problem. This work begins with an exploration of the methods used to explore students' scientific modeling in the learning sciences space and the frameworks developed to characterize student modeling practices. Learning analytics can be used to leverage these frameworks of scientific modeling practices to explore questions around students' scientific models and their modeling practices. These analyses are focused around the use of EcoSurvey, a collaborative, digital tool used in high-school biology classrooms to model the local ecosystem. This tool was deployed in ten biology classrooms and used with varying degrees of success. There are significant teacher-level differences found in the activity sequences of students using the EcoSurvey tool. The theoretical metrics around scientific modeling practices and automatically extracted feature sequences were also used in a classification task to automatically determine a particular student's teacher. These results underline the power of learning analytics methods to give insight into how modeling practices are realized in the classroom. This work also informs changes to modeling tools, associated curricula, and supporting professional development around scientific modeling.","pdf":"Scientific Modeling: Using learning analytics to examine student practices and classroom variation  David Quigley University of Colorado Boulder Institute for Cognitive Science  Department of Computer Science  1777 Exposition Drive Boulder, Colorado  david.quigley@colorado.edu  Jonathan Ostwald University Corporation for  Atmospheric Research Digital Learning Sciences 3090 Center Green Drive  Boulder, Colorado ostwald@ucar.edu  Tamara Sumner University of Colorado Boulder Institute for Cognitive Science  1777 Exposition Drive Boulder, Colorado  tamara.sumner@colorado.edu  ABSTRACT Modeling has a strong focus in current science learning frame- works as a critical skill for students to learn. However, un- derstanding students scientific models and their modeling practices at scale is a difficult task that has not been taken up by the research literature. The complex variables in- volved in classroom learning, such as teacher differences, in- crease the difficulty of understanding this problem. This work begins with an exploration of the methods used to ex- plore students scientific modeling in the learning sciences space and the frameworks developed to characterize stu- dent modeling practices. Learning analytics can be used to leverage these frameworks of scientific modeling prac- tices to explore questions around students scientific models and their modeling practices. These analyses are focused around the use of EcoSurvey, a collaborative, digital tool used in high-school biology classrooms to model the local ecosystem. This tool was deployed in ten biology class- rooms and used with varying degrees of success. There are significant teacher-level differences found in the activ- ity sequences of students using the EcoSurvey tool. The theoretical metrics around scientific modeling practices and automatically extracted feature sequences were also used in a classification task to automatically determine a particu- lar students teacher. These results underline the power of learning analytics methods to give insight into how model- ing practices are realized in the classroom. This work also informs changes to modeling tools, associated curricula, and supporting professional development around scientific mod- eling.  CCS Concepts Human-centered computing  Collaborative inter- action; Applied computing  Interactive learning environments; Collaborative learning; Computing  Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of the United States government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ACM ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027420  methodologies  Classification and regression trees; Sci- entific visualization;  Keywords Scientific Modeling; Collaborative Modeling; Teacher Differ- ences; Classification  1. INTRODUCTION Scientific models represent ideas, processes, and phenom-  ena by describing important components, their characteris- tics, and their interactions. Models are constructed across a broad spectrum of scientific disciplines, such as the food web in biology, the water cycle in Earth science, or the struc- ture of the solar system in astronomy. Models are central to the work of scientists for understanding phenomena, and for constructing and communicating theories. Constructing and using models to explain scientific phenomena is also an essential practice in contemporary science classrooms. In A Framework for K-12 Science Education [23], developing and using models is one of the eight core practices deemed essen- tial for science learning and instruction. According to the Framework, [s]cientists use models... to represent their cur- rent understanding of a system (or parts of a system) under study, to aid in the development of questions and explana- tions, to generate data that can be used to make predictions, and to communicate ideas to others [24].  Scientific models can take many forms, such as textual descriptions, visual diagrams, computer simulations, and mathematical equations. For instance, in elementary phys- ical science, Schwarz et al [29] studied the development of students modeling practices by having students sketch mod- els depicting how light interacts with objects to produce shadows. Bryce et al [2] asked students to construct a clay model of a cell. Even these simple modeling activities push students to represent their current knowledge and to use this knowledge to explain new phenomena. Models are often more complex, involving visual representations or computer simulations. Such models may focus on the complex interac- tions between components (e.g. predator-prey interactions in a food web) or depict how a substance changes state over time (e.g., how water changes from liquid to gas as it moves through stages in the water cycle).  In this research, we study the development of student modeling practices in secondary biology classrooms. In these classrooms, students used a web-based software tool - Eco-    Survey - to characterize organisms and their interrelation- ships found in their local urban ecosystem. Students use EcoSurvey to: (1) photograph, map and characterize local species, (2) document how species interact around shared re- sources such as food, and (3) identify resources and species that are important to the resilience of their environment. EcoSurvey follows in a rich tradition of computer-based mod- eling tools [32, 14, 5]. These digital modeling tools pro- vide built-in affordances that foreground important scien- tific modeling practices, and are explicitly designed to scaf- fold students modeling activities, through the careful design of the interface and prompts promoting reflection and ap- propriate action [27, 5]. As such, they support students to develop more complex models that would be difficult to cre- ate using traditional tools and these models can be quickly revised thanks to their digital nature.  Digital modeling tools also provide an opportunity for in- strumentation to unobtrusively capture usage. Reflecting contemporary software architectures, EcoSurvey is a cloud- based software tool, where all changes and refinements to student models are centrally captured and stored, providing researchers with a fine-grained record of student modeling practices at scale, across potentially thousands of students in a wide range of classroom settings. These rich data of- fer opportunities for new learning analytic methods to bet- ter characterize student scientific modeling practices and to examine classroom level differences. In this paper, we use learning analytics and machine learning techniques to an- swer the following questions:  1) What variation do we see in the models created by students to support explanations of scientific phenomena, in our case, ecosystem biodiversity  2) What variation do we see in student modeling practices across different teachers  3) Can the action sequences used by students during mod- eling be used to predict each students teacher  We analyzed EcoSurvey usage data collected from over 200 secondary students across ten classrooms. We observed large variations in the completeness and complexity of stu- dent models, and large variations in their iterative refine- ment processes. We also observed large differences in stu- dent modeling practices across different classrooms and teach- ers, and we were able to predict a students teacher based on the observed modeling practices with a high degree of accuracy without significant tuning of the predictive model. These results highlight the value of this approach for ex- tending our understanding of student engagement with an important contemporary science practice, as well as the po- tential value of analytics for identifying critical differences in classroom implementation. These results shed light on potential improvements in tools and curricula. Before dis- cussing our approach and results further, we first present the education and learning sciences theories underpinning this work and describe our research context and the EcoSurvey tool in more detail.  2. THEORY AND RELATED WORK A central goal of our approach is to develop theoretically-  grounded analytic methods. Education research and the learning sciences offer insights into three areas critical to our approach: the elements of a good student model, how to characterize student modeling practices, and variation in classroom implementation across teachers.  2.1 Scientific models that support complete ex- planations of phenomena  Scientific models are tools for explanation and prediction. A complete scientific explanation should explain observed relationships between variables and describe the mechanisms that support cause and effect inferences about them [23]. Thus, to support student explanations, a scientific model of a phenomenon should include important components (vari- ables), their interactions ( relationships), and define the mechanisms involved. When modeling an ecosystem, these correspond to the organisms in the ecosystem (animals, plants, insects, fungi, etc), how these organisms interact with each other and the environment (predator, prey, producer, de- composer, etc), and the involved processes (abiotic, biotic, etc). Professional biologists use this information to measure the biodiversity of an ecosystem in terms of species richness, evenness, and divergence [10, 18, 6].  In this work, we characterize variation in students models by examining the number of organisms present, the vari- ety of types of organisms present, the number of interac- tions between organisms that students have identified, and the diversity of these interaction types. We also look at how these features are distributed within a model. These measures are used to understand the complexity of a stu- dent model. Interestingly, understanding the complexity of an ecosystem has been shown to support students to de- velop empathy and other affective stances towards nature [13]. Student understanding the flow of matter and energy through ecosystems has also been shown to vary strongly across cultural boundaries [1], providing further motivation for understanding variation in student models and student modeling practices.  2.2 Strong student scientific modeling practices Constructing scientific models is part of the inquiry tra-  dition in science education, where students learn scientific concepts through hands-on doing[15]. Understanding what students are doing at a fine-grained level can provide teach- ers with useful insights into learning processes, as well as provide teachers with feedback as to where and when stu- dents need additional assistance. Towards this end, several scholars have developed frameworks characterizing effective student modeling practices [29, 2]. Schwarz et al. [29] iden- tify a series of seven practices: (1) identifying the anchoring phenomena to be modeled, (2) constructing a model, (3) testing the model, (4) evaluating a model, (5) comparing the model against other ideas, (6) revising the model, and (7) using the model to predict or explain phenomena. Bryce et al [2] identify a similar set of practices as important to support student learning during modeling, namely (1) ob- servation (paralleling the anchoring phenomena), (2) model construction, (3) model use, (4) model evaluation, and (5) model revision. Their research suggests that supporting stu- dents to engage in these practices can lead to positive learn- ing outcomes [29].  Here, we focus on a subset of these practices - construct- ing, evaluating, revising, and using models - incorporating them into our analysis framework [3]. We focus on these four practices as they are directly supported through the EcoSur- vey interface and can be readily observed and tracked in the usage log. In addition to these four practices, we examine the degree to which students engaged in iterative design of their models. Iteration occurs when students cycle between    the other four modeling practices, where the four practices correspond directly to individual actions in the EcoSurvey interface, such as adding an organism or relationship (con- struction), editing an organism or relationship (revision), or generating a graph of the entire ecosystem to support explanations (using). Iteration is an important modeling practices that is used to both expand the scope of a model and to improve its accuracy [11, 2]. Learning analytic tech- niques are used to identify the degree to which students used these practices and to examine variations in student mod- eling practices. While these usage log analysis methods are an excellent passive way to collect data on student prac- tices [25], it is important to note that these methods do not capture information about how students are reasoning with their models. Exploring student reasoning with models and how they generate explanation using models is beyond the scope of this study, and would require deep exploration of students cognitive processes using think-alouds, cognitive interviews or other learning and cognitive sciences research methods (e.g. [29]).  2.3 Teacher Differences Student learning outcomes vary widely across teachers [12,  21]. Students with a top-performing math teacher can be expected to perform .266 standard deviations better on a standardized math test than those with a median teacher [12]. Similarly, McNeill et al [21] evaluated 22 high school ecology classrooms across the US and found that teacher differences accounted for 34.5% of the variance on scores from a multiple choice assessment and 42.5% of the variance on scores from an open ended assessment. Differences in student learning outcomes can be attributed, in part, to differences in their opportunities to learn different topics [20, 22]. For instance, in a classroom setting, the opportunity for iteration can be driven by the structure of the class: students will not expand or refine their model if they are not given the opportunity to do so.  Differences in student learning can also be attributed to differences in the curriculum being utilized, and differences in how teachers implement curriculum in their specific class- room [12]. Large variations in how teachers implement STEM inquiry-oriented curriculum have been routinely observed [16, 28], and curriculum integrating modeling is no excep- tion. Windschitl et al [35] conducted a series of studies ex- amining how K-12 teachers integrated student modeling into their classrooms and found significant variance in teacher understanding and adoption. For many teachers, the tra- ditional scientific method notion of generating a hypothesis is deeply ingrained in their views of science practices. Sub- sequently, these teachers had difficulty adopting a scientific practice that required them to ground ideas and predictions in an initial model. In some cases, they found that teach- ers simply rejected the model-based inquiry approach, citing that providing students with opportunities to engage in it- erative practices took too much classroom time and added unnecessary complexity.  In our analysis, we examine variations in student model- ing across classrooms and teachers, analyzing both students opportunities to learn and variations in the degree to which they engaged in specific modeling practices. For these anal- yses, we use measures of frequency and variety as features [31]. Frequency characterizes how often students were able to engage in the different modeling practices, whereas vari-  ety captures the breadth of practices that they engaged in. Frequency and variety have been shown to reliably predict the uptake and adoption of new technologies across differ- ent groups of users [31, 19]. Here, we use these features to study the different patterns of uptake and adoption of modeling practices across classrooms.  We also explore the ways in which an individual students modeling processes can be indicative of teacher differences. We use sequence classification techniques [36] to detect re- curring patterns, called sequential patterns or action se- quence features, in students modeling practices, as they engage in cycles of creating, evaluating, revising, and using their models. We explore the degree to which automatically extracted and optimized action sequence features are able to correctly predict a specific students teacher. These pat- tern mining methods have been used by learning analytics researchers to address questions related to course selection trajectories [4] and group work dynamics [26]. Automatic feature optimization is a common technique used in data mining to identify the features that carry predictive value for classification [9]; the resulting features can reveal in- sights into processes important for differentiating between categories [7]. In our case, we are using these sequences to detect and understand potential differences in modeling curriculum implementation across teachers.  3. RESEARCH CONTEXT: INQUIRY HUB AND ECOSURVEY  EcoSurvey was developed as part of a larger collaborative design-based research project called the Inquiry Hub, which is focused on supporting teachers in developing student- centered approaches to curriculum and teaching [30]. In- quiry Hub Biology is a digital high school biology curricu- lum developed in partnership with a large urban school dis- trict in the midwestern United States. Within the ecosys- tems unit of this curriculum, students are asked to choose a tree to plant on their school grounds or other designated site that will improve their local ecosystems biodiversity and resilience. Classes use EcoSurvey to create a collective model of their local ecosystem. They use these models to provide evidence and construct arguments to support their choice about the type of tree they choose to plant. The recommended type of tree is then planted on the site, in col- laboration with the local Parks and Recreation Department, based on the students arguments and evidence. Thus, the models students create using EcoSurvey support them to construct arguments with real world consequences. To il- lustrate the use of EcoSurvey within this context, we follow the experience of Maria, a fictional student in Ms. Smiths 3rd period class.  3.1 Data Collection and Creating the Model Ms. Smith instructs students to map the ecosystem within  a selected site on their school grounds or in the local area, taking pictures and making field notes on the organisms and interactions between organisms that they observe. Marias group makes observations along the creek that runs next to the school. They find a lady beetle, a honey locust tree, some mushrooms, a gray squirrel, and a few other organ- isms. Using their smartphones, they take pictures of these organisms and upload them to EcoSurvey, creating a card for each organism while out in the field. Each card automat-    ically captures information about the date, time, and loca- tion of the observation being recorded. Cards also include a relations field to capture interactions between organisms and information about the organisms role in the ecosystem. Students begin entering this information as they observe it in the field, and then continue to augment this information back in the classroom through additional research. In Fig- ure 1, we see Marias lady beetle card under construction. While in the field, she created the card, uploaded a picture, and added details about interactions they saw. At the same time, her team members are also creating cards for other organisms they are observing.  Figure 1: Edit view for Marias Lady Beetle card.  3.2 Evaluating the Model As students create cards, their organisms are added to a  shared class survey. The survey view shows all of the or- ganism cards and their detailed information, ordered by how recently they were edited. Maria can see that her classmates have created many cards, including a Blue Jay card (Figure 2).  Ms. Smith organizes the student groups into pairs and asks each group to review the others cards for correctness and completeness. Marias group is paired with Group 2, who completed several cards. Andre, a member of Group 2, asks Maria to first review the red tailed hawk card he created. Maria uses the search feature of the survey view to quickly find the hawk among the cards. She notices that this card is missing many details, including interactions with other organisms.  Figure 2: Main view of Ms. Smiths class survey.  3.3 Revising the Model Maria recommends that Group 2 do further research into  how the hawk contributes to the local ecosystem. She also takes the chance to update her groups gray squirrel and honey locust cards. She discovered that hawks prey upon squirrels and nest in honey locust trees during her earlier  research. She didnt realize that their school ecosystem in- cluded hawks until she reviewed the work of her classmates, as her group did not see a hawk. Once Maria has completed editing her groups cards, she continues her review of Group 2s cards. She uses the group select function to view only the cards created by members of Group 2.  Group 2 notices that two people in Marias group cre- ated duplicate lady beetle cards. Maria decides to add her lady beetle information to the other card, since it is more detailed, and uses the delete function to remove her lady beetle card from the model.  3.4 Iterating the Model In reviewing Group 2 cards, Maria sees a card for geese,  but notices that the group did not add a predatory relation- ship to grass, even though she observed geese eat the grass on the soccer field. She uses the search functionality and discovers that no one in class created a card to document grass as an observed organism. Maria adds a new card for grass and includes a predatory-prey relationship with geese. By cycling back through earlier modeling practices (creating new cards), Maria is iteratively improving the class model to be more complete and accurate.  3.5 Using the Model Once the class has created a robust model of their local  ecosystem, students use this model to construct arguments for choosing a particular tree to plant. Maria presses the create relation graph button, which generates the graph representation of the model and exports it to a digital graph- ing tool(Figure 3). Maria and her team study the result- ing diagram that enables them to visualize the relationships (links) between all the organisms (nodes) they have cata- loged. It is clear from looking at her graph that the English Oak trees are an important keystone species in their site, in- volved in a large number of relationships with a wide variety of organisms. The geospatial locations in the observational data indicate that there are only two English Oak trees lo- cated in their site; Maria and her group recommend planting an additional tree of this type.  Figure 3: A section of Marias final graph.  3.6 Analyzing EcoSurvey Use Marias scenario illustrates how EcoSurvey supports stu-  dents to engage in the practices of creating, evaluating, revis- ing, iterating, and using models. To use a learning analytics approach to study modeling practices, we must map specific actions, or sequences of actions, taken in the EcoSurvey in-    Table 1: EcoSurvey Actions  Modeling Practice Description EcoSurvey Actions Create Model Create a new entry in the model New Card Evaluate Model Explore the organisms and interactions in the current  model Group Select, Search  Revise Model Edit or delete organisms and interactions included in the current model  Edit, Delete  Use Model Export a representation of the model for use (e.g. con- structing an argument)  Generate Graph, Download  Iterate Cycle between creation, revision, and use practices New Card, Edit, Delete, Generate Graph, Download  terface to specific modeling practices. Table 1 describes the mapping between modeling practices and specific EcoSurvey interface actions that we use in our analyses. As students interact with EcoSurvey, the system captures and logs each of the actions shown in Table 1. Each log entry includes the time, user, survey, and action type.  4. METHODS Here, we describe data used in our analyses as well as the  specific analytic techniques used to answer each of our three research questions. All teachers names are pseudonyms.  4.1 Study Data EcoSurvey usage log data was collected from 262 students,  across 10 high school classrooms, during Fall 2015. These 10 classes were taught by three different teachers: Ander- son, Baker, and Chavez. Anderson taught two periods of high school biology, which she elected to combine into one group to produce a single ecosystems model. Baker taught three periods, while Chavez taught five. From the sample, we recorded actions for 204 students, while 58 students did not record any activity. All classrooms in this sample fol- lowed a 3:1 device deployment where three students used one laptop together; thus it is not surprising that there are students with no recorded activity. A total of 9 models were created, which included 586 organism cards and 545 inter- actions, generating 3160 action logs.  4.2 Variation in Student Scientific Models Our first research question examines variation within stu-  dent models, focusing specifically on the richness of students models in terms of the number of organisms and their rela- tionships. We analyze the relative number of organisms and interactions within each class survey. We also look at the balance of interactions per organism by evaluating both the average number of interactions per organism and variance in the distribution of interactions. Examining variance al- lows us to distinguish different patterns in the assignment of interactions to organisms. Some classes may create models where most organisms have a similar number of interactions, while other classes may create models where only a few or- ganisms have been assigned many interactions.  4.3 Variation in Modeling Practices Our second research question examines variation in stu-  dent modeling practices, focusing on action variety, frequency, and iteration. Action variety refers to the range of actions a student performed. For example, some students may have only created and edited cards, while others may have used  the full range of EcoSurvey actions. Frequency refers to the total number of actions completed by an individual student and the number of usage sessions they engaged in. Sessions are defined by a series of actions from a single user without a large break in activity (greater than two hours). Defining a session using a two hour gap allows for any student activ- ity within a long class period to occur within one session; several of our classrooms employ 1.5 hours block periods.  To characterize iteration practices, we look for evidence of design cycles within the log information. Design cycles can be recognized when students engage in multiple sequences of construct-revise-use practices. This focus on a sequence of practices is consistent with Schwarz et al [29], which charac- terized modeling practices as a series of steps. By extension, a design cycle consists of returning to a previous modeling step after moving on in the sequence (e.g. creating a new card after editing a different card). We counted the number of cycles as a measure of iteration.  Combined, these three metrics - action variety, frequency, and iteration - yield an eight feature vector for each student consisting of total number of EcoSurvey actions, total num- ber of create actions, total number of evaluate actions, total number of revise actions, total number of use actions, total number of EcoSurvey action types taken, number of ses- sions, and number of iterations. We combined the feature vectors for students with the same teacher, and performed a Kruskal-Wallis H test [17] for each feature to determine differences between teachers. A Kruskal-Wallis H test is a non-parametric adaptation of an ANOVA to compare sam- ples of different sizes, as we have in our groups. We further explored these differences using Tukeys HSD test [33] to test the significance of pairwise differences between teachers.  4.4 Predictive Value of Modeling Practices Our third research question examines the degree to which  we can use sequences of student modeling actions to pre- dict that students teacher. For this prediction task, we use the previously described features of variety, frequency, and iteration as well as automatically extracted sequence pat- terns. This sequence pattern approach is inspired by the feature-based sequence classification methods summarized by Xing, Pei, and Keogh [36]. In our work, a sequence pat- tern consists of a series of EcoSurvey actions (e.g. New Card, Edit, Generate Graph) embedded within a stu- dents complete action log. To extract sequence patterns, we used the Colibri Core [34] software package. This soft- ware package, originally designed for natural language pro- cessing tasks, treats every action as a token and determines the frequency of consecutive token sequences (n-grams) from    Table 2: Final models for each class.  Survey # Users Organisms Interactions Interactions Per Organism Interaction Variance Anderson 4 & 7 29 155 264 1.7 4.35  Baker 1 28 47 7 0.149 0.297 Baker 2 27 25 5 0.2 0.24 Baker 4 29 19 0 0 0  Chavez 1 27 88 70 0.795 0.663 Chavez 2 29 45 27 0.6 1.31 Chavez 6 30 60 57 0.95 3.78 Chavez 7 31 81 82 1.012 5.72 Chavez 8 32 66 33 0.5 0.826  student usage logs. These token sequences can include wild- card actions (skip-grams). For instance, the software will extract the sequence New Card, Edit, Generate Graph as either an n-gram or as the skip-gram New Card, {*}, Generate Graph. This skip-gram will capture similar se- quence patterns, where one action occurs between New Card and Generate Graph actions. This yielded 2,893 unique se- quence patterns, that occurred at least three times, across all student usage logs. Once we extracted these sequence patterns, we used them as a new series of features to aug- ment each students existing feature vector. This approach parallels that used by dAquin et al [4], where they used se- quential pattern mining to study student course enrollment patterns.  To understand which features that characterize a students modeling actions are most predictive of his or her teacher, we input subsets of each students feature vector into four Naive Bayes classifiers using Weka [8]. The first classifier used the eight features related to variety, frequency, and it- eration of actions. The second classifier used the full set of sequence pattern extracted by Colibri Core for each stu- dent. The third classifier implemented a best-first search [9], which automatically reduced the full set of sequence patterns to the eighteen most predictive features. The last classifier combines the eight variety, frequency, and iteration features with the eighteen most predictive sequence patterns. Each test was run using 10-fold cross validation.  5. RESULTS Results are presented for each of our three research ques-  tions.  5.1 (RQ1) What variation do we see in the mod- els created by students  As shown In Table 2, there are substantial variations in the models created by students in different classrooms. We see that Andersons students documented many more or- ganisms (155) and interactions (264) than all other classes. Though Anderson had both of her classes work together to create one survey, the total number of students contribut- ing to this model is comparable to the number of students contributing in other classrooms. We also see that students in Bakers three classes each documented significantly fewer organisms and interactions. One class only documented 19 organisms (less than one per student) and did not document any interactions. Chavezs classes exhibit wide variation, particularly in the numbers of interactions documented by each class.  The number of interactions per organism, a broad mea- sure of model complexity, further illustrates apparent class- room differences, with Andersons class creating more com- plex models than Bakers and Chavezs classes. To better understand classroom differences, we examine variance in the number of interactions per organism. In Andersons class, we see a high variance in comparison to the inter- actions per organism metric, which indicates that there are a small number of organisms with lots of interactions and many organisms with few interactions.  Chavez P1 and P7 classes provide a particularly inter- esting case to examine this variation. On reviewing Table 2, we see that the variance in the number of interactions assigned to each organism is significantly lower in P1 than in P7, while the actual number of organisms and interac- tions are comparable. Further analyses reveal that students in Chavezs P1 did not assign any interactions for 39% of their organisms, while students in P7 did not assign interac- tions to 74% of their organisms. A similar analysis revealed that 42% of the organisms documented in Andersons model did not include interactions. In most classes, the majority of organisms have no documented interactions. It appears that students engaged significantly more with describing or- ganisms, and spent far less time consistently documenting interactions.  5.2 (RQ2) What variation do we see in stu- dent modeling practices across different teachers  There are significant differences between the student ac- tion sequences of our three teachers on all eight metrics re- lated to variety, frequency, and iteration (p < .001). Our Tukeys HSD test for each feature shows that the three groups are each distinct to a significant degree in Create, Revision, and Iteration frequency (Figure 4a, p < .05), as well as Overall Actions, Session Count, and Action Vari- ety (Figure 4b, p < .05). We also see Andersons students performed significantly more Evaluate and Use actions than the other two teachers students (Figure 4a, p < .05), though the differences between Bakers and Chavezs students are not significant. Andersons class also used EcoSurvey twice as much, as measured by session counts. Overall, Ander- sons students engaged in more modeling practices than both of the other two groups, and Chavezs students engaged in more modeling practices than Bakers.  There were also differences in the modeling practices that students employed. Students in Bakers classes rarely en- gaged in three of the five modeling practices we are study- ing: revisions, iteration, or use. Chavezs class engaged with    (a) The average number of actions by modeling prac- tice type.  (b) The average number of actions, types of actions, and action sessions.  Figure 4: Student modeling practices for each teachers students.  four of the five practices, but appeared to rarely use their models.  5.3 (RQ3) Can the action sequences used by students during modeling be used to pre- dict each students teacher  As shown in Table 3, student action sequences can predict their teacher with varying degrees of reliability depending upon the features used. Our baseline assumes that each stu- dent is in one of Chavezs classes; almost 52% of the students in this study were in one of his classes. All of the feature sets we studied improved performance over the baseline. Clas- sifying based on all 2,893 sequence patterns improved our classification accuracy by almost 12%, whereas classifying solely based on our variety, frequency, and iteration fea- tures improved performance by over 15%. We also trained a model on the best sequence patterns, that is, the 18 most  predictive patterns identified by Wekas Attribute Selection tool [9]; this yielded a nearly 25% improvement in perfor- mance. The best performing model was one that combined the most predictive sequence patterns with our variety, fre- quency, and iteration features. This combination resulted in a 30% improvement over baseline, correctly predicting a students teacher 80% of the time.  The most useful features for classification accuracy are the 18 best sequence patterns (Table 4). A closer examina- tion reveals that these sequence patterns correspond to our five modeling practices in interesting ways. These patterns prioritize model revision, evaluation, and iteration as dis- tinguishing features, which correspond to the differences in classroom modeling practices discussed under research ques- tion 2.  To better understand the types of errors that our best performing model makes, we generated a confusion matrix  Table 3: Predictive accuracy of each action sequence feature set.  Feature Set # Attributes Naive Bayes Acc Baseline 0 51.96%  All Sequence Patterns 2,893 63.73% Variety, Frequency, and Iteration Features 4 67.65%  Best Sequence Patterns 18 75.00% Combined Features 22 80.39%    Table 4: The most predictive action sequences.  New card, New card, {*}1, New card, {*}, {*}, New card  New card, {*}, Group Select, {*}, New card  Group Select  Group Select, {*}, Group Select Group Select, {*}, {*}, {*}, Group Select  Group Select, {*}, New card, {*}, New card  Group Select, Search Search, {*}, {*}, {*}, Edit Edit Edit, Edit Edit, {*}, Edit Edit {*} {*} Edit Edit, Search Edit, Generate Graph, Download Edit, Generate Graph, Download,  Edit Generate Graph Download Generate Graph, Download  (Table 5). We see that 75% of the errors are due to the misclassification of 30 of Chavezs students as Bakers stu- dents. One possible reason for this misclassification is that some students in Chavezs classes performed very few mod- eling actions overall, similarly to the majority of students in Bakers classes.  Table 5: Combined features confusion table.  Classified As Anderson Baker Chavez  Correct Class Anderson 29 0 1  Baker 1 64 3 Chavez 5 30 71  6. DISCUSSION In this study, we demonstrated the utility of learning an-  alytic methods for characterizing variation in students sci- entific models and their modeling practices. We also showed that an individual students modelling action sequences can be used to predict his or her teacher. Our results support Windschitl et als findings documenting large variations in how teachers implement modeling in their classrooms [35]. While we did not conduct direct classroom observations, our analysis revealed profound, quantifiable differences in the models that students constructed across different classrooms and significant differences in their classroom learning expe- riences as depicted in the range of modeling practices that they engaged in.  Student models exhibited large variance in the number of organisms and interactions documented. These differences could be due to a variety of factors, such as the time allo- cated to modeling during class, the degree to which mod- eling practices were incorporated into instruction, or their teachers dispositions and knowledge about scientific mod- eling. Our results suggest that such teacher level differences do matter. Another source of variation could be differences in ability and knowledge that individual students bring to the modeling task. In our current work, we are revising the Inquiry Hub curriculum to provide better guidance to teachers to integrate modeling into their classroom, and we are providing more opportunities for students to engage in modeling throughout the unit.  Our analysis of student models also revealed a disturbing similarity across all classrooms and teachers: all the mod- els contained significant percentages of organisms that did  1A {*} refers to a wild card in a skip-gram, which can be compelted with any value.  not have a single defined interaction with another organism. Thus, these student models are missing critical elements of a complete and sound ecosystem model. It is unlikely that these models can support students to develop comprehensive explanations and predictions as called out in the Framework [23]. There are multiple possible explanations for these be- haviors, including weaknesses in the Inquiry Hub curricu- lum, the associated teacher professional development, or the design of the EcoSurvey tool. As a first step, we have made major changes to the design of EcoSurvey version 2 to make it easier for students to establish relationships from multiple parts of the interface, to visualize established relationships through an integrated graph view, and to see which organ- isms are not connected to others in the model.  The large variance we observed in student modeling prac- tices provides evidence of significant teacher-level differences. Clearly these teachers are implementing EcoSurvey and the corresponding lessons differently in their classrooms, with wildly varying results. When teachers devoted more time to modeling, as measured by sessions, their students engaged in a richer variety of modeling practices. Prior research sug- gests that there is a linkage between student engagement in modeling practices and future learning outcomes [29, 2]. Thus, it appears that students in several of our participat- ing classrooms lacked critical opportunities to learn [20, 22], that could ultimately impact their academic performance. In future work, we plan to examine the relationships be- tween student engagement in modeling practices and their learning outcomes as measured by end-of-course school dis- trict assessments.  Our predictive analysis provided further evidence of sig- nificant teacher-level differences. The feature selection al- gorithm honed in on the presence or absence of three mod- eling practices - evaluation, revision, and iteration - as the features that best predicted a students teacher. This sug- gests that future professional development and curriculum design should focus on these specific practices, ensuring that all students get an opportunity to participate in these parts of the modeling process. In EcoSurvey version 2, we have expanded features designed to support evaluation, revision, and iteration practices. For instance, we have implemented generating a visual graph of their model directly into the tool, rather than exporting this information into a 3rd party graphic tool. By facilitating students to use (visualize) their models more frequently, we hope that this will prompt them to notice shortcomings and engage in more iterative refine- ments. The most accurate classifier also benefited from additional features characterizing action variety, frequency (number of actions), and iteration. These features further highlight differences in student engagement, with some stu-    dents missing the opportunity to explore, develop, and use their models over time.  A core aspect of our analytic approach explicitly linked specific user interface actions in the EcoSurvey tool to indi- vidual modeling practices identified through prior research: creating, evaluating, revising, using, and iterating [29, 2, 11]. This approach enabled us to work with theoretically and em- pirically sound features identified through prior classroom research. And, this approach enabled us to interpret the action sequences identified as salient by our algorithms in a theoretically-informed way, enabling us to link our findings back to instructional concerns, such as curriculum design and professional development. This method of linking inter- face actions to identified modeling practices could support generalizing this analytic approach to other tools that sup- port scientific modeling.  While this study yielded many results that have informed our partnership design work, there are several limitations that are important to note. First, we are working with a limited data set, containing data from only three teachers and 9 models. While we generated interesting insights into differences between these classrooms, it is difficult to gen- eralize our findings to a broader spectrum of classrooms. Second, we cannot attribute our observed variation in mod- els and modeling practices to student-level differences, due to the shared and collaborative nature of the deployment. All our participating classrooms asked students to work in groups and each group shared a single laptop computer; we are actually observing the collaborative modeling practices of small groups rather than individual students.  7. CONCLUSION We have demonstrated that learning analytics can be used  to study student scientific models and student modeling practices at a scale that has previously been impossible. We used quantitative statistical measures to study varia- tion across models and teachers. We also used methods drawn from data mining and machine learning to identify critical differences in student modeling practices and to ex- plore which features of student modeling sequences are use- ful for classification.  This work opens the door for a wide variety of further research. Future directions could incorporate student de- mographics and examine potential differences in the uptake of modeling practices across various populations. Future work could also incorporate student assessment data to look at connections between engagement in modeling practices and student learning outcomes. Other work could further explore teacher-level differences, combining classroom ob- servations with learning analytics to better understand the different approaches teachers take during classroom imple- mentation.  The work presented here has already informed the Inquiry Hub partnerships effort. The design-based research team is making evidence-based changes to our curriculum, pro- fessional development, and classroom tools based on these results. Other research groups studying student scientific modeling can apply these theories and analytic techniques in their settings to understand variation in models, modeling practices, and classroom implementation.  8. ACKNOWLEDGMENTS  This material is based in part upon work supported by the National Science Foundation under Grant Numbers 1555550 and 1147590. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Na- tional Science Foundation. In addition, the authors would like to thank the Gordon and Betty Moore Foundation for their support.  9. REFERENCES [1] M. Bang, D. L. Medin, and S. Atran. Cultural mosaics  and mental models of nature. Proceedings of the National Academy of Sciences of the United States of America, 104(35):1386813874, 2007.  [2] C. Bryce, V. B. Baliga, K. de Nesnera, D. Fiack, K. Goetz, L. M. Tarjan, C. Wade, V. Yovovich, S. Baumgart, D. Bard, D. Ash, I. M. Parker, and G. S. Gilbert. Exploring Models in the Biology Classroom. The American Biology Teacher, 8(1):3542, 2016.  [3] M. Cukurova, K. Avramides, D. Spikol, R. Luckin, and M. Mavrikis. An analysis framework for collaborative problem solving in practice-based learning activities: A mixed-method approach. In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, LAK 16, pages 8488, New York, NY, USA, 2016. ACM.  [4] M. dAquin and N. Jay. Interpreting data mining results with linked data for learning analytics: Motivation, case study and directions. In Proceedings of the Third International Conference on Learning Analytics and Knowledge, LAK 13, pages 155164, New York, NY, USA, 2013. ACM.  [5] E. B. Fretz, H.-K. Wu, B. Zhang, E. A. Davis, J. S. Krajcik, and E. Soloway. An investigation of software scaffolds supporting modeling practices. Research in Science Education, 32(4):567589, 2002.  [6] L. H. Gunderson. Ecological ResilienceIn Theory and Application. Annual Review of Ecology and Systematics, 31:425439, 2000.  [7] I. Guyon and A. Elisseeff. An Introduction to Variable and Feature Selection. Journal of Machine Learning Research, 3(3):11571182, 2003.  [8] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The WEKA data mining software. ACM SIGKDD Explorations, 11(1):1018, 2009.  [9] M. Hall and G. Holmes. Benchmarking attribute selection techniques for data mining. IEEE Transactions on Knowledge and Data Engineering, 15(6):14371447, 2003.  [10] C. S. Holling. Resilience and Stability of Ecological Systems. Annual Review of Ecology and Systematics, 4:123, 1973.  [11] J. B. Homer. Why We Iterate: Scientific Modeling in Theory and Practice. System Dynamics Review, 12(1):119, 1996.  [12] T. J. Kane, D. F. Mccaffrey, T. Miller, and D. O. Staiger. Have we identified effective teachers validating measures of effective teaching using random assignment, 2013.  [13] S. R. Kellert and E. O. Wilson. The biophilia hypothesis. Island Press, 1995.    [14] D. J. Ketelhut, B. C. Nelson, J. Clarke, and C. Dede. A multi-user virtual environment for building and assessing higher order inquiry skills in science. British Journal of Educational Technology, 41(1):5668, 2010.  [15] J. L. Kolodner, P. J. Camp, D. Crismond, B. Fasse, J. Gray, J. Holbrook, S. Puntambekar, and M. Ryan. Problem-based learning meets case-based reasoning in the middle-school science classroom: Putting learning by design(tm) into practice. Journal of the Learning Sciences, 12(4):495547, 2003.  [16] J. Krajcik, P. Blumenfeld, R. Marx, K. Bass, J. Fredricks, and E. Soloway. Inquiry in Project-Based Science Classrooms: Initial Attempts by Middle School Students. Journal of the Learning Sciences, 7(3):313350, 1998.  [17] W. H. Kruskal and W. A. Wallis. Use of Ranks in One-Criterion Variance Analysis. Journal of the American Statistical Association, 47(260):583621, 1952.  [18] N. W. H. Mason, D. Mouillot, W. G. Lee, J. B. Wilson, and H. Setala. Functional richness, functional evenness and functional divergence: The primary components of functional diversity. Oikos, 111(1):112118, 2005.  [19] K. E. Maull, M. G. Saldivar, and T. Sumner. Understanding digital library adoption: a use diffusion approach. Proceedings of the 11th annual international ACM/IEEE joint conference on digital libraries, pages 259268, 2011.  [20] L. M. McDonnell. Opportunity to learn as a research concept and a policy instrument. Educational Evaluation and Policy Analysis, 17(3):305322, 1995.  [21] K. L. McNeill, D. S. Pimentel, and E. G. Strauss. The impact of high school science teachers beliefs, curricular enactments and experience on student learning during an inquiry-based urban ecology curriculum. International Journal of Science Education, 35(15):26082644, 2011.  [22] Y. Mo, K. Singh, and M. Chang. Opportunity to learn and student engagement: A HLM study on eighth grade science achievement. Educational Research for Policy and Practice, 12(1):319, 2013.  [23] National Research Council. A framework for K-12 science education: Practices, crosscutting concepts, and core ideas. National Academies Press, 2012.  [24] National Research Council. A framework for K-12 science education: Practices, crosscutting concepts, and core ideas, page 57. National Academies Press, 2012.  [25] R. Pelanek, J. Rihak, and J. Papousek. Impact of Data Collection on Interpretation and Evaluation of Student Models. Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, pages 4047, 2016.  [26] D. Perera, J. Kay, I. Koprinska, K. Yacef, and O. R. Zaane. Clustering and sequential pattern mining of online collaborative learning data. IEEE Transactions on Knowledge and Data Engineering, 21(6):759772, 2009.  [27] C. Quintana, B. J. Reiser, E. a. Davis, J. Krajcik, E. Fretz, R. G. Duncan, E. Kyza, D. Edelson, and E. Soloway. A scaffolding design framework for  software to support science inquiry. Journal of the Learning Sciences, 13(3):337386, 2004.  [28] L. Schauble, R. Glaser, R. a. Duschl, S. Schulze, and J. John. Students Understanding of the Objectives and Procedures of Experimentation in the Science Classroom. Journal of the Learning Sciences, 4(2):131166, 1995.  [29] C. V. Schwarz, B. J. Reiser, E. A. Davis, L. Kenyon, A. Acher, D. Fortus, Y. Shwartz, B. Hug, and J. Krajcik. Developing a learning progression for scientific modeling: Making scientific modeling accessible and meaningful for learners. Journal of Research in Science Teaching, 46(6):632654, 2009.  [30] S. Severance, W. R. Penuel, T. Sumner, and H. Leary. Organizing for Teacher Agency in Curricular Co-Design. Journal of the Learning Sciences, 25(4):531564, 2016.  [31] C.-F. Shih and A. Venkatesh. Beyond adoption: Development and application of a use-diffusion model. Journal of Marketing, 68(1):5972, 2004.  [32] E. Soloway, A. Z. Pryor, J. S. Krajcik, S. Jackson, S. J. Stratford, M. Wisnudel, and J. T. Klein. Sciencewares model-it: Technology to support authentic science inquiry. T.H.E. Journal, 25(3):5456, 1997.  [33] J. W. Tukey. Comparing individual means in the analysis of variance. Biometrics, 5(2):99114, 1949.  [34] M. van Gompel and A. van den Bosch. Efficient n-gram, Skipgram and Flexgram Modelling with Colibri Core. Journal of Open Research Software, 4(1):e30, 2016.  [35] M. Windschitl, J. Thompson, and M. Braaten. Beyond the scientific method: Model-based inquiry as a new paradigm of preference for school science investigations. Science Education, 92(5):941967, 2008.  [36] Z. Xing, J. Pei, and E. Keogh. A brief survey on sequence classification. ACM SIGKDD Explorations Newsletter, 12(1):40, 2010.    "}
{"index":{"_id":"41"}}
{"datatype":"inproceedings","key":"Crossley:2017:PMP:3027385.3027399","author":"Crossley, Scott and Liu, Ran and McNamara, Danielle","title":"Predicting Math Performance Using Natural Language Processing Tools","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"339--347","numpages":"9","url":"http://doi.acm.org/10.1145/3027385.3027399","doi":"10.1145/3027385.3027399","acmid":"3027399","publisher":"ACM","address":"New York, NY, USA","keywords":"educational data mining, natural language processing, on-line tutoring systems, predictive analytics, sentiment analysis","Abstract":"A number of studies have demonstrated links between linguistic knowledge and performance in math. Studies examining these links in first language speakers of English have traditionally relied on correlational analyses between linguistic knowledge tests and standardized math tests. For second language (L2) speakers, the majority of studies have compared math performance between proficient and non-proficient speakers of English. In this study, we take a novel approach and examine the linguistic features of student language while they are engaged in collaborative problem solving within an on-line math tutoring system. We transcribe the students' speech and use natural language processing tools to extract linguistic information related to text cohesion, lexical sophistication, and sentiment. Our criterion variables are individuals' pretest and posttest math performance scores. In addition to examining relations between linguistic features of student language production and math scores, we also control for a number of non-linguistic factors including gender, age, grade, school, and content focus (procedural versus conceptual). Linear mixed effect modeling indicates that non-linguistic factors are not predictive of math scores. However, linguistic features related to cohesion affect and lexical proficiency explained approximately 30% of the variance (R2 ","pdf":"Predicting Math Performance  Using Natural Language Processing Tools   Scott Crossley  Georgia State University  25 Park Place, Ste 1500   Atlanta, GA 30303  01+404-413-5179   scrossley@gsu.edu   Ran Liu   Carnegie Mellon University   5000 Forbes Avenue  Pittsburgh, PA 15213   01+412-449-9168  ranliu@cmu.edu   Danielle McNamara  Arizona State University   PO Box 872111  Tempe, AZ 85287  01+480-727-5690   dsmcnamara1@gmail.com   ABSTRACT A number of studies have demonstrated links between linguistic  knowledge and performance in math. Studies examining these  links in first language speakers of English have traditionally relied  on correlational analyses between linguistic knowledge tests and  standardized math tests. For second language (L2) speakers, the  majority of studies have compared math performance between  proficient and non-proficient speakers of English. In this study,  we take a novel approach and examine the linguistic features of  student language while they are engaged in collaborative problem  solving within an on-line math tutoring system. We transcribe the  students speech and use natural language processing tools to  extract linguistic information related to text cohesion, lexical  sophistication, and sentiment. Our criterion variables are  individuals pretest and posttest math performance scores. In  addition to examining relations between linguistic features of  student language production and math scores, we also control for  a number of non-linguistic factors including gender, age, grade,  school, and content focus (procedural versus conceptual). Linear  mixed effect modeling indicates that non-linguistic factors are not  predictive of math scores. However, linguistic features related to  cohesion affect and lexical proficiency explained approximately  30% of the variance (R2 = .303) in the math scores.    Categories and Subject Descriptors K.3.1 [Computer Uses in Education]: Computer-assisted  Instruction (CAI); J.5 [Computer Applications: Arts and  Humanities]: Linguistics   General Terms Algorithms, Measurement, Performance   Keywords On-line tutoring systems, educational data mining, natural  language processing, sentiment analysis, predictive analytics   1. INTRODUCTION It has long been argued that there are strong links between  language skills and the ability to engage with math concepts and  problems. For instance, success in math is argued to be partially  based on the development of language that affords children the  ability to participate in math instruction in the classroom as well  as engage quantitatively with the world outside the classroom.  [1]. Similarly, strong math skills are presumed to interact with  language ability because math literacy is not just about knowing  numbers and symbols, but also understanding the words  surrounding those numbers and symbols. Thus, strong overlap is  thought to exist between math and print literacy [2].   Notably, it may not only be language skills that are related to  math ability, but also a number of other cognitive predictors that  are developed before formal education. Along with linguistic  skills, cognitive skills such as a spatial attention and quantitative  ability may be related to early math skills in young children [3].  Nonetheless, linguistic skills may be one of the more important  factors. For instance, Cummins [4] identified language difficulties  in second language (L2) speakers as a key obstacle in solving  math problems and linked difficulty in transferring cognitive  operations across math and language domains as a barrier to  success in math.    One problem with previous studies linking math success and  linguistic factors is that the studies have generally relied on  correlational analyses among standardized tests of math and  linguistic knowledge. For instance, several studies have examined  links between tests of language proficiency (e.g., syntax,  knowledge of language ambiguity, verbal ability, and  phonological skills) and success on tests of math knowledge  including algebraic notation, procedural arithmetic, and arithmetic  word problems [1, 5]. Other studies have compared success on  standardized math tests between first language (L1) speakers of  English and second language speakers of English, who have lower  linguistic ability [6, 7, 8]. To our knowledge, no studies have  examined the relationship between language complexity and  language affect in student discourse to their success on math  assessments.   The purpose of this study is to fill that gap by examining the  language used by students engaged in collaborative math problem  solving in an on-line tutoring system. To do so, we transcribed  recordings of student discourse during math problem solving and  analyzed the language produced for a number of linguistic  features related to text cohesion, lexical sophistication, and  sentiment that were derived from natural language processing  (NLP) tools. In this study, we examined the extent to which the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4870-6/17/03$15.00 DOI: http://dx.doi.org/10.1145/3027385.3027399    derived linguistic features are predictive of students pretest and  posttest math scores. We also examine a number of non-linguistic  factors that are potentially predictive of math success including  age, gender, school, and content focus (procedural versus  conceptual). Our goal is to directly investigate links between  linguistic production and math success.    1.1 Math and Language Connections  A number of studies have examined links between math skills and  language abilities in first language (L1) speakers of English.  These studies generally indicate that there are strong links  between language proficiency and math ability. For instance,  Macgregor and Price [5] examined the relations between three  cognitive indicators of language proficiency (metalinguistic  awareness of symbols, syntax, and language ambiguity) and  algebraic notation. Their data came from pencil-and-paper tests  taken by 1500 students aged between 11 and 15 whose length of  algebra instruction varied between 1 and 4 years. The majority of  students who scored high on language tests also scored high on  the algebra test. A follow-up study included a more difficult  algebra test that led to greater variance in high and low scores  math scores and indicated a stronger relationship between  language ability and algebraic notation. The authors concluded  that limited metalinguistic awareness seemed to negatively affect  success in algebra learning.   In a similar study, Vukovic and Lesaux [1] investigated links  between linguistic skills (i.e., general verbal ability and  phonological skills), symbolic number skills and arithmetic  knowledge (procedural arithmetic and arithmetic word problems).  They also included working memory and visualspatial ability as  control variables. Their data came from 287 third graders enrolled  at five different schools using the same curriculum for math  education. A path model analysis indicated that the linguist skills  differed in their degree of relation with arithmetic knowledge.  While phonological skills were found to be directly related to  arithmetic knowledge, general verbal ability was indirectly related  through symbolic number skills. They concluded that general  verbal ability is involved in how children reason numerically  whereas phonological skills are involved in executing arithmetic  problems. (p.90).  Hernandez [9] investigated links between math and language  skills indirectly, by examining relationships between reading  ability and math achievement levels. He hypothesized that there  was a positive correlation between reading skills and math scores.  To test this hypothesis, he analyzed 652 ninth-grade students  scores from the reading and math sections of the Texas  Assessment of Knowledge and Skills. Correlations between the  reading scores and the math scores were computed for texts taken  in sixth, seventh, and eighth grades. The results revealed  significant positive correlations (with small and medium effect  sizes) between reading ability and math achievement. Hernandez  suggested that students reading skills should be taken into  account in order to provide more effective math instruction,  especially for poor readers. Such instruction could include reading  strategy training and collaboration between reading and math  teachers.   However, not all studies have found significant links between  language skills and math knowledge. LeFevre et al. [3] conducted  a longitudinal study that followed childrens math progress. The  study focused on a year-long data collection from 182 children  ages 4 to 8 (37 in preschool and 145 in kindergarten), including  linguistics skills (receptive vocabulary and phonological  awareness) and non-linguistic skills such as quantitative   knowledge, spatial attention, early numeracy skills (number  naming and nonlinguistic arithmetic). The outcome measures in  the study included standardized and research-based tests of math  knowledge. Path modeling resulted in three paths which showed  that linguistic skills were significantly related to number naming,  that quantitative abilities were related to processing numerical  magnitudes, and that spatial attention was related to a variety of  numerical and math tests. The last two paths, related to  quantitative predictors of arithmetic knowledge, found that non- linguistic features were stronger predictors of math success.    An additional source of evidence for connections between  linguistic and math abilities have come from studies comparing  L1 and L2 English language speakers. The basic notion behind  these studies is that students with lower language skills in English  (i.e., second language speakers of English who are less proficient)  will have lower math skills in English based classroom. And, it is  further assumed that once L2 students reach a threshold of  language proficiency, they will have the resources to perform on  par with L1 speakers [4].    The assumption that L2 students do not perform as well as L1  students is supported by the US Department of Education [10],  which reports that over a five-year period (from 1st to 5th grade),  L1 speakers of English report higher math scores than proficient  L2 speakers who, in turn, report higher math scores low proficient  L2 students.    For the most part, results from research investigating the  differences in math skills between L1 and L2 speakers of English  concur with US Department of Education report. For instance, Alt  et al. [6] investigated relations between math and language  achievement among school-age children (ages 7-10) who were  grouped into native students (N=21), L2 learners whose first  language was Spanish (N=20), and students with specific  language impairment (SLI) (N=20). The researchers hypothesized  that there would be differences in math skills between the groups  due to language proficiency differences. Data were collected  using two standardized math tests (one in English and one in  Spanish) and three experimental tasks (number comparison,  quantity comparison and concept mapping games). The tests and  tasks were categorized as either heavy or light processing in terms  of language, symbol, and visual working memory. For instance,  the math test in English was classified as heavy language  processing, heavy symbol processing, and light on visual working  memory. The math test in Spanish was classified as light language  processing, heavy symbol processing, and light on visual working  memory. The concept mapping game was classified as light  language processing, light symbol processing, and heavy on visual  working memory. The results showed that students with SLI  achieved performed significantly worse than native speakers in all  tests and tasks. When L1 and L2 speakers were compared, Alt et  al. found that L1 students significantly outperformed the L2  students only in language-heavy tests and games. These results led  Alt et al. to conclude that language proficiency is a crucial factor  in math success for students who have language-related  challenges.    Martinello [8] investigated item difficulty differences across math  tests between L1 and L2 students in terms of different levels of  linguistic complexity and contextual support provided by pictures  and schemas. Standardized math test scores for 68,839 fourth- grade students, 3179 of which were non-native speakers of  English, were used in the study. The test scores consisted of 39  items that assessed knowledge of number sense and operations,  patterns and relations, algebra, geometry, measurement, and     probabilities. For each item in the test, two researchers rated the  grammatical and lexical complexity of the item. The results  showed that linguistic complexity and the non-linguistic  representations that accompanied the items accounted for around  66% of the variation in scores between native and non-native  students such that linguistically complex items were found to be  more difficult for nonnative speakers. These items included  complex grammatical structures and low frequency non-math  words that were central to the items and hard to guess from the  context. Non-linguistic representations (especially schemas) were  found to decrease the difficulty of more linguistically complex  items.    Similar findings that support the notion that L2 speakers of  English are at a disadvantage in math performance when  compared to L1 speakers have been reported in a number of  studies [11, 12, 13]. Of course, while language is a predictor of  success, it is not the only consideration. Language skills can  interact with background differences such as parent education,  levels of poverty, and ethnicity [10], courses taken [12], and  immigrant status [13]. Nonetheless, correlational studies generally  support the threshold hypothesis [4] that proficiency in the  language of instruction is necessary for academic achievement in  disciplines such as math.   1.2 Current Study  In summary, a number of studies have demonstrated strong links  between linguistic knowledge and success in math. Studies  examining these links in L1 speakers have traditionally relied on  correlational analyses between linguistic knowledge tests and  standardized math tests [1, 3, 5]. For L2 speakers, the majority of  studies have compared math success between proficient and non- proficient speakers of English [6, 10, 11, 12, 13]. In this study, we  take a novel approach and examine the linguistic features of  students language production during math problem solving in an  on-line tutoring system. To derive our linguistic features of  interest, we transcribe student speech and use a number of natural  language processing tools to extract linguistic information related  to text cohesion, lexical sophistication, and sentiment. Thus, in  contrast to previous studies, our interest is not on linguistic  performance as measured by standardized tests, but on linguistic  performance as a function of language production during  collaborative math learning activities. Our criterion variables are  pretest and posttest math performance scores. In addition to  examining relations between linguistic features of student  language production and math scores, we also control for a  number of non-linguistic factors including gender, age, grade,  school, and content focus (procedural versus conceptual). Thus, in  this study, we address three research questions:    1. Are non-linguistic factors significant predictors of math  performance in a collaborative on-line tutoring environment   2. Are linguistic factors related to lexical sophistication,  cohesion, and affect significant predictors of math  performance in a collaborative on-line tutoring environment   3. Are linguistic features stronger predictors of math  performance than non-linguistic factors      METHOD  2.1 Procedure  The data used in this study come from an experiment that  compared the effectiveness of collaborative versus individual  learning of fraction concepts and procedures from an intelligent  tutoring system. Students in the study were randomly assigned to   one of two conditions: Collaborative, in which two students  worked with a partner through the full tutor curriculum (i.e.,  collaborative dyad), and Individual, in which students worked by  themselves on the entire tutor. Since audio recordings of student  dialogue were only collected for the Collaborative condition, the  present investigation only applies to students in that condition.  The fractions tutoring system that students used is online  software, built using an extension of Cognitive Tutor Authoring  Tools [14] designed to support collaborative learning [15]. The  fractions tutoring system helps students become better at  understanding and using fractions. It covers six sub-topics,  including naming, picturing, equivalent, ordering, adding, and  subtracting fractions. Its effectiveness has previously been  demonstrated in prior classroom deployment studies [16, 17].  These studies showed that students mistakes decrease as they  progress through the tutor; students score higher on a fractions test  after using the tutoring system compared to before; and scores  remain higher than pre-tutoring a week after they have finished  using the tutoring system.   Figure 1. Example problems from the adding fractions section of  the collaborative fractions tutor.   In this study, the tutoring system was designed to support  collaboration between students. Although each student worked on  the tutoring system on his or her own computer screen, each  student in a pair could control only part of the screen. The  students needed to work together to finish the problem (i.e., one  student could not do everything). Students worked together at the  same time and, ideally, talked about what they were doing, asked  for help from their partner, defended a position or explained why     they thought something was the correct answer, and built off of  each others contributions.   All collaborative dyads randomly received a problem set focused  on either procedural or conceptual knowledge building. The  procedural versus conceptual comparison had been included to  investigate whether there were any interactions between  collaborative learning and type of knowledge acquired. Figure 1  shows one example of each type of problem, conceptual and  procedural. The top and bottom panels show example problems  from the conceptual and procedural knowledge conditions,  respectively. The figure depicts correctly completed screens;  student-input fields are marked with either green text or borders.  The study took place over five consecutive days. On the first day,  students individually took a pretest to establish their baseline  fractions knowledge.  In the following three days, students in the  Collaborative condition worked through the tutoring system with  a partner. On the last day, students individually took a posttest  that also tested fractions knowledge, with content similar to the  pretest.   1.3 Participants  A total of 104 fourth and fifth graders participated in the  Collaborative condition of the study.  There were 19 fifth graders  from one classroom of one school and 50 fourth graders and 35  fifth graders from a second school. Of these, only a subset of  students completed the full study (pretest, posttest, and three days  of tutoring system use), had the same partner during the entire  study (no absences for either individual), and consented to audio  recording of their dialogue. For consistency purposes, we only  analyzed data from students who fit all of these criteria. Thus, our  analyses were done on this subset of 36 students (14 fifth graders  from the first school, and 16 fourth graders and 6 fifth graders  from the other school). There were 15 males and 21 females in the  analysis subset. Student pairs were determined by the teachers.  Teachers were asked to pair each student with a partner that they  would get along with, and who was at a similar knowledge level.   1.4 Transcriptions  A professional transcriber transcribed each of the speech samples  collected from the participants. The transcriptions contained the  speakers words, some metalinguistic data (singing, laughing,  sighing) and filler words (e.g., ummm, ahhhh). Disfluencies that  were linguistic in nature (e.g., false starts, word repetition, repairs)  were also retained. If any portion of the audio was not  transcribable, the words were annotated either with an underscore  or the flag INAUDIBLE depending on the transcriptionist. The  files were cleaned so that metalinguistic data, filler words,  untranscribale portions were removed prior to analysis.   1.5 Linguistic Variables  The transcripts were separated by learner and then cleaned to  remove all non-linguistic information including metadata and  non-linguistic vocalizations such as coughs and laughs. Each  transcript was run through a number of natural language  processing tools including the Tool for the Automatic Analysis of  Lexical Sophistication (TAALES) [18], the Tool for the  Automatic Analysis of Cohesion (TAACO) [19] and the  SEntiment ANalysis and Cognition Engine (SEANCE) [20]. The  selected tools reported on language features related to lexical  sophistication, text cohesion, and sentiment analysis respectively.  The tools are discussed in greater detail below.   1.5.1 TAALES  TAALES is a computational tool that is freely available and easy  to use, works on most operating systems (Windows, Mac, Linux),  allows for batch processing of text files, and incorporates over  150 classic and recently developed indices of lexical  sophistication. These indices measure word frequency, lexical  range, n-gram frequency and proportion, academic words and  phrases, word information, lexical and phrasal sophistication, and  age of exposure. Each of these are discussed briefly below. For  more detailed accounts of TAALES please see Kyle and Crossley  [18].  Word frequency indices. TAALES calculates a number of word  frequency indices with frequency counts retrieved from Thondike- Lorge [21], Kucera-Francis [22], Brown [23], and SUBTLexus  databases [24]. In addition, TAALES derives frequency counts  from the British National Corpus (BNC) [25]. TAALES calculates  scores for all words (AW), content words (CW), and function  words (FW).    Range indices. In addition to frequency information, TAALES  includes a number of range indices which calculate how many  texts within a corpus a word appears (i.e., specificity). Range  indices are calculated for the spoken (574 texts) and written  (3,083 texts) subsets of the BNC, SUBTLEXus (8,388 texts) and  Kucera-Francis (500 texts).   N-gram frequency and proportion indices. TAALES calculates  bigram and trigram frequencies and proportion scores (i.e., the  proportion of n-grams in a text that are common in a reference  corpus) from both the written (80 million words) and spoken  subcorpora (10 million words) of the BNC.   Academic list indices. TAALES includes word and n-gram level  academic lists. These indices are calculated from the Academic  Word List (AWL) [26] and the Academic Formula List (AFL)  [27].   Word information indices. Word information scores are derived  from the MRC Psycholinguistic Database [28, 29, 30]. Word  information scores are calculated for word familiarity,  concreteness, imageability, meaningfulness, and age of  acquisition.   1.5.2 TAACO  TAACO (Crossley et al., in press-b) incorporates over 150 classic  and recently developed indices related to text cohesion. For a  number of indices, the tool incorporates a part of speech (POS)  tagger from the Natural Language Tool Kit [31] and synonym sets  from the WordNet lexical database [32]. The POS tagger affords  the opportunity to look at content words (i.e., nouns, verbs,  adjectives, adverbs) as well as function words (i.e., determiners,  propositions). TAACO provides linguistic counts for both  sentence and paragraph markers of cohesion and incorporates  WordNet synonym sets. Specifically, TAACO calculates type  token ratio (TTR) indices (for all words, content words, function  words, and n-grams), sentence overlap indices that assess local  cohesion for all words, content words, function words, POS tags,  and synonyms, paragraph overlap indices that assess global  cohesion for all words, content words, function words, POS tags,  and synonyms, and a variety of connective indices such as logical  connectives (e.g., moreover, nevertheless), causal connectives  (because, consequently, only if), sentence linking connectives  (e.g., nonetheless, therefore, however), and order connectives  (e.g., first, before, after).     1.5.4 SEANCE  SEANCE is a sentiment analysis tools that relies on a number of  pre-existing sentiment, social positioning, and cognition  dictionaries. SEANCE contains a number of pre-developed word  vectors developed to measure sentiment, cognition, and social  order. These vectors are taken from freely available source  databases such as SenticNet [33, 34] and EmoLex [35, 36]. In  some cases, the vectors are populated by a small number of words  and should be used only on larger texts that provide greater  linguistic coverage in order to avoid non-normal distributions of  data as found in the Lasswell dictionary lists [37] and the Geneva  Affect Label Coder (GALC) [38] lists. For many of these vectors,  SEANCE also provides a negation feature (i.e., a contextual  valence shifter [39]) that ignores positive terms that are negated  (e.g., not happy). The negation feature, which is based on Hutto  and Gilbert [40], checks for negation words in the 3 words  preceding a target word. SEANCE also includes the Stanford part  of speech (POS) tagger [41] as implemented in Stanford CoreNLP  [42]. The POS tagger allows for POS tagged specific indices for  nouns, verbs, and adjectives.   1.6 Statistical Analysis  We first conducted a paired samples t-test to examine if there  were differences between pretest and posttest scores for the data.  We then conducted linear mixed effect (LME) models to answer  our three research questions. The purpose of the LME was to  determine if linguistic features in the students language output  along with other fixed effects could be used to predict the  students pretest and posttest math scores. Thus, the LME model  modeled the pretest and posttest results in terms of random factors  (i.e., repeated variance explained by the students as they moved  through the intervention longitudinally) and fixed or between  factors (e.g., the linguistic features in their transcripts, gender,  age, school). Such an approach allows us to examine math growth  over time for individual learners using random factors as well as  investigate if individual differences related to the learner such as  demographic information, age, and linguistic ability predict math  development. Lastly, the approach allows us to also examine if  different classroom interventions influence math scores (i.e.,  procedural versus conceptual approaches to teaching math in the  classroom).   Prior to the LME analysis, we first checked that the linguistic  variables were normally distributed as well as controlled for  multicollinearity between all the linguistic variables (r > .700).  We used R [43] for our statistical analysis and the package lme4  [44] to construct linear mixed effects models (LME). We also  used the package lmerTest [45] to analyze the LME output and  derive p-values for individual fixed effects. Final model selection  and interpretation was based on t and p values for fixed effects  and visual inspection of residuals distribution. To obtain a  measure of effect sizes, we computed correlations between fitted  and predicted residual values, resulting in an R2 value for both the   fixed factors and the fixed factors combined with the random  factor (i.e., the repeated participant data from the pretest and the  posttest). We first developed a baseline model that included  gender, grade, condition, and school as fixed effects and  participants as random effect. We next developed a full model that  included gender, grade, condition, and school as fixed effects  along with linguistic features and participants as random effect.   2. RESULTS  2.1 Math Gains  A paired t-test examining differences between the pretest and the  posttests scores indicated significant differences between the  pretest (M= .469, SD=.170) and the posttest (M= .603, SD= .185);  t(35)= 5.988, p < .001.   2.2 Baseline Model  A baseline model considering all fixed effects aside from  linguistic revealed no significant effects on math scores. Table 1  displays the coefficients, standard error, t values, and p values for  each of the non-linguistic fixed effects. Inspection of residuals  suggested the model was not influenced by homoscedasticity. The  non-linguistic variables explained around 2% of the variance (R2  = .016) while the fixed and random variables together explained  around 55% of the variance (R2 = .553). Thus, the majority of  change found in the pretest and posttest was due to time.   2.3 Full Model  A full model was developed that including the nested baseline  model and linguistic fixed effects. The model included five  linguistic features related to cohesion (sentence linking  connectives and adjacent overlap of adjectives), affect (respect  terms), and lexical proficiency (number of function word types  and verb hypernymy). None of the variables showed suppression  effects. The model indicated that a greater number of sentence  linking connectives (e.g., nonetheless, therefore, however),  function word types (e.g., prepositions, connectives, and articles),  and overlap of adjectives predicted higher math scores.  Conversely, more respect terms and greater use of more specific  words (i.e., greater hypernymy scores) related to lower math  scores. Table 2 displays the coefficients, standard error, t values,  and p values for each of the fixed effects ordered by strength of t  value. A log likelihood comparisons found a significant difference  between the baseline and full models, (2(2) = 42.486, p < .001),  indicating that the inclusion of linguistic features contributed to a  better model fit. Together, the fixed factors including the  linguistic and non-linguistic variables explained around 30% of  the variance (R2 = .303) while the fixed and random variables  combined to explain around 82% of the variance (R2 = .823).       Table 1. Baseline model for predicting math scores    Fixed Effect Coefficient Std. Error t p   (Intercept) 0.564 0.059 9.543 < .001   Gender (male) -0.039 0.061 -0.650 0.521   Grade (5) -0.029 0.082 -0.350 0.729   Condition (procedural) -0.024 0.060 -0.397 0.694   School 0.038 0.086 0.436 0.666     3. DISCUSSION  Previous studies that have investigated links between language  use and math performance have reported strong links between the  two indicating that language skills are an important prequisite for  effectively engaging with math concepts and problems. These  previous studies have traditionally relied on analyzing links  between language proficiency tests and/or surveys and  standardized math scores. Similar studies have also examined  differences in math performance between L1 and L2 speakers of  English to test threshold hypotheses predicated on the notion that  less proficient speakers of English will have more difficulty in  math classes taught in English.   Our study takes a novel approach to understanding links between  math performance and language use by examining the actual  language produced during math problem solving and examining if  features of this language are predictive of math performance in  standardized tests. Beyond language features, this study also  examined a number of non-linguistic student factors including  gender, age, grade, school, and content focus (procedural versus  conceptual). The findings indicate that the non-linguistic factors  were not significant predictors of math performance. However,  time between the pretest and posttest was a strong predictor of  performance. In addition, linguistic features were significant  predictors of math performance. We address each of these below.   That non-linguistic features were not significant predictors of  math performance has important implications for understanding  math performance. Specifically, male students performed no  better than female students and 4th grade students performed no  better than 5th grade students. In addition, no differences were  reported for students from two different schools. These findings  provide evidence that learning within a math tutoring system may  not favor one gender over another nor grade or school. Lastly, the  two types of knowledge conditions (conceptual or procedural)  showed no difference in performance indicating equivalence  between the two. However, performance did increase between the  pretest and the posttest according to the paired samples t-test  indicating that significant learning occurred as a result of  interacting with the online math tutor. Much of this increase can  be attributed to the effects of repeated measures across time (i.e.,  the random effects). These random effects explained above 50%  of the variances in the math performance scores.          The full model LME model demonstrated that a number of  linguistic features were significant predictors of math  performance. Specifically, a greater number of sentence linking  connectives and function words were predictive of math  performance. These findings indicate that math performance is  likely linked with the production of more complex syntactic  structures such as those found in coordinated sentences and  sentences with more structural components (i.e., function words).  Lexically, math performance is associated with the production of  more abstract words (i.e., words with greater hypernymy scores).  Intuitively this makes sense because math solutions are based on  abstract thinking and language use. In addition, a greater overlap  of adjectives between sentences is a strong predictor of math  performance likely indicating that the repetition of math  adjectives such as greater than and less than may be related to  math performance. Lastly, our analysis demonstrated that math  performance was related to the use of fewer words related to  respect. This finding may seem counter-intuitive, but performance  within a math tutoring system that requires collaboration and  timed completion of problems may favor curt and direct discourse  between participants that may be interpreted as less respectful. In  total, the linguistic factors explained about 28% of the variance in  the math performance data over and above the 2% explained by  the non-linguistic factors. When both fixed and random factors  were included in the model, over 80% of the variance in the math  performance was predicted.    To provide examples of the linguistic features above, we extracted  excerpts from a student dyad on the last day of the study. The first  student (Student 137) in the dyad had the highest posttest score  (97%) and showed a 15% gain from the pretest. The second  student (Student 128) scored low on the posttest (44%) and had  the third lowest score on the pretest (22%). The students posttest  score was almost double that of his pretest score though, showing  strong gains in learning.              Table 2. Full model for predicting math scores    Fixed Effect Coefficient Std. Error t p   (Intercept) 0.557 0.055 10.106 < .001   Gender (male is contrast) 0.007 0.057 0.121 0.905   Grade (5th grade is contrast) -0.021 0.077 -0.284 0.778   Condition (procedural content is contrast) -0.036 0.057 -0.639 0.527   School 0.032 0.080 0.401 0.691   Sentence linking connective 0.059 0.018 3.246 < .001   Number of function word types 0.044 0.0193 2.273 < .050   Respect words -0.032 0.013 -2.518 < .050   Adjacent overlap of adjectives 0.039 0.015 2.549 < .050   Verb hypernymy -0.038 0.017 -2.265 < .050     Table 3: Text excerpts from students that completed and did  not complete the EDM MOOC   Sentence linking examples      STUD_137: Yeah it is, but it could be 80/80. It could  why  didn't they just have a card with a one    STUD_137: It's equal. You're rushing. And just look at the  numbers and then you put one in.     Function words and Respect example     STUD_128: How did you get them all wrong  STUD_137: You got two of them wrong.   STUD_128: You got them all wrong.  STUD_137: Wait. If the cement is green.  STUD_128: Why did you get them all wrong  STUD_137: You did.  STUD_128: Why did you get them all wrong   Adjective overlap example     STUD_137: No, two-ninths is greater than one-ninth.  STUD_128: Two-ninths is greater.    STUD_137: It's greater than. It was either greater than or equal  to.  STUD_128: It was greater than.     Hypernymy example      STUD_137: And that one you have too. You should be able to  figure that one out.  STUD_128: Three, four, five. Three-fifths.  STUD_137: Dude, when are you going to  what are you  doing Oh. Wait, hold on.  STUD_128: You have it.  STUD_137: Oh, they're equal.   STUD_128: You had it the whole time.        Linguistically, the excerpts provide illustrations for the trends  reported in the statistical analysis. For instance, Student 137 links  many sentences together with connectors such as but and and. In  addition, both students tend to use a greater number of function  words such as how, did, you, all, of, if, and the. The use of a  greater number of function words indicates the use of stronger  sentence-based structural elements, which may be important in  discussing more abstract ideas. In terms of abstract words, the  excerpts show that these two students use a number of abstract  words that are less specific such as that, one, have, able, go, are,   do, you, it, they, and time. The two students also demonstrate a  directness with one another that could be viewed as disrespectful  from an outsiders perspective. For instance, the two students  seem comfortable accusing one another of getting the answers  wrong. Lastly, in terms of argument overlap, the excerpts provide  instances of students repeating adjectives related to math solving  problems (i.e., greater). In total, the excerpts provide illustrations  of what the natural language processing tools are likely capturing  in their estimations of math performance. These excerpts help  provide contextualized details for the math discourse in the data.   4. CONCLUSION  The findings from this study have practical implications for  understanding math performance and math instruction.  Specifically, the findings provide support for the notion that  language proficiency is strongly linked to math performance such  that more complex language and a greater overlap of adjectives  equates to higher performance. Similarly, discourse that contains  fewer terms related to respect equates to higher performance.  From an instructional perspective, the findings also indicate that  collaborative, on-line tutoring instruction can lead to improved  math performance. These findings could inform math pedagogy  practices by providing support for language instruction within the  math classroom. For instance, it may be the case that providing  students with a solid math vocabulary foundation would improve  students math success by providing them with the means to  discuss complex math problems in a collaborative environment. It  is likely that focusing both on abstract math principles and on the  language needed to communicate these principles would push  students over the language threshold needed for success in the  math classroom.  Additionally, in terms of respect, it is likely that  students that show less deference and are more likely to challenge  ideas are more successful in the math classroom.   Future studies can build on the results presented here by sampling  larger populations of students that come from more diverse  backgrounds and more varied grade levels. Such a study would  build on the relatively low sample size found in this paper and  provide greater evidence for the importance of linguistic  proficiency and math success. Of interest in future replications of  this research would be to examine if the results reported here  persist with older students, in educational settings outside of an  intelligent tutoring environment, and with different math topics.  Such research would help extend the current study past the single  context on which it focused and provide evidence that linguistic  proficiency is an important indicator or math success in a variety  of learning contexts.    5. ACKNOWLEDGMENTS  This research was supported in part by the Institute for Education  Sciences and National Science Foundation (IES R305A080589,  IES R305G20018-02, and DRL- 1417997). Ideas expressed in this  material are those of the authors and do not necessarily reflect the  views of the IES or the NSF.   6. REFERENCES  [1] Vukovic, R. K., & Lesaux, N. K. (2013). The relationship   between linguistic skills and arithmetic knowledge. Learning  and Individual Differences, 23, 87-91.   [2] Adams, T. L. (2003). Reading math: More than words can  say. The Reading Teacher, 56(8), 786-795.   [3] LeFevre, J. A., Fast, L., Skwarchuk, S. L., Smith-Chant, B.  L., Bisanz, J., Kamawar, D., & Penner-Wilger, M. (2010).     Pathways to math: Longitudinal predictors of  performance. Child development, 81(6), 1753-1767.   [4] Cummins, J. (1979). Linguistic interdependence and the  educational development of bilingual children. Review of  Educational Research, 49, 222-251.   [5] MacGregor, M., & Price, E. (1999). An exploration of  aspects of language proficiency and algebra learning. Journal  for Research in Math Education, 449-467.   [6] Alt, M., Arizmendi, G. D., & Beal, C. R. (2014). The  relationship between math and language: Academic  implications for children with specific language impairment  and English language learners. Language, speech, and  hearing services in schools, 45(3), 220-233.   [7] Hampden-Thompson, G., Mulligan, G., Kinukawa, A., &  Halle, T. (2008). Math Achievement of Language-Minority  Students During the Elementary Years. Washington, DC:  U.S. Department of Education, National Center for  Education Statistics.   [8] Martiniello, M. (2009). Linguistic complexity, schematic  representations, and differential item functioning for English  language learners in math tests. Educational  assessment, 14(3-4), 160-179.   [9] Hernandez, F. (2013). The Relationship Between Reading  and Math Achievement of Middle School Students as  Measured by the Texas Assessment of Knowledge and  Skills (Doctoral dissertation).   [10] Hampden-Thompson, G., Mulligan, G., Kinukawa, A., &  Halle, T. (2008). Math Achievement of Language-Minority  Students During the Elementary Years. Washington, DC:  U.S. Department of Education, National Center for  Education Statistics.    [11] Ardasheva, Y., Tretter, T., Kinny, M. (2012). English  Language Learners and Academic Achievement: Revisiting  the Threshold Hypothesis. Language Learning, 62(3), 769- 812.    [12] Mosqueda, E., & Maldonado, S. I. (2013). The effects of  English language proficiency and curricular pathways:  Latina/os math achievement in secondary schools. Equity &  Excellence in Education, 46(2), 202-219.   [13] Wang, J., & Goldschmidt, P. (1999). Opportunity to learn,  language proficiency, and immigrant status effects on math  achievement. The Journal of Educational Research, 93(2),  101-111.   [14] Aleven, V., McLaren, B.M., Sewall, J., & Koedinger, K.R.  (2009). A New Paradigm for Intelligent Tutoring Systems:  Example-Tracing Tutors. International Journal of Artificial  Intelligence in Education, 19(2), 105-154.    [15] Olsen, J. K., Belenky, D. M., Aleven, V., Rummel, N., &  Ringenberg, M. Authoring collaborative intelligent tutoring  systems. . In Lane, H. C., Yacef, K., Mostow, J., & Pavlik, P.  (Eds.). Proceedings of the Artificial Intelligence in Education  (AIED) Conference. Heidelberg, Germany: Springer.   [16] Rau, M. A., Aleven, V., & Rummel, N. (2009, July).  Intelligent Tutoring Systems with Multiple Representations  and Self-Explanation Prompts Support Learning of Fractions.  In Proceedings of the Artificial Intelligence in Education  (AIED) Conference. (pp. 441-448). Heidelberg, Germany:  Springer.    [17] Rau, M. A., Aleven, V., Rummel, N., & Rohrbach, S. (2012).  Sense making alone doesnt do it: Fluency matters too! ITS  support for robust learning with multiple representations. In  International Conference on Intelligent Tutoring Systems  (pp. 174-184). Springer Berlin Heidelberg.   [18] Kyle, K., & Crossley, S. A. (2015). Automatically assessing  lexical sophistication: Indices, tools, findings, and  application. TESOL Quarterly, 49(4), 757-786.  doi:10.1002/tesq.194    [19] Crossley, S. A., Kyle, K., & McNamara, D. S. (in press). The  tool for the automatic analysis of text cohesion (TAACO):  Automatic assessment of local, global, and text cohesion.  Behavior Research Methods.   [20] Crossley, S. A., Kyle, K., & McNamara, D. S. (in press).  Sentiment Analysis and Social Cognition Engine (SEANCE):  An Automatic Tool for Sentiment, Social Cognition, and  Social Order Analysis. Behavior Research Methods.  (Thorndike & Lorge, 1944)   [21] Thorndike, E. L., & Lorge, I. (1944). The teacher's wordbook  of 30,000 words. New York: Columbia University, Teachers  College: Bureau of Publications.   [22] Kuera, H., & Francis, N. (1967). Computational analysis of  present-day American English. Providence, RI: Brown  University Press.   [23] Brown, G. D. (1984). A frequency count of 190,000 words in  theLondon-Lund Corpus of English Conversation. Behavior  Research Methods, Instruments, & Computers, 16(6), 502- 532.    [24] Brysbaert, M., & New, B. (2009). Moving beyond Kuera  and Francis: A critical evaluation of current word frequency  norms and the introduction of a new and improved word  frequency measure for American English. Behavior Research  Methods, 41(4), 977-990. doi:10.3758/brm.41.4.977   [25] The British National Corpus, version 3 (BNC XML Edition).  2007. Distributed by Oxford University Computing Services  on behalf of the BNC Consortium. URL:  http://www.natcorp.ox.ac.uk/   [26] Coxhead, A. (2000). A new academic word list. TESOL  Quarterly, 34(2), 213-238.    [27] Simpson-Vlach, R., & Ellis, N. C. (2010). An academic  formulas list: New methods in phraseology research. Applied  LInguistics, 31(4), 487-512.    [28] Brysbaert, M., Warriner, A. B., & Kuperman, V. (2014).  Concreteness ratings for 40 thousand generally known  English word lemmas. Behavior Research Methods, 46(3),  904-911.Coltheart, 1981   [29] Coltheart, M. (1981). The MRC psycholinguistic database.  The Quarterly Journal of Experimental Psychology, 33(4),  497-505.    [30] Kuperman, V., Stadthagen-Gonzalez, H., & Brysbaert, M.  (2012). Age-of-acquisition ratings for 30,000 English words.  Behavior Research Methods, 44(4), 978990.    [31]  Bird, S., Klein, E., & Loper, E. (2009). Natural language  processing with Python. O'Reilly Media, Inc.   [32] Miller, G. A. (1995). WordNet: A lexical database for  English. Communications of the ACM, 38(11), 3941.      [33] Cambria, E., Grassi, M., Hussain, A., & Havasi, C. (2012).  Sentic computing for social media marketing. Multimedia  tools and applications, 59(2), 557-577.    [34] Cambria, E., Speer, R., Havasi, C., & Hussain, A. (2010).  SenticNet: A Publicly Available Semantic Resource for  Opinion Mining. Paper presented at the AAAI fall  symposium: commonsense knowledge.   [35] Mohammad, S. M., & Turney, P. D. (2010). Emotions  evoked by common words and phrases: Using Mechanical  Turk to create an emotion lexicon. Paper presented at the  Proceedings of the NAACL HLT 2010 workshop on  computational approaches to analysis and generation of  emotion in text.   [36] Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing  a wordemotion association lexicon. Computational  Intelligence, 29(3), 436465.     [37]  Lasswell, H. D., & Namenwirth, J. Z. (1969). The Lasswell  Value Dictionary. New Haven: Yale University Press.   [38] Scherer, K. R. (2005). What are emotions And how can they  be measured Social science information, 44(4), 695-729   [39] Polanyi, L., & Zaenen, A. (2006). Contextual Valence  Shifters. In J. G. Shanahan, Y. Qu, & J. Wiebe (Eds.),  Computing Attitude and Affect in Text: Theory and  Applications (pp. 1-10). Dordrecht: Springer Netherlands.   [40] Hutto, C. J., & Gilbert, E. (2014). Vader: A parsimonious  rule-based model for sentiment analysis of social media text.  Paper presented at the 8th Int. AAAI Conf. on Weblogs and  Social Media, Ann Arbor, MI.   [41]  Toutanova, K., Klein, D., Manning, C. D., & Singer, Y.  (2003). Feature-rich part-of-speech tagging with a cyclic  dependency network. Paper presented at the Proceedings of  the 2003 Conference of the North American Chapter of the  Association for Computational Linguistics on Human  Language Technology-Volume 1.   [42] Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard,  S. J., & McClosky, D. (2014). The Stanford CoreNLP  Natural Language Processing Toolkit. Paper presented at the  52nd Annual Meeting of the Association for Computational  Linguistics: System Demonstrations, Baltimore, MA.   [43] R Team (2014). R: A language and environment for  statistical computing. R Foundation for Statistical  Computing, Vienna, Austria. 2013: ISBN 3-900051-07-0.   [44] Bates, D., Mchler, M., Bolker, B., & Walker, S. (2015).  Fitting linear mixed-effects models using lme4. arXiv  preprint arXiv:1406.5823.    [45]  Kuznetsova, A., Brockhoff, P. B., & Christensen, R. H. B.  (2015). Package lmerTest. R package version, 2.0-29.     "}
{"index":{"_id":"42"}}
{"datatype":"inproceedings","key":"Mouri:2017:LAS:3027385.3027408","author":"Mouri, Kousuke and Ogata, Hiroaki and Uosaki, Noriko","title":"Learning Analytics in a Seamless Learning Environment","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"348--357","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027408","doi":"10.1145/3027385.3027408","acmid":"3027408","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, seamless learning, ubiquitous learning","Abstract":"This paper describes seamless learning analytics methods of VASCORLL (Visualization and Analysis System for COnnecting Relationships of Learning Logs). VASCORLL is a system for visualizing and analyzing the learning logs collected by the seamless learning system, which supports language learning in the real-world. As far, several studies have been made in the seamless learning environments in order to bridge formal learning over informal learning. However, their focus was the implementation of the seamless learning environment in education. This study focuses on visualizing and analyzing learning logs collected in the seamless learning environment. This paper describes how our analytics could contribute to bridging the gap between formal and informal learning. An experiment was conducted to evaluate 1) whether our developed VASCORLL is effective in connecting the words learned in formal learning to the ones learned in informal learning, 2) which social network algorithm is effective to enhance learning in the seamless learning environment. Twenty international students participated in the evaluation experiment, and they were able to increase their learning opportunities by using VASCORLL. In addition, it was found that the betweenness centrality is useful in finding central words bridging formal and informal learning.1","pdf":"Learning Analytics in a Seamless Learning Environment    Kousuke Mouri  Faculty of Arts and Science   Kyushu University  Japan   mourikousuke@gmail.com   Hiroaki Ogata  Faculty of Arts and Science   Kyushu University  Japan   hiroaki.ogata@gmail.com   Noriko Uosaki  Center for International Education and   Exchange, Osaka University  Japan   n.uosaki@gmail.com   ABSTRACT   This paper describes seamless learning analytics methods of   VASCORLL (Visualization and Analysis System for COnnecting   Relationships of Learning Logs). VASCORLL is a system for   visualizing and analyzing the learning logs collected by the   seamless learning system, which supports language learning in the   real-world. As far, several studies have been made in the seamless   learning environments in order to bridge formal learning over   informal learning. However, their focus was the implementation   of the seamless learning environment in education. This study   focuses on visualizing and analyzing learning logs collected in the   seamless learning environment. This paper describes how our   analytics could contribute to bridging the gap between formal and   informal learning. An experiment was conducted to evaluate 1)   whether our developed VASCORLL is effective in connecting the   words learned in formal learning to the ones learned in informal   learning, 2) which social network algorithm is effective to   enhance learning in the seamless learning environment. Twenty   international students participated in the evaluation experiment,   and they were able to increase their learning opportunities by   using VASCORLL. In addition, it was found that the   betweenness centrality is useful in finding central words bridging   formal and informal learning. 1   CCS CONCEPTS    Human-centered computing   Ubiquitous and mobile   computing  Ubiquitous and mobile computing systems and   tools    KEYWORDS   Ubiquitous learning, seamless learning, learning analytics   1 INTRODUCTION   Seamless learning is defined as an approach when a person   experiences a continuity of learning, and consciously bridges the   multifaceted learning efforts, across a combination of locations,   times, technologies or social settings [28].  Several researchers in                                                                     Permission to make digital or hard copies of all or part of this work for personal or   classroom use is granted without fee provided that copies are not made or distributed   for profit or commercial advantage and that copies bear this notice and the full   citation on the first page. Copyrights for components of this work owned by others   than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,   or republish, to post on servers or to redistribute to lists, requires prior specific   permission and/or a fee. Request permissions from Permissions@acm.org.   LAK17, March 13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027408   the seamless learning field have pointed out that mobile and   ubiquitous technologies have enabled students to learn   continuously across different contexts [10], [11]. The main   characteristics of seamless learning are shown as follows: (1)   Encompassing formal and informal learning, (2) Encompassing   personalized and social learning, (3) Across time, (4) Across   locations, (5) Ubiquitous knowledge access, (6) Encompassing   physical and digital worlds, (7) Combined use of multiple device   types, (8) Seamless switching between multiple learning tasks, (9)   Knowledge synthesis, (10) Encompassing multiple pedagogical or   learning activity models.   One of its most important issues is how to connect formal learning   with informal one, because this is inevitable in designing both in-  school and out-of-school activities in order to link what they have   learned in school with their daily life experiences and vice versa,   that is to say, to connect what they have learned in their daily lives   to their experiences in class. In this paper, the term formal   learning is defined as the learning under intentionally organized   environments and the term informal learning is defined as the   learning out of organized environments. For example, inside-of-  class learning is one of the formal learning situations and out-of-  class self-learning is one of the informal learning situations.   One of the application domains of seamless learning is language   learning. For example, Wong et al. [27] reported a seamless   learning system called MyCLOUD (My Chinese UbiquitOUs   learning Days), which allows students to learn Chinese language   in both in-school and out-of-school learning spaces. Uosaki et al.   [25] reported a seamless learning system called SMALL System   (Seamless Mobile-Assisted Language Learning support system) in   order to support Japanese students who aim to learn English   language in a formal and an informal setting.   Most of these studies focused on realizing a seamless learning   environment at school or university. Once realized, the students   learning logs have been accumulated into their server. Therefore,   we contend that learning efficacy can be enhanced by visualizing   and analyzing their accumulated learning logs. So far, little   attention has been paid to this aspect. The research issues of   learning analytics based on seamless learning environments are as   follows:   (1) How to visualize and analyze learning logs accumulated in   formal and informal learning systems.    (2) How the analytics can bridge the gap between formal learning   and informal learning.   (3) How the analytics can enhance and support students learning   experiences.                  To tackle these issues, this paper proposes a visualization and   analysis system called VASCORLL (Visualization and Analysis   System for Connecting Relationships of Learning Logs).   VASCORLL is to connect logs collected by using e-book system   with logs collected by using a ubiquitous learning system called   SCROLL (System for Capturing and Reminding of Learning Log)   [21]. The rest of this paper is constructed as follows. Section 2   discusses related works regarding e-book-based learning analytics   and ubiquitous learning analytics. Section 3 introduces our   proposed seamless learning design and the system that   implements with SCROLL. Section 4 describes the methods of   VASCORLL that works in the seamless learning environments.   Section 5 and 6 describes the implementation and initial   evaluation experiment on the VASCORLL. Section 7 summarizes   the contributions made by the work and future works.   2 RELATED WORKS   2.1 E-book-based Learning Analytics   Nowadays, majority of textbooks are not only published in printed   format, but are also created as electronic text-book (e-book)   format available online or on mobile devices [2], [24]. Japanese   govemment has announced their future policy to introduce   instead of as their policy that they plan to introduce e-books in   all K12 schools by 2020. Many countries e-book policies only   focus on introducing the technology of e-books into K12 schools   [22]. However, little attention has been paid to visualizing and   analyzing important information from the e-book activity logs.   Therefore, it is necessary to explore various analytics in this   aspect.   This paper calls visualizing and analyzing of e-book activity logs   E-book-based Learning Analytics (ELA). In such analytics,   some researchers at Kyushu University in Japan reported several   analytics using a document viewer system called BookLooper   [22], [23], [29]. The objective of their studies is as follows: (1)   improving of learning materials, (2) analyzing learning patterns,   (3) detecting the students comprehensive level, (4) predicting   final grades, and (5) recommending e-books in accordance with   personalization. On the other hand, Kiyota et al. [8] proposed a   seamless learning system with EPUB (Electronic PUBlication:   one of the e-book formats). It is available on various mobile   devices including general smartphones, and it is easy to obtain   information, such as location, acceleration and rotation of the   terminal via their integrated sensors while students read the books.   The most common idea of those projects is to visualize and   analyze either logs collected in a formal setting or an informal   setting, especially, logs collected in a formal setting. However,   VASCORLL aims to visualize and analyze learning logs   accumulated in both learning environments (formal and informal   setting).   2.2 Ubiquitous Learning Analytics   In recent years, ubiquitous learning (u-learning) or mobile   learning (m-learning) has been the focus of attention in   educational research across the world [14], [19]. U-learning has   been carried out using ubiquitous technologies such as RFID tags   and cards, wireless communications, mobile phones, Personal   Digital Assistants (PDAs), and wearable computers [20]. These   types of learning include not only in-class learning (formal   learning), but also a variety of out-of-class learning (informal   learning) in spaces such as homes, libraries, and museums. In   such learning approaches, the majority of researchers have been   constructing a context-aware u-learning system, which integrated   learning materials and contextual information by using ubiquitous   technologies. For example, Hwang et al. [6] developed a context-  aware ubiquitous learning system with the attached RFID tag on   the plants. The application domain of their studies is nature   science. When a learner arrives in front of a plant with an RFID   tag, the system asks him or her questions about the plants   features, such as its trunk, shape, and color after they received it   using an RFID reader. This enables learners to understand deeply   by connecting knowledge about the plant with the real life   experience.   On the other hand, Ogata et al. [21] developed their u-learning   system called SCROLL, which allows users to share with others   by recording what learners have learned in their daily lives using a   web browser and mobile device anytime and anywhere. The   application domain of their studies is mainly language learning.   Using SCROLL, international students can learn new knowledge   through their experiences in their daily life with photos, audios,   and context such as location and place.   Aljohani et al. [17] described learning analytics called Ubiquitous   Learning Analytics (ULA) in order to analyze enormous learning   data, including contextual information accumulated by using these   u-learning systems. The value of the ULA is discussed by   considering two possible kinds of interactions. The first is the   interaction between learners and their contexts, referred to as   learners-to-context interaction. The second is the interaction   between learners and context-based knowledge, referred to as   learner-to-context-based learning materials interaction. They   suggested that the use of learner contextual data can enhance the   interaction between learners and their mobile devices, and   between learners and objects in their learning environments. In   addition, analyzing or visualizing contextual data has a potential   to improve knowledge of the patterns of learners interactions   with their contexts. One of the issues of the ULA is how to   visualize and analyze two interactions: learners-to-context and   learner-to-context-based words.   Mouri et al. [12], [13] tackled the issues and reported innovative   visualization and analysis methods. However, the focus of their   studies was to visualize and analyze learning logs accumulated in   ubiquitous learning environments (informal setting). Our   VASCORLL enables students to bridge the gap between formal   and informal learning by visualizing and analyzing what they   have learned in the classroom using a document viewer system   and what they have learned outside the classroom using SCROLL.   By providing the results of visualization and analysis, it is   expected that students can apply what they have learned in the   classroom to what they have learned outside the classroom.   3 SYSTEM DESIGN   3.1 Scroll   As described in Section 2.2, this paper utilizes SCROLL to   support international students in the real-world language learning   and share their experiences with each other. In the SCROLL      3   project, Ubiquitous Learning Log (ULL) is defined as a digital   record of what learners have learned in their daily lives. To   simplify the process of capturing learners learning experiences,   SCROLL provides an easy-to-use interface. It adopts an approach   to share contents with other users based on a LORE (Log-  Organize-Recall-Evaluate) model proposed by Ogata et al. [21].   How the model supports each learning process is described as   below.    (1) Log: International students are likely to face some problems  such as how to read, write and pronounce words in their daily   life. They can save what they have learned with photo, such   as location (latitude and longitude), learning place, and date   and time of creation as a ULL as shown in Figure 1.   (2) Organize: When an international student adds a new ULL,  SCROLL compares it with his past ULL and those of other   users, categorize it and shows him related ULLs. By showing   it with information on when and where they learned it, past   learning and current learning can be linked and their   knowledge will be reorganized and reinforced [26].   (3) Recall: Learners are likely to forget what they have learned  before. It is necessary to support re-calling their past ULLs.   During this learning process, the system support learners to   recall what they have learned by using quiz.   (4) Evaluate: It is important to recognize what and how the  learner has learned by analyzing the past ULLs, so that he or   she can improve what and how to learn in the future. Mouri   et al. [12] developed an innovative visualization system that   implemented Time-Map [7], network graph based graph   theory in order to support this learning process.   3.2 Seamless Learning System based on e-Book   The above SCROLL is instantiated as the seamless learning   environment to support formal and informal language learning.   Figure 2 shows the design framework of our system that   integrated SCROLL with EPUB. There are two ways of   supporting learning activities in our framework:    (1) E-book-based learning activity: The teachers or instructors   create e-book contents using PowerPoint and Keynote before   class, and use them in their courses. The uploaded e-book   contents are converted to EPUB format and it is supported to   access the contents by using smartphone and PCs. After   international students choose a target e-book in their course   in accordance with their language level, they can read those   learning materials on their web browser using EPUB viewer   as shown in Figure 3. They can underline, mark unknown   words in the e-books, and save it using SCROLL (Figure 1).   In addition, they use the EPUB viewer to prepare before   class, and to review after class. Therefore, they can read the   learning materials not only during class but also outside class,   such as their home or libraries. Students action logs such as   opening a book, zooming, and page turning are collected into   E-book logs and learning materials database.   (2) Authentic learning activity: In out-of-class activity,   international students proactively observe things, grasp the   meanings and review on their daily encounters, and apply   their past experiences to other learning situations. For   example, when international students learn how to read, write   and create (passbook) in an e-book during class, then   they can actually apply their experience through the e-book   to their real-life experience at the bank with the tips from   SCROLL.   4 METHOD   4.1 Visualization in the Seamless Learning   Environment   In order to visualize and analyze learning logs accumulated in the   seamless learning environment, this paper uniquely defines them   as two types of three-layer structures as shown in Figure 4:   Formal Learning Structure (FLS) and Informal Learning Structure   (ILS). FLS includes three layers, which are called formal   Figure 1: Interface of adding a ULL.      .         Figure 2: Design of the Seamless learning system.      .                       learners, formal words, and learning materials.    (1) Formal learners: The upper layer shows learners studying in   a formal setting, such as lecture room and classroom.    (2) Formal words: The intermediate layer shows words that they   have learned in a formal setting using SCROLL with EPUB.   (3) Learning materials: The Lowest layer shows learning   materials uploaded by teachers or instructors.    In order to visualize the relationships among formal learner,   formal words and learning materials, this paper visualizes the   relationships using network directed graph. How our visualization   method connects relationships of each node For example, if a   learner learns and saves a newly learned word using SCROLL   with EPUB interface during class, our visualization method will   connect the learner node in the upper layer in the FLS to the   word node in the intermediate layer in FLS. Besides, the word   node will connect to the learning material nodes in the lowest   layer in FLS. By visualizing these links, teachers and students can   grasp which e-book and which page that word appears.   ILS includes three layers, which is called Informal learners,   Informal words, and Locations   (1) Informal Learners: The upper layer shows learners studying  in an informal setting such as museums, restaurants and city   halls.   (2) Informal words: The intermediate layer shows words that  they have learned in an informal setting using SCROLL.   (3) Locations: The lowest layer shows contextual data such as  location and place where they have learned in an informal   setting.   According to Mouri et al. [14], [15], how to connect the   relationships in ILS are as follows: If a learner learns and saves a   new word in an informal setting using SCROLL, it will connect   the learner node in the upper layer in ILS to the word node in the   intermediate layer in ILS. Then, the word node will connect to   the node of the location where they have learned it. For example,   if the learner learned natto (a traditional Japanese food made   from fermented soybeans) at the supermarket, it will connect   natto in the intermediate layer to supermarket in the lowest   layer.    In order to bridge the relationships between FLS and ILS, the   visualization will connect same words which appear in the   intermediate layers both in the FLS and ILS (e.g. natto in the   intermediate layer in FLS and natto in the intermediate layer in   ILS). The analytics using two types of three-layers have the   following advantages:    (1) On the FLS side, the formal words with a large number of  links to related learning materials mean students learned it in   many classes. For example, if a student learns the word   passbook using a learning material during class, the   Figure 4: Visualization methods in the seamless learning environments: Formal Learning Structure (FLS) and   Informal Learning Structure (ILS)   Figure 3: SCROLL with EPUB interface.      .            5   visualization informs them of other learning material context   where it is used. In addition, the formal words with a large   number of links to the formal learners mean words which   were learned by many students during class.   (2) On the ILS side, the informal words that are related to many  places are the words can be learned in various places. When   a learner experiences tea ceremony of a traditional Japanese   culture, for instance at the university, they are likely to learn   such tea ceremony related words as maccha (special tea for   tea ceremony), seiza (to sit in the correct manner on a   Japanese tatami mat). They can also be learned in other   places. Maccha can be learned at the supermarket, and the   seiza can be learned at the martial arts gym.   4.2 Seamless Learning System based on e-Book   Based on the network graph described above, this paper analyzes   using social network analysis as shown in Table 1.   Degree, closeness and betweenness centrality are a fundamental   measurement concept for the social network analysis [3], [9].   Especially, we hypothesize that betweenness centrality could be to   bridge the gap between formal learning and informal learning. For   example, if an international student learns word natto on e-book   content in a formal setting, it might be able to be applied to   various learning places such as supermarkets, shopping malls, and   restaurants in an informal setting. However, it is difficult for   him/her to know whether it can be learned in other learning   environments nor where it can be learned. In addition, it is   difficult to know which word could play the most important role   to bridge over formal and informal learning to realize the seamless   learning environments. By using betweenness centrality, this   analysis could find most important words between FLS and ILS   side.    Table 1: Social network analysis   Algorithm Formula (graph G:=(V,E) with V vertices   and E edges)   Details   Degree   =     1     Degree centrality is defined as the number of links incident upon   a node. That is, it is the sum of each row in the adjacency matrix   representing the network. N is the number of node and is the   degree of the node i.   Closeness   = () =    1   ,    Closeness centrality is that the distance of a node to all others in   the network. is the shortest path length between i and j, and     is the average distance from I to all the other nodes.   Betweenness   =  1  (  1)(  2)    ()     ,,     Betweenness centrality is that the number of shortest paths   between any two nodes that pass via a given node.  is the   number of the shortest path between j and k, and () is the   number of the shortest path between j and k that contains node i.      Figure 5: Visualization and analysis interface                 5 IMPLEMENTATION   5.1 Interface of VASCORLL   In the field of network graph studies, the majority of them have   focused on advantages such as good-quality results, flexibility,   simplicity, and interactivity.    For example, a network layout called force-directed uses the   force vector algorithm proposed in the Gephi software,   appreciated for its simplicity and for the readability of the network,   which helps visualization [4], [18]. A network layout called   Yifan Hu multilevel uses a very fast algorithm to reduce   complexity [5]. The repulsive forces on one node from a cluster of   distant nodes are approximated by a Barners-Hut calculation,   which treats them as one super-node [1].   On the other hand, Mouri et al. [16] proposed Ubiquitous   Learning Graph (ULG), which is divided into four areas: top-left,   top-right, bottom-left, and bottom-right. In their evaluation   experiment, they reported it is important to establish their nodes   position on the network graph for readability and ease-of-use   when visualizing the relationships in the real-world language   learning.   With these points in mind, we developed a network layout called   Seamless Learning Graph (SLG), which is divided into six areas   as shown in Figure 5: upper-left (Formal learners), center-left   (Formal words), bottom-left (Learning materials), upper-right   (Informal learners), center-right (Informal words), and bottom-  right (Locations). These areas represent the layers described in the   section 4.1.   As described in Section 4.2, the interface implements three   centrality based on social network analysis. From the (1) to (3)   buttons mean each centrality. By clicking them, VASCORLL will   automatically visualize and analyze all learning logs accumulated   in SCROLL with EPUB. The node size is based on the numerical   value of each centrality.    Figure 6 (left) shows the enlarged graph in both formal and   informal word areas. There are two learning scenarios by utilizing   the results of visualization, which are called Learning via formal   words and Learning via informal words as shown in Figure 6   (right).   (1) Learning via formal words: As shown in Figure 6 (left), the   word natto is the biggest size in the formal words areas. By   clicking it, the system moves to the page where the word   natto appears. That way, learners can grasp which e-book   and which page includes it.   (2) Learning via informal word: After learning natto in the e-  book contents, learners can find natto in the informal   words areas. By clicking it, the system moves to the ULLs   (natto pages of SCROLL) learned in the informal setting.   Unlike the above learning method (1), by utilizing the ULL,   they can learn other learners learning experiences (not only   words but also time, location and place information) that   Figure 6: The enlarged graph both formal and informal words area and each hyperlink      7   cannot be learned in the formal setting.   5.2 Color coding of the visualized nodes   To avoid having learners get confused when they see each node   since there are many visualized nodes, it is definitely necessary to   establish some criteria for the distinction of each node. To   effectively distinguish each node, we created a color coding   scheme for the nodes as shown in Table 2.   6 EVALUATION   6.1 Participants and Design   Twenty international students who are studying at the University of   Tokushima in Japan participated in the evaluation experiments.   They were from China, Malaysia, Thailand and Mongolia aged   from 21 to 36. Their length of stay in Japan ranged from 1 month to   5 years.    The evaluation experiment was designed to evaluate the following   three points:   (1) Whether VASCORLL can increase the participants learning   opportunities (Learning opportunities means that the number   of ULLs that the learner uploaded to the system during the   evaluation period).   (2) Whether VASCORLL would be benefit in terms of usability in   finding important words in the seamless learning environment.   (3) Which centrality is effective in supporting learning in the   seamless learning environment   6.2 Method   Figure 7 shows the experimental procedure.   Before the evaluation experiment began, a Japanese instructor   uploaded e-book contents to SCROLL server. The uploaded e-book   contents were created based on the JLPT. Since they had never used   SCROLL with EPUB before, they practiced using it for one week   before using VASCORLL. In addition, based on the uploaded ULLs   during the practice, the students were divided into two groups:   Group A (experimental group) and Group B (control group). Group   A consisted of 5 Chinese, 4 Mongolians and 1 Malaysian. Group B   consisted of 3 Chinese, 5 Mongolians and 2 Thais. Group A learned   words in their daily lives and words in the ebook contents using   SCROLL with EPUB and VASCORLL. Group B learned words in   their daily lives and words in the e-book contents using   SCROLLL with EPUB. Participants used their own smart-phones   (iPhone 4s or Android device) to upload their ULLs in a formal   and an informal setting anytime and anywhere. The mobile   devices used in the evaluation experiment were three iPhone 4s,   fourteen iPhone 5s, and three Samsung Galaxy Note 3s.   In the next phase, both Group A and B students evaluated, during   u-learning activity, each centrality based on social network   analysis. Participants learned words using three centrality: degree,   closeness, and betweenness. They were asked to use the   prearranged one centrality (e.g. participants firstly had to use   degree centrality for one day). After the evaluation experiment,   the participants were asked to complete five-point-scale   questionnaires to evaluate the system performance and usability,   as well as the user-friendliness of understanding the contents and   finding ULLs using each centrality in VASCORLL.   6.3 Result and Discussion   6.3.1 Whether VASCROLL can increase the participants  learning opportunities   In order to examine the increase of learning opportunities by   VASCORLL, we compared the number of the uploaded ULLs of   Group A with that of Group B using F-test. Table 3 shows the   number of ULLs that the participants uploaded during the   evaluation. In total, Group A students uploaded 189 ULLs and   Group B students uploaded 127 ULLs to the system. The means   and standard deviations were 18.9 and 6.41 for Group A, and 12.7   and 6.75 for Group B. It was found that the learning opportunities   of the two groups were significantly different with F = 4.41 (p   Table 2: Color coding to distinguish the kinds of nodes   Node Layer Node color   Formal learners Upper in FLS Red   Formal words Intermediate in FLS Yellow   Learning materials Lowest in FLS Blue   Informal learners Upper in ILS Pink   Informal words Intermediate in ILS Green   Locations Lowest in ILS Light blue      Table 3: Number of The Uploaded ULLs   Group Number   of ULLs   Mean SD Adjusted   mean   F   Group A 189 18.9 6.41 17.83 4.41   Group B 127 12.7 6.75 12.83       Figure 7: Experimental procedure                 < .05), implying that VASCROLL was able to increase their   learning opportunities. The adjusted mean of Group A (17.83) is   statistically higher than that of Group B (12.83). That means that   VASCORLL was a useful tool to increase their learning   opportunities.   6.3.2  Whether VASCROLL would be benefit in terms of in  finding important words   The results of a five-point-scale questionnaire are presented in   Table 4 (Best: 5, Worst: 1).    Q1 asks about whether the participants were able to find that the   words that they learned during class were connected to the words   that other learners learned outside class. Similarly, Q2 asks about   whether the participants were able to find that the words they have   learned outside class were connected to the words that others have   learned in class. The results of Q1 and Q2 revealed that the   participants found the relationships of words between formal and   informal learning. For example, some students learned Japanese   word,  (natto) in e-book contents in class. By uploading  natto to the system, the system could show them that other   students had learned it at the shopping mall and supermarkets.   That way VASCORLL was able to connect the words between   formal and informal learning.   Q3 asks about whether the participants were able to learn and find   the relationships between words and places by using VASCORLL.   For example, when a participant learned Japanese word,  (Cuisine) in class, he/she could find it was connected to the   experiences of others at places such as schools and restaurants. By   sharing the authentic experiences that are rarely able to acquire in   formal setting, VASCORLL enabled them to experience indirectly   what other people experienced thanks to the system, which   connected their formal learning to the informal one.   Q4 asks about whether the participants were able to find their   newly learned words in other e-book contents. For example, when   a participant learned  (Use) in an e-book material titled   Japanese Learning Beginner Vol.1, the system connected it to   other e-book materials such as Japanese Learning Beginner   Vol.2 and Onomatopoeia Japanese Learning Vol.1. That way,   they could learn that it is a frequently used word in the Japanese   language.   Q5 asks about whether VASCORLL was easy to use. They were   asked to evaluate the usability in terms of the operability and   readability of the visualized graph. The response shows that many   participants felt that VASCORLL was not easy to use. We asked   them to give comments regarding this problem. Examples of the   negative comments are as follows:   (1) The speed of visualizing and analyzing logs in the system is   too slow (It took about 20~30 sec.).   (2) If visualizing logs using a mobile device, it is hard to read   the nodes on the device because my screen is very small.   However, if logs are visualized on a personal computer, they   become very easy to read.   (3) It was a little bit difficult to understand how to use the   system.   From the comments (1) and (2), the participants would suggest   that the system developers need to improve functionality in   accordance with their mobile device and system speed in   visualizing a large mount of logs. The comment (3) shows that   even though we explained the usage of VASCORLL before the   evaluation experiment, some participants did not understand fully   how to use it. Thus, our next evaluation ought to be more   carefully planned.   6.3.3  Which centrality is effective in supporting seamless  learning   The results of five-point-scale questionnaire for evaluating each   centrality (degree, closeness and betweenness) are presented in   Table 5 (Best: 5, Wrong: 1). In addition, the participants were   asked questions such as Which centrality is the easiest in   understanding or finding central words and Which centrality is   the most effective for learning in order to evaluate each centrality   in the seamless learning environment.   Q1-Q3 asked about whether the participants were able to   understand and find central words using fundamental social   network analysis: degree centrality, closeness centrality, and   betweenness centrality. From the results of the questionnaire,   many participants preferred to learn and find central words using   betweenness centrality because the mean score of the Q3 is the   most highest.    In order to find the most effective centrality for them to learn   central words, we interviewed the participants to compare   betweenness centrality with other centrality.   (1) Degree centrality versus Betweenness centrality   Table 4:  Result of the five-point-scale questionnaire for   evaluating usability and effectiveness of system (Group A)   Question Mean SD   Q1. Were you able to connect words   inside-class to out-side-class learning by   using VASCORLL   3.7 0.82   Q2. Were you able to connect words out-  side-class to inside-class learning by using   VASCORLL   3.9 0.99   Q3. Were you able to learn and find the   relationship between words and places by   using VASCORLL   3.4 0.96   Q4. Were you able to learn and find the   relationships between words and e-book   contents by using VASCORLL   3.5 0.87   Q5. Was VASCORLL ease of use 2.6 1.23      Table 5. Result of the five-point-scale questionnaire for   evaluating each centrality (N=20)   Question Mean SD   Q1. Was the degree centrality easy of   understanding or finding central words   3 0.91   Q2.  Was the closeness centrality easy of   understanding or finding central words   2.7 1.08   Q3.  Was the betweenness centrality easy   of understanding or finding central words   3.7 0.86         9   Degree centrality enabled the participants to find merely nodes   that have many links. Two participants selected the centrality in   terms of usability and effectiveness for learning because it is   simple and easy to understand the characteristics. However, some   participants commented that it was difficult to find words bridging   formal learning over informal learning. When comparing   betweenness centrality with the degree centrality, the processing   speed of betweenness centrality was little, but it was very useful   in finding central words which linked formal and informal   learning.   (2) Closeness centrality versus Betweenness centrality   When comparing closeness centrality with the closeness   centrality, the closeness centrality was not useful to find central   words in the seamless learning environment. There was no   numerical value difference between formal words in FLS and   informal words in ILS, so that the participants could not find   central words. Therefore, this paper concluded that the closeness   centrality was not a useful centrality in finding central words in   the seamless learning environment if using our visualization and   analysis method.   As shown in Figure 8, the majority of the participants preferred to   use betweenness centrality than other centrality. We asked them   why you preferred to the betweenness centrality than other   algorithms. Their feedbacks are as follows:    Because it is easy to find words in my e-book content  linking to informal words, the betweenness centrality is very   good.    It was easy to understand. And I learned some words.  Then, it recommended some useful words to me (e.g. the   size of green or yellow node).    The betweenness centrality turned out a very good centrality in   terms of easiness to find words bridging formal and informal   learning. In addition, we compared betweenness centrality with   other centrality. Fifteen answered that the betweenness centrality   is helpful to find central words in the seamless learning   environment. Seventeen answered it worked effectively in   language learning.   7 CONCLUSION   This paper described a system called VASCORLL for visualizing   and analyzing learning logs collected in the seamless learning   environment in order to bridge the gap between formal and   informal learning. VASCORLL works on cyber-physical setting   to link learners in the real world and learning logs that are   accumulated in the cyber spaces using the ubiquitous learning   system called SCROLL with EPUB. SCROLL with EPUB   enabled international students to learn through two learning   activities: e-book-based learning activity and authentic learning   activity.   Through those learning activities, we proposed visualization and   analysis methods based on graph theory, social network analysis   and graph drawing algorithms in order to find pivotal words in the   seamless learning environment. Two types of three-layer   structures called FLS and ILS were adapted as the visualization   methods. That way, teachers and students could easily grasp   words bridging between words in FLS and ILS. In addition, this   paper evaluated whether they were able to find the most pivotal   words on the network graph using each centrality based on social   network analysis.   The evaluation was conducted after the implementation of   VASCORLL. A questionnaire with a five-point-scale conducted   after the evaluation showed that VASCORLL was a useful tool to   find central words bridging formal and informal learning. The   result of questionnaires for evaluating each centrality showed the   most effective centrality for learning was betweenness centrality.   Therefore, we concluded that the betweenness centrality is the   most important centrality in the seamless learning environment.   VASCORLL will be evaluated repeatedly, with the processing   speed of visualizing and analyzing learning logs improved. In   addition, our future works include applying VASCORLL to other   application domains such as math, physics, and science education,   and long-term evaluations with an enough number of participants.   8 ACKNOWLEDGMENTS   This part of this research was supported by the Grant-in-Aid for   Scientific Research No.25282059, No.26560122, No.25540091,   No.26350319 and No.16J05548 from the Ministry of Education,   Culture, Sports, Science and Technology (MEXT) in Japan. The   research results have been partly achieved by Research and   Development on Fundamental and Utilization Technologies for   Social Big Data (178A03), the Commissioned Research of   National Institute of Information and Communications   Technology (NICT), Japan.   9 REFERENCES   [1] Barnes, J. & Hut, P., 1986. A Hierarchical O (n log n) Force-  Calculation Algorithm, Nature 324(4), 446449.   [2] Fang, H., Liu, P. & Huang, R., 2011. The Research on E-  book-oriented Mobile Learning System Environment   Application and Its tendency, International Conference on   Computer Science and Education, 1333-1338.   [3] Freeman, l.-C., 1979, Centrality in social networks:   Conceptual clarification, Social Networks, Vol.1, 215-239. Figure 8: Number of selected centrality by each participant                 [4] Fruchterman, M.-E.-J. & Reingold, E.M., 1991. Graph   drawing by force-directed placement, Sofw Pract. Exper.   21(11), 11291164.   [5] Hu, Y.-F. & Scolt, J.-A., 2001. A Multilevel Algorithm for   Wavefront Reduction, SIAM J. Sci. Comput., 23(4), 1352  1375.   [6] Hwang, G.-J., Chu, H.-C., Liu, Y.-S. & Tsai, C.-C., 2011. A   knowledge acquisition approach to developing Mindtools for   organizing and sharing diffetentating knowledge in a   ubiquitous learning environment, Computer & Education,   Vol.57, No.1, 1368-1377.   [7] Johson, I. & Wilson, A., 2009. The Time-Map Project:   Developing Time-Based GIS Display for Cultural data,   Journal of GIS in Archaeology 1, 123-135.   [8] Kiyota, M., Mouri, K. & Ogata, H., 2015. Seamless Learning   System Based on e-Book, Proc. of the 23nd International   Conference on Computers in Education (ICCE 2015), 448-  452.   [9] Latora, V. & Marchiori, M., 2007. A measure of centrality   based on the network efficiency, Journal of Physics, Vol.9, 1-  12.   [10] Looi, C.-K., Sun, D. & Xie, W., 2015. Exploring Students   Progression in an Inquiry Science Curriculum Enabled by   Mobile Learning, IEEE Transactions on Learning   Technologies, Vol.8, No.1, 43-54.   [11] Milrad, M., Wong, L.-H., Sharples, M., Hwang, G.-J., Looi,   C.-K. & Ogata, H., 2013. Seamless learning: An international   perspective on next generation technology enhanced learning,   In Z. L. Berge and L. Y. Muilenburg, Handbook of mobile   learning (Chapter 9), 95-108.   [12] Mouri, K. & Ogata, H., 2015. Ubiquitous Learning Analytics   in the Real-world Language Learning, Journal of Smart   Learning Environment, Vol.2, No.15, 1-18.   [13] Mouri, K., Ogata, H. & Uosaki, N., 2015. Analysis of   Ubiquitous Learning Logs Using Social Network Analysis,   International Journal of Mobile Learning and Organisation   (IJMLO), Vol.9, No.2, 101-123.   [14] Mouri, K., Ogata, H. & Uosaki, N., 2015. Ubiquitous learning   analytics in the context of real-world language learning, Proc.   of International Conference Learning Analytics and   Knowledge (LAK 15), 378-382.   [15] Mouri, K., Ogata, H. & Uosaki, N., 2015. Visualization and   Analysis System for Connecting Relationships of Learning   Logs, proc. of the 23nd International Conference on   Computer in Education (ICCE 2015), 357-366.   [16] Mouri, K., Ogata, H., Uosaki, N. & Liu, S. Visualization for   Analyzing Ubiquitous Learning Logs, proc. of the 22nd   International Conference on Computers in Education (ICCE   2014), 461-470.   [17] Aljohani, N.-R. & Davis, H.-C., 2012. Learning analytics in   mobile and ubiquitous learning environments, In 11th World   Conference on Mobile and Contextual Learning.   [18] Noack, A., 2009. Modularity clustering is force-directed   layout, Physical Reviewe E, Vol.79, No.2, 1-8.   [19] Ogata, H. & Yano, Y., 2004. Context-aware support for   computer-supported ubiquitous learning, Proc. of IEEE   International Workshop on Wireless and Mobile Technologies   in Education, 27-34.   [20] Ogata, H., Hou, B., Li, M., Uosaki, N. & Mouri, K. Role of   Passive Capturing in a Ubiquitous Learning Environment,   International association for Development of the Information   Society, 117-124.   [21] Ogata, H., Hou, B., Li, M., Uosaki, N., Mouri, K. & Liu, S.,   2014. Ubiquitous Learning Project Using Life-logging   Technology in Japan, Educational Technology and Society   Journal, Vol.17, No.2, 85-100.   [22] Ogata, H., Yin, C., Okubo, F., Shimada, A., Kojima, K. &   Yamada, M., 2015. E-Book-based Learning Analytics in   University Education, International Conference on Computer   in Education (ICCE 2015), 401-406.   [23] Shimada, A., Okubo, K., Yin, C., Kojima, K., Yamada, M. &   Ogata, H., 2015. Informal learning behavior analysis using   action logs and slide features in e-textbooks, Proc. of IEEE   International Conference on Advanced Learning   Technologies, 116-117.   [24] Shin, J.-A., 2012. Analysis on the digital textbooks different   effectiveness by characteristics of learner, International   Journal of Education and Learning, Vol.1, No.2, 23-38.   [25] Uosaki, N., Li, M., Hou, B., Ogata, H. & Yano, Y., 2010.   Supporting an English Course Using Handhelds in a Seamless   Learning Environment, Workshop of the 18nd International   Conference on Computers in Education (ICCE 2010), 185-  192.   [26] Uosaki, N., Ogata, H. & Mouri, K., 2015. How We Can Boost   Up Outside-class Learning: Effectiveness of Ubiquitous   Learning Log System, International Journal of Mobile   Learning and Organisation (IJMLO), Vol. 9, No.2, 160-181.   [27] Wong, L.-H., Chai, C.-S., King, R. & Zhang, X., 2014.   Unpacking the researcher-teacher co-design process of a   seamless language environment with the TPACK framework,   22nd International conference on computer in education   (ICCE 2014), 886-895.   [28] Wong, L.-H., Milrad, M. & Specht, M., 2015. Seamless learning in   the age of mobile connectivity, Singapore: Springer.   [29] Yin, C., Okubo, F., Shimada, A., Kojima, K., Yamada, M.,   Ogata, H. & Fujimura, N., 2014. Smart Phone based Data   Collecting System for Analysing Learning Behaviors,   International Conference on Computer in Education (ICCE   2014), 575-577.       "}
{"index":{"_id":"43"}}
{"datatype":"inproceedings","key":"Holstein:2017:SIL:3027385.3027450","author":"Holstein, Kenneth and McLaren, Bruce M. and Aleven, Vincent","title":"SPACLE: Investigating Learning Across Virtual and Physical Spaces Using Spatial Replays","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"358--367","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027450","doi":"10.1145/3027385.3027450","acmid":"3027450","publisher":"ACM","address":"New York, NY, USA","keywords":"blended learning, causal modeling, classroom, intelligent tutoring systems, teachers, visualizations","Abstract":"	Classroom experiments that evaluate the effectiveness of educational technologies do not typically examine the effects of classroom contextual variables (e.g., out-of-software help-giving and external distractions). Yet these variables may influence students' instructional outcomes. In this paper, we introduce the Spatial Classroom Log Explorer (SPACLE): a prototype tool that facilitates the rapid discovery of relationships between within-software and out-of-software events. Unlike previous tools for retrospective analysis, SPACLE replays moment-by-moment analytics about student and teacher behaviors in their original spatial context. We present a data analysis workflow using SPACLE and demonstrate how this workflow can support causal discovery. We share the results of our initial replay analyses using SPACLE, which highlight the importance of considering spatial factors in the classroom when analyzing ITS log data. We also present the results of an investigation into the effects of student-teacher interactions on student learning in K-12 blended classrooms, using our workflow, which combines replay analysis with SPACLE and causal modeling. Our findings suggest that students' awareness of being monitored by their teachers may promote learning, and that gaming the system behaviors may extend outside of educational software use.","pdf":"SPACLE: Investigating learning across virtual and  physical spaces using spatial replays   Kenneth Holstein, Bruce M. McLaren, and Vincent Aleven  Human-Computer Interaction Institute   Carnegie Mellon University  Pittsburgh, PA 15213   {kjholste, bmclaren, aleven}@cs.cmu.edu  ABSTRACT  Classroom experiments that evaluate the effectiveness of  educational technologies do not typically examine the effects of  classroom contextual variables (e.g., out-of-software help-giving  and external distractions). Yet these variables may influence  students instructional outcomes. In this paper, we introduce the  Spatial Classroom Log Explorer (SPACLE): a prototype tool that  facilitates the rapid discovery of relationships between within- software and out-of-software events. Unlike previous tools for  retrospective analysis, SPACLE replays moment-by-moment  analytics about student and teacher behaviors in their original  spatial context. We present a data analysis workflow using  SPACLE and demonstrate how this workflow can support causal  discovery. We share the results of our initial replay analyses using  SPACLE, which highlight the importance of considering spatial  factors in the classroom when analyzing ITS log data. We also  present the results of an investigation into the effects of student- teacher interactions on student learning in K-12 blended  classrooms, using our workflow, which combines replay analysis  with SPACLE and causal modeling. Our findings suggest that  students awareness of being monitored by their teachers may  promote learning, and that gaming the system behaviors may  extend outside of educational software use.   CCS Concepts   Applied computing ~ Computer-assisted instruction   Human-centered computing ~ Activity centered design     Keywords  Intelligent tutoring systems, visualizations, causal modeling,  blended learning, classroom, teachers   1. INTRODUCTION  In recent years, there has been a growing interest within the  learning analytics and educational data mining communities in  multi-modal learning analytics: the collection and integrated  analysis of diverse data streams (e.g., computer log files, motion  sensor logs, field observations, and audio recordings) to obtain a  richer picture of student learning (e.g., [32, 33, 43]). Some of this   work has focused on blended learning contexts  introducing  methods for measuring and studying learning-related interactions  that cross physical and virtual spaces. For example, Baker et al.  studied relationships between students behavior patterns within  educational software and their interactions with peers and teachers  in the physical classroom by analyzing computer log streams that  were synchronized with quantitative field observations [13].   In parallel, there have been recent calls for added rigor in the  design of learning analytics tools for teachers and students. If  monitoring, awareness, and reflection tools for classrooms are to  be effective, the design of these tools will likely benefit from a  theoretically and empirically informed understanding of the causal  mechanisms by which they could positively impact student  learning [30, 33]. This may include, for example, understanding  the dynamics of learning environments in which such tools will be  used, and identifying any existing teacher or student practices  with which they may conflict [32, 46]. In addition, a better  understanding of the nature and effects of both teacher and student  decision-making in such environments will likely be essential to  the design of tools that can promote more effective decision- making  [33, 37].  We are currently designing real-time learning analytics tools to  help K-12 teachers more effectively guide their students as they  work with adaptive educational technologies in the classroom. To  inform our design process, we wish to examine the effects that  teacher-student interactions have on student learning over  relatively short time periods (seconds to hours, corresponding to  Newells cognitive and rational bands of action [44]).    To this end, we introduce the Spatial Classroom Log Explorer  (SPACLE), a prototype tool that facilitates the discovery of  relationships between teacher activity, classroom layout, and  student learning and behavior in blended classrooms. Unlike  existing tools for retrospective analysis of blended class sessions  (e.g., [1, 15]), SPACLE visualizes user-selected, moment-by- moment analytics about student and teacher behaviors within their  spatial context. Although students out-of-software behaviors and  spatial positions in the classroom are very rarely considered in  analyses of log streams from educational software, there is reason  to suspect that spatial factors may impact learning (e.g., [21]).  Some existing tools allow researchers to quickly alternate between  analyzing software log data and examining webcam or screen  recordings from students computers (e.g., [1]). While such tools  can reveal rich features of individual and small-group learning  sessions (including on-task conversations and student affect), they  are not designed to reveal broader relationships between the  spatiotemporal dynamics of the classroom and students learning.  SPACLE enables researchers to visualize moment-by-moment  analytics about both out-of-software interactions and students  current learning and behavioral states (as computed from software  logs) over a spatial map of the classroom in which the data was     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada Copyright is  held by the owner/author(s). Publication rights licensed to ACM.   ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027450     collected. By visualizing a relatively small set of features within  an interactive replay of a class session, SPACLE may enable  faster detection of qualitative patterns across students than would  be feasible from higher-dimensional stimuli such as live  classroom observations or video recordings. And like other forms  of log replay  including screen replays [28, 39] and low-fidelity  text-based replays [40] of students interactions within  educational software  SPACLE allows researchers to examine  different sets of analytics across multiple replay sessions, using  the same data set.   In this paper, we illustrate how SPACLE supports analysis of  classroom behaviors and provide initial findings from an  investigation of student-teacher interactions in classrooms using  intelligent tutoring systems. These findings suggest, for instance,  that student gaming the system behaviors [13, 34] may extend  to interactions occurring outside of the software. Finally, we  discuss how SPACLE can inform the evidence-based design of  real-time monitoring and awareness tools for teachers working in  blended classrooms.    2. BACKGROUND  Intelligent tutoring systems (ITSs) are advanced learning  technologies that allow students to work at their own pace:  providing step-by-step guidance during complex problem-solving  practice and continuously adapting instruction to students current  state (a set of measured variables, which may include estimates of  student knowledge, affective states, metacognitive skills, and  more) [10, 25]. Several meta-reviews have indicated that ITSs can  enhance student learning in classroom settings, compared with  traditional classroom instruction and other forms of educational  technology [17, 19, 20]. Another key advantage of adaptive  educational technologies such as ITSs may be that, when they are  used in classrooms, they can free the teacher to move throughout  the classroom and provide one-on-one support to students who  may need it most [8, 23].   ITSs also generate a wealth of data from students interactions  within the software, which have enabled fine-grained process  analyses of student learning and behavior. For example  educational data mining methods have been used to study the  effects of students off-task and gaming-the-system behaviors  (e.g., [6, 13]), cognitive and affective states such as frustration,  concentration, and confusion (e.g., [12]), and various micro-level  features of ITSs instructional design (e.g., [7]) on student  learning with these systems.    Although ITSs are often designed for use in K-12 schools, the ITS  literature has rarely studied the effects of elements of classroom  context on students learning with ITSs [1, 24]. For example:  recent field studies suggest that a large proportion of K-12  students help-seeking behavior, when using ITSs in classrooms,  may occur entirely outside of the software; but the ITS literature  tends to study the effects of within-software aspects of students  help-related behaviors rather than out-of-software behaviors such  as asking a teacher for help [4, 6]. While in-vivo classroom  studies aim to study the effectiveness of ITSs in the presence of  contextual variables that are likely to be present in real-world  classrooms (e.g., help from a teacher or peer, external distractions  affecting individuals or groups of students, collaboration between  students, etc.), they do not typically measure the effects of the  contextual variables themselves  instead treating these as noise  [26, 27, 37] (though see [1, 13]).   There is reason to expect, however, that some of these contextual  variables may be important mediators of student learning. In  particular, gaining a better understanding of the effects of teacher-  student interactions in classrooms using ITSs may be crucial to  understanding these systems effectiveness in real-world contexts.  For example, a large-scale, two-year evaluation study of Carnegie  Learnings Algebra I tutor suggested that variability in the out-of- software support teachers provided to students may have been at  least partly responsible for inconsistent results across evaluation  years [29]. Similarly, recent work has found that the extent to  which teachers override ITSs built-in, mastery learning based  problem selection may negatively impact student learning [31].   3. EXPLORING MULTIMODAL  CLASSROOM DATASETS THROUGH LOG  REPLAY  Even when an in-vivo classroom study is primarily designed to  test preconceived hypotheses (e.g. the effectiveness of a particular  educational technology design), researchers sometimes collect  qualitative classroom observations during the course of the study.  These observations can allow researchers to gain a richer picture  of what went on during a given class session, which may in turn  help explain and interpret study results. Often times, these  qualitative observations lead to unexpected discoveries, which can  later be investigated more thoroughly through targeted follow-up  experiments and observation sessions or educational data mining.  For example, classroom observations of students misuse of ITSs  inspired a line of experimental and data mining work dedicated to  uncovering the underlying causes behind these behaviors, as well  as design work dedicated to intervening on these underlying  causes [34]. Similarly, our current work was originally inspired by  informal classroom observations of teachers interactions with  their students during lab sessions, in the context of in-vivo  experiments that were not intended to study (and did not explicitly  consider the effects of) teacher-student interactions.   We have developed SPACLE1 to extend this observation process,  by enabling researchers to interactively re-examine classroom  ITS-use sessions, within a virtual map of the classroom layout  (c.f. [9]), while visualizing moment-by-moment analytics about  individual students. SPACLE replays are multimodal in the sense  that they combine multiple data streams  visualizing both  analytics about students out-of-software interactions (e.g.,  whether or not a student is raising her/his hand, talking to a peer,  or talking to the teacher), and analytics generated from students  interactions within the software, such as whether students are  inactive, abusing the tutors help functions [6], making frequent  careless errors [11], stuck on a current activity [5], confused,  frustrated, or engaged in their current task [2].    In each replay session, SPACLE allows researchers to specify the  analytics they would like to examine about the teacher, the  students, and/or any summary information they would like to  display at the class level (e.g. the percentage of the class that is  stuck on their current task at a given time). These analytics can  be implemented as custom plugin scripts, subject to minimal input  and output constraints. Then, given a map of the classroom layout  where observations took place, as well as a mapping from student  identifiers to their seating positions within the classroom (both of  which may be obtained, in approximate form, by asking a teacher  to provide a printed or hand-drawn copy of the seating chart),  SPACLE can generate visual replays that preserve potentially  important spatial information.                                                                         1 Available at: https://github.com/d19fe8/SPACLE     Specifically, researchers can import a class roster and an image  (e.g., a scanned drawing) of a classroom layout into SPACLE, and  then construct a virtual map of the classroom within the interface,  by dragging, rotating, and resizing graphical representations of  students (which are automatically generated, and pre-labeled,  based on the class roster) into place, using the image as a guide.  Each student is represented as a small circle with a rectangle  directly above it (representing the students computer screen) and  a name or other identifier directly below it. Researchers can then  choose to visualize moment-by-moment analytics about students  by assigning certain analytics to appear either in students circles  (typically used to visualize out-of-software behaviors such as  hand raising), or on their computer screens (typically used to  visualize analytics about within-software interactions). In  addition, if analytics about teacher behavior are present in a  synchronized dataset, these can be visualized via a free-floating   circle, which can change position on the map to represent the  teachers location in the classroom at a given time.   Aside from teacher position, all other analytics are visualized  through color. For continuous-valued or ordinal analytics, colors  can be assigned to two arbitrary end points within the range of  values a given metric can assume, and these analytics will be  visualized by interpolating between the two colors. For  categorical analytics, colors can be assigned individually to  different categories. Figure 1 shows a series of screenshots from a  replay session (showing time slices several minutes apart). In this  replay, the time elapsed since each students last within-software  interaction is displayed on their computer screens, with end  points of 30 seconds and 90 seconds. So, if a student has spent 30  or fewer seconds inactive, that students screen will appear black,  and if the student has spent 90 seconds or more inactive, the  screen will appear bright green. In between 30 and 90 seconds of  inactive time, a students screen will appear to gradually transition  from black to green. The teachers position and current activities  are also visualized in this replay, with on-task conversation  indicated by an orange circle, and inactive/distracted or off-task  conversation indicated by a blue circle. What is striking is the  amount of inactivity in the third frame, during a period when the  teacher is inactive, in the back of the classroom.   By examining a limited set of variables within a single replay  session, researchers may be able to detect qualitative patterns  across multiple students more rapidly than would be possible by  watching video recordings or conducting live classroom  observations [35]. And by visualizing different sets of analytics  across multiple replay sessions, researchers can iteratively explore  questions about potential mediators of student learning and  behavior within the software. After formulating hypotheses based  on replay analyses of a small number of classrooms, researchers  can investigate further through quantitative modeling on larger  samples. In addition to facilitating interactive replays, SPACLE  can generate synchronized datasets that enable educational data  mining techniques to be easily applied.   SPACLE is currently designed to work with ITS log data from  DataShop, an open repository for data from educational  technologies that currently houses over 700 data sets, many of  which have been used in secondary analyses.2 Prior to generating  replays, SPACLE first synchronizes records of out-of-software  events in the classroom (e.g. student and teacher behaviors, or  class-level disruptions) with log data generated from students  interactions within the software. The records of out-of-software  behaviors may be generated by hand (i.e., field observations  conducted by human observers), or, in the future, via automated  means such as sensors placed throughout the classroom (e.g. [21,  32]) or machine-learned detectors that attempt to infer out-of- software behaviors from ITS log data (e.g. [3]). The primary  requirements SPACLE imposes on these out-of-software logs are  that they either include continuous measurements (e.g. moment-  by-moment recordings of a teachers location and movements in  the classroom) or discrete observations marked with approximate  start and end times for a given behavior.   In our work thus far we have focused on using SPACLE to better  understand and interpret the effects of classroom dynamics on  student learning with ITSs  though we have also begun exploring  broader uses of SPACLE as a design tool (see Discussion). In the  next section, we illustrate how weve used SPACLE as a bridge                                                                       2 https://pslcdatashop.web.cmu.edu   Figure 1. A sequence of screenshots from a replay of a class  session generated by SPACLE. In the displayed classroom   there is a long row of desks in the center of the room,  oriented vertically, and several horizontal rows of desks on  either side of it. Student names are obscured in this image.   Students inactive time, ranging from 30 seconds or less  (black) to 90 seconds or more (bright green), is visualized on   their computer screens. The teacher circle takes on two  colors (orange: on-task conversation with a student, blue:   inactive/distracted or off-task conversation).        between qualitative analysis of classroom observation data and  larger-scale data mining, in our own early investigations into  potential effects of teacher behavior in ITS classrooms. After  confirming that the ITS used in our study was effective overall  (via analysis of students pre- and posttest scores), we began  exploring potential mediators of student learning (both within and  outside of the software). Over the course of these explorations, we  have gradually moved from contextually richer methods  (classroom observations and replay analysis on small samples) to  more generalizable methods (correlational analyses and causal  modeling on larger samples), and then back again to aid in  interpretation and additional exploration (see Figure 2).    4. CASE STUDY  4.1 Data Collection  The data we report in this study were originally collected during a  classroom experiment aimed at evaluating how analytics  generated from students interactions with an ITS, presented on a  prototype teacher dashboard, could help teachers plan more  effective lectures for subsequent class sessions. However, the data  analyzed in this paper are from a class period during which  students worked with ITSs and teachers did not yet have access to  a dashboard. Thus, these teachers often relied on direct  observations of their students computer screens, while walking  around the classroom, in order to monitor their students progress.  This is a typical situation when teachers use ITSs in their classes.   In this study, 299 middle school students used Lynnette, an ITS  for algebraic equation solving [14, 16], for 60 minutes, spread  across up to two class sessions. Students performance in equation  solving was measured before and after using Lynnette via  computer-based pre- and posttests, which were focused on  measuring procedural skills. We used two test forms, which were  identical up to the particular numbers used in equations. Test  forms were presented in counterbalanced order across pre- and   post-test. Test items were graded automatically, based on the  correctness of students final responses (i.e. without providing  partial credit for intermediate steps in equation solving).   We collected live classroom observations from a sample of 9 out  of 17 classrooms taught by 4 teachers, with a total of 151 students.  Students who were absent during any of the pretest, ITS-use  sessions, or posttest were excluded from subsequent analyses,  leaving 135 students in total. In the remainder of this paper, only  data from these 135 students are considered. Due to privacy  concerns, we were unable to video record class sessions. Instead,  during each class session, a member of our research team sat in  the back of the classroom (in order to minimize any disturbance  caused by their presence) and recorded coarse-grained field  observations of teacher and student behavior. We recorded  observations using LookWhosTalking3, a tool for coding live  classroom observations, developed at our institution, which was  customized with a coding scheme we designed to facilitate both  coding and eventual analyses. This coding scheme was adapted  from the Baker-Rodrigo observation method protocol (BROMP)  [42], and the TA observation protocol developed by Stang et al.  [18]. We extend the TA observation protocol by distinguishing  between different types of teacher interactions with students   namely, distinguishing whether a teacher is monitoring/observing  a student or holding a conversation with that student, and further  distinguishing between on-task and off-task teacher-student  conversations (c.f. [13, 42]).  Following BROMP, up-to-date seating charts were elicited from a  teacher prior to each class session, both to enable coding of  student-teacher interactions, and for use as classroom maps during  replay analysis [42]. Field observers recorded instances in which  students raised or lowered their hands, and coded teacher behavior  with reference to 6 broad categories:    1. On-task conversation: The teacher is engaged in a  discussion with a student about the activity she/he is  currently working on   2. Off-task conversation: The teacher is engaged in an  unrelated discussion with the student.   3. Talking to class: The teacher is addressing the entire  class (e.g., giving a mini-lecture based on observations  made during a lab session)   4. Monitoring: The teacher is watching the class from a  fixed location (e.g., the teachers desk), or standing  behind a student and scanning that students computer  screen over her/his shoulder (disambiguated by the  teachers current location, as described below)   5. Outside the room: the teacher is not in the classroom  6. Inactive: the teacher is in the classroom, but engaged in   an activity other than one of the above (e.g., grading  papers or checking email)   Within each of the broad behavior categories above, the position  of the teacher in the classroom was recorded if the behavior  persisted for at least two seconds. The teachers position was  coded either as the name of a student the teacher was standing  behind (if the teacher was directly monitoring that student, or  engaged in an on-task conversation), or a description of another  location in the classroom, such as the teachers desk. These field  observations were then synchronized offline, using SPACLE, with  the DataShop log data generated by Lynnette.                                                                        3 Available at: https://bitbucket.org/dadamson/lookwhostalking       Figure 2. A spectrum of methods for understanding student   learning in classrooms using educational technologies.        4.2 Analyses and Results  4.2.1 Pre-post analysis  A students prior knowledge of equation solving (as measured by  the pretest) was a strong predictor of their posttest score (r = 0.79,  p < .001). Students went from an average of 43% on the pretest to  52% on the posttest  a significant improvement (F(1, 133) =  17.66, p < .001).    4.2.2 Replay Analysis  On average, teachers spent roughly 47% of their time either  inactive or outside of the room. The proportions of time teachers  were observed engaging in each of the other coded activities,  within the remainder of the time, are reported in Table 1.      Table 1. Frequency of coded teacher and student behaviors  during teachers active time. Top row: average percentage of   teachers active time that was spent engaged in each of the  coded behavior categories. Bottom row: average percentage of   students for which a category was observed at least once.         Teacher- student:  On-task  conversa  tion   Teacher- student:  Off-task  conversa  tion   Teacher:  Talking  to class   Teacher:  Monitori  ng   Student:  Hand- raising   Teacher  time 33% 19% 4% 44% n/a   % of  students 28% 7% n/a 34% 26%      In examining replays of a small number of class sessions, we  observed a number of unexpected patterns  often re-running the  replay with different combinations of analytics in order to explore  particular questions more deeply. Almost immediately, we noticed  that the teachers in our study tended to actively monitor their  students in concentrated bursts, interleaved with (often lengthy)  idle periods in which the teacher might either monitor the whole  class from a fixed position in the room, or attend to an unrelated  activity. During periods in which teachers were walking around  the classroom, they occasionally provided students with  apparently unsolicited feedback (i.e. feedback that was not  preceded by the student raising her/his hand) based on their  observations while watching a students computer monitor over  her/his shoulder.   In these replays, teachers appeared to selectively monitor certain  students while consistently passing others by. In interviews with  some of these teachers, they noted that they monitor their students  strategically during computer lab sessions, relying on prior  knowledge about their students abilities and behavioral  tendencies. In particular, two of the teachers we interviewed  emphasized that they tend to focus on monitoring students who  they expect are more likely to be off-task (e.g. browsing external  websites instead of working with the software). However, replays  displaying the amount of time each student spent inactive in the  software suggested that teachers tended to neglect certain regions  of the classroom, and overlooked students who truly tend towards  greater time off-task.    For example, Figure 3 shows a group of students, on the right side  of the classroom, who spent a large amount of time inactive over  the course of a class session. Yet the teacher spent very little time   in this region of the room, and almost no time directly monitoring  any of these students screens. This may be viewed as evidence  that teachers intuitions are limited when it comes to judging  which students are more likely to engage in off-task behavior. It is  also possible that students sitting in regions of the room where a  teacher is more active are more likely to remain on-task. Indeed,  our replay analyses lend some support to this interpretation, as  students frequently appeared to go off-task when the teacher  moved to another region of the classroom, but then resumed  working with the software once the teacher started moving in their  general direction. And many students appeared to go off-task  during periods in which the teacher was either inactive or outside  of the room (see Figure 1).    A major takeaway from these replay analyses was that we might  have previously underestimated the importance of spatial factors  in the classroom when analyzing ITS log data. Although our  original goal in collecting classroom observation data was to  investigate the effects and predictors of teachers helping  behaviors in the classroom, replay analyses revealed that teachers  proximity seemed to have much more salient effects on student  learning and behavior. A teachers location in the classroom  appeared to be related to whether or not particular students chose  to be on-task, and the activity of students sitting next to one  another often appeared to be temporally synchronized (similar to  the distraction ripples observed by Raca et al. [21]).  Furthermore, when the teacher was either distracted or outside of  the classroom, many students appeared to stop working with the   Figure 3. A time-lapse image of a SPACLE replay,  summarizing a 60-minute class session. In this replay, the   amount of time each student spent inactive during the entire  session is displayed  ranging from black (less time) to   bright green (more time). Student names are obscured in  this image. Brighter yellow student circles indicate more   frequent hand raising, and more faded colors of the teacher  circle indicate less time spent with a particular student.        software entirely. And students willingness to raise their hands  (as well as their likelihood of receiving help from the teacher as a  result) appeared to increase during time intervals in which the  teacher was nearby.   4.2.3 Relationships between student-teacher  interactions and student learning outcomes  After using replay analysis to gain a richer qualitative picture of  what went on in a small set of class sessions, we conducted  quantitative analyses on the synchronized logs generated by  SPACLE in order to investigate the robustness of some of the  patterns we observed. Since we are ultimately interested in  students learning outcomes, we began by examining relationships  between frequencies of various student-teacher interactions  (evaluated per-student) and students pre-post gains.    As shown in Table 2, neither a students frequency of on-task  conversations with the teacher nor their frequency of requesting  help (via hand-raising) were significantly correlated with their  performance at posttest, even when controlling for the students  pretest score. Interestingly, the frequency with which a teacher  directly monitored a student was the only measured aspect of  students and teachers interactions in the classroom that  correlated significantly with posttest, and the relationship with  direct monitoring remains significant even when controlling for  pretest.     Table 2. Zero-order and partial correlations (controlling for  pretest) between student-teacher interactions and posttest   scores.   p < 0.05, ** p < 0.01, *** p < 0.001     On-task  conversation   Off-task  conversation   Direct  monitoring   Hand  raising   Zero-order  correlation   0.00     0.13 0.39***     -0.02     Partial  correlation   -0.08 -0.14 0.20* -0.08      In order to better understand the mechanisms by which this  apparent link might arise, we adopt a causal model search  approach, using directed acyclic graphs (DAGs) to represent the  qualitative causal structure among measured variables. We used  the PC algorithm in the Tetrad V program4 to search for an  equivalence class of graphs that are consistent with a set of  conditional independence constraints [22]. We included  background knowledge about our experimental design as a search  constraint: namely, that the pretest precedes all process variables,  which in turn are all prior to the posttest. The PC algorithm is  asymptotically reliable, and its primary limitations lie in its  assumptions that the underlying causal dependencies between  variables can be modeled with linear functions, and that there are  no unmeasured common causes among variables.    To relax the second of these assumptions, we also used the FCI  algorithm to learn an equivalence class of graphs, represented by  partial ancestral graphs (PAGs). PAGs are representationally                                                                       4 Available at http://www.phil.cmu.edu/projects/tetrad/   richer than DAGs, and may contain edges representing  uncertainty over the nature of pairwise relationships between  variables [22]:    X  Y: X causes Y in every member of the equivalence  class represented by this PAG.    X  Y: X and Y share a latent common cause in every  member of the equivalence class represented by this  PAG.    X o Y: Either X causes Y, X and Y share a common  cause, or both.    X oo Y: X is a cause of Y or Y is a cause of X.  Alternatively, X and Y may share a latent common  cause (either in the absence of a direct causal link  between the two variables, or in addition to one).   Figure 4 shows the model found by PC, with path coefficient  estimates included. The model fits the data well (2 = 6.03, df =  10, p = .81)5, and contains a number of properties that are  consistent with findings in prior literature on the effects of student  help-seeking behaviors on learning gains with ITSs. For example,  under this model increased use of the ITSs hint functionality  appears to inhibit learning, overall [6]. Also, compatible with  previous findings that on-task conversations with peers and  teachers during ITS use may be negatively related to student  learning overall, we find that on-task conversations with teachers  appear to increase students error rates within the software [13].  However, we did not replicate Baker et al.s finding of a negative  relationship between on-task conversations and learning gains,  instead finding no relationship  (perhaps owing, in part, to  differences in the quality and effects of peer help and teacher  help). Note that the observation of a negative relationship between  on-task conversations and student error rates, and the absence of  an observed relationship with learning gains may be, at least in  part, due to a selection effect. Students who have more on-task  conversations with the teacher are likely those who are having  more difficulties in the software (for reasons that may not be  captured by their performance on the pretest alone), and who are  in turn likely to learn less [6, 13]. In addition, it is possible that a  finer-grained coding of the nature or content of these on-task  conversations would have revealed particular circumstances under  which such conversations produce a measurable increase or  decrease in student learning, as measured by posttest.    The observed positive relationship between the frequency of  direct monitoring by the teacher and student posttest scores  appears to have been mediated, in part, by students hint-use  behavior. One possibility this suggests  made more plausible by  our observations during replay analyses  is that students who are  more aware that the teacher is monitoring them are less likely to  engage in maladaptive learning behaviors such as abusing  software-provided hints, and are therefore more likely to learn the  material. It is also possible, however, that the apparent link  between teachers direct monitoring and student learning gains                                                                       5 Note that in path analysis, the null hypothesis is that the  estimated model is the true model, and the p-value represents the  probability that a difference between the estimated and the  observed covariance matrices at least as large as the realized  difference would have been observed under the null hypothesis.  As such, a p-value above a specified threshold (conventionally  alpha = .05) implies that the model cannot be rejected.        reflects a selection effect. For example, teachers may tend to more  frequently monitor students who show signs of making progress  in the software (or who the teacher believes are more likely to  make progress). Interestingly, this model suggests that students  with higher pretest scores may have been somewhat more likely to  receive additional monitoring from the teacher. In a follow-up  interview, one of the teachers in our study claimed to have  intentionally placed a group of students in a relatively isolated and  inaccessible area of the classroom, as these students were a pain   to deal with  hinting at possible mechanisms by which this  apparent bias could have arisen.   Stang et al. recently found similar results at the university level  [18]. In their study of interactions between teaching assistants  (TAs) and students in the hands-on laboratory sections of large  introductory physics courses, these authors found that the  frequency of TA-student interactions was a strong and positive  predictor of student engagement (defined as on-task behavior),  which was in turn a stronger predictor of student learning gains  than their pretest scores. Compatible with our findings, the  authors found that this relationship held for interactions that were  initiated by TAs, but not for those initiated by students. In  addition, very brief visits by the TA appeared to be just as  effective as lengthy interactions. The authors posited that this  might be due either to a policing effect (i.e., frequent  interactions motivate students to not stray off-task), or a  ventilation effect (i.e., TA-initiated visits open the door for  productive conversations with students). To gain a sense of the  relative plausibility of these two explanations in our own dataset,  we ran follow-up replay analyses with SPACLE, across two  teachers and class sessions -- visualizing the rate of student hint  requests on each students computer screen by displaying a  flash of color each time a student asked for a hint. These replays  suggested that students might have been less likely to request  hints when the teacher was nearby. In addition, students who were  observed asking for multiple hints in rapid succession appeared to  stop (or at least, pause) this behavior when the teacher was nearby  or directly monitoring them  lending some support to Stang et  al.s policing hypothesis, while also remaining compatible with  their ventilation hypothesis.   Given the potential for confounding factors, we used the FCI  algorithm to learn a PAG causal model, relaxing the assumption  of no unmeasured common causes (see Figure 5). The learned  structure is largely the same, except that this model encodes the  possibility that pretest may be related to direct monitoring, off- task conversation, hint use, and/or error rate by a common  unmeasured cause, and that the same may be true for the  relationships between direct monitoring and hint use, and on-task  conversation and its children (hand raises and error rate). In  addition, the learned structure suggests that students frequency of  hand-raising shares common unmeasured causes with their  frequency of off-task conversation and their within-software error  rates (which in turn may share a common cause with students  rate of hint-use)  perhaps indicating that these behaviors are  symptoms of unmeasured cognitive, motivational, and affective  states such as confusion and frustration [34]. However, the  positive link between direct monitoring and student learning gains  remains in every member of the equivalence class found by FCI.   5. DISCUSSION AND FUTURE WORK  We have introduced SPACLE, a prototype tool that facilitates  exploratory, retrospective analyses of learning-related interactions  that may cross over between virtual and physical spaces. In  addition, we have demonstrated how SPACLE can support  hypothesis generation, by using replay analysis to inform our own  investigations into the effects of teacher-student interactions on K- 12 students learning with intelligent tutoring systems. We used  SPACLE replays to inform quantitative log analyses in two ways:  first as a means to quickly explore multimodal classroom datasets  and identify important classroom behaviors that likely have an  impact on learning, and second to continuously evaluate the  relative plausibility of various, alternative hypotheses that were  consistent with the results of our quantitative analyses.     Figure 4. The model found by PC, with parameter estimates  included. This model fits the data well: 2 = 11.31, df = 12, p   = .50.     Figure 5. The PAG equivalence class found by FCI, which   encodes the possibility of unmeasured common causes.        Furthermore, through a combination of causal modeling and  replay analysis with SPACLE, we have presented some  convergent evidence for positive effects of teachers monitoring  behaviors on student learning in classrooms using ITSs.  Specifically, our findings from causal modeling suggest that  students who receive more frequent monitoring from teachers in  ITS classrooms may learn more, and that this effect may be  partially mediated by students hint-use behavior within the  software. Our use of SPACLE replays on a small subset of our  data throughout the analysis process enabled us to evaluate the  relative plausibility of various hypotheses that were compatible  with these causal models. Students who are monitored by their  teachers more frequently tend to engage less often in gaming the  system behaviors such as hint abuse, and may also be less likely  to go off-task.   These findings extend those of Stang et al. [13] by suggesting that  more frequent visits from a teacher may promote engagement and  learning not only at the university level, but also among  considerably younger students (7th-8th grade). Our findings also  lend support to the authors prediction that their observed  relationship between teacher visits and student engagement would  generalize beyond their studys setting (inquiry-based laboratory  sessions in an introductory physics course). In addition, our  findings may help interpret Stang et al.s observation that a  teachers frequency of interaction with a student predicts student  engagement, independent of the length of these interactions. Our  findings suggest that teachers interactions may not need to have a  verbal component in order to be effective  that is, K-12 students  mere awareness of being monitored may have a positive impact  on their learning with self-paced systems such as ITSs.    Without using SPACLE for our initial explorations, we likely  would not have turned our attention, in the first place, to studying  potential effects of teachers monitoring behaviors. Rather, we had  originally collected classroom observations on teachers  monitoring behavior in order to study potential teacher blind spots  during blended lab sessions (e.g., failing to notice important  opportunities to help students learn the material, or exhibiting an  unconscious bias towards helping and monitoring certain subsets  of students). Informally, SPACLE replays suggested that teachers  tended to spend a significant amount of time inactive during lab  sessions and often overlooked students who tended to spend more  time off-task. Another one of our initial goals for these analyses  was to model and understand how teachers decide which students  to help, in order to understand how their help giving might be  better allocated. Contrary to our initial expectations, however, the  frequency of teachers verbal interactions with students was not a  significant predictor of student learning, overall, even when  examining on-task conversations only.   These results should not be interpreted as suggesting that on-task  conversations with a teacher cannot be helpful. Indeed, we expect  that there exist many scenarios in which help from a human  teacher is likely to be more effective than the support ITSs can  currently offer. As mentioned, a selection effect may be  responsible, at least in part, for the absence of an observed  relationship between on-task teacher-student conversations and  student learning gains. However, this absence does suggest that  any overall positive effect of such conversations is not strong  enough to offset the selection effect. It may also be that,  consistent with prior work on the effects of student hint-use within  ITSs, on-the-spot support from a human teacher during blended  lab sessions is helpful only under particular circumstances [6]. For  example, it may be that current ITSs are generally more effective  at teaching procedural skills, whereas human teachers can be more   effective at teaching conceptual knowledge [36]. Under this  interpretation, our pre- and posttests may not have been able to  capture the effects of students on-task conversations with their  teachers, since these assessments were primarily designed to test  students procedural knowledge in equation solving. It may also  be that the effectiveness of a particular on-task conversation with  a teacher depends jointly upon student traits (e.g., the students  inclination to self-explain ideas presented by the teacher) and the  type and quality of the help the teacher provides (e.g., completing  a problem for the student as a worked example, versus prompting  the student to work through the problem while verbalizing her/his  thought process).    Although the coding scheme used in the current study was not  fine-grained enough to capture such distinctions, we view the  investigation of circumstances under which help from a human  teacher is more beneficial than help from an ITS (or vice-versa) as  a promising direction for future work. Such research could inform  the design of more synergistic blended curricula  combining the  complementary strengths of both human teachers and ITSs. It  could also inform the design of learning analytics tools to help  teachers more effectively support their students while they work  with learning technologies such as ITSs in the classroom.   Several limitations of this work should be mentioned. The causal  models shown in Figures 4 and 5 should not be viewed as the  true models. First, although our data are from an experimental  study, the data reported in this paper are from a portion of the  study in which we did not directly intervene on any of the  measured variables between pre- and posttest (except insofar as  running an in-vivo classroom study can be considered an  intervention in itself). As such, our data should be considered  observational, and future experimental investigation is required to  evaluate the causal nature of each link identified in our causal  models. Second, by no means can we rely on the assumption  made by our search algorithms, that the underlying relationships  between our modeled variables are truly linear. Nonetheless, this  model assumption is not unreasonable, as the relationships in the  data we modeled appear approximately linear. Third, although our  sample of 135 students is relatively large compared to many ITS  studies, the reliability of our model search algorithms would be  improved with access to larger samples, and it is generally  impossible to compute confidence bounds when dealing with  finite samples [45]. Fourth, although SPACLE allows us to  quickly run exploratory replay analyses that capture  spatiotemporal factors, time-series analyses could enable deeper  analysis of individual links in our causal models by leveraging  temporal precedence as a cue to causality (i.e., a scalable  formalization of part of what human researchers do when  observing SPACLE replays [35]). In our future work, we plan to  apply algorithms for causal modeling from time-series data.   It is also worth noting that we observed teachers over a relatively  brief period (60 minutes) in this study, as we were interested in  investigating student-teacher interactions on a small scale. It  would be interesting to observe ITS classrooms over longer time  periods, in order to study how teacher practices (and perhaps also  their effects on student learning) may evolve over time. And  finally, in this study a single human observer manually collected  classroom observations of teacher-student interactions. This  required us to use a very coarse-grained coding scheme, and also  limited the number of classrooms we could feasibly observe. In  future work, we will automate parts of the classroom data  collection process (building on recent work by Prieto et al. [32]),  using a combination of low-cost sensors and manual observations.  A semi-automated approach may enable more detailed coding     schemes by freeing human observers to focus on recording  higher-level observations (e.g. semantic features of on-task  conversations in the classroom).    In addition to using SPACLE for exploratory data analysis, we  have also begun exploring the use of SPACLE as a design tool.  First, weve begun using SPACLE in our own design work, to  support the iterative design and prototyping of analytics for use in  real-time teacher dashboards. SPACLE allows designers to  experiment with alternative analytics (e.g. different alert  thresholds for behavior detectors, or different measures of the  same psychological construct) and examine the consequences of  particular choices in a tangible way. Second, we have begun  exploring the use of SPACLE as a means to investigate the  distance between teachers actual behavior in the classroom and  their recollections of their activities. For example, after observing  class sessions, we have asked teachers to walk us through their  activities while student were working on computers, drawing their  physical paths over top maps of their classrooms in the process. In  comparing teachers recollections with SPACLE replays, weve  observed that the replay often reveals a much lower amount of  teacher activity (often fairly concentrated in particular regions of  the classroom) than teachers recollections would suggest. In the  future, we would like to explore the use of SPACLE-generated  replays as after action reviews for teachers, to encourage them  to reflect on their own activity patterns in the classroom.  However, in order for such reflection tools to be used in teachers  daily practice, outside of exploratory design studies, the collection  of classroom observations would need to be heavily automated.   In conclusion, our results have implications for the learning  analytics, educational data mining, and intelligent tutoring  systems communities. Using replay analysis with SPACLE, we  generated a number of questions about the nature and effects of  teachers on-the-spot decision-making during blended lab  sessions. Through both replay analyses and causal modeling, we  observed rich relationships between students out-of-software  interactions in blended lab sessions using ITSs and their within- software learning and behavior. Some of the most salient observed  effects involved no verbal interactions between students and their  teachers, but rather appeared to be due to spatial factors (e.g. the  teachers position in the room, relative to a student) and perhaps  classroom layout. We view these observations as suggestive that  the influence of such out-of-software factors on student learning  with ITSs and similar educational technologies has perhaps been  under studied previously. Our finding that the frequency with  which teachers monitor students is predictive of learning gains  may indicate that one possible mechanism by which classroom  monitoring tools such as real-time dashboards might be effective  in promoting student learning is by simply making students aware  that they are being monitored. In our future work, we plan to use  SPACLE replays in conjunction with causal modeling, to  construct models of teacher decision-making [32, 37] and identify  additional links between teacher behavior and student learning.  These models could in turn help inform the design of more  effective learning analytics tools for teachers. Our findings also  suggest that students may systematically take advantage of  affordances offered by the physical classroom (e.g. teachers  limited attention and perceptual abilities) in order to decide  whether and when to go off-task or abuse hints (consistent with  previously reported informal classroom observations [34]). This  hints at the usefulness of a broader notion of gaming the system  than has been used previously  taking into account student  behaviors that extend outside of the software.   6. ACKNOWLEDGMENTS  This work was supported by NSF Award #1530726, and by the  Institute of Education Sciences, U.S. Department of Education,  through Grant R305B150008 to Carnegie Mellon University. The  opinions expressed are those of the authors and do not represent  the views of the Institute or the U.S. Department of Education. In  addition, we thank the DataShop and CTAT/TutorShop teams,  Franceska Xhakaj, Jasper Tom, Ran Liu, Amos Glenn, Mary Beth  Kery, and all participating teachers and students.   7. REFERENCES  [1] Liu, R., Davenport, J., & Stamper, J. (2016). Beyond log   files: Using multi-modal data streams towards data-driven  KC model improvement. In EDM 2016, 436-441.   [2] Baker, R.S.J.d, Gowda, S.M., Wixon, M., Kalka, J., Wagner,  A.Z., Salvi, A., Aleven, V., Kusbit, G., Ocumpaugh, J., &  Rossi, L. (2012). Towards Sensor-Free Affect Detection in  Cognitive Tutor Algebra. In EDM 2012, 126-133.   [3] Miller, W. L., Baker, R. S., Labrum, M. J., Petsche, K., Liu,  Y. H., & Wagner, A. Z. (2016). Automated detection of  proactive remediation by teachers in Reasoning Mind  classrooms. In LAK 2015. ACM.   [4] Ogan, A., Walker, E., Baker, R. S., Rebolledo Mendez, G.,  Jimenez Castro, M., Laurentino, T., & De Carvalho, A.  (2012). Collaboration in cognitive tutor use in Latin  America: Field study and design recommendations. In CHI  2012, 1381-1390. ACM.   [5] Beck, J. E., & Gong, Y. (2013). Wheel-spinning: Students  who fail to master a skill. In AIED 2013, 431-440. Springer  Berlin Heidelberg.   [6] Aleven, V., Roll, I., McLaren, B. M., & Koedinger, K. R.  (2016). Help helps, but only so much: research on help  seeking with intelligent tutoring systems. IJAIED, 26(1),  205-223.   [7] Rau, M., Scheines, R., Aleven, V., & Rummel, N. (2013).  Does representational understanding enhance fluency  or  vice versa Searching for mediation models. In EDM 2013,  161-168.   [8] Schofield, J. W. (1995). Computers and classroom culture.  Cambridge University Press.   [9] Mavrikis, M., Gutierrez-Santos, S., & Poulovassilis, A.  (2016). Design and evaluation of teacher assistance tools for  exploratory learning environments. In LAK 2016, 168-172.  ACM.   [10] VanLehn, K. (2006). The behavior of tutoring systems.  IJAIED, 16(3), 227-265.   [11] San Pedro, M. O. C. Z., Baker, R. S., & Rodrigo, M. M. T.  (2011). Detecting carelessness through contextual estimation  of slip probabilities among students using an intelligent tutor  for mathematics. In AIED 2011, 304-311. Springer Berlin  Heidelberg.   [12] Fancsali, S. (2014). Causal discovery with models: behavior,  affect, and learning in cognitive tutor algebra. In EDM 2014,  28-35.   [13] Baker, R. S., Corbett, A. T., Koedinger, K. R., & Wagner, A.  Z. (2004). Off-task behavior in the cognitive tutor classroom:  when students game the system. In CHI 2004, 383-390.  ACM.     [14] Long, Y., & Aleven, V. (2013). Supporting students self- regulated learning with an open learner model in a linear  equation tutor. In AIED 2013, 219-228. Springer Berlin  Heidelberg.   [15] Dyke, G., Kumar, R., Ai, H., & Ros, C. P. (2012).  Challenging assumptions: Using sliding window  visualizations to reveal time-based irregularities in CSCL  processes. In ICLS 2012, 1, 363-370.   [16] Long, Y., & Aleven, V. (2016). Supporting shared  student/system control over problem selection with an open  learner model in a linear equation tutor. In ITS 2016, 90-100.  Springer International Publishing.   [17] Steenbergen-Hu, S., & Cooper, H. (2013). A meta-analysis  of the effectiveness of intelligent tutoring systems on K12  students mathematical learning. Journal of Educational  Psychology, 105(4), 970-987.   [18] Stang, J. B., & Roll, I. (2014). Interactions between teaching  assistants and students boost engagement in physics  labs. Physical Review Special Topics-Physics Education  Research, 10(2).   [19] Ma, W., Adesope, O. O., Nesbit, J. C., & Liu, Q. (2014).  Intelligent tutoring systems and learning outcomes: A meta- analysis. Journal of Educational Psychology, 106(4), 901- 918.   [20] Kulik, J. A., & Fletcher, J. D. (2015). Effectiveness of  Intelligent Tutoring Systems A Meta-Analytic Review.  Review of Educational Research, 42-78.   [21] Raca, M., & Dillenbourg, P. (2013). System for assessing  classroom attention. In LAK 2013, 265-269. ACM.   [22] Spirtes, P., Glymour, C. N., & Scheines, R. (2000).  Causation, prediction, and search. MIT press.   [23] King, A. (1993). From sage on the stage to guide on the  side. College Teaching, 41(1), 30-35.   [24] Roll, I., & Wylie, R. (2016). Evolution and revolution in  Artificial Intelligence in Education. IJAIED, 26(2), 582-599.   [25] Anderson, J. R., Corbett, A. T., Koedinger, K. R., &  Pelletier, R. (1995). Cognitive tutors: Lessons learned. The  Journal of the Learning Sciences, 4(2), 167-207.   [26] Aleven, V. A., & Koedinger, K. R. (2002). An effective  metacognitive strategy: Learning by doing and explaining  with a computer-based Cognitive Tutor. Cognitive  Science, 26(2), 147-179.   [27] Koedinger, K. R., Aleven, V., Roll, I., & Baker, R. (2009). In  vivo experiments on whether supporting metacognition in  intelligent tutoring systems yields robust learning. Handbook  of Metacognition in Education, 897-964.   [28] Aleven, V., McLaren, B.M., Roll, I., & Koedinger, K.  (2004). Toward tutoring help seeking. In ITS 2004, 227-239.  Springer Berlin Heidelberg.   [29] Pane, J. F., Griffin, B. A., McCaffrey, D. F., & Karam, R.  (2013). Effectiveness of cognitive tutor algebra I at  scale. Educational Evaluation and Policy Analysis, 2013.   [30] Gaevi, D., Dawson, S., & Siemens, G. (2015). Lets not  forget: Learning analytics are about  learning. TechTrends, 59(1), 64-71.   [31] Ritter, S., Yudelson, M., Fancsali, S. E., & Berman, S. R.  (2016). How mastery learning works at scale. In L@S 2016,  71-79. ACM.   [32] Prieto, L. P., Sharma, K., Dillenbourg, P., & Rodriguez  Triana, M. J. (2016). Teaching analytics: Towards automatic  extraction of orchestration graphs using wearable sensors. In  LAK 2016, 148-157. ACM.   [33] Martinez-Maldonado, R., & Martinez, R. (2016). Seeing  learning analytics tools as orchestration technologies:  Towards supporting learning activities across physical and  digital spaces. In 1st International Cross-LAK Workshop at  LAK 16, 70-73.   [34] Baker, R. S. (2011). Gaming the system: a retrospective  look. Philippine Computing Journal, 6(2), 9-13.   [35] Baker, C., Saxe, R., & Tenenbaum, J. B. (2005). Bayesian  models of human action understanding. In NIPS 2005, 99- 106.   [36] VanLehn, K. (2011). The relative effectiveness of human  tutoring, intelligent tutoring systems, and other tutoring  systems. Educational Psychologist, 46(4), 197-221.   [37] Borko, H., Roberts, S. A., & Shavelson, R. (2008). Teachers  decision making: From Alan J. Bishop to today. In Critical  Issues in Mathematics Education, 37-67. Springer US.   [38] Xhakaj, F., Aleven, V., & McLaren, B. M. (2016). How  Teachers Use Data to Help Students Learn: Contextual  Inquiry for the Design of a Dashboard. In EC-TEL 2016,  340-354. Springer International Publishing.   [39] De Vicente, A., & Pain, H. (2002). Informing the detection  of the students motivational state: an empirical study. In ITS  2002, 933-943. Springer International Publishing.   [40] Baker, R. S., Corbett, A. T., & Wagner, A. Z. (2006). Human  classification of low-fidelity replays of student actions.  In Proceedings of the Educational Data Mining Workshop at  ITS 2006, 29-36.   [41] Aleven, V., Xhakaj, F., Holstein, K., McLaren, B.M.  (2016).  Developing a teacher dashboard for use with  intelligent tutoring systems. In 4th International Workshop on  Teaching Analytics at EC-TEL 16, 15-23.   [42] Ocumpaugh, J. (2015). Baker Rodrigo Ocumpaugh  Monitoring Protocol (BROMP) 2.0 Technical and Training  Manual. Technical Report. New York, NY: Teachers  College, Columbia University. Manila, Philippines: Ateneo  Laboratory for the Learning Sciences.   [43] Blikstein, P., & Worsley, M. (2016). Multimodal learning  analytics and education data mining: Using computational  technologies to measure complex learning tasks. Journal of  Learning Analytics, 3(2), 220-238.   [44] Newell, A. (1994). Unified theories of cognition. Harvard  University Press.       [45] Robins, J. M., Scheines, R., Spirtes, P., & Wasserman, L.  (2003). Uniform consistency in causal  inference. Biometrika, 90(3), 491-515.   [46] Rodriguez Triana, M. J., Prieto, L. P., Vozniuk, A., Shirvani  Boroujeni, M., Schwendimann, B. A., Holzer, A. C., &  Gillet, D. (in press). Monitoring, Awareness and Reflection  in Blended Technology Enhanced Learning: a Systematic  Review. In IJTEL.       "}
{"index":{"_id":"44"}}
{"datatype":"inproceedings","key":"Whitelock-Wainwright:2017:SWT:3027385.3027419","author":"Whitelock-Wainwright, Alexander and Gavsevi'c, Dragan and Tejeiro, Ricardo","title":"What Do Students Want?: Towards an Instrument for Students' Evaluation of Quality of Learning Analytics Services","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"368--372","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027419","doi":"10.1145/3027385.3027419","acmid":"3027419","publisher":"ACM","address":"New York, NY, USA","keywords":"action research, learning analytics, service quality","abstract":"Quality assurance in any organization is important for ensuring that service users are satisfied with the service offered. For higher education institutes, the use of service quality measures allows for ideological gaps to be both identified and resolved. The learning analytic community, however, has rarely addressed the concept of service quality. A potential outcome of this is the provision of a learning analytics service that only meets the expectations of certain stakeholders (e.g., managers), whilst overlooking those who are most important (e.g., students). In order to resolve this issue, we outline a framework and our current progress towards developing a scale to assess student expectations and perceptions of learning analytics as a service.","pdf":"1      What do students want Towards an instrument for   students evaluation of quality of learning analytics   services Alexander Whitelock-Wainwright   School of Psychology   The University of Liverpool   Liverpool, UK   A.Wainwright@liverpool.ac.uk   Dragan Gaevi  Moray House School of Education and    School of Informatics   The University of Edinburgh   Edinburgh, UK   dragan.gasevic@ed.ac.uk   Ricardo Tejeiro  School of Psychology   The University of Liverpool   Liverpool, UK   tejeiro@liverpool.ac.uk      ABSTRACT  Quality assurance in any organization is important for ensuring that   service users are satisfied with the service offered. For higher edu-  cation institutes, the use of service quality measures allows for   ideological gaps to be both identified and resolved. The learning   analytic community, however, has rarely addressed the concept of   service quality. A potential outcome of this is the provision of a   learning analytics service that only meets the expectations of cer-  tain stakeholders (e.g., managers), whilst overlooking those who   are most important (e.g., students). In order to resolve this issue,   we outline a framework and our current progress towards develop-  ing a scale to assess student expectations and perceptions of   learning analytics as a service.   CCS Concepts    Human-centered computing  Accessibility design and   evaluation methods    Keywords   Service quality; action research; learning analytics   1. INTRODUCTION  In recent years, Learning Analytics (LA) movements within the   domain of higher education have been growing. As a result, stu-  dents are becoming immersed in a new culture where greater   feedback and insight into their learning are readily available. From   small-scale LA implementations designed to monitor fluctuations   in emotions [29], to large-scale LA initiatives at institutions such   as the Open University UK that aim to improve retention rates [5],   there is an underlying commitment to optimize learning and the   environments in which it occurs [1]. In other words, LA can be   conceptualized as a service that aims to provide students with edu-  cational support during their learning. Thus, by thinking of LA in   this way, it thereby creates a commitment for higher education   institutes (HEIs), as service providers, to meet student   expectations.   Up until now, however, any research concerned with student   expectations has focused mainly upon ethical and legal issues [8,   12, 14, 28, 31, 40]. Nevertheless, these authors have recognized   important issues that can facilitate the development of an evalua-  tion framework for future LA services. On the other hand, a   students expectation of a LA service will not simply relate to   whether ethical practice is followed. Rather, ethical and privacy   issues will only reflect a handful of expectations, in what could be   considered a plethora of expectations that students will hold to-  wards LA as a service. The latter is exemplified by [40], who stress   the challenge of creating LA outputs that are meaningful to the   student population. If an issue such as the latter arose in a large-  scale implementation of LA, how would this be fed back so that the   system could be redesigned to meet students expectations of and   improve their experience with LA Without a resolution, this   missing link between the running of a LA service and the   incorporation of student feedback could jeopardize what could   effectively be quality analytics.   It would be an oversight to suggest that evaluation procedures   have not received sufficient attention in LA research [3, 27].   Existing studies have sought to acquire feedback on LA tools that   have been introduced. However, for a continuous improvement of   institutional LA services, a broader approach needs to be   undertaken that explores student expectations and experiences with   the LA services on offer. In so doing, discrepancies in student   expectations and actual service provision can be readily   acknowledged and solutions introduced, which should lead to an   improved quality of service for students.   In this short paper, we intend to outline our on-going work   towards the development of a scale that aims to assess students   expectations and perceptions of LA services. Section one briefly   discusses the topic of expectations and service quality. Section two   focuses on the current gap within the LA literature of only a limited   amount of attention being paid towards service evaluation. In sec-  tion three, we introduce our hypothesized model of how service   quality can be included within the continual evolution and devel-  opment of LA systems. Finally, section four provides readers with   a summary of our progress towards creating a LA service quality   scale.   2. SERVICE QUALITY  Judgments of service quality are believed to be based on a users   subjective assessment of the extent to which their needs or expecta-  tions were met [24, 41]. As a result, organizations become reliant   upon providing a good quality of service, as it can be the pivotal   factor towards enticing service users to utilize their service over   and above those offered by competitors [26]. Furthermore, these   Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights   for components of this work owned by others than ACM must be   honored. Abstracting with credit is permitted. To copy otherwise, or   republish, to post on servers or to redistribute to lists, requires prior   specific permission and/or a fee. Request permissions from   Permissions@acm.org.   LAK '17, March 13-17, 2017, Vancouver, BC, Canada     2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027419        2      user evaluations of service quality are not only constrained to face-  to-face organizational settings, but extend to online service provi-  sion [25, 41], and even the online or offline services offered by   HEIs [2, 16].   Student evaluations of teaching and general student experi-  ence surveys are commonly used as approaches to collecting   students feedback on quality of service in higher education [33].   Although such surveys are commonly tailored for individual insti-  tutions, in some countries there are national initiatives. For   example, in the UK, the National Student Survey (NSS) has be-  come an important measure of service quality that allows final year   undergraduate students to provide feedback as to their overall satis-  faction with a course [13]. In conjunction with helping prospective   students make informed decisions about where they choose to   study, these results also assist HEIs in bringing about positive   changes. Thus, HEIs are thereby engaging in the process of quality   assurance to make sure that standards in their provision of educa-  tion are continually met.   The process of exploring students expectations and percep-  tions of HEIs services is important. Without such procedures in   place, there is a possibility that an ideological gap could persist   [18]. This particular gap can be thought as a clear separation be-  tween student expectations of the universitys service and what the   HEI believe the service they are providing should be [2, 19]. With-  out identification or resolution of such gaps, it can be readily   assumed that dissatisfaction with the provided service will entail.    Going beyond using only institutional and national surveys to   understand student opinions of the service delivered by a HEI,   researchers have sought to explore potential discrepancies between   stakeholders by using alternative measures. A popular approach   taken has been the use of SERVQUAL [26], which measures ser-  vice quality across five dimensions of Tangibles, Reliability,   Responsiveness, Assurance, and Empathy. This scale can, depend-  ing upon the time between assessments [20], explore expectations   and perceptions of a service [42]. Perceived quality is then assumed   to be the difference between expectation and perception scores   [26]. Alternatively, the perception-only scale of SERVPERF has   shown to capture more of the variance in service quality, whilst   also supporting the view of service quality as being an attitude [9].   Commonly, in practice, organizations apply the expectation discon-  firmation theory (EDT) to assess user satisfaction [21, 38], through   the use of SERVQUAL. In other words, a users level and direction   of satisfaction with a service is based upon whether the perfor-  mance aligned with their initial expectations or not [22].    When applied in HEI settings, both the aforementioned varia-  tions in scales (i.e., expectations and perceptions together, or   perceptions alone) have shown utility in measuring service quality   [36, 42].  As [36] found SERVQUAL enabled them to go beyond   simply identifying issues with the syllabus. Rather, this scale al-  lowed the authors to emphasize the importance of abstract features   in education that often go overlooked (e.g., teaching staff being   more responsive to the need of students). Thus, stakeholders within   HEIs can utilize the findings from such measures to effectively   identify service issues that may remain unnoticed, and introduce   strategies designed to provide resolutions to any gaps identified.   Therefore, it can be expected that by improving the services of-  fered, HEIs could improve overall student satisfaction. Conversely,   without implementing the practice of assuring quality in a service,   dissatisfaction in the student population may ensue.    In the case of LA, it too can be regarded as a service provided   by a HEI, as the underlying foundation of the field is to support   students during their learning. As a result of it being conceptualized   in this way, LA as a service should then be subject to quality assur-  ance measures. Otherwise, it is plausible to assume that without a   continual evaluation of the LA services that are in place, problem-  atic features may endure without resolution. This will certainly   result in HEIs ignoring ideological gaps that would effectively   jeopardize the quality of the LA service they are providing.   3. SERVICE QUALITY AND LEARNING  ANALYTICS  Expectations can be defined in terms of the users pre-trial beliefs   about a service [23]. In the context of LA, those who use the ser-  vice can be considered as part of the following stakeholders groups:   learners, teachers, managers, and policymakers [7]. Each of these   respective stakeholder groups will have different expectations of   LA as a service. For example, a teacher may expect to be provided   with real-time updates on how their students are performing in a   course. Whereas, a manager would expect feedback on how a hand-  ful of modules are running. A potential outcome of this variability   in needs across stakeholders is a LA service that mainly satisfies   the expectations of one group above the rest.   Perceptions, in contrast, are defined as the users judgement of   how the service performed in reference to their prior expectations   [26]. This post-usage comparison reflects what is known as discon-  firmation, where the user determines whether performance   exceeded, met, or fell below what was expected [21]. The outcome   of a user assessing whether performance aligns with expectations   or not can subsequently determine their level of satisfaction with   the service provided [21]. This conceptualization of service quality   through the perspective of EDT has been important in health care   settings. For example, [4] assessed whether patients expectations   of health care had been met. Through the use of a pre-visit ques-  tionnaire composed of expectation items, and a post-visit   questionnaire made up of perception items, this enabled researchers   to identify discrepancies in the health service provided. With these   findings, organizations are able to assess the quality of service and   gain valuable information as to how they can make improvements.   The LA community does consider the student population to be   the most important stakeholders in any form of LA service [10].   Research efforts, as well, have kept students in mind, from design-  ing visualization to monitor progress [15, 27], to creating software   aimed at regulating metacognitive abilities [39]. In spite of how   beneficial this research will be for students, LA has seemingly   overlooked the importance of student expectations and perceptions   of the service provided. Instead, LA services have seemingly been   implemented in a top-down fashion directed by the beliefs of re-  searchers, managers, and policymakers without much consideration   of what students expect from such a service. Thus, LA is not facili-  tating the evolution and development of the services it could offer   to students, as evaluation processes are rarely being implemented.    It would, however, be incorrect to suggest that evaluations of   LA tools in general have been overlooked, as [30] developed a   framework of quality indicators by working with experts in the   field. Although, the limitation here is that these indicators are help-  ing to establish a standard by which LA tools should be measured,   as opposed to investigating what students expect from a service. In   the same way, when various research efforts have incorporated   evaluation procedures, these have been directed towards an as-  sessment of a tools utility and value [3, 27]. Therefore, to some   extent the LA community has not ignored the importance that eval-  uative processes can have in the design and implementation of LA     3      tools and services. On the other hand, these are not addressing how   students perceived the quality of the LA service provided to be.    As it stands, we posit that a movement towards developing   and understanding how LA services can be regulated in a quality   assurance framework should be supported. Irrespective of size   (e.g., university-wide LA service, or a LA service confined to one   module/course/degree programme), a LA service cannot be naively   assumed as being unproblematic. Inevitably, there will be gaps   between what students expect from and perceive the service to be.   It then becomes the responsibility for the LA community to intro-  duce measures that will identify such issues and suggest resolutions   that will facilitate re-developments of a LA service that is consid-  ered high in quality. Without the adoption of this approach, it can   be assumed that students could become dissatisfied with the service   as their needs are not met, and inevitably decrease the usage of LA   output in their learning.   4. CONCEPTUAL MODEL   To address the issues discussed in the previous section, we propose   an evaluation framework that can allow for student perceptions to   feedback into the continuous evolution and development of LA as a   service. To achieve this objective of assimilating evaluations of   service quality into the development and implementation of LA   service, we posit that an action research approach should be under-  taken. This was decided upon as the evaluative process are carried   out by those involved in the LA service (e.g., teaching staff, man-  agers, etc.) with an aim to improve the service offered [6]. In   addition, the process should be cyclical in nature, going from plan-  ning, acting, observing, and reflecting [6]. Put differently, we view   the practice of assuring quality in LA services as an enquiry that   encourages LA practitioners to engage in a process of investigating   and evaluating their work [17].      To illustrate this approach in practice, take a LA service of-  fered to students that provides real-time updates about their studies   through a dashboard. Those LA practitioners who designed and   setup this service may hold a preconceived belief that this   dashboard would address all students needs and help improve   academic performance. Log data can be used by the LA practition-  ers to investigate whether students are making use out of this   particular tool. However, this approach naively assumes that the   LA service is catering to the needs and expectations of the students.   When, in actuality, students may be expecting less frequent up-  dates as to prevent themselves from being overloaded by   information. Thus, this method of introducing a LA service from a   top-down perspective can effectively create an ideological gap. As   practitioners are holding beliefs about what they think a service   should be, without acknowledging what students expect the service   to be like. This both perpetuates the ideological gap and creates a   risk that students could become dissatisfied with the LA service.   Under our proposed action research approach, the practition-  ers would follow the procedure of planning and implementing the   LA service. Following this, there is a need to observe of how the   service is running, which can be accomplished through the use of a   service quality self-report measure. Theoretically, this should allow   the LA practitioners to gain an insight into student views about a   service en masse. As up to now, evaluations taken from small   groups of students are not going to reflect the divergent opinion   towards a service held by the general population of students. These   results can then be fed back to the practitioners, who can then re-  flect and decide how the LA service can be altered to meet the   expectations of the student population. This then brings the process   back round to start the cycle again.   As previously mentioned, there are scales available that can   measure service quality [2, 26]. In the case of SERVQUAL, the   wording can be adapted to fit a particular environment. However,   as LA is a relatively new field, and there has been no research to   explore student expectations of LA as a service, we first explored   how issues discussed in the literature relate can relate to service   quality. On completion of this step, we felt it was more appropriate   to create a new scale that could measure service quality in LA.   5. SCALE DEVELOPMENT  For the application of an action research approach aimed at   assuring quality in LA services, a scale is required that can measure   service quality. Researchers have incorporated evaluation processes   into their methodologies [3, 27], but these were limited to small   groups testing out new LA tools. What is required is a scale that   can assess student expectations and perceptions of LA as a service   offered by an institution. In doing so, it can provide the foundation   for HEIs adoption of LA to be subject to regulatory measures.   The initial step taken to create a new scale that would meas-  ure LA service quality was to conduct a review of the LA literature.   Attention was paid towards articles discussing ethical and legal   issues in the field of LA [14, 31, 40], or frameworks on how to   assimilate LA into HEIs [32, 38].    A number of themes emerged from this review that we believe   would be important for measuring service quality. These themes   can be grouped as followed: Ethics and Privacy, Meaningfulness,   Agency, and Intervention. Ethics and Privacy expectations cover a   multitude of topics that have been continually debated within LA,   such as reassuring students that their data is kept securely. Mean-  ingfulness expectations center on the need for LA feedback to be   both relatable and clear so it can be effectively incorporated into   students learning. Agency expectations are the degree to which LA   should be student-centered, so that students themselves decide how   to interpret and use any feedback provided. Finally, Intervention   expectations are concerned with what students expect LA interven-  tions to be aimed at (e.g., the development of academic skills, or   emotional support).   The design of the items for this scale was motivated by the   EDT model proposed by [21]. As the objective of this scale was to   measure disconfirmation between expectations and perceptions, the   questions were phrased to reflect these. It is important to note,   however, that we have only considered expectations as reflecting   what a user wants from a service (e.g., the level of service I expect   to happen in reality). The issue here, according to [37], is that this   conceptualization of expectation creates a situation whereby user   satisfaction would result from a poor service if they expected this   to occur. To compensate for this problem, the authors deconstruct-  ed expectations into predictive (i.e., a users pre-usage belief of   what they anticipated the service would achieve) and desired (i.e., a   users pre-usage belief of what they wanted in a service; e.g., I   hope for this ideally). In doing this, they were able to extend the   EDT by showing that satisfaction was the result of desired expecta-  tions being met. Feelings of indifference, however, were caused by   predictive expectations being met; whereas, dissatisfaction occurs   under circumstances when these predictive expectations are not   met. This approach of using predicted and desired expectations has   been used by [4] in the context of health care settings. They found   that this deconstruction of expectations offered more explanatory   power than using one type of expectation alone. With this in mind,   we believed using desired and predictive expectations would be   more beneficial for our LA service quality scale.   Taking into account the abovementioned findings and identi-    4      fied themes, we developed a scale containing 79 items. These items   were broken down into two subscales (e.g., predictive and desire   expectations). This questionnaire was then subject to peer review   by two LA experts. The opinion of these experts centered on mak-  ing the questions more focused and reducing the number of items   as there were instances of overlapping topics. These comments into   consideration, we refined the number of items in the questionnaire   to 37 (Appendix), each of which contains two subscales (e.g., pre-  dictive and desire expectations).    In the next methodological step, we will be running a pilot   study on a small group of students. This pilot will allow for stu-  dents to provide feedback about the questionnaire to identify any   issues with it (e.g., the clarity of wording used). Next, the pilot   questionnaire will be distributed to a larger sample. The collected   results will then be subject to reliability analysis, with items being   removed if an improvement in coefficient alpha is possible. A fac-  tor analysis will then be run on the remaining items to extract the   underlying factor structure. This should then leave us with a final   questionnaire, which will be distributed across various universities   so we can explore student expectations and perceptions of LA as a   service.   The overarching aim of this research is to develop a frame-  work by which students perspective of service quality and   satisfaction in LA services can be assessed. Various EDT meas-  urement methods have been proposed to calculate quality of a   service [11]. The most prominent of which has been exemplified by   SERVQUAL, where perceived quality is the difference between the   perception and expectation ratings of each item [26]. As an ap-  proach, however, it is problematic due to not measuring   disconfirmation directly [35]. Thus, a decision was made to adopt   the additive difference model as used by [34]. The first step in this   method is to calculate the average score across the items of three   scales (i.e., predictive and desired expectations, and perceived per-  formance; Appendix). Next, participants answer questions   measuring desires congruency (e.g., difference between what I de-  sired and what I received) and expectations congruency (e.g., how   good or bad is this difference). For each item on this scale,   individuals are asked to make a subjective assessment of whether   the performance aligned with the desired/predictive expectation,   and if this difference was good or bad. The scores on these two   respective scales for the desired/predictive expectation items are   multiplied for each item and then averaged, which provides average   congruency scores (i.e., desires congruency and expectations dis-  confirmation). As in [34] research, we will also introduce items   relating to satisfaction (e.g., overall, how do you feel about the   learning analytic services you received), and overall service quali-  ty (e.g., overall, what is the level of service quality you received   from learning analytics services). A decision to incorporate these   items were based upon the findings that desires are predictive of   satisfaction, whilst predictive expectations indirectly affect   judgements of service quality through perceived performance [34].   Thus, the abovementioned points provides a foundation to develop   a model that will enable practitioners to understand how expecta-  tions and performance affect students satisfaction with LA   services and their judgements of overall LA service quality.   6. CONCLUSION  LA is a valuable service in education, from being a tool to improve   a HEIs retention rate, to guiding students down the optimum learn-  ing pathway [32]. An examination of the literature, however, does   show an important gap within the field of LA itself, which is to   develop measures of quality assurance. As with any other service,   there is a need to meet the needs and expectations of users. If these   are regularly overlooked, it may perpetuate ideological gaps where   the service providers are promoting their expectations over and   above their users needs.   As HEIs are increasing their use of LA services, it becomes   imperative for practitioners to start considering how quality assur-  ance can be guaranteed. To resolve this issue, we have suggested   the use of an action research framework that allows for a continual   evaluation of the LA service in place. In addition, we outline our   current progression towards developing a scale that will measure   the expectations and perceptions of LA as a service. It is intended   that this will deter complacency and stress the importance of   engaging in continual evaluations and re-developments of the LA   tools and services in place.   7. APPENDIX  Pilot Questionnaire Link: http://bit.ly/LASQE    8. REFERENCES   [1] 1st International Conf. on Learning Analytics and Knowledge   2011 | Connecting the technical, pedagogical, and social   dimensions of learning analytics: 2011.   https://tekri.athabascau.ca/analytics/. Accessed: 2016-01-19.   [2] Abdullah, F. 2006. The development of HEdPERF: a new   measuring instrument of service quality for the higher educa-  tion sector. International J. of Consumer Studies. 30, 6   (2006), 569581.   [3] Ali, L., Hatala, M., Gaevi, D. and Jovanovi, J. 2012. A   qualitative evaluation of evolution of a learning analytics tool.   Computers & Education. 58, 1 (2012), 470489.   [4] Bowling, A., Rowe, G., Lambert, N., Waddington, M.,   Mahtani, K., Kenten, C., Howe, A. and Francis, S. 2012. The   measurement of patients expectations for health care: a re-  view and psychometric testing of a measure of patients   expectations. Health Technology Assessment. 16, 30 (2012),   1532.   [5] Calvert, C.E. 2014. Developing a model and applications for   probabilities of student success: a case study of predictive an-  alytics. Open Learning: The J. of Open, Distance and e-  Learning. 29, 2 (2014), 160173.   [6] Carr, W. and Kemmis, S. 1986. Becoming critical education,   knowledge, and action research. Falmer Press.   [7] Clow, D. 2012. The learning analytics cycle. Proc. of the 2nd   International Conf. on Learning Analytics and Knowledge -   LAK 12 (New York, New York, USA, 2012), 134.   [8] Cormack, A. 2016. A data protection framework for learning   analytics. J. of Learning Analytics. 3, 1 (2016), 91106.   [9] Cronin, J.J. and Taylor, S.A. 1992. Measuring Service Quali-  ty: A Reexamination and Extension. J. of Marketing. 56, 3   (1992), 55.   [10] Drachsler, H. and Greller, W. 2012. The pulse of learning   analytics understandings and expectations from the stakehold-  ers. Proc. of the 2nd International Conf. on Learning   Analytics and Knowledge - LAK 12 (New York, New York,   USA, 2012), 120.   [11] Elkhani, N. and Bakri, A. 2012. Review on expectancy dis-  confirmation theory (EDT) Model in B2C E-Commerce. J. of   Information Systems Research and Innovation. 2, 12 (2012),   95102.   http://bit.ly/LASQE   5      [12] Ferguson, R., Hoel, T., Scheffel, M. and Drachsler, H. 2016.   Guest editorial: Ethics and privacy in learning analytics. J. of   Learning Analytics. 3, 1 (2016), 515.   [13] Higher Education Funding Council for England: 2016.   http://www.hefce.ac.uk/. Accessed: 2016-10-14.   [14] Ifenthaler, D. and Schumacher, C. 2016. Student perceptions   of privacy principles for learning analytics. Educational   Technology Research and Development. 64, 5 (2016), 923-  938.   [15] Kim, J., Jo, I.-H. and Park, Y. 2015. Effects of learning ana-  lytics dashboard: analyzing the relations among dashboard   utilization, satisfaction, and learning achievement. Asia Pacif-  ic Education Review. 17, 1 (2015), 1324.   [16] Kim-Soon, N., Rahman, A. and Ahmed, M. 2014. E-Service   Quality in Higher Education and Frequency of Use of the Ser-  vice. International Education Studies. 7, 3 (2014), 110.   [17] McNiff, J. and Whitehead, J. 2011. All You Need to Know   About Action Research. SAGE.   [18] Ng, I.C.L. and Forbes, J. 2009. Education as Service: The   Understanding of University Experience Through the Service   Logic. J. of Marketing for Higher Education. 19, 1 (2009),   3864.   [19] Nguyen, A. and Rosetti, J. 2013. Overcoming potential nega-  tive consequences of customer orientation in higher education:   closing the ideological gap. J. of Marketing for Higher   Education. 23, 2 (2013), 155174.   [20] Oldfield, B.M. and Baron, S. 2000. Student perceptions of   service quality in a UK university business and management   faculty. Quality Assurance in Education. 8, 2 (2000), 8595.   [21] Oliver, R.L. 1980. A Cognitive Model of the Antecedents and   Consequences of Satisfaction Decisions. J. of Marketing   Research. 17, 4 (1980), 460.   [22] Oliver, R.L. 1989. Processing of the satisfaction response in   consumption: A suggested framework and research proposi-  tions. J. of Consumer Satisfaction, Dissatisfaction, and   Complaining Behaviour. 2, (1989), 116.   [23] Olson, J.C. and Dover, P. 1976. Effects of Expectation Crea-  tion and Disconfirmation on Belief Elements of Cognitive   Structure. In NA - Advances in Consumer Research Volume   03. B.B. Anderson, eds. Cincinnati, OH: Association for   Consumer Research. 168175.   [24] Parasuraman, A. 2005. E-S-QUAL: A Multiple-Item Scale for   Assessing Electronic Service Quality. J. of Service Research.   7, 3 (2005), 213233.   [25] Parasuraman, A., Zeithaml, V.A. and Berry, L.L. 1985. A   conceptual model of service quality and its implications for   future research. J. of Marketing. 49, 4 (1985), 4150.   [26] Parasuraman, A., Zeithaml, V.A. and Berry, L.L. 1988.   SERVQUAL: A Multiple-Item Scale for Measuring Consum-  er Perceptions of Service Quality. J. of Retailing. 64, 1   (1988), 1240.   [27] Park, Y. and Jo, I.-H. 2015. Development of the Learning   Analytics Dashboard to Support Students Learning Perfor-  mance. J. UCS. 21, 1 (2015), 110133.   [28] Prinsloo, P. and Slade, S. 2016. Student vulnerability, agency,   and learning analytics: An exploration. J. of Learning   Analytics. 3, 1 (2016), 159182.   [29] Ruiz, S., Charleer, S., Urretavizcaya, M., Klerkx, J., Fernn-  dez-Castro, I. and Duval, E. 2016. Supporting learning by   considering emotions: tracking and visualization a case study.   In Proc. of the Sixth International Conf. on Learning   Analytics & Knowledge (LAK 16), 254263.   [30] Scheffel, M., Drachsler, H., Stoyanov, S. and Specht, M.   2014. Quality indicators for learing analytics. Educational   Technology and Society. 17, 4 (2014), 117132.   [31] Sclater, N. 2016. Developing a code of practice for learning   analytics. J. of Learning Analytics. 3, 1 (2016), 1642.   [32] Sclater, N., Peasgood, A. and Mullan, J. 2016. Learning Ana-  lytics in Higher Education: A Review of UK and International   Practice. Jisc, Bristol.   https://www.jisc.ac.uk/sites/default/files/learning-analytics-in-  he-v3.pdf   [33] Spooren, P., Brockx, B. and Mortelmans, D. 2013. On the   Validity of Student Evaluation of Teaching The State of the   Art. Review of Educational Research. 83, 4 (2013), 598  642.   [34] Spreng, R.A. and Mackoy, R.D. 1996. An empirical examina-  tion of a model of perceived service quality and satisfaction. J.   of retailing. 72, 2 (1996), 201214.   [35] Spreng, R.A. and Page, T.J. 2003. A test of alternative   measures of disconfirmation. Decision Sciences. 34, 1   (2003), 3162.   [36] Stodnick, M. and Rogers, P. 2008. Using SERVQUAL to   measure the quality of the classroom experience. Decision   Sciences J. of Innovative Education. 6, 1 (2008), 115133.   [37] Swan, E.I. and Trawick, F. 1980. Satisfaction related to pre-  dictive vs. desired expectations. Refining concepts and   measures of consumer satisfaction and complaining   behavior. H.K. Hunt and R.L. Day, eds. Bloomington: School   of Business, Indiana University. 712.   [38] Tse, D.K., Nicosia, F.M. and Wilton, P.C. 1990. Consumer   Satisfaction as a Process. Psychology & Marketing (1986-  1998). 7, 3 (1990), 177.   [39] West, D., Heath, D. and Huijser, H. 2016. Lets Talk Learning   Analytics: A Framework for Implementation in Relation to   Student Retention. Online Learning. 20, 2 (2016), 150170.   [40] Winne, P.H. and Hadwin, A.F. 2013. nStudy: Tracing and   Supporting Self-Regulated Learning in the Internet. Interna-  tional Handbook of Metacognition and Learning   Technologies. R. Azevedo and V. Aleven, eds. Springer Inter-  national Handbooks of Education. 293308.   [41] Xu, D., Benbasat, I. and Cenfetelli, R.T. 2013. Integrating   Service Quality with System and Information Quality: An   Empirical Test in the E-Service Context1. MIS Quarterly. 37,   3 (2013), 777794.   [42] Yooyen, A., Pirani, M. and Mujtaba, B.G. 2011. Expectations   versus realities of higher education: gap analysis and universi-  ty service examination. Contemporary Issues in Education   Research. 4, 10 (2011), 25.     "}
{"index":{"_id":"45"}}
{"datatype":"inproceedings","key":"Allen:2017:WYS:3027385.3027445","author":"Allen, Laura K. and Perret, Cecile and Likens, Aaron and McNamara, Danielle S.","title":"What'D You Say Again?: Recurrence Quantification Analysis As a Method for Analyzing the Dynamics of Discourse in a Reading Strategy Tutor","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"373--382","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027445","doi":"10.1145/3027385.3027445","acmid":"3027445","publisher":"ACM","address":"New York, NY, USA","keywords":"corpus linguistics, dynamics, intelligent tutoring systems, natural language processing, reading, stealth assessment","Abstract":"In this study, we investigated the degree to which the cognitive processes in which students engage during reading comprehension could be examined through dynamical analyses of their natural language responses to texts. High school students (n ","pdf":"   Whatd You Say Again Recurrence Quantification  Analysis as a Method for Analyzing the Dynamics of   Discourse in a Reading Strategy Tutor    Laura K. Allen   Arizona State University  PO Box 872111      Tempe, AZ, 85287  01+404-414-5200     LauraKAllen@asu.edu      Aaron Likens  Arizona State University   PO Box 872111     Tempe, AZ, 85287  01+404-414-5200     alikens@asu.edu   Cecile Perret  Arizona State University   PO Box 872111     Tempe, AZ, 85287  01+404-414-5200     cperret@asu.edu     Danielle S. McNamara   Arizona State University   PO Box 872111     Tempe, AZ, 85287   01+404-414-5200    Danielle.McNamara@asu.edu       ABSTRACT  In this study, we investigated the degree to which the cognitive  processes in which students engage during reading comprehension  could be examined through dynamical analyses of their natural  language responses to texts. High school students (n = 142)  generated typed self-explanations while reading a science text.  They then completed a comprehension test that measured their  comprehension at both surface and deep levels. The recurrent  patterns of the words in students self-explanations were first  visualized in recurrence plots. These visualizations allowed us to  qualitatively analyze the different self-explanation processes of  skilled and less skilled readers. These recurrence plots then  allowed us to calculate recurrence indices, which represented the  properties of these temporal word patterns. Results of correlation  and regression analyses revealed that these recurrence indices  were significantly related to the students comprehension scores at  both surface- and deep levels. Additionally, when combined with  summative metrics of word use, these indices were able to  account for 32% of the variance in students overall text  comprehension scores. Overall, our results suggest that recurrence  quantification analysis can be utilized to guide both qualitative  and quantitative assessments of students comprehension.         Categories and Subject Descriptors   Computing methodologies~Natural language processing     Applied computing~Computer-assisted instruction  Applied  computing~Psychology    General Terms  Algorithms, Measurement, Performance, Languages, Theory   Keywords  Intelligent Tutoring Systems, Natural Language Processing,  stealth assessment, corpus linguistics, dynamics, reading   1 INTRODUCTION  Literacy is a critically important skill for success in modern  society, as individuals are increasingly reliant on text-based  communication in their daily lives, classrooms, and workplaces  [17; 40]. The ability to learn from and communicate through text  relies on an intricate set of processes that include understanding  the basic content in the text and generating connections between  this new information and prior knowledge of the concepts [32].  Unfortunately, the complexity of these tasks often presents  difficulties in students acquisition of strong literacy skills, as  evidenced by consistent reports of low performance on  standardized assessments of reading comprehension and writing  [37]. Further, teachers often lack the time and resources to provide  students the individualized instruction and feedback they need to  improve these skills.   In response to this need, researchers have developed educational  technologies with the aim of enhancing the quality of the reading  and writing training that students receive, as well as their  opportunities for deliberate practice (see [10] for an overview).  For instance, automated writing evaluation systems deliver  automated feedback on students essay writing [43; 51]. Similarly,  Intelligent Tutoring Systems (ITSs) provide students with  instruction and automated feedback that can be adapted to their  knowledge and skills. For instance, the DSCoVAR (Dynamic   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than ACM must be honored. Abstracting with  credit is permitted. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.    LAK '17, March 13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027445        Support of Contextual Vocabulary Acquisition for Reading)  system targets students vocabulary knowledge by providing  opportunities to practice reading difficult words across multiple  contexts. Additionally, the system provides individualized  feedback on students performance [16].    These literacy-focused adaptive technologies build on a strong  foundation of research on the use of artificial intelligence in  education. Traditionally, this field has focused on the  development and use of ITSs that target instruction in well- defined domains, such as mathematics and physics [36; 38; 47].  The strength of these systems is largely grounded in their ability  to adapt the instruction, practice problems, and feedback that  students receive based on on-line assessments of their  performance, affective states, and knowledge. These systems have  been shown to be highly effective, with a recent overview  reporting no significant differences in effect size between ITSs  and expert one-on-one human tutoring [48].    Despite their obvious similarities, however, educational  technologies that target literacy skills (as well as skills in other ill- defined domains) differ from more traditional ITSs in a number of  important ways. Perhaps the most salient of these differences is  the nature of students responses to the tutoring system. For  example, ITSs that target math instruction can present students  with high numbers of multiple-choice questions in a single  training session, each of which has a set of right and wrong  answers for students to select. Based on the measured  performance on these items, the system can adapt additional  practice problems and feedback to students individual needs [47].    Conversely, ITSs for literacy instruction often prompt students to  respond to the tutor using natural language. For instance,  iSTART  an ITS for reading strategy training  prompts students  to type self-explanations (i.e., explanations of the meaning of the  text material to oneself) of texts as they read [33]. Similarly, We- Write  a tutoring system that provides training on self-regulation  strategies for writing  prompts students to generate and revise  essays in the system [52]. In the current study, we describe recent  work that aims to develop more robust assessments of students  natural language responses in ITSs such as these. In particular, we  extend work on discourse analyses by examining the temporal  properties of the language that students produce during learning  tasks and relate these properties to students performance at  multiple levels.    1.1 Adaptivity in Educational Technology  Educational technologies rely on assessments of student  performance to drive adaptive instruction and feedback. In an  effort to not distract from the learning process, system developers  have increasingly relied on measures that can be collected within  the learning task itself [45-46]. These stealth assessments can be  informed by a wealth of data commonly collected by intelligent  tutoring systems, such as the choices students make during  learning tasks, the trajectories of their mouse movements, and the  keystrokes they press while typing. For example, log data (e.g.,  students clicks in the system) has been used to develop detectors  of students engagement [24] and affect [5; 13] during learning  tasks.     Once these stealth assessments have been developed, they can be  used to develop models of student users. These models then allow  the system to individualize the instruction and feedback that  students receive based on their strengths and weaknesses [7].  Importantly, these models can be continuously updated in the  system as additional data is collected. This ensures that the system   is appropriately accounting for changes in students knowledge  and skills over the course of their training in the system.    1.1.1 Language Assessment in Educational  Technologies   Albeit still rare, ITS developers increasingly incorporate natural  language and natural language processing (NLP) techniques into  tutoring systems in an effort to increase adaptivity and learning  [18-21; 34; 42]. For instance, Why2 Atlas  a tutoring system for  physics  engages students in natural language dialogue related to  their qualitative explanations of physics problems [42]. Prior  research suggests that these interactions with NLP-based tutoring  systems lead to significant learning gains compared to non- interactive learning tasks [18; 49].    More recently, researchers have begun to use the data collected  from these natural language responses to develop more nuanced  stealth assessments of students characteristics and performance  [3; 12; 34]. For instance, DMello and colleagues (2009) [12]  found that they could predict the proportional occurrence of  students affective states through analyses of the cohesion in their  dialogues with an automated tutor. Similarly, McNamara and  colleagues (2007) [34] found that natural language processing  indices could be used to accurately score the quality of students  self-explanations during text reading.    Despite this significant progress, NLP-based tutoring systems  have plenty of room for improvement. One issue relates to the  ability of these systems to measure the on-line cognitive and  affective processes of student users. Analyses of students  language typically rely on aggregate measures of language use  (e.g., the most common words used by students, total number of  words produced) and, as such, provide little information about the  processes in which students are engaged. In order to provide more  nuanced assessments that can target students needs, ITSs should  analyze the properties of students language as it unfolds over  time.    1.1.2 Dynamical Analyses of Natural Language  In the current study, we rely on computational techniques from  dynamical systems theory to analyze the temporal organization of  students natural language responses to text. Dynamic  methodologies provide a novel means with which researchers can  characterize patterns that emerge from students behaviors (e.g.,  language, system choices) during learning tasks. Traditional  statistics often aggregate variables across time, potentially  discarding important information about learning and performance.  In contrast, dynamic methodologies consider time to be a critical  component of the analysis and explicitly seek to characterize  temporal patterns. Thus, rather than treating behavior as a static  process, these dynamic analyses more accurately account for the  complex, changing nature of behavior. Although the current study  is one of the first to use dynamic analyses to assess students  natural language responses to an intelligent tutoring system, these  techniques have previously been used across a wide variety of  domains as a means to understand the complex patterns that  manifest in individuals behaviors over time [e.g., 4; 11; 41; 44].    To illustrate the potentially important value of these dynamic text  analyses, consider that you have been asked to read a text on a  complex topic and explain the text to yourself as you read. How  might the topics you reference change over the course of this task,  compared to when you are reading a text more passively It may  be the case that when you read the text passively, you simply  explain the meaning of the individual sentences to yourself,        without referencing the previous material in the text or your  outside knowledge. When you are reading the text more deeply,  however, you might read sentences, but consistently refer to  previous material in order to generate connections and develop a  deeper understanding of the concepts in the text.    The differences described in this example may play an important  role in modeling the processes that students are engaging in  during text comprehension, which can ultimately help to develop  more nuanced assessments of their performance. For instance, it is  possible that a students comprehension of text-based information  (i.e., information that does not require the reader to make  connections across sentences or paragraphs in the text) can be  detected with simple, traditional analyses of the frequent words  occurring in their natural language responses to the text. Their  deep comprehension of the text (i.e., their performance on items  that require the reader to generate inferences), however, may be  missed if the temporal nature of these responses is not taken into  account. In this scenario, dynamic analyses that account for the  temporal distributions of the words in students text responses  may prove more informative than static measures.       1.1.3 Recurrence Quantification Analysis  Here, we utilize a dynamic methodology  recurrence  quantification analysis -- to visualize and quantify the extent to  which recurrent patterns in students natural language text  responses relate to their reading comprehension processes.  Recurrence quantification analysis (RQA) is a nonlinear data  analysis technique that provides information about patterns of  repeated behavior (i.e., the number and duration of recurrences) in  a continuous or categorical time series [30]. Like many techniques  used in the dynamical systems theory framework, this  methodology has been used in a variety of domains, both within  and outside the realm of human behavior [11; 44]. For example,  researchers have utilized recurrence quantification analyses to  examine patterns of heart-rate variability [30], postural  fluctuations [41] and eye movements [4].    Beyond these physiological measures, RQA has the potential to  provide important information about recurrence in the content of  students language. Dale and Spivey (2005) [11], for example,  have revealed that RQA can be applied to categorical data sets,  such as the words in a particular conversation. This flexibility of  the RQA technique (i.e., the fact that it can be applied to both  continuous and categorical data sets) may be particularly salient  for the study of natural language. In particular, recurrence can be  measured at multiple levels of the text (e.g., word, semantic),  rather than relying only on one level of analysis.   The starting point of RQA is the development of a recurrence plot,  which is a visualization of a matrix wherein the individual  elements represent points in a time series that are visited more  than once (i.e., they recur). In other words, this plot represents the  times in which a dynamical system visits the same area in a phase  space [29]. Within this plot, each point represents a particular  state that is revisited by the system. If multiple points occur  continuously, they form diagonal lines, which represent times  when the system is revisiting an entire sequence of states.    As a simple illustration, consider the following sentence: The ice  cream man brought ice cream on Friday. To generate a  recurrence plot for this sentence, the words in the sentence are  first placed on both the X and Y axes of a 2-dimensional plot (see  Figure 1). Each time a word appears both the X and Y axes, a dot  is placed in that location on the plot. Because this sentence is  being plotted against itself, the recurrence plot is symmetrical   with a diagonal line through the center  the line of identity (LOI).  The points of interest in these recurrence plots are the points that  do not occur on the main diagonal. Individual points off the main  diagonal represent the times that a word is repeated later in the  sentence. When multiple points occur simultaneously, these points  form diagonal lines (e.g., ice cream in Figure 1), which  represent sequences of words that are repeated in time.      Figure 1. Example recurrence plot   Visualizing recurrent patterns is informative, but researchers also  need to quantify the structure contained in recurrence plots.  Recurrence quantification analysis offers multiple metrics that  help to quantify recurrent patterns to allow for statistical  comparisons of recurrence plots [53]. Below, we briefly describe  the most commonly used metrics in recurrence quantification  analyses. For more detailed information, see [9].   Recurrence Rate. The recurrence rate is a measure of the density  of points represented in a recurrence plot. A recurrence plot is  calculated by dividing the total number of points in a plot by the  square of the length of the overall time series. This metric  represents the overall amount of recurrence that is present in the  recurrence plot, regardless of the distributions of the points.    Determinism. Determinism is a measure of the number of  recurrent points that tend to fall on diagonal lines (ignoring the  LOI) in the recurrence plot. Thus, this metric provides  information about the distribution of the recurrent points.  Diagonal lines in recurrence plots reflect time periods when the  system is revisiting a particular sequence of states. Thus, systems  with low determinism can exhibit short moments of repetitive  states; however, they are considered less ordered than highly  deterministic systems.   Average Line Length. This metric calculates the average length  of the diagonal lines in the recurrence plot. Thus, when the system  repeats a sequence of states, this metric provides information  about the typical length of those sequences.   Maximum Line Length. This metric calculates the length of the  longest diagonal line in the recurrence plot. Therefore, this metric  reveals whether a system revisits a long sequence of states at  some point in time.           Entropy. Entropy is calculated as the Shannon entropy of the  distribution of the line lengths in the recurrence plot. This metric  quantifies the degree to which the trajectory of the system exhibits  order. Thus, entropy will be higher if the system revisits a wider  variety of state sequences over time. Dynamic systems that  continually revisit the same, or similar, sequences of states, will  have lower entropy.   1.2 iSTART  This study aims to refine the adaptive capabilities of the  Interactive Strategy Training for Active Reading and Thinking  (iSTART) system, an intelligent tutoring system that teaches high  school and college students self-explanation strategies to improve  their comprehension of complex texts [28; 33]. Self-explanation  has repeatedly been shown to be beneficial for improving higher  order skills such as deep comprehension of text, inference  generation, and problem solving [8]. In this study, we intend to  maximize iSTARTs ability to produce a user model in order to  improve the system's adaptability to individual student's needs.    iSTART is based on the Self-Explaining and Reading Training  (SERT) intervention, which was created to teach students  effective strategies for self-explaining a text followed by practice  on how to use them as they read [31]. Previous research has  demonstrated the effectiveness of SERT, as well as iSTART, in  improving students reading comprehension skills of complex texts  [28]. iSTART focuses on self-explanation training through two  principal modules within the system: training and practice.    In the training module, students are taught the self-explanation  strategies (comprehension-monitoring, paraphrasing, prediction,  elaboration, and bridging) through the use of animated videos  presented by a pedagogical agent.  Each strategy is taught through  the use of definitions, mnemonic devices, and examples. Students  then answer a set of checkpoint questions to determine their  comprehension of the lesson. Once all students have received a  75% score on each of the lesson checkpoints, they view a  summary lesson and are then prompted to practice using the  strategies in an initial practice activity. In this phase, students use  the self-explanation strategies and receive feedback for two texts.  Once completed, students are ushered into the practice module of  the system.   The practice module is composed of a variety of practice  activities, all falling in one of two categories: identification and  generative. Identification mini-games are all games in which  students are prompted to read a text with an associated self- explanation and then must select the specific strategy that was  used for that self-explanation. Generative practice, however, is  composed of both game-based practice as well as non-game  practice activities. These activities prompt students to generate  their own self-explanations to a designated target sentence as they  read a text. Students are then given feedback on their response  based on a complex algorithm that uses linguistic indices to  determine the quality of the student's response. Generative  practice activities are designed to allow teachers the opportunity  to insert their own texts for their students, thus the evaluation  algorithm that iSTART uses to score the self-explanations must be  flexible and accurate to optimize the system's capabilities to  improve student learning outcomes.    1.2.1 iSTART Evaluation Algorithm   iSTART is designed to assess and score students self- explanations immediately after each individual submission to the  system. Since analyses are always conducted on a local basis, the   system is dependent on a limited set of available information. This  includes the students response, the specific target sentence  prompting the self-explanation, and the previous sentences of the  text. The algorithm uses both word-based indices and Latent  Semantic Analyses (LSA) to assess the quality of the self- explanation and determine the appropriate feedback. Lower-level  assessments are informed by word-based indices that include  response length as well as quantity of content-word overlap.  Typically, these provide an initial report on whether the response  is too short, too similar or identical to the topic sentence, or  entirely irrelevant.  After these initial analyses, more information  is taken into consideration using LSA, which is capable of  producing a more holistic assessment by determining how well  the self-explanation is related to the text as well as outside content  (considered as a students prior-knowledge).   The algorithm produces a score using word-based indices and  LSA-indices on a scale from 0 to 3. A score of 0 is given when the  self-explanation is either too short, irrelevant, or too similar to the  target sentence. A score of 1 demonstrates that the student wrote a  self-explanation that solely relates to the target sentence. A score  of 2 is generated if the student wrote a self-explanation that relates  to both the target sentence and previous portions of the text.  Students receive a score of 3 when their self-explanations derive  information from the target sentence, previous sentences of the  text, as well as external information not directly related, though  relevant, to the text. This implies that the student not only  produced inferences throughout the text, but elaborated on the  available information using background knowledge. Research  using the iSTART algorithm has shown that it scores as accurately  as humans and that it can offer a summary of the cognitive  processes used in reading comprehension [25].   1.2.2 Aggregated Self-Explanation Analyses   Previous research on the iSTART system has relied on NLP  techniques to analyze student's self-explanation responses [1-3;  50]. Initial work focused on local sentence-level analyses of  students individual self-explanations to texts. However, recent  research has begun to observe how a set of self-explanations that  span an entire text can be aggregated and evaluated to provide  analyses at a more global level. Such analyses reveal a far more  comprehensive interpretation of the comprehension processes  involved in reading a text.    Research on these aggregated self-explanations was motivated  by the possibility of increasing the bandwidth of available  information to analyze.  Researchers used analyses of aggregated  self-explanations to determine whether evaluating responses at a  larger window size (i.e., aggregated self-explanations for a single  text as opposed to individual self-explanations) would improve  upon a student model [3; 50]. Results showed that analyzing the  aggregated responses accounted for an additional 10% of the  variance that was already accounted for by the original iSTART  algorithm [50].  These studies also show a positive relationship  between the aggregated NLP scores and iSTART algorithm scores  and pretest reading scores [3].    Additional research studies have assessed which specific  linguistic indices provide more accurate predictions of potential  connections within the text. Studies have determined that indices  relating to local and global cohesion are most likely to reveal  relevant connections being made across self-explanations.  Specifically, when students aggregated self-explanations display  a higher incidence of causal cohesion, these students also exhibit  better comprehension of the text [1-2].        Recently, Allen, Jacovina, and McNamara (2016) [1] discovered  that over the course of multiple sessions of practice within  iSTART, the global cohesion of students aggregated self- explanations increased. This implies that over time students learn  to generate more inferences and create deeper connections across  the text they read.  This finding demonstrates that extended  training within iSTART improves student comprehension  processes. Ultimately, these linguistic signatures can provide the  system with information on students' performance over time, thus  helping to determine what type of practice is optimal for  individualized training.   1.3 Current Study  The purpose of the current study is to investigate the degree to  which the cognitive processes in which students engage during  reading comprehension can be examined through dynamical  analyses of their natural language responses to the text. We use  dynamic visualizations and quantifications of students natural  language text responses to measure their performance on a  comprehension test. In particular, we examine whether the  patterns of students word usage during their text responses reflect  differences in their cognitive processes, as reflected by their  performance on surface- and deep-level comprehension questions.  Additionally, we present visualizations of these patterns and  provide qualitative assessments of these visualizations to  demonstrate their potential to drive student feedback.   2 METHODS  2.1 Participants  The data for this study was collected as part of a larger, five- session study. In total, 149 high school and college freshmen  (6.7%) students participated in this study located in the  southwestern United States. On average, the students were 15.69  years of age (range = 13-19). Of these students, 55% were female  and 16.8% reported speaking English as a second language  Additionally, 43.6% were Caucasian, 32.2 %were Hispanic, 8.7 %  were African-American, 7.4 % were Asian, and 8.1 % reported  other nationalities. Seven students were dropped from the  analyses due to data loss and attrition; thus, we analyzed data for  142 total students.   2.2 Study Procedure  The data included in this study was collected over the course of  two sessions, which lasted between one and two hours. In the first  session, students general world knowledge, reading  comprehension and writing skills, and attitudes were assessed  using the following measures: Demographics questionnaire,  Alternate Uses task [22]; selected items from the Remote  Associative task (RAT) [35]; Motivated Strategies for Learning  Questionnaire (MSLQ) [39]; Gates MacGinitie Reading test  (Gates-MacGinitie (4th ed.) reading skill test (form S) level  10/12) [27]; 30 question multiple-choice test on general  knowledge in literature, science, and history; 25 minute timed- essay writing task; and a Component Processes test [23].   The data collected in session two was collected one to three days  after session one and contained the following measures: Cognitive  Reflection Test (CRT) [15]; Self-Explanation and Reading  Comprehension Test; On-line Motivation Questions [6]; Learning  Orientation and Performance Orientation task (LO/PO) [26]; and a  Grit assessment [14].    For the purposes of the current study, we only analyzed the data  from the Demographics questionnaire in session one and the Self- Explanation and Reading Comprehension test in session two.   2.3 Self-Explanation and Reading  Comprehension Test   A Self-explanation and Reading Comprehension Test was  administered to students to analyze the on-line reading processes  students employed during reading, as well as their comprehension  of the text at the surface (text-based) and deep (bridging) levels.  Students read and self-explained one of two science texts during  session two related to heart disease or red blood cells. This text  was presented one segment (i.e., two to three sentences) at a time,  with each segment separated by a target sentence in bold. For each  target sentence, students were instructed to write a self- explanation of the information they had just read. In total, each  student wrote nine self-explanations for the text.    Immediately following this self-explanation and reading  procedure, the students were asked to answer eight  comprehension questions. The comprehension test consisted of 4  text-based and 4 bridging open-ended questions. The text was not  visible to students while they answered these questions. Text- based questions were based on information found within one  sentence in the text, whereas bridging questions required students  to refer to information from two or more sentences within the text.  Each question was worth one point, but allowed partial credit.  Thus, the maximum number of points that a student could receive  on this test was eight. The comprehension questions were  independently scored by two expert raters for at least 14% of the  responses. Raters resolved discrepancies and repeated the process  until they received 95% exact agreement, with a kappa of at least  0.8.  Once interrater reliability was achieved, one coder completed  the remainder of the scoring.   Table 1. Recurrence Quantification Analysis Indices   Description   Recurrence Rate Proportion of the recurrence plot that is  composed of recurrent points     Determinism Proportion of recurrent points that form  diagonal line structures (defined as 2 or  more recurrent points in a row)   Line Number Total number of lines in the recurrence  plot.    Max Line Length of the longest diagonal line in  the plot, excluding the main diagonal   Average Line Average length of the lines in the  recurrence plot   Entropy Shannon information entropy of  diagonal line lengths    Normalized  Entropy   Entropy variable normalized by the  number of lines in the plot   2.4 Data Processing  For the purpose of generating and quantifying the recurrence  plots, students individual, sentence-level self-explanations were  aggregated. Therefore, each student had one aggregated self- explanation file that included the nine self-explanations they  produced while reading.    To prepare the data for the RQA, the texts were first cleaned. All  punctuation in the texts was first removed and the words were all        converted to lower case and stemmed. Once the texts were  cleaned, the series of words was converted to series of categorical  numeric codes, which each represented the unique words in each  self-explanation. For instance, the sentence, The bird ate bird  food. would be converted to the series: {1, 2, 3, 2, 4}.   2.5 Recurrence Quantification Analyses  We used the crqa library in R [9] to generate the recurrence plots  and calculate the recurrence indices for students self- explanations. The resulting indices are described in Table 1.    2.6 Text Analyses  In addition to the RQA indices, descriptive indices of students  aggregated self-explanations were calculated to provide summary  information about the words in students self-explanations.  Specifically, we calculated the total number of words, the number  of letters per word, and the type-token ratio. The type-token ratio  is a measure of the number of unique words in the text divided by  the total number of words. We included these basic text indices in  our analysis to determine whether the recurrence quantification  analysis metrics accounted for different and unique variance  beyond these basic descriptive indices.    2.7 Statistical Analyses  To assess the degree to which the patterns of recurrence in  students self-explanations were associated with their  comprehension of the text, we generated recurrence plots and  calculated Pearson correlations and regression analyses. The  recurrence plots allowed us to visualize the recurrent word  patterns across students self-explanations of the text.  Additionally, these recurrence plots allowed us to quantify the  properties of these plots with seven RQA indices (see Table 1).     Normality of the indices was assessed with skew, kurtosis, and  visual data inspections. Two indices, Line Number and Average  Line were strongly skewed; therefore, we calculated the log  transformation for this index.    Pearson correlations were used to assess relations between word  recurrence (as defined by the RQA indices) and comprehension  scores. We calculated these correlations for students overall  comprehension scores, as well as their text-based and bridging  comprehension scores. Finally, stepwise regression analyses were  conducted to follow-up the correlation analyses in order to  provide an indication of the variables that accounted for the most  variability in the dependent variables. For this analysis, we  included the three basic descriptive indices and the RQA indices  to determine whether the RQA indices accounted for unique  variance in the model once the basic indices were included.  Multicollinearity was assessed among the indices (r > .90)  included in the regression analysis; however, no indices  demonstrated multicollinearity. Additionally, the self-explanations  of eleven students contained fewer than 100 words, which did not  provide enough data points for the Entropy RQA indices to be  calculated. Therefore, we conducted pairwise deletion to account  for this missing data in our correlation and regression analyses.   3 RESULTS  3.1 Qualitative Analysis of Recurrence Plots  To visualize the temporal distribution of words in students self- explanations, recurrence plots for each student were calculated  using the procedure described in the previous sections. These  recurrence plots varied considerably among the students and  provided us a means to qualitatively analyze differences in the   word recurrence in the self-explanations of students who received  low and high scores on the comprehension test.      Figure 2. Recurrence Plot for a Student with a Low Text   Comprehension Score     Figure 3. Recurrence Plot for a Student with a High Text   Comprehension Score  Figures 2 and 3 illustrate two recurrence plots that were generated  using two students actual self-explanations from the current  study. Although the students self-explanations had a similar total  number of words (Figure 2 = 224; Figure 3 = 251), the plots  demonstrate that these students exhibited strongly different  patterns of word recurrence throughout their self-explanations.    Figure 2 illustrates the recurrence plot of a student who received a  score of 1 (out of 8) on the comprehension test (text-based  comprehension score = 1; bridging comprehension score = 0). As  can be seen in the plot, this student rarely produced self- explanations with similar words from their previous explanations.  Additionally, in the situations when this student did exhibit word        recurrence, the words tended to occur in isolation, rather than in  sequences (diagonal lines) of words.  In other words, the  recurrence plot suggests that this student did not generate explicit  connections between the information explained in different  sections of the text.    In contrast, the plot depicted in Figure 3 comes from a student  who received a perfect score of 8 on the comprehension test (text- based comprehension score = 4; bridging comprehension score =  4).  Unlike the previous student, this student exhibited a high  degree of recurrence across self-explanations. Additionally, many  of the recurrent points fell on diagonal lines, suggesting that this  student was repeatedly referring to sequences of words, rather  than individual words. Thus, while reading through the text, the  student continued to explain the new text information in  connection with previously encountered text information.    Overall, these recurrence plots provide a means through which the  comprehension processes of skilled and less skilled readers can be  differentiated. Despite the fact that these two students generated a  similar amount of text during the self-explanation procedure, the  temporal distribution of the words they used varied widely. In  particular, these plots reveal that the student who continuously  repeated words and phrases while self-explaining ultimately  developed a deeper comprehension of the text. In comparison, the  student who rarely repeated information across self-explanations  demonstrated low text comprehension.   3.2 Text Comprehension  The qualitative analyses of the recurrence plots provided  preliminary evidence that skilled and less skilled readers exhibited  strong differences in their word recurrence during self- explanation. To empirically test these findings, we conducted  quantitative analyses of these plots.    Table 3. Correlations between RQA indices and  Comprehension Scores   RQA Index Text-Based Bridging Total  Recurrence Rate .119 (M) .101  .126 (M)  Determinism -.058 .071  .001  Log of Line Number .413** .481**  .505**  Max Line .132 (M) .142*  .155*  Log of Average Line .002 .177*  .093  Entropy .011 .229*  .124 (M)  Normalized Entropy -.204* .019 -.116  p <.001**; p <.05*; Marginal = M   Pearson correlations were first calculated between the RQA  indices and students text comprehension scores (see Table 3).  Results from these analyses indicated that students  comprehension scores were significantly related to a number of  the RQA indices. In particular, these results reveal that skilled  readers did not simply repeat words more often than less skilled  readers. Rather, they differed from less skilled readers in their  more frequent repetition of longer sequences of words.  Importantly, the relations between the RQA indices and  comprehension scores differed between text-based and bridging  questions. These findings suggest that these recurrence  characteristics are able to provide nuanced information about  students comprehension processes that go beyond holistic  comprehension scores.   We conducted three stepwise regression analyses with the RQA  indices and three basic text indices (i.e., total number of words,   the number of letters per word, and the type-token ratio) as  predictors and the comprehension scores (i.e., total, text-based,  and bridging) as the dependent variables. The purpose of these  analyses was to assess the amount of variance accounted for by  the RQA indices, as well as to determine whether these indices  accounted for variance in the comprehension scores when  summative text measures were taken into account.    The three regression analyses yielded significant models. The  analysis of students total comprehension scores [F (2, 118) =  27.58, p < .001; R2 = .32] retained two variables: Log of Line  Number [ = .54, p < .001] and Number of Letters per Word [ =  .25, p < .01].  The analysis of students text-based comprehension scores [F (3,  117) = 11.60, p < .001; R2 = .23] retained three variables: Log of  Line Number [ = .48, p < .001], Number of Letters per Word [  = .19, p < .05], and Determinism [ = -.18, p < .05].  Finally, the analysis of students bridging comprehension scores  [F (4, 116) = 11.18, p < .001; R2 = .38] retained four variables:  Log of Line Number [ = .70, p < .001], Number of Letters per  Word [ = .25, p < .01], Normalized Entropy [ = .32, p < .01]  and Determinism [ = -.26, p < .05].  The results of these analyses suggest that students text  comprehension was most strongly predicted by the number of  diagonal lines in their recurrence plots, as well as the size of their  words. This provides confirmation of the qualitative analyses by  indicating that the skilled readers more frequently repeated  sequences of words, rather than individual words. In addition, the  words that skilled readers use tend to be longer, or less frequent  words, which provides a proxy for students vocabulary.   Additionally, the analyses revealed that the recurrence metrics  were more strongly related to students performance on bridging  questions than text-based questions. Thus, comprehension  questions that required students to generate connections across  multiple sentences in the text were more strongly related to the  word recurrence in students self-explanations.     4 DISCUSSION  Educational technologies across a variety of domains increasingly  incorporate natural language components for the purpose of  increasing interactivity and providing students with adaptive  instruction and feedback [10; 20; 42]. While these systems  generally provide accurate holistic feedback [34; 43; 51], they  often lack the more nuanced information that is needed to drive  formative feedback related to beneficial learning processes. The  objective of many natural language assessments is to deliver  accurate scores that match an experts ratings of quality.  However, the indices used in these analyses often exist in a black  box and can be difficult to translate into actionable feedback for  students. Additionally, these assessments do not often take the  temporal aspects of language into account, which may play a  critical role in the assessment of students performance at more  fine-grained sizes.    In this study, we addressed these research gaps through  computational analyses of students natural language responses to  a text. We leveraged dynamic modeling techniques to capture the  temporal properties of students language use and to relate those  properties to students performance on a comprehension test.  Importantly, this analysis did not solely rely on statistical  assessments of student performance. We were able to generate  metrics that could provide both qualitative and quantitative  information about students comprehension performance. We  anticipate that these metrics will be able to drive summative        feedback in educational technologies, but also provide students  with meaningful visualizations of their work. These visualizations  may ultimately help students to ground the system feedback in  specific examples from their own work, which can lead to  improvements in their understanding and uptake of the feedback.    The results of the current study support our hypotheses that the  temporal, recurrent properties of students text responses can  provide important information about their comprehension. The  qualitative analyses of students recurrence plots indicated that  successful comprehension processes could be observed through  visualizations of students word use over time. Specifically, the  skilled reader depicted in Figure 3 consistently repeated sequences  of words across self-explanations, whereas the less skilled reader  (Figure 2) referred to previously mentioned concepts much less  frequently. This is an important finding because it indicates that  the temporal variability in students natural language responses  can provide important information about their comprehension  processes. Further, these analyses revealed that visualizations of  these language sequences can be used to deliver meaningful  information about these different comprehension processes.    The RQA indices generated from these recurrence plots were  additionally able to provide important information about students  comprehension performance. In particular, the results of the  correlation and regression analyses indicated that 32% of the  variance in students comprehension scores were accounted for  using a combination of summative metrics of word use (i.e., total  number of words, the number of letters per word, and the type- token ratio), as well as indices related to recurrent patterns of this  word use. These analyses speak to the importance of accounting  for temporal patterns in analyses of students language. Natural  language processing techniques tend to rely on summative metrics  of text features; however, the results of the current study suggest  that expanding these analyses to include temporality can provide  critical information about students learning processes.     The correlation analyses additionally revealed similarities and  differences between the relationships between these recurrence  metrics and the text-based and bridging comprehension scores.  Performance on both the text-based and bridging questions was  related to a greater number of recurrent word sequences (Log of  Line Number) and a longer maximum recurrent sequence (Max  Line). This is an interesting finding and suggests that  comprehension at multiple levels can be enhanced through the  generation of connections among text information. In particular,  both text-based and bridging scores demonstrated medium  relationships to the number of lines in students recurrence plots.  Thus, feedback driven by these metrics could potentially be  developed to prompt students to generate greater connections  among ideas in order to improve their understanding of the text  content.     In addition to this similarity in recurrent lines, the correlations  were indicative of some interesting differences between the text- based and bridging scores. For instance, while bridging scores  were positively associated with the raw entropy index for the line  lengths in students plots, text-based comprehension scores were  negatively related to the normalized entropy metric. This suggests  that the processes underlying students surface- and deep-level  comprehension performance may differentially manifest in the  temporal properties of their response to texts. This has important  implications for future system adaptability. If these findings were  to be replicated with more descriptive information in follow-up  studies, it suggests that text-based and bridging comprehension   performance could be assessed and, therefore, addressed in  different ways through system feedback.    As a final note, in the current study, we only focused on the  individual words in students self-explanations, and did not  account for the numerous properties that can be calculated in  linguistic analyses. This methodological choice was made to  provide a demonstration of the power of the recurrence  quantification technique when only words are considered. In  reality, however, this technique is highly flexible and can be used  to analyze any number of features of language. For instance,  categorical recurrence quantification analyses (such as this one)  can be used to analyze recurrent patterns in the parts-of-speech or  topics of students language. Additionally, recurrence  quantification analyses can be applied to model continuous data,  such as word frequency or similarity to the topic. Future studies  should be conducted to build on the results of the current study to  account for the multi-dimensional properties of the language that  students generate.    Overall, our results suggest that recurrence quantification analysis  can be utilized to guide both qualitative and quantitative  assessments of students comprehension. Our eventual goal is to  use these indices to develop more nuanced stealth assessments  and formative feedback in the iSTART system. More broadly, the  current study suggests that dynamic visualizations and analyses  can be used as a step towards more adaptive educational  technologies for literacy, as well as for any system that collects  students natural language responses. Although this is only a first  step, and a number of studies remain to be conducted, this study  provides a strong initial foundation because it demonstrates the  feasibility of such measures for modeling student performance.    5 ACKNOWLEDGMENTS  This research was supported in part by: IES R305G020018-02,  IES R305G040046, IES R305A080589, and NSF REC0241133,  and NSF IIS-0735682. Opinions, conclusions, or  recommendations do not necessarily reflect the views of the IES  or NSF. We also thank Matt Jacovina, Scott Crossley, Rod  Roscoe and Jianmin Dai for their help with the data collection and  developing the ideas found in this paper.   6 REFERENCES  [1] Allen, L. K., Jacovina, M. E., and McNamara, D. S. 2016.   Cohesive features of deep text comprehension processes. In  J. Trueswell, A. Papafragou, D. Grodner, and D. Mirman  (Eds.), Proceedings of the 38th Annual Meeting of the  Cognitive Science Society in Philadelphia, PA, 2681-2686.  Austin, TX: Cognitive Science Society.   [2] Allen, L. K., McNamara, D. S., and McCrudden, M. T. 2015.  Change your mind: Investigating the effects of self- explanation in the resolution of misconceptions. In D. C.  Noelle, R. Dale, A. S. Warlaumont, J. Yoshimi, T. Matlock,  C. D. Jennings, and P. Maglio, (Eds.), Proceedings of the  37th Annual Meeting of the Cognitive Science Society (Cog  Sci 2015), 78-83. Pasadena, CA: Cognitive Science Society.   [3] Allen, L. K., Snow, E. L., and McNamara, D. S. 2015. Are  you reading my mind Modeling students' reading  comprehension skills with Natural Language Processing  techniques. In J. Baron, G. Lynch, N. Maziarz, P. Blikstein,  A. Merceron, and G. Siemens (Eds.), Proceedings of the 5th  International Learning Analytics & Knowledge Conference  (LAK'15), 246-254. Poughkeepsie, NY: ACM.        [4] Anderson, N. C., Bischof, W. F., Laidlaw, K. E., Risko, E. F.  and Kingstone, A. 2013. Recurrence quantification analysis  of eye movements. Behavior research methods, 45(3), 842- 856.   [5] Baker, R., and Ocumpaugh, J. 2015. Interaction-based affect  detection in educational software. In R. Calvo, S. D'Mello, J.  Gratch & A. Kappas (Eds.), The Oxford handbook of  affective computing, 233-245. New York: Oxford University  Press.   [6] Boekaerts, M., 2002. The on-line motivation questionnaire:  A self-report instrument to assess students context  sensitivity. Advances in motivation and achievement, 12, 77- 120.   [7] Brusilovsky, P. 1994. The construction and application of  student models in intelligent tutoring systems. Journal of  Computer and Systems Science International, 23, 70-89.   [8] Chi, M., Bassok, M., Lewis, M., Reimann, P., and Glaser, R.  1989. Self-explanations: How students study and use  examples in learning to solve problems. Cognitive Science,  13, 145-182.   [9] Coco, M. I. and Dale, R. 2013. Cross-recurrence  quantification analysis of categorical and continuous time  series: an R package. arXiv preprint arXiv:1310.0201.   [10] Crossley, S. A. and McNamara, D. S. (Eds.). 2016. Adaptive  educational technologies for literacy instruction. New York:  Taylor & Francis, Routledge.   [11] Dale, R. and Spivey, M. J., 2005. Categorical recurrence  analysis of child language. In Proceedings of the 27th annual  meeting of the cognitive science society, 530-535. Mahwah,  NJ: Lawrence Erlbaum.   [12] DMello, S., Dowell, N., and Graesser, A. 2009. Cohesion  relationships in tutorial dialogue as predictors of affective  states. In Dimitrova V., Mizoguchi R., du Boulay B.,  Graesser A. (eds.) Proceedings of the 14th International  Conference on Artificial Intelligence in Education, 916.  IOS Press, Amsterdam.   [13] DMello, S. and Graesser, A. 2015. Feeling, thinking, and  computing with affect-aware learning technologies. In R.  Calvo, S. D'Mello, J. Gratch & A. Kappas (Eds.), The Oxford  handbook of affective computing, 419-434. New York:  Oxford University Press.   [14] Duckworth, A. L., Peterson, C., Matthews, M. D., and Kelly,  D. R. 2007. Grit: Perseverance and passion for long-term  goals. Journal of personality and social psychology, 92(6),  1087-1101.   [15] Frederick, S. 2005. Cognitive reflection and decision  making. The Journal of Economic Perspectives, 19(4), 25- 42.   [16] Frishkoff, G. A., Collins-Thompson, K., Hodges, L., and  Crossley, S., 2016. Accuracy feedback improves word  learning from context: evidence from a meaning-generation  task. Reading and Writing, 29(4), 609-632.   [17] Geiser, S. and Studley, R. 2001. UC and SAT: Predictive  validity and differential impact of the SAT I and SAT II at  the University of California. Oakland, CA: University of  California.   [18] Graesser, A. C., Chipman, P., King, B., McDaniel, B., and  D'Mello, S. 2007. Emotions and learning with auto tutor.   Frontiers in Artificial Intelligence and Applications, 158,  569-571.   [19] Graesser, A. C., Chipman, P., Haynes, B. C. and Olney, A.  2005. AutoTutor: An intelligent tutoring system with mixed- initiative dialogue. IEEE Transactions on Education, 48(4),  612-618.   [20] Graesser, A. C., Lu, S., Jackson, G. T., Mitchell, H. H.,  Ventura, M., Olney, A., and Louwerse, M. M. 2004.  AutoTutor: A tutor with dialogue in natural language.  Behavior Research Methods, Instruments, & Computers,  36(2), 180-192.   [21] Graesser, A. C., VanLehn, K., Ros, C. P., Jordan, P. W. and  Harter, D. 2001. Intelligent tutoring systems with  conversational dialogue. AI magazine, 22(4), 39-51.   [22] Guilford, J. P., Christensen, P. R., Merrifield, P. R., and  Wilson, R. C. 1978. Alternate uses: Manual of instructions  and interpretation. Orange, CA: Sheridan Psychological  Services.   [23] Hannon, B. and Daneman, M. 2001. A new tool for  measuring and understanding the individual differences in  the component processes of reading comprehension. Journal  of Educational Psychology, 93, 103-128.    [24] Haswell, R. H. 2006. Automatons and automated scoring:  Drudges, black boxes, and dei ex machina. In: P. F. Ericsson  and R. H. Haswell (Eds.), Machine scoring of student essays:  Truth and consequences, 5778. Logan, UT: Utah State  University Press.    [25] Jackson, G. T., Guess, R. H., and McNamara, D. S. 2010.  Assessing cognitively complex strategy use in an untrained  domain. Topics in Cognitive Science, 2, 127-137.   [26] Jha, S. and Bhattacharyya, S. S. 2013. Learning orientation  and performance orientation: scale development and its  relationship with performance. Global Business  Review, 14(1), 43-54.   [27] MacGinitie, W. H. and MacGinitie, R. K. 1989. Gates  MacGinitie reading tests. Chicago, IL: Riverside.   [28] Magliano, J., Todar, S., Millis, K., Wiemer-Hastings, K.,  Kim, H., and McNamara, D. 2005. Changes in reading  strategies as a function of reading training: A comparison of  live and computerized training. Journal of Educational  Computing Research, 32, 185-208.   [29] Marwan, N., Romano, M. C., Thiel, M., and Kurths, J., 2007.  Recurrence plots for the analysis of complex  systems. Physics reports, 438(5), 237-329.   [30] Marwan, N., Wessel, N., Meyerfeldt, U., Schirdewan, A.,  and Kurths, J. 2002. Recurrence-plot-based measures of  complexity and their application to heart-rate-variability  data. Physical review E, 66(2), 1-8.   [31] McNamara, D. S. 2004. SERT: Self-explanation reading  training. Discourse Processes, 38, 1-30.   [32] McNamara, D. S. and Magliano, J. P. 2009. Towards a  comprehensive model of comprehension. In B. Ross (Ed.),  The psychology of learning and motivation. New York, NY:  Elsevier Science.   [33] McNamara, D. S., Levinstein, I. B., and Boonthum, C. 2004.  iSTART: Interactive strategy trainer for active reading and  thinking. Behavioral Research Methods, Instruments, &  Computers, 36, 222-233.        [34] McNamara, D. S., Boonthum, C., Levinstein, I. B., and  Millis, K. 2007. Evaluating self-explanations in iSTART:  Comparing word-based and LSA algorithms. In T. Landauer,  D.S. McNamara, S. Dennis, and W. Kintsch (Eds.),  Handbook of Latent Semantic Analysis, 227-241. Mahwah,  NJ: Erlbaum.   [35] Mednick, S., 1962. The associative basis of the creative  process. Psychological review, 69(3), 220-232.   [36] Murray, T. 1999. Authoring intelligent tutoring systems: An  analysis of the state of the art. International Journal of  Artificial Intelligence in Education, 10, 98-129.   [37] National Assessment of Educational Progress. 2011. The  nations report card: Writing 2011. Retrieved Nov. 5, 2012,  nces.ed.gov/nationsreportcard/writing.   [38] Nkambou, R., Mizoguchi, R., and Bourdeau, J. (Eds.).  2010. Advances in intelligent tutoring systems (Vol. 308).  Springer Science & Business Media.   [39] Pintrich, P. R. and De Groot, E. V. 1990. Motivational and  self-regulated learning components of classroom academic  performance. Journal of educational psychology, 82(1), 33- 40.   [40] Powell, P. 2009. Retention and writing instruction:  Implications for access and pedagogy. College Composition  and Communication, 66, 664-682.   [41] Riley, M. A., Balasubramaniam, R., and Turvey, M. T.,  1999. Recurrence quantification analysis of postural  fluctuations. Gait & posture, 9(1), 65-78.   [42] Ros, C. P., Jordan, P., Ringenberg, M., Siler, S., VanLehn,  K. and Weinstein, A. 2001. Interactive conceptual tutoring in  Atlas-Andes. In Proceedings of AI in Education 2001  Conference, 151-153.   [43] Shermis, M. and Burstein, J. (Eds.). 2003. Automated essay  scoring: A cross-disciplinary perspective. Mahwah, NJ:  Erlbaum.   [44] Shockley, K., Santana, M. V., and Fowler, C. A., 2003.  Mutual interpersonal postural constraints are involved in  cooperative conversation. Journal of Experimental   Psychology: Human Perception and Performance, 29(2),  326-332.    [45] Shute, V. J. 2011. Stealth assessment in computer-based  games to support learning. In S. Tobias & J. D. Fletcher  (Eds.), Computer games and instruction, 503-524. Charlotte,  NC: Information Age Publishers.   [46] Shute, V. J. and Kim, Y. J. 2013. Formative and stealth  assessment. In J. M. Spector, M. D. Merrill, J. Elen, and M.  J. Bishop (Eds.), Handbook of Research on Educational  Communications and Technology (4th Edition), 311-323.  New York, NY: Lawrence Erlbaum Associates, Taylor &  Francis Group.    [47] VanLehn, K. 2006. The behavior of tutoring systems.  International Journal of Artificial Intelligence in Education,  16, 227-265.    [48] VanLehn, K. 2011. The relative effectiveness of human  tutoring, intelligent tutoring systems, and other tutoring  systems. Educational Psychologist, 46, 197-221.   [49] VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P.,  Olney, A., and Rose, C. P. 2007. When are tutorial dialogues  more effective than training Cognitive Science, 31, 3-62.   [50] Varner, L. K., Jackson, G. T., Snow, E. L., and McNamara,  D. S. 2013. Does size matter Investigating user input at a  larger bandwidth. In C. Boonthum-Denecke and G. M.  Youngblood (Eds.), Proceedings of the 26th International  Florida Artificial Intelligence Research Society (FLAIRS)  Conference, 546-549. Menlo Park, CA: AAAI Press.   [51] Warschauer, M., and Ware, P. 2006. Automated writing  evaluation: Defining the classroom research agenda.  Language Teaching Research, 10, 124.   [52] Wijekumar, K. K., Harris, K. R., Graham, S., and Meyer, B.  J. F. 2016. We-Write. In S. A. Crossley and D. S. McNamara  (Eds.) Adaptive educational technologies for literacy  instruction, 184-203. New York: Taylor & Francis,  Routledge.   [53] Zbilut, J. P. and Webber, C. L., 1992. Embeddings and  delays as derived from quantification of recurrence  plots. Physics letters A, 171, 3-4, 199-203.        "}
{"index":{"_id":"46"}}
{"datatype":"inproceedings","key":"Wise:2017:HSL:3027385.3027446","author":"Wise, Alyssa Friend and Cui, Yi and Jin, Wan Qi","title":"Honing in on Social Learning Networks in MOOC Forums: Examining Critical Network Definition Decisions","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"383--392","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027446","doi":"10.1145/3027385.3027446","acmid":"3027446","publisher":"ACM","address":"New York, NY, USA","keywords":"discussion forum, massive open online courses, network partitioning, social network analysis, tie extraction","Abstract":"This study examines the impact of content-based network partitioning and tie definition on social network structures and interpretation for MOOC discussion forums. Using dynamic interrelated post and thread categorization [5] based on a previously developed natural language model [27], 817 threads containing 3124 discussion posts from 567 learners in a MOOC on the use of statistics in medicine were characterized as either related to the learning of course content or not. Content-related, non-content, and unpartitioned interaction networks were constructed based on five different tie definitions: Direct Reply, Star, Direct Reply+Star, Limited Copresence, and Total Copresence. Results showed content-related and non-content networks to have distinct characteristics at the network, community, and individual node levels, validating the usefulness of the content/non-content distinction as an analytic tool. Network properties were less sensitive to differences in tie definition with the exception of Total Copresence, which showed distinct characteristics presenting dangers for general use, but usefulness for detecting inflated social status due to superthread initiation","pdf":"Honing in on Social Learning Networks in MOOC Forums:  Examining Critical Network Definition Decisions   Alyssa Friend Wise  New York University   82 Washington Square East, 7th Floor  New York, NY 10003 USA    1-212-998-5348  alyssa.wise@nyu.edu   Yi Cui  Simon Fraser University   250-13450 102nd Avenue  Surrey, B.C. V3T 0A3 Canada    1-778-782-8046  yca231@sfu.ca  Wan Qi Jin  Simon Fraser University   250-13450 102nd Avenue  Surrey, B.C. V3T 0A3 Canada    1-778-782-8046  wanqij@sfu.ca     ABSTRACT  This study examines the impact of content-based network  partitioning and tie definition on social network structures and  interpretation for MOOC discussion forums. Using dynamic  interrelated post and thread categorization [5] based on a  previously developed natural language model [27], 817 threads  containing 3124 discussion posts from 567 learners in a MOOC  on the use of statistics in medicine were characterized as either  related to the learning of course content or not. Content-related,  non-content, and unpartitioned interaction networks were  constructed based on five different tie definitions: Direct Reply,  Star, Direct Reply+Star, Limited Copresence, and Total  Copresence. Results showed content-related and non-content  networks to have distinct characteristics at the network,  community, and individual node levels, validating the usefulness  of the content/non-content distinction as an analytic tool. Network  properties were less sensitive to differences in tie definition with  the exception of Total Copresence, which showed distinct  characteristics presenting dangers for general use, but usefulness  for detecting inflated social status due to superthread initiation.     CCS Concepts   Applied computing~E-learning   Keywords  Massive open online courses; social network analysis; discussion  forum; network partitioning; tie extraction   1. INTRODUCTION  Massive open online courses (MOOCs) present many exciting  opportunities for expanding learning by opening up accessibility  to college-style courses while at the same time bringing together  self-selected students from all over the world. However, these  online learning environments have faced substantial challenges  including low completion rates and less than satisfactory learning  experiences in many cases [11; 16]. One commonly cited  shortcoming contributing to these problems is the lack of social  interaction in MOOCs [21]. Interaction is an important element of  quality in online learning generally [25] and of particular   importance for engagement in MOOCs [16], therefore increasing  interaction is a promising route for addressing the challenges in  completion and satisfaction.    While interaction is a worthy goal, the tremendous numbers of  students involved and the diversity in learner backgrounds, needs  and intents effectively prohibit sufficient interaction of the  conventional student-to-instructor form. Many MOOCs therefore  rely on peer-to-peer communication as the primary vehicle for  interaction [15], with online discussion forums serving as the  central medium. Despite the potential for peer interaction to  improve student experiences and learning, the actual benefits of  MOOC discussion forums reaped thusfar are questionable. First,  MOOC discussions are often plagued by a host of problems that  prevent them from meeting their full potential. These problems  include low levels of participation [2], overwhelming quantity and  disorganization of posts [19], and a lack of responsivity between  learners [1]. Second, examinations of the relationship between  MOOC forum participation and learning outcomes have yielded  mixed and contradictory findings [12; 23]. These two issues have  prompted intense interest in investigating the interactions  occurring in MOOC forums and their relationship to learning (e.g.  [7; 8; 12; 15]). One common tool used in such studies is social  network analysis (SNA). SNA is a useful method to investigate  interaction in online discussion because of its focus on the  connections between actors [3; 29]. However, MOOC forum  discussions differ from those in conventional online learning  environments in several important ways. Specifically, there is an  exponentially greater number of learners interacting in perpetually  different configurations in a relatively unstructured activity for a  broad range of purposes [17]. Thus there is a need to apply SNA  methods in ways that take into account the complexity and distinct  characteristics of MOOC discussion activities.  One notable characteristic of activities in MOOC discussion  forums is that the discussion posts are made on highly diversified  topics. Unlike more traditional formal online learning discussion  forums which are usually designed and used for targeted  discussions about the course content, MOOC discussion forums  are generally open and thus host posts on topics ranging from  clarification of course content to logistical questions about  assignments, and from sharing deep personal connections with the  learning material to lightweight social interactions [24]. The  diversity of forum activities offers one possible explanation for  the lack of clear relationship to learning outcomes found thus far,  as different mode of interaction may serve different purposes. For  example, discussions directly about the course content can play a  different role in the learning process than those of a social nature  [26]. Distinctions may be more nuanced as well; for example past  work from the higher education literature indicates that  academically-related social interactions are more impactful for   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13-17, 2017, Vancouver, BC,  Canada  2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00  DOI: http://dx.doi.org/10.1145/3027385.3027446     retention than purely social ones [18]. This highlights network  partitioning as a critical but under-addressed research area in  applying SNA to MOOC discussion forum data.  Another notable characteristic of MOOC discussion forum is the  diversity of participation configurations. Unlike in formal online  learning environments where learners generally participate in  consistent ways with a regular group of people following  requirements set by the course, MOOC learners can initiate, join  or abandon discussions at any time and for any reason. These  different conditions lead to a voluminous number of threads of  vastly different sizes with shifting configurations of participants.  This in turn has implications for the meaning of and what should  be taken as an indication of interaction, and makes defining the  nature of social ties in these environments even more challenging  than in conventional online learning environments. MOOC studies  that use SNA methods have adopted several different tie  definitions to conceptualize and operationalize social ties [9; 30];  however there has not yet been a clear articulation of the typology  of these different definitions, the rationales for them, and the  implications for the results and interpretations of the analysis.  This study addresses the two underexplored areas of network  partitioning and tie definition by empirically comparing different  approaches for conceptualizing and constructing social networks  from MOOC discussion forum data.   2. LITERATURE REVIEW  2.1 SNA and MOOC Discussion Forums  SNA has been useful in studying MOOC discussions because of  its ability to extract patterns of connections between learners. It  has been used to generate understanding about the general  characteristics of social networks in MOOC discussions and  explore their relationship with learning. For instance, [15] studied  the structure of peer support networks formed in two MOOCs  designed for educators (on digital learning and mathematics  learning) and examined factors that might account for such  structures. The discussions in each course were studied as a single  network of interaction with a directed edgelist constructed based  on the exact reply structure. The study found some cross-course  consistency in general network measures and participation  patterns: both networks had clear core-periphery structure and low  edge weight; learners participation fell into four patterns,  including mutual interactions, extensive but non-mutual  interactions, thread initiation, and unresponded interaction  attempts; reciprocity was found in both networks. However, the  associations between network connections and demographic  factors were largely inconsistent across courses: the tendency that  learners connect more with those who have the same activity  pattern, teach at the same schooling level (elementary vs high  school), or live in the same state or country were only found in the  course on digital learning.   [14] also investigated the factors that influence social connections.  The study was conducted on two instances of a programming  MOOC, offered in English and Spanish respectively. A directed  social network was extracted from the whole forum, based on the  exact reply structure. Like [15], this work found some consistent  cross-offering results, such as reciprocity and performance-based  homophily; they also failed to find cross-offering consistency in  the association between social connection and learners similarity  in geographical location. In the same study, [14] examined the  association between social centrality degree (the number of direct  connections a node has), closeness (average of the shortest path  lengths from a node to all other nodes in the network),   betweenness (the number of times a node is part of the shortest  path between any two other nodes in the network) and academic  performance (operationalized as completion and distinction).  Weighted degree was found to be significantly associated with  learning outcome across offerings; effect of betweenness and  closeness were only found in the Spanish offering.   [12] also examined associations between social centrality and  academic performance (operationalized as no certificate,  completion, or distinction). They conducted the study on MOOCs  in algebra and finance. Undirected social networks were extracted  from the whole discussion forums based on copresence in the  same thread or subthread (how these distinctions were made was  not explained). The results found from the two courses were  inconsistent: degree and betweenness were positively correlated  with learning performance in the algebra course while no  significant correlation was found between any centrality index and  learning performance in the finance course. Contradicting to the  findings of [14], [12] found learners tend to talk to those who are  in different performance groups than themselves.  In summary, these studies findings about social networks and  learning are largely inconsistent or contradictory. One possible  explanation is MOOC discussion forums are used for highly  diversified purposes, such as understanding learning materials,  clarifying course policy, and developing social connections [4;  24]. Consequently, analyzing the discussion forum as one social  network may compile interactions with distinct natures together,  confounding relationships and concealing important patterns.  Another possible explanation relates to the different decisions  made around how to define ties in the network.  2.2 Partitioning Social Networks  2.2.1 Exogenous Partitioning   To address the problems with studying MOOC discussion forum  as a whole network, some researchers have examined the  interactions and networks in a more refined manner by paying  specific attentions to defining network boundaries. One  straightforward approach to doing so uses the presence of sub- forum designations for categorization. For example, [7] created an  exogenously defined boundary by partitioning the social network  in a business MOOC based on the presence of seven sub-forums,  including Readings, Lectures, Cases, Final Project, Course  Material Feedback, Technical Feedback, and Study Groups. [7]  also examined sub-forum networks week by week. The networks  were extracted based on copresence within a thread. It was found  although all sub-forum networks followed a similar trend in  structural shift across the different weeks (from centralized to  dispersed), they differed in several ways: sub-forums were  participated in largely by distinct groups of learners and only a  small percentage of learners participated across sub-forums;  participants in different sub-forums showed different levels of  persistence over time, with the Cases sub-forum for discussing  learning material showing the highest level of persistence.   In another study, [8] explored how social network structures  influenced information diffusion in two successive offerings of a  business MOOC. Social networks were extracted based on  copresence in thread and partitioned based on eight sub-forums,  including Readings, Lectures, Cases, Final Project, Questions for  Professor, Course Material Feedback, Technical Feedback, and  Study Groups. In addition, networks formed based only on  frequent ties were compared with the unfiltered networks.  Interesting cross-offering results were found. First, in both  offerings, the proportion of one-off ties (edges with a weight of     one) was different across sub-forums: the Feedback sub-forum  (containing posts mostly related to technical support and gratitude  expressions) had the highest proportion of one-off ties while the  Cases sub-forum had the fewest. This indicates learners  engagement in different types of sub-forum activities follow  different conventions. Second, sub-forums demonstrated different  densities of interaction: the Cases and Final Project sub-forums  were more cohesive than the Study Groups sub-forum; as a result,  information diffusion efficiency also differed across sub-forums.  Taken together, these results indicate that patterns of interaction  vary across different kinds of forum activities; notably there  seemed to be key differences between sub-forums whose purpose  directly related to the learning of course content and those which  did not. In yet another example, [10] used sub-forums to explore  social networks in a finance MOOC. They mapped networks in  sub-forums dedicated to content-related issues (lectures, exercises  and quizzes) and investigated user roles from both social and  semantic perspectives.  These studies examined networks in MOOC forums in a more  nuanced way by partitioning the whole forum into the component  sub-forums. However, segmenting networks in a structural way  based on sub-forums is non-optimal for two reasons. First, each  MOOC sets different course-specific sub-forums, therefore the  generalizability of findings based on such divisions are limited.  Second, prior studies have shown that cross posting is common in  MOOCs [22]. Thus, there is no guarantee that learners make posts  in the appropriate designated sub-forum. Consequently, social  networks built based on sub-forums may not accurately reflect the  nature of relationships formed in forum interactions.    2.2.2 Endogenous Partitioning  In contrast to using sub-forums as pre-defined boundaries, [6]  created an endogenous boundary by conducting separate network  analyses for all learners and only active learners that made >4  posts. [6] looked at learners language characteristics and  examined social networks from the perspectives of learning  performance and social position in a MOOC on infrastructure.  The social network was constructed based on the direct reply  structure. By analyzing learners discussion forum contributions,  they selected five discourse dimensions to focus on: narrativity,  deep cohesion, referential cohesion, syntactic simplicity, and word  concreteness. They then conducted mixed-effects modeling to  identify the association between discourse features and social  centrality (indexed by degree, closeness, and betweenness), as  well as the association between discourse features and the final  course grade. In conducting these analyses they compared results  computed separately for all forum users and the group of active  users. Discourse features were found useful for predicting  learners performance and social centrality, and the model worked  better for active users than for all users. It was also found learners  with higher social centrality had different discourse features than  those with higher performance, such as lower referential cohesion,  less abstract words and simple syntactic structure. The distinction  between socially central learners and high performers and the  association of the social learners with features indicting more  superficial discourse dramatically indicates the need to consider  forum activities directly related to the learning of course content  and those that focused on social purposes separately.   In another study, [20] also focused on the type of activities  learners participated in while examining the development of social  networks and analyzed separately all users and the group of  regular users who participated in at least three weeks in a MOOC  on solar energy. An undirected social network was extracted   based on copresence in the thread. Adding an additional layer of  endogenous boundaries to their network, they classified the posts  contributed by regular users into five categories: cognitive task  (comments about quizzes and assignment), social task (learner  emotions about tasks), cognitive non-task (learners engage with  subject outside of assignments), social non-task (purely social  aspects), and an additional category for administrative and  technical issues. They then analyzed what qualitative attributes  were associated with regular participants network formation.  First, regular users were found to have different social networks  than all learners when examined at both the individual and group  levels. Second, social non-task and cognitive task were the  dominant type of posts in the discussion forum. [20] also found  the topic of conversations were not significant for network  formation modeling. The authors suggested this might have to do  with the fact that the importance of topics may vary in different  stages in the course, but interactions in different course stages  were not modeled separately.     Findings from the above two studies as well as [7] and [8]  disagree on the relationship between how students interact in  discussions and whether or not the interactions are related to the  course content, highlighting the need for further investigation.  Content-related and non-content interactions refer to different  genres of topics that play different roles in the learning process.  Content-related interactions are directly related to the learning of  the course material while non-content interactions may serve a  highly diversified array of purposes (for example logistic or  social) that can contribute to, but are generally more distal in their  impact on learning. As shown in [6] these different ways of  engaging may attract distinct collections of learners. Even when  learners participate in both kinds of interactions, they may play  different roles and show different participation patterns in the two  contexts. Furthermore, from a theoretical perspective there is an  expectation that participation in content-related interactions and  social networks would be more predictive of learning outcomes  than participation in non-content interactions. Thus for both  conceptual and empirical reasons, it makes sense to analyze the  social networks formed based on these activities separately.   2.3 Defining Social Ties   As noted previously SNA studies of MOOCs have used varying  definitions to construct social ties. For example, the studies cited  in this paper thusfar have used copresence and (directed or  undirected) direct reply to construct ties. Tie definition is critical  for SNA studies because different ways of establishing ties carry  different assumptions about the nature of the interaction in social  networks that have implications for network outcomes and their  interpretation. This issue, however, has not been well addressed in  the literature. The majority of studies simply establish their tie  definition without giving any explanation or rationale.  Two types of social networks have been commonly constructed  using the reply structure of threaded discussions in discussion  forums that adopt multi-level post structure. The first type is the  network of speaking interaction. In this type of network, a tie is  defined as speaking to someone. This type of network formation  mechanism is based strictly on the reply relationship in the  forums. One approach that adopts this mechanism is direct reply  relationship, in which a tie is constructed only if there is a direct  reply relationship between two nodes in the same thread, either  between the thread starter and a reply post addressed to it, or  between a reply post and its further reply, which is commonly  seen in discussion forums that support multiple levels of posts.  Direct Reply only maps the speaking interaction between users,     without making any assumption about others who may have been  informed by the post but not replied to it directly. This is perhaps  the most straightforward approach to tie formation in online  discussions and is used by many studies [e.g. 14; 15]. However,  an important problem with this scheme is there is no guarantee  that when making a post, forum users always choose the  appropriate location and level. In addition, some discussion  forums may only support limited levels of posts. For instance,  discussion forums on the Coursera platform only support three  levels of posts. When a response is made to a level 3 post, it is  displayed as another level 3 post. How accurately the reply  structure produced in such forums reflects the actual relations  among learners is questionable. As the Direct Reply tie definition  extracts social ties strictly according to the logged reply structure,  it is most vulnerable to such problems.   To work around such concerns, [30] proposed the Star tie  definition for network extraction when investigating relationships  between course engagement, performance and social connectivity.  Star also defines a tie as speaking to someone; but different than  Direct Reply which distinguishes multiple levels of posts and map  ties as direct contact between nodes on different levels, Star  considers all posts in the same thread being tied only to the thread  starter. The rational is even if a reply post was not addressed  directly to the starter, it was made in the context of the thread and  should address the topic set by it, thus the tie is considered a  traceable contact within the scope of a thread. Star structure  highlights the importance of thread starter; however, as it does not  distinguish between different levels of replies, it overlooks  connections formed between learners within the same thread.     A hybrid scheme that combines Direct Reply and Star was  introduced by [9] in their investigation of social structure in online  discussion forums using natural language processing and SNA  methods. This scheme also makes use of traceable reply  relationships to map ties. Ties are constructed both between posts  on different levels and between each post and the thread starter.  This scheme produces a more comprehensive network than the  first two schemes alone, but still only considered the act of  speaking in a threaded discussion. As a learner could access  multiple posts made by people interested in the same topic, social  relationships could thus form among them through listening to  each other. Therefore, the methods that strictly follow the  speaking contacts could leave out the interactions and  relationships between learners who never speak to each other  directly but have spoken on the same topic in the same thread.   A different approach that addresses this issue is to create a  copresence network that embodies coparticipation relationships  among nodes. In a copresence network, a tie is defined as being  present in the same part of a discussion. Two nodes can have a  tie as long as they are in the same thread or subthread, without the  need to have directly replied to each other. Thus ties are formed  both hierarchically between child-parent nodes and horizontally  between two nodes on the same level in the reply structure. This  type of network formation mechanism represents the notion of  online discussions as collective conversations rather than single  streams of individual replies. Within the genre of copresence  networks, the usual scheme is total copresence where any two  nodes in the same thread are considered having a tie [9].  However, when this scheme is used to map interaction, the size of  threads can be problematic. It might be reasonable to assume that  a participant in a thread with a small number of replies has ties  with all others in the same thread through reading their posts, but  this assumption becomes less reasonable when the number of   replies is very large. To address this problem, another scheme is  to set a cap on the reasonable number of posts in the same thread  (or subthread) that a participant would read and use this to create a  measure of limited copresence. It is also possible to assign  deceased weight to ties in large threads.  The five tie definitions  are summarized in Figure 1.      Figure 1. Tie definitions on a continuum   3. STUDY FRAMING  In this study, we examine the effects of partitioning a MOOC  forum social network according to whether or not the discussion  interactions are related to the course content. This can bring many  benefits. First, examining the two networks separately can provide  understanding of network properties and formation mechanisms  for different kinds of interactions and allows for more accurate  attribution of factors that influence such processes. Second, it can  reveal the characteristics of participants in the two networks and  help to identify learner subpopulations so as to allow for a more  specialized understanding of different learning needs. Third, it  may allow for more nuanced prediction of learning outcomes.  Finally, as content-related MOOC forum activities share many  characteristics with conventional online learning forums, this  approach allows for more aligned comparisons with previous  online discussion SNA research findings. In addition, we use five  tie formation definitions to investigate their impact on the  consequent networks extracted. To allow for parallel comparisons  across tie definitions, this phase of the work examines only  undirected networks.  RQ1: What effects do different tie formation mechanisms have on  the resulting network characteristics and interpretation  RQ2: In what ways do social networks extracted from content- related and non-content activities show distinct characteristics  from each other and the unpartitioned network   4. METHODS  4.1 Data Source   This study used data from StatMed14, a completed MOOC  offered in 2014 on Stanford open-source platform Lagunita. The  course is an introductory course on probability and statistics with  a special focus on statistics in medical studies. The course  provided a discussion forum for interaction in nine topic areas,  including General, Video, Homework, Course Material Feedback,  External Resources, Tech Support, Introductions, Study Group,  and Platform Feedback. Learners were invited to post questions  and comments for response by peers, the TA and the instructor.  Forum information provided in the dataset included the following:  thread id; post id; user id; post position in thread (thread starting  post, reply post, or reply-to-reply post); parent post; post text; post  creation date and time; and number of votes post received. Thread  titles were not included. The discussion forum was participated by  568 unique users. They made 817 thread starting posts, 1277 reply  posts, and 1035 reply-to-reply posts in the forums. Of the 817  threads, 117 received no reply. Five reply posts that contained  non-English language or only punctuation were removed, leaving  a total corpus of 817 threads with 2307 replies made by 567 users.     4.2 Thread Classification  The 817 threads were classified as either being content-related or  non-content using a unigram and bigram based-model built on  manually-coded starting posts from a prior offering of the course  [28]. In previous work, the model demonstrated good reliability  on StatMed14 data for both thread starting and reply posts  (accuracy > .81) [27; 28]. This model was used in conjunction  with dynamic interrelated post and thread categorization DIPTiC  [5] to categorize threads by comparing the model classification of  thread starting post and distribution of replies. This additional step  increased the estimation of classification accuracy to .88 [5].  Using this comprehensive characterization method, a total of 468  threads containing 1446 replies were labeled as content-related  and a total of 349 threads containing 861 replies were labeled as  non-content.   4.3 Network Construction  4.3.1 Network Participants  The nodelist was extracted from discussion forum data using user  id of posts. It was found that of the 567 forum users extracted  from the cleaned data, 178 participated only in content-related  threads, 232 participated only in non-content threads, and 157  participated in both kinds of threads. Thus the number of nodes  for the unpartitioned network, content-related network, and non- content networks were 567, 335, and 389 respectively.   4.3.2 Tie Extraction  Edgelists were extracted using five tie definitions.  Direct Reply: The author of each post is connected with the  author of its parent post; this represents the actual reply structure  (see Figure 2a). Using this definition, a total of 2307 ties were  extracted from the unpartitioned network; after removing 286 self- loops, 2021 ties representing 1086 unique edges remained,  including 1249 content-related ties representing 625 unique edges  and 772 non-content ties representing 551 unique edges.   Star: The author of each reply and reply-to-reply post is  connected with the author of the thread starting post (see Figure  2b). Using this definition, a total of 2307 ties were extracted from  the unpartitioned network; after removing 502 self-loops, 1805  ties representing 1116 unique edges remained, including 1092  content-related ties representing 625 unique edges and 713 non- content ties representing 558 unique edges.  Direct Reply + Star: Ties defined in both Direct Reply and Star  were included but the same tie was never counted more than one  time (see Figure 2c). Using this definition, a total of 3339 ties  were extracted from the unpartitioned network; after removing  683 self-loops, 2656 ties representing 1292 unique edges  remained, including 1697 content-related ties representing 747  unique edges and 959 non-content ties representing 643 unique  edges.   Total Copresence: All authors in the same thread are connected  with each other (see Figure 2d). Using a VBA script written for  this definition, a total of 15299 ties were extracted from the  unpartitioned network; after removing 1992 self-loops, 13307 ties  representing 5578 unique edges remained, including 7018  content-related ties representing 1133 unique edges and 6289 non- content ties representing 4641 unique edges.   Limited Copresence: All users in small threads (<5 replies) are  connected to each other; in larger threads users are connected to  all other users in their sub-thread and the thread starter only (see  Figure 2e). Of all 489 subthreads in larger threads, only 69 (14%)  have more than four posts. Using a VBA script written for this   definition, a total of 5313 edges were extracted from the  unpartitioned network; after removing 1066 self-loops, 4247 ties  representing 1456 unique edges remained, including 2879  content-related ties representing 848 unique edges and 1368 non- content ties representing 724 unique edges.      Figure 2. Ties based on five definitions    4.3.3 Network Construction and Visualization   To investigate the influence of tie definitions on the  characteristics of the resultant networks, as well as to investigate  the characteristics of social networks formed in content-related  and non-content activities, three undirected networks  (unpartitioned, content-related, and non-content) were constructed  based on each of the five definitions. Gephi 0.9.1. for mac was  used to compute network measures and visualize the networks  employing the Force Atlas layout algorithm.    5. RESULTS AND DISCUSSION   5.1 Networks Formed by Five Tie Definitions  The characteristics of the fifteen networks revealed several  findings about the impact of tie definition. First, reply structure- based tie definitions (Direct Reply, Star, and Direct Reply+Star)  produced networks with similar structures and network measures  (see Figure 3a - c and Table 1). Although Limited Copresence  extracts network based on coparticipation, the resulted networks  are similar to those produced using reply-structure-based  definitions, especially Direct Reply+Star (see Figure 3d and Table  1). It is worth investigating to what extent this is dependent on the  technical limitations of the MOOC to three levels of threading as  well as the actual patterns of post distribution in this particular  data set. This might usefully be done with synthetically generated  data. It would also be interesting to examine how different limits  for the maximum number of presumably shared posts in a thread  impacts the structure of Limited Copresence networks.    Table 1. Network measures of five unpartitioned networks    DR S DR+S LC TC  # of edges 1086 1116 1292 1456 5578  Avg node   degree (SD)  3.83   (14.54)  3.94   (12.92)  4.56   (15.48)  5.14   (16.69)  19.68   (36.63)  Avg edge   weight (SD)  1.86   (3.38)  1.62   (2.50)  2.06   (4.36)  2.92   (9.49)  2.39   (14.98)   DR = Direct Reply S = Star  DR+S = Direct Reply+Star LC =  Limited Copresence TC = Total Copresence   Second, Total Copresence appeared to produce dramatically  distinct networks than other tie definitions. For instance, the other  four unpartitioned networks all have three major modules, each  dominated by a single node of high centrality (see Figure 3a1, b1,  c1, d1) and the two largest modules containing u1 and u417 are        Only the primary components are shown in this figure. Node size represents degree. Color indicates module.  Figure 3. Social networks constructed using five definitions  relatively more connected with each other than with the module  containing u2. Such structure indicates interactions in these  networks revolve around these three focal nodes with participants  in the modules containing u1 and u417 tending to interact more  with each other than with participants in the module containing  u2. In contrast the Total Copresence network shows a very  different picture (see Figure 3e1): one module contains the  dominant central nodes of u1 and u417, while the second module  has blown up into a balloon of many similar-degree  interconnected nodes.  An examination of the posts made by u1, u2, and u417 revealed  both u1 and u417 were part of the instructional team who  responded to and received responses from a large number of  learners across multiple threads. This explains their prominence  across networks made with each of the five tie definitions. In  contrast, u2 was a learner who started a single socializing thread  at the beginning of the course which received 82 replies and 10  replies to replies. Although u2 only made this single post, as a  thread starter he/she gained high degree in Direct Reply, Star, and   Direct Reply+Star networks. Moreover, as this thread had mostly  top-level replies, Limited Copresence resulted in very similar  results to the reply-based networks. In Total Copresence networks  however, all nodes in this giant thread have the same degree  contribution from the thread, reducing the central role of u2 as the  thread starter. This result suggests that the Total Copresence tie  definition presents dangers for general use due to the  disproportionate influence of large threads; however can be useful  for detecting users with inflated social status due to  superthreads initiation.     5.2 Content vs Non-Content Networks  5.2.1 Network and Module Levels  A comparison of networks built using all five definitions revealed  content-related and non-content networks to have distinct  characteristics. First, the difference is revealed by their network  measures. Using the Limited Copresence tie definition, the  content-related network had an average node degree of 5.06  compared to 3.72 in the non-content network; similarly, the     average edge weight in the content-related network was 3.40,  compared to 1.89 for the non-content network.  These differences suggest users in the content-related network  both interacted with more people and had more repeated  interactions with the same people than those in non-content  network. Moreover, the major modules in the two networks show  different patterns (see Figure 3d2 and d3). The content-related  network contains two closely interconnected modules with u1 and  u417 as hubs. In the non-content network, all three major modules  have less interconnections between them, especially the module  containing u2, which is less tightly connected to the ones  containing u1 and u417. This suggests there were more  interactions between members of different modules in the content- related network than in the non-content one.   Another important finding at network level across all five tie  definitions was that the unpartitioned network highly resembled  the non-content network in several ways, such as the number of  major modules, the relations between the modules, as well as the  dominant participants. While this may be the result of specific  features of content-related and non-content interactions in this  data set, it illustrates well the danger that without partitioning,  non-content interactions may mask or distort important content- related connection in the network analysis.   5.2.2 Individual Level: Learners  To examine content-related and non-content networks at  individual level, the top 10 learners ranked by degree centrality in  unpartitioned, content-related, and non-content networks built  through Limited Copresence were identified and both their  properties in the network and actual discussion forum posts were  examined (see Table 2).   Table 2. Top 10 learners ranked by degree centrality in three   networks (Limited Copresence)      First, except for u2 and u30 who only participated in non-content  network, other top learners all participated to some extent in both  networks. However, for the 15 remaining distinct learners on the  lists, only u10, u216, u21 appear on the high degree lists for both  content-related and non-content networks while the rest of the top  ranked learners have high degree in one network but not both.  This indicates the top players in the two networks are largely  different people. Learners who are highly connected in the  content-related network are not necessarily highly connected in  non-content network, and vice versa. The overall list thus  represents a combination of two very different kinds of top users.    5.2.3 Individual Level: Instructors  To investigate instructors roles in discussing the course content,  the modules that contained instructional team members u1 (169  nodes) and u417 (88 nodes) in the content-related network based  on Limited Copresence were compared (see Figure 4).    Several similarities and differences between the two modules  were noted. First, in both modules, a large proportion of learners  are connected only to the instructor and not to any peers in the      Figure 4. Instructor modules in content-related network   (Limited Copresence)   module, indicating that although peer interaction is considered a  potential source of learning support in MOOCs [16], instructor- learner interaction can often eclipse this and is still the primary  form of interaction for many learners. Second, an important  difference between the two modules is that there are more  interconnections among learners in Instructor Module 1 while  Instructor Module 2 is largely hub-and-spoke shaped: the average  degree (excluding the central node) in Instructor Module 1 is 2.91  compared to 1.93 for Instructor Module 2. These characteristics  indicate that learners in Instructor Module 1 had more learner- learner interactions than those in Instructor Module 2. To explore  potential instructor influence that may partly account for the  differences in learner-learner connections, the two instructors  posts in all content-related threads were examined, revealing two  distinct characteristics of instructors forum activities.   First, the two instructors had distinct posting patterns. Instructor  u1 made 353 posts in 240 content-related threads, including 216  reply posts to thread starting posts, and 137 reply posts to  subthread starting posts or other posts in subthreads. This diverse  posting pattern indicates the instructor not only revisited the  threads that he/she has participated in, but commented on other  learners replies to learner-initiated threads. In contrast, instructor  u417 made 121 replies to 119 content-related thread starting posts  and never made any replies to subthreads. This indicates that this  instructor only addressed the subset of all students that initiated  threads (and not learners who participated only as responders). It  is also notable that neither instructor initiated any content-related  threads. While thread-starting can lead to centrality in a network,  these instructors achieved the same effect simply through the  volume of their replies and the attention they attracted.  Second, the instructors appeared to use distinct intervention and  social techniques in posting. In many cases, instructor u1 used  diverse techniques that seemed intended to inspire and help  learners to work out the answer or solution to a question  themselves (e.g. Looks like you are making great progress!...You  are correct about dealing with categorical data, and the  observations are certainly correlated!  Think about it again using  the hint and let me know if you have any other questions).  These techniques may have not only invited the original poster to  post again, but also encouraged other learners to participate and  form a tie with the original poster. U1 also used a variety of social  presence indicators such as greetings and social presence in their  messages. In contrast, instructor u417 usually provided  straightforward answers or instructions to learners questions and  seldom used social techniques. Future research can probe in depth  the ways in which instructors different approaches to reply and     social techniques may be associated with and influence learner- learner interaction in MOOC discussions.    6. IMPLICATIONS   The complexity of interactions in MOOC discussion forum  challenges the research community to come up with suitable  methods to understand learning and teaching. In this study, we  investigated characteristics of social networks partitioned based  on content-relatedness and found that the content-related social  network has distinct characteristics compare to the non-content  network. An important value of this partitioning method is that it  is not affected by variations in sub-forum structures, thus allowing  for cross course and platform comparisons. Moreover, we  investigated the effect of different tie definitions on network  characteristics and interpretation. We found Total Copresence  produces social networks with distinct characteristics and is useful  for detecting inflated degree due to superthreads. These findings  have important research and pedagogical implications for MOOC  and large-scale online learning in general.  First, our work indicates that network analysis method should take  into account the nature of MOOC discussion forums and partition  social networks based on pedagogical purpose. Given the  complexity of activities and users, unpartitioned MOOC networks  may be too comprehensive to be useful for specific research  purposes, such as identifying competent learners [13] and  investigating correlations between social properties and learning  outcome [12; 14]. For instance, in the discussion forum we  studied, 41% of forum participants in the unpartitioned network  never participated in any content-related interaction. Including  them into the network when analyzing learning-related social  connections may result in distorted network properties. Moreover,  as revealed by our study, the unpartitioned network can be  strongly influenced by the non-content network, thus obscuring  the characteristics of the content-related network. This means  analyzing the unpartitioned network can lead to overlooking the  characteristics of learning-related interactions. To make sure that  content-partitioned networks accurately represent the intended  division and are comparable across studies, it is critical to use a  valid and generalizable partition method. The endogenous  content/non-content partitioning used here appears superior to  exogenous partitioning based on sub-forums both because it  circumvents the problem of cross-posting across sub-forums [22]  and because it allows for comparisons across courses using  different sub-forum divisions.  Second, our research also improved understanding of the  assumptions and implications of different network tie definitions.  While the Direct Reply, Star, Direct Reply+Star, and Limited  Copresence definitions produced similar networks, the Total  Copresence networks revealed a disproportionate influence of  large threads. This leads us to caution the interpretation of  networks created with this tie definition and disfavor its use. We  believe this is an important contribution for SNA work in MOOCs  moving forward due to common presence of large threads in  MOOC discussions (and previously prevalent use of this tie  definition [e.g. 7, 8, 12, 20]). It is worthwhile to note, however,  that networks based on Total Copresence ties can be useful as a  diagnostic for detecting inflated social status due to superthreads.  Finally, this research also has important pedagogical implications.  First, instructors can identify competent and influential learners in  content-related network who are suitable candidates for  community TA or help giver roles. Second, they could identify  disconnected or marginalized users who require additional  assistance. Third, instructors could identify learners that only   participate in non-content networks and invite them to take part in  learning-related interactions. Fourth, learners network statuses  could also also be provided to them directly to help them make  sense of their learning behaviors. Showing learners the extent to  which they are a central or peripheral part of a connected structure  in a content-related network and how their status may be related  to their learning could help them adjust their participation patterns  for better learning outcomes. Finally, instructors could be  presented with the association between their participation patterns  and the patterns in learner-learner interactions in content-related  network to help them make informed participation decision and  adjust their facilitation techniques.   The goal of this study was to investigate the basic importance and  usefulness of distinguishing different tie definitions and  partitioning social networks based on content-relatedness. As only  a single dataset was used, we would not expect the specific  findings from this study to generalize to other MOOC forum data.  However, the importance of purposefully selecting an appropriate  tie definition and examining content/non-content distinctions was  established. Future work can move forward to examine cross  course similarities and differences as well as patterns in the  growth and evolution of networks over time.    7. ACKNOWLEDGMENTS  We thank Stanford University and the MOOCPosts Dataset team  for their assistance in accessing and working with the data.   8. REFERENCES  [1] Agrawal, A., Venkatraman, J., Leonard, S., and Paepcke, A.   2015. YouEDU: addressing confusion in MOOC discussion  forums by recommending instructional video clips. In  Proceedings of the 8th International Conference on  Education Data Mining (Madrid, Spain, June 26 - 29, 2015).  ACM, New York, NY, USA, 297-304.    [2] Breslow, L., Pritchard, D. E., DeBoer, J., Stump, G. S., Ho,  A. D., and Seaton, D. T. 2013. Studying learning in the  worldwide classroom research into edX's first MOOC.  Research & Practice In Assessment, 8, 13-25.    [3] Cho, H., Gay, G., Davidson, B., and Ingraffea, A. 2007.  Social networks, communication styles, and learning  performance in a CSCL community. Computers and  Education, 49, 2, 309-329.    [4] Cui, Y., and Wise, A. F. 2015. Identifying content-related  threads in MOOC discussion forums. In Proceedings of the  2nd ACM Conference on Learning @ scale (Vancouver,  Canada, March 14-18, 2015). ACM, New York, NY, USA,  299-303. DOI= 10.1145/2724660.2728679.    [5] Cui, Y., Wise, A. F., and Jin, W.Q. (in review). Humans and  machines together: Improving characterization of large scale  online discussions through dynamic interrelated post and  thread categorization (DIPTiC).   [6] Dowell, N., Skrypnyk, O., Joksimovi, S., Graesser, A. C.,  Dawson, S., Gaevi, D., Vries, P. d., Hennis, T., and  Kovanovi, V. 2015. Modeling learners social centrality and  performance through language and discourse. In Proceedings  of the 8th International Conference on Educational Data  Mining (Madrid, Spain, June 26-29, 2015). ACM, New York,  NY, USA, 250-257.   [7] Gillani, N., and Eynon, R. 2014. Communication patterns in  massively open online courses. The Internet and Higher  Education, 23, 18-26.     [8] Gillani, N., Yasseri, T., Eynon, R., and Hjorth, I. 2014.  Structural limitations of learning in a crowd: communication  vulnerability and information diffusion in MOOCs. Nature  Scientific Reports, 4. DOI= 10.1038/srep06447.   [9] Gruzd, A.A., and Haythornthwaite, C. 2008. Automated  discovery and analysis of social networks from threaded  discussions. In Proceedings of the International Network of  Social Network Analysts 2008, St. Pete Beach (St. Pete  Beach, USA.2008). Retrieved Sep 28, 2016:    http://hdl.handle.net/10150/105081.   [10] Hecking, T., Chounta, I. A., and Hoppe, H. U. 2016.  Investigating social and semantic user roles in MOOC  discussion forums. In Proceedings of the 6th International  Conference on Learning Analytics and Knowledge  (Edinburgh, UK, April 25-29, 2016) ACM New York, NY,  USA, 198-207. DOI= 10.1145/2883851.2883924.   [11] Hew, K.F., and Cheung, W.S. 2014. Students and  instructors use of massive open online courses (MOOCs):  motivations and challenges. Educational Research Review,  12, 45-58. DOI= 10.1016/j.edurev.2014.05.001   [12] Jiang, S., Fitzhugh, S. M., and Warschauer, M. 2014. Social  positioning and performance in MOOCs. In Proceedings of  Graph-Based Educational Data Mining Workshop at the 7th  International Conference on Educational Data Mining  (London, United Kingdom, 2014). CEUR-WS, 55-58.    [13] Jiang, Z., Zhang, Y., Liu, C., and Li, X. 2015. Influence  analysis by heterogeneous network in MOOC forums: what  can we discover In Proceedings of the 8th International  Conference on Education Data Mining (Madrid, Spain, June  26 - 29, 2015). ACM, New York, NY, USA, 242-249.    [14] Joksimovi, S., Manataki, A., Gaevi, D., Dawson, S.,  Kovanovi, V., & De Kereki, I. F. 2016. Translating network  position into performance: importance of centrality in  different network configurations. In Proceedings of the 6th  International Conference on Learning Analytics &  Knowledge (Edinburgh, UK, April 25-29, 2016) ACM New  York, NY, USA, 314-323. DOI= 10.1145/2883851.2883928.   [15] Kellogg, S., Booth, S., and Oliver, K. 2014. A social network  perspective on peer supported learning in MOOCs for  educators. The International Review of Research in Open  and Distributed Learning, 15, 5. DOI=  10.19173/irrodl.v15i5.1852.   [16] Khalil, H., and Ebner, M. 2013. How satisfied are you with  your MOOC - a research study on interaction in huge  online courses. In Proceedings of EdMedia 2013 (Victoria,  Canada, June 24, 2013). AACE, 830-839.    [17] Kizilcec, R. F., Piech, C., and Schneider, E. 2013.  Deconstructing disengagement: analyzing learner  subpopulations in massive open online courses. In  Proceedings of the 3rd International Conference on  Learning Analytics and Knowledge (Leuven, Belgium, April  8 - 12, 2013). ACM New York, NY, USA, 170-179. DOI=  0.1145/2460296.2460330.   [18] Kuh, G. 2002. From promise to progress: how colleges and  universities are using student engagement results to improve  collegiate quality. National Survey of Student Engagement  Annual Report. Bloomington, IN: Indiana University.   [19] McGuire, R. 2013. Building a sense of community in  MOOCs. Campus Technology, 26, 12, 31-33.    [20] Poquet, L., and Dawson, S. 2016. Untangling MOOC learner  networks. In Proceedings of the 6th International Conference  on Learning Analytics and Knowledge (Edinburgh, UK,  April 25-29, 2016) ACM New York, NY, USA, 208-212.  DOI= 10.1145/2883851.2883919.   [21] Ros, C. P., and Ferschke, O. 2016. Technology support for  discussion based learning: from computer supported  collaborative learning to the future of Massive Open Online  Courses. International Journal of Artificial Intelligence in  Education, 26, 2, 660-678.   [22] Rossi, L.A., and Gnawali, O. 2014. Language independent  analysis and classification of discussion threads in Coursera  MOOC forums. In Proceedings of 2014 IEEE 15th  International Conference on Information Reuse and  Integration (San Francisco, USA, August 13 - 14, 2014).  IEEE, 654-661. DOI= 10.1109/IRI.2014.7051952.    [23] Santos, J.L., Klerkx, J., Duval, E., Gago, D., and Rodrguez,  L. 2014. Success, activity and drop-outs in MOOCs: an  exploratory study on the UNED COMA courses. In  Proceedings of the 4th International Conference on Learning  Analytics and Knowledge (Indianapolis, USA, March 24 -  28, 2014). ACM New York, NY, USA, 98-102. DOI=  10.1145/2567574.2567627.   [24] Stump, G. S., DeBoer, J., Whittinghill, J., and Breslow, L.  2013. Development of a framework to classify MOOC  discussion forum posts: methodology and challenges. In  Proceedings of NIPS 2013 Workshop on Data Driven  Education (Lake Tahoe, United States, December 5 - 8,  2013). NIPS Foundation, 1-20.    [25] Trentin, G. 2000. The quality-interactivity relationship in  distance education. Educational Technology, 40, 1, 17-27.    [26] Wise, A. F., Chang, J., Duffy, T. M., and del Valle, R. 2004.  The effects of teacher social presence on student satisfaction,  engagement, and learning. Journal of Educational  Computing Research, 31, 3, 247-271.   [27] Wise, A. F., Cui, Y., Jin, W., and Vytasek, J. 2017. Mining  for gold: identifying content-related MOOC discussion  threads across domains through linguistic modeling. The  Internet and Higher Education, 32, 11-28.    [28] Wise, A. F., Cui, Y., and Vytasek, J. 2016. Bringing order to  chaos in MOOC discussion forums with content-related  thread identification. In Proceedings of the 6th International  Conference on Learning Analytics and Knowledge  (Edinburgh, UK, April 25-29, 2016) ACM New York, NY,  USA, 188-197. DOI= 10.1145/2883851.2883916.    [29] Yusof, N., and Rahman, A. A. 2009. Students' interactions in  online asynchronous discussion forum: a social network  analysis. In Proceedings of 2009 International Conference  on Education Technology and Computer (Singapore,  Singapore, Apr 17  20, 2009). IEEE, 25-29.   [30] Zhu, M., Bergner, Y., Zhang, Y., Baker, R., Wang, Y., and  Paquette, L. 2016. Longitudinal engagement, performance,  and social connectivity: a MOOC case study using  exponential random graph models. In Proceedings of the 6th  International Conference on Learning Analytics and  Knowledge (Edinburgh, UK, April 25-29, 2016). ACM, New  York, NY, USA, 223-230. DOI= 10.1145/288385.     "}
{"index":{"_id":"47"}}
{"datatype":"inproceedings","key":"Slater:2017:UCT:3027385.3027438","author":"Slater, Stefan and Baker, Ryan and Almeda, Ma. Victoria and Bowers, Alex and Heffernan, Neil","title":"Using Correlational Topic Modeling for Automated Topic Identification in Intelligent Tutoring Systems","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"393--397","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027438","doi":"10.1145/3027385.3027438","acmid":"3027438","publisher":"ACM","address":"New York, NY, USA","keywords":"correlational topic modeling, intelligent tutoring systems, mathematics education, natural language processing, topic modeling","Abstract":"Student knowledge modeling is an important part of modern personalized learning systems, but typically relies upon valid models of the structure of the content and skill in a domain. These models are often developed through expert tagging of skills to items. However, content creators in crowdsourced personalized learning systems often lack the time (and sometimes the domain knowledge) to tag skills themselves. Fully automated approaches that rely on the covariance of correctness on items can lead to effective skill-item mappings, but the resultant mappings are often difficult to interpret. In this paper we propose an alternate approach to automatically labeling skills in a crowdsourced personalized learning system using correlated topic modeling, a natural language processing approach, to analyze the linguistic content of mathematics problems. We find a range of potentially meaningful and useful topics within the context of the ASSISTments system for mathematics problem-solving.","pdf":"Using Correlational Topic Modeling for Automated Topic  Identification in Intelligent Tutoring Systems   Stefan Slater  Ryan Baker   University of Pennsylvania  3700 Walnut St.   Philadelphia, PA 19104  {slater.research,   ryanshaunbaker}@gmail.com   Ma. Victoria Almeda  Alex Bowers   Teachers College Columbia University  525 W. 120th St.   New York, NY 10027  {mqa2000,bowers}@tc.columbi  a.edu   Neil Heffernan  Worcester Polytechnic Institute   100 Institute Rd.  Worcester, MA 01609    nth@wpi.edu   ABSTRACT  Student knowledge modeling is an important part of modern  personalized learning systems, but typically relies upon valid  models of the structure of the content and skill in a domain. These  models are often developed through expert tagging of skills to  items. However, content creators in crowdsourced personalized  learning systems often lack the time (and sometimes the domain  knowledge) to tag skills themselves. Fully automated approaches  that rely on the covariance of correctness on items can lead to  effective skill-item mappings, but the resultant mappings are often  difficult to interpret. In this paper we propose an alternate  approach to automatically labeling skills in a crowdsourced  personalized learning system using correlated topic modeling, a  natural language processing approach, to analyze the linguistic  content of mathematics problems. We find a range of potentially  meaningful and useful topics within the context of the  ASSISTments system for mathematics problem-solving.    CCS Concepts   Information systems~Document topic models  Keywords  Topic Modeling; Correlational Topic Modeling; Natural  Language Processing; Mathematics Education; Intelligent  Tutoring Systems   1. INTRODUCTION Accurate estimation of student knowledge in online learning  environments generally relies on the existence of skill model  frameworks that map specific problems to a broader theme, topic,  or skill. Models such as Bayesian Knowledge Tracing (BKT; [9])  and Performance Factors Analysis (PFA; [18]) utilize skill models  as an underlying structure for drawing inferences about student  knowledge. However, this process of associating problems in an  intelligent tutoring system (ITS) with annotations of the skills and  knowledge that are associated with the problem benefits more  than just knowledge tracing models. Teachers and researchers can  use information about the underlying skill model to determine if    skills differ in important ways, such as determining which skills  are more likely to be associated with disengagement (e.g. [1]) or  negative affect [11], and to study how hint requests and other  metacognitive behaviors vary between different skills [24].  Skill  models can also help to inform teachers assessments of student  learning and performance, and identify areas where students may  need additional practice or scaffolding in order to succeed.   However, the human creation and curation of models of domain  structure can be challenging [22] especially in online learning  environments which utilize crowdsourced or teacher-generated  problem content. With the rising interest in scaling high-quality  online education, there is also increasing effort to engage a  broader community including teachers in creating content [12].  However, teachers frequently lack the time and training to  produce high-quality annotations of the skills associated with  specific content, and there are no guarantees that skill tags will be  consistent between different authors using a crowd-sourced  system. Definitions of skills, and the granularity of skills  associated with particular problems, may vary from author to  author, and render the overall skill model across the system  uninterpretable, or worse, inconsistent.   While newer knowledge estimation approaches such as recurrent  neural networks (RNNs) do not require expert-coded domain  structure knowledge or skill models [19], it is unclear whether  RNNs offer a tangible increase in performance for knowledge  estimation compared to more traditional approaches such as  Bayesian Knowledge Tracing (BKT) (see discussion in [15]).  Additionally, RNNs have poor interpretability, as their predictions  cannot be straightforwardly tied to specific skills or features of the  problems themselves. As such, RNNs are an incomplete substitute  for having a skill-problem mapping. When it is not feasible to  manually author the mapping between skills and problems,  automatically deriving this model may have substantial value.   There have been efforts to automate the process of determining  which skills are associated with each of a set of problems by using  the co-occurrence of correctness across problems. For example,  [2] derived the mapping between test items and latent skills by  taking several initial mappings (with randomized restart), testing  their fit to the data, and using a search algorithm to enhance the  mapping.  Other approaches to automatically deriving mappings  between problems and skills, such as those used by [10] and [23],  utilize matrix factorization to infer skills based on student  responses. These approaches assume that student patterns of  correct and incorrect responses have less variance within skills  than between skills, and that this difference in variance can be  used to draw inferences about the latent skill structure of the data.  This approach has been shown to have high accuracy, provided   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   2017 ACM.  ISBN 978-1-4503-4870-6/17/03 $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027438    that (1) there are relatively few latent skills in the data and (2) that  the skills are substantially different from one another. These  assumptions are difficult to maintain in a specialized mathematics  tutor that considers potentially hundreds of individual skills, all  within the same overall domain. Another limitation to these  approaches is that they typically assume that knowledge is static  when analyzing covariance, but knowledge in online learning  environments is in fact changing as it is being measured.    An alternative to methods utilizing patterns of correct and  incorrect answers is topic modeling approaches, such as Latent  Semantic Analysis (LSA; [16]), Latent Dirichlet Analysis (LDA;  [5]), and Correlational Topic Modeling (CTM; [4]) Topic models  are not dependent on human tagging of skills  the only input  required is the textual content of the problem itself. Using topic  modeling approaches to label skills and topics utilizes both the  relationships between words and symbols within problem texts, as  well as inferred knowledge about the absence of particular words,  a similar approach to how students themselves come to  understand and learn material [16]. Novice learners often come to  understand complex material by first learning via semantic,  surface-level features, and relies heavily on information that is  available perceptually [13]. Therefore, an approach which utilizes  the textual information contained in a problem may serve to better  map to students emerging understanding of skills, rather than  experts higher-order determinations. Additionally, this approach  can model the underlying lexical similarity that exists within  problems that share a common skill.   Topic modeling is a form of natural language processing that  utilizes word co-occurrence patterns to identify clusters of words,  called topics, that appear in large collections of documents.  Topic modeling can be loosely characterized as factor analysis  conducted on words, rather than numerical variables. Topic  models have been used for a range of applications, such as  automated matching of reviewers to scientific papers within  particular fields of study [17] and utilizing latent topics within  capital facility finance bond elections to examine which topics are  more likely to pass [6].   From this family of models, we select correlated topic modeling  (CTM) [4], which models the intercorrelations of words in text to  infer topics  as the most appropriate modeling approach to use  for our data. LDA assumes that all topics are present in differing  proportions across the component documents [3], an assumption  which almost certainly doesnt hold across the scope of math  topics present within ASSISTments. Additionally, mathematics  problems tend to be highly focused on a single or very small  selection of topics, and so modeling the proportionality of topics  between problems is unnecessary. Finally, although LDA can  model proportions of topics present in a document or documents,  it cannot measure the correlations that may be present between  documents  for example, a math problem about trigonometry is  much more likely to involve geometry than fractions. LSA suffers  from this same limitation. This is the aspect of CTM that makes it  valuable for this particular data  because of the possible  relationships existing between skills, we expect a strong  underlying correlational structure between the topics that we are  attempting to model. Because of this, as well as the wide scope of  problems within our dataset, we selected CTM as our modeling  approach of choice.   2. METHODS  Data for this research comes from the ASSISTments platform  [12]. ASSISTments is an online intelligent tutoring system used   by over 50,000 students, primarily in the northeastern United  States. ASSISTments provides formative and summative  assessment, as well as student support, scaffolding, and detailed  teacher reports.    Within the ASSISTments system, students work through problem  sets, consisting of mathematics questions which they provide  answers to. There are several types of problem sets: complete all  problem sets, which require all problems to be answered correctly;  if-then-else problem sets, which require a certain percentage of  problems to be answered correctly; and skill builder problem sets,  which require students to answer three consecutive problems  correctly in a row. Within most problems, students have the  opportunity to request hints from the system. On many problems,  students providing incorrect answers are asked to complete  scaffolding problems  sub-problems which work on a specific  aspect or skill associated with the base problem. Problem  responses are generally fill in the blank, but can also be multiple  choice or short answers. Problem sets can be assigned as  classwork, to be completed with the supervision of a teacher, or as  homework, to be completed when the student has study hall or is  at home.   Figure 1. Example of a problem in ASSISTments        We analyzed the textual content of 112,526 problems, nearly all  of them mathematics problems, and most of them developed and  used by teachers in the 2012-2013 school year. In ASSISTments,  teachers author problems and hints using a text editor, and have  the ability to add in mathematical symbols, images, embed video,  and use rich HTML formatting.   2.1 Text Processing  Preprocessing of text is an important step for NLP approaches.  ASSISTments text data includes HTML tags and markup and  HTML characters, which must be removed before they can be  used in modeling. Additionally, because a majority of the  problems included numbers and mathematical symbols, we had to  develop additional coding methods for numbers and mathematical  symbols.    CTM is a bag-of-words approach to text analysis  it solely  considers what words are present, and does not consider ordering  or other relationships between words, or any special qualities of  the words themselves. This approach has advantages and  disadvantages to working with the data for this research. For  example, special symbols such as the degree symbol or the square  root operator are HTML encoded (as &deg; and &sqrt;. A bag of  words model will treat these strings like any other word, while     more sophisticated tools which tag words with semantic  categories or other linguistic information (e.g. [20]), may struggle  to represent this information correctly. However, there are  disadvantages, especially when working with numeric  information. A bag of words tagger will differentiate between 1,  2, 3, and so on, even when there is no reason to think, for  example, that the equations 4 + 3 and 2 + 5 are different in any  relevant way.   To address this limitation of the modeling approach, and to  attempt to capture differences in meaning within the mathematics  problems that would not be readily apparent to a pure bag-of- words approach, we developed several dummy codes for the data.  These dummy codes were implemented using string search and  replace routines. The original text, and their replacements, can be  found in Table 1. We selected our converted text notation in such  a way that it would not resemble actual words and phrases used in  the problems, This approach was necessary in order to reduce the  variance that the CTM would attempt to identify between  numbers, decimals, and degrees which are semantically very  similar.   Table 1. Conversions between raw texts and their  corresponding labels in the data   Original Text Converted Text  {0-9} xxnumxx  {10+} xxmanynumxx   Decimals, e.g. 1.11 xxdecimalxx  Fractions, e.g. 2/3 xxfracxx   Dollar Amounts, e.g. $3.50 xxmoneyamtxx  Percents, e.g. 76% xxpercentxx   Degree Amounts, e.g. 90 xxdegreesxx  Explicit Numbers, e.g. #41 xxexplnumxx   1 Teachers often used this notation to denote questions which  came from a worksheet or textbook that students had access  to.      After performing string replacement, all HTML elements within  the problems  such as embedded videos, image links (URLs),  and font changes  were removed. These HTML elements  contained little data that could be meaningfully parsed by a bag of  words model  URLs, for instance, were almost always unique by  problem. Additionally, the textual content contained within the  HTML tags was not visible to the learner. Punctuation and  mathematical operators were all removed, as well as excess  whitespace. While punctuation and mathematics operators serve a  distinct purpose within the text, there was too much inconsistency  between use cases and problem authors conventions for this  information to be extracted reliably. The R package topicmodels  was used to remove common grammatical words and words  which contained less than three characters. This package was also  used to stem the dataset (converting words such as contained,  contains, and containing to contain  [14]. A sparse matrix was  created with d = 0.9999, removing all words which appeared in  less than 0.0001% of problems. These steps produced a final  dataset consisting of 4,058 words mapped to 112,526 problems.   Construction of the model was performed in R. Term frequency  weightings were applied to the document term matrix, and three  CTM models were calculated  one with five predicted topics, one  with 15 predicted topics, and one with 25 predicted topics.  Because of the exploratory nature of this work, optimal tf-idf  weightings and optimal numbers of topics were not calculated. Tf-  idf (term frequency/inverse document frequency) weightings can  help with the identification and exclusion of extremely common  or extremely uncommon words, as well as weight frequencies of  common and uncommon words more effectively. The current skill  label proxy variable within ASSISTments estimates close to 330  skills  a number which was not computationally feasible for our  hardware at this time. In future work, we plan to continue this  work on a cloud computing setup. Goodness of the resulting  models was calculated via the perplexity of each model.  Perplexity scores measure the ability of topic models to generalize  to new and unseen text (in the case of these models, a test set of  problems). A perplexity score can be thought of as the number of  probable words that could follow any given word within the  model, therefore, a lower perplexity represents a better model fit.    3. RESULTS  The perplexity scores for each of the three models are presented in  Table 2. The 25-topic model was found to have the lowest  perplexity among the three models tested, indicating that is it the  best fit among the three models presented here. The downward  trend in perplexity for higher K suggests that additional topics  could more appropriately model problem content within  ASSISTments.      Table 2. Perplexity scores for the three topic models  K = Perplexity   5 319.91  15 227.40  25 189.28     The 25 topics identified, along with the five most common words  associated with each topic, are presented in Table 3.     Table 3. 25 Topics identified by algorithm  Topic Correlated Terms Topic Label   1 Many, student, xxpercentxx,  look, take   Number/percentage  conversion1   2 Left, attempt, xxexplnumxx,  xxmanynumxx, xxdecimalxx   System generated  you  have XX attempts left.   3 Origin, problem, let, try,  solution   System generated, after  scaffolding  Lets try  the original problem   4 Xxdecimalxx, express,  divide, paper, point   Teacher reminder   express your answer as   a decimal to the  hundredths point   5 Step, one, problem, break,  button   System reminder  do  not press break this  problem into steps   6 Question, sorry, next,  incorrect, attempt   System generated   Sorry, thats incorrect.   Lets move onto the next  question   7 Fraction, number, answer,  mix, improper   Improper and mixed  fractions   8 Triangle, angle, length,  figure, side   Side length and angles  of triangles   9 Xxexplnumxx, page, unit,  xxmanynumxx   Textbook and worksheet  problems; Page 25 #4   10 Equation, line, variable,  write, slope   Slope problems   11 Nearest, round, place,  answer, hundredth   Teacher reminder   round your answer to     the nearest hundredth  12 Best, choose, follow, part,   two  A vs. B comparison   problems  13 Day, xxnumxx, time, play,   month  Time problems   14 Xxmanynumxx, point, score,  game, name   Sports problems   15 Xxmoneyamtxx, number,  cost, answer, total   Currency questions   16 Xxnumxx, power,  xxdecimalxx,   xxmanynumxx, number   Metric explanation  problems2   17 Answer, make, type, fraction,  enter   Teacher reminder  how  to enter fraction answers   18 Area, xxnumxx, scale,  square, xxdecimalxx   Area problems   19 Xxfracxx, number, whole,  fraction, example   Whole fraction problems   20 Mile, xxnumxx, per, ball, car Distance problems  21 Xxnumxx, divid, conversion,   formula, number  Unit conversion   problems  22 Xxnumxx, xxmanynumxx,   number, find, value  Simple algebra problems   23 Xxdecimalxx, fraction,  numerator, multiply,   denominator   Decimal  fraction  conversion problems   24 Xxnumxx, factor, simplify,  follow   Factorization problems   25 Follow, correct, select,  subtract, label   Instructions about  subsequent parts of a   problem  1 These problems often took the form of interpreting a pie  chart, and calculating the number of students who constituted  a given percentage of the overall population.  2 These problems scaffolded students by explaining the nature  of the metric system in base ten, using decimals to show that  relationship. Powers of ten was a common phrase as well.    There are a number of surprising findings in the CTM results.  First, we expected that CTM would distinguish between different  topics within problems, which it appears to be able to do. For  example, topics 10 (slope problems), 15 (currency problems), 21  (unit conversion problems), and 23 (fraction conversion problems)  appear to be well-formed. What we didnt anticipate, however,  was that CTM would also pick up on common phrases or hints  that the system provided to students, such as topic 2 (reminders  about the number of attempts a student has left), topics 4 and 11  (reminders about significant figures), and topic 3 (when a student  returns to an original problem after a scaffolding problem).    Additionally, the model appears to identify non-mathematics  themes, such as topic 14 (sports) and topic 15 (currency). Similar  approaches to identifying the semantic content of problems have  been used before [21], and topic modeling may be an additional  approach to identifying themes that are present within problems.  While these categories and the reminders/hints to students dont  lend themselves towards the goal of automated skill tagging of  problems, they are interesting for assessing features of problem  construction, such as the use of reminder texts and feedback about  student performance, or the use of specific themes in word  problems. If CTM is able to reliably tag these features, then it is  possible to use them in models assessing the relationship between  problem design and student affect, learning, and behavior. As   such, even the categories which are less useful for our initial  research goal are likely to have other potentially productive uses.   4. CONCLUSION  In this paper we have developed a correlational text model (CTM)  to attempt to identify common topics within a mathematics tutor.  These approaches are important for being able to estimate student  knowledge, as well as for guiding and informing teacher feedback  and identification of student performance gaps. We developed a  CTM which used 25 topics, and determined that it had better  model fit than 15- and 5-topic alternatives. The CTM was able to  identify not only mathematics subjects such as fraction problems,  slope problems, and area problems, but also instances of system- generated scaffolding and hints (such as reminders about  rounding) and non-mathematics subjects, such as problems  concerning sports and money.   This approach is not without its limitations though  one of our  goals in this effort was to develop an automated method of skills  tagging, and the results of the CTM are somewhat murky in that  respect. Only 9 of the 25 topics identified appear to be about a  clearly defined mathematics skill/concept, the rest of the topics  identifying either system-generated text or non-mathematics  subjects. In other words, the CTM attempts to identify differences  in problem content, problem structure and problem theme, all at  the same time, and the resulting model is somewhat muddy as a  result. It is unclear whether an increase in the number of topics  will capture more skills and find more fine-grained skills, or just  add more noise and variance to the underlying model.   It is also likely that this is not an optimal mapping of the domain  structure of these problems. Performing CTM beyond 25 topics  was computationally limiting, but the ASSISTments system  identifies roughly 330 unique skills within the database. Future  efforts at using CTM to identify domain structure will need to  utilize cluster or cloud computing, as this approach is  computationally demanding. However, this expansion of topics  comes with a cost  research by Chang et al. suggests that, while  CTM tends to outperform LSA and LDA in terms of model fit and  word intrusion metrics, it does so at the cost of human  interpretability. In other words, while CTM does a better job of  clustering topics than LSA and LDA, the topics themselves may  not be as interpretable to human judges, especially as the number  of topics to be modeled increases [7].    An additional limitation of this modeling approach is the depth of  structure that can be assessed. As the CTM does not take into  account the order of words, grammar, and rhetoric, it may provide  an oversimplified categorization of the math problems within the  tutoring system. In particular, this approach may combine  problems with similar surface features, but with different deep  structure - the underlying principle that is necessary for a solution  [8]. Analysis and tagging of deep structure of problems, therefore,  currently still needs to be more reliant on human coders and  expert judgment than the automated approach used here; trying to  expand the depth of our categorizations with more sophisticated  linguistic approaches will be an important area of future work.   Finally, mathematics notation represents something of an  unsolved problem. While string replacement worked to a degree,  improvement in the identification of topics in mathematics  problems will benefit from an improvement in the capacity for  text analysis tools to work with and process mathematical  notation, such as equations, unit notations (such as ft, in, km), and  variables (none of which were captured with the current text  replacement scheme). However, the analysis of mathematics text     is not well-developed and this work further highlights the  necessity for specialized tools for exploring data involving  mathematics symbols and notation with NLP tools. Differences in  the uses of numbers, equations, variables, operators, and symbols  represent a large source of potential variance and structure within  the data that cannot currently be explored in an effective way, and  an enhanced ability to parse mathematics items in text could  greatly enhance the ability of topic models to successfully  distinguish individual skills.   Future work in this domain may attempt to use the results from a  CTM as a skill matrix for various knowledge inference  techniques, such as BKT or PFA. If CTM and other forms of topic  modeling can achieve an acceptable level of agreement with  expert skill tagging, then we should expect to see improved model  fit for these NLP-derived skill models compared to previous  methods for automated skill tagging, at lower cost and time to  implement than manual skill tagging. These improvements in the  scalability of skill tagging would serve to improve the quality and  consistency of skill identification in ITS environments, improving  both the quality of personalized learning while making it easier  for researchers to develop models that build on skill models and  use these models to understand and enhance student learning.   5. ACKNOWLEDGMENTS  This research was supported by the National Science Foundation  (NSFDRL 1252297). Any opinions and findings expressed are the  authors and do not necessarily reflect the views of the NSF.  ASSISTments has benefited from multiple US DOE and NSF  grants and awards. Our thanks to the University of Pennsylvania  Research Apprenticeship Course, for their feedback and thoughts  on earlier drafts of this paper. Our thanks also to Vitomir  Kovanovic for his insights on dealing with mathematics equations  and texts within the data.  6. REFERENCES  [1] Baker, R.S.J.d., de Carvalho, A.M.J.A., Raspat, J., Aleven,   V., Corbett, A.T., Koedinger, K.R. (2009) Educational  Software Features that Encourage and Discourage  Gaming  the System . Proceedings of the 14th International  Conference on Artificial Intelligence in Education, 475-482.   [2] Barnes, T., Bitzer, D., & Vouk, M. (2005). Experimental  analysis of the q-matrix method in knowledge discovery.  In International Symposium on Methodologies for Intelligent  Systems. Springer Berlin Heidelberg, 603-611.   [3] Blei, D. M. (2012). Probabilistic topic models.  Communications of the ACM, 55(4), 77-84.   [4] Blei, D. M., & Lafferty, J. D. (2007). A correlated topic  model of science. The Annals of Applied Statistics, 17-35.   [5] Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent  dirichlet allocation. Journal of Machine Learning  Research, 3(Jan), 993-1022.   [6] Bowers, A.J., Chen, J.(2015) Ask and Ye Shall Receive  Automated Text Mining of Michigan Capital Facility  Finance Bond Election Proposals to Identify which Topics  are Associated with Bond Passage and Voter Turnout.  Journal of Education Finance, 41(2), 164-196.   [7] Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J. L., & Blei,  D. M. (2009). Reading tea leaves: How humans interpret  topic models. In Advances in neural information processing  systems, 288-296.   [8] Chi, M. T., Glaser, R., & Farr, M. J. (2014). The nature of  expertise. Psychology Press, xvii-xxi.   [9] Corbett, A. T., Anderson, J. R. (1995). Knowledge tracing:  Modeling the acquisition of procedural knowledge. User  Modeling and User-Adapted Interaction 4, 4, 253278.    [10] Desmarais, M. C. (2012). Mapping question items to skills  with non-negative matrix factorization. ACM SIGKDD  Explorations Newsletter, 13(2), 30-36.   [11] Doddannara, L., Gowda, S., Baker, R.S.J.d., Gowda, S., de  Carvalho, A.M.J.B (2013) Exploring the relationships  between design, students affective states, and disengaged  behaviors within an ITS. Proc. of the 16th International  Conference on Artificial Intelligence and Education, 31-40.   [12] Heffernan, N. T., & Heffernan, C. L. (2014). The  ASSISTments ecosystem: building a platform that brings  scientists and teachers together for minimally invasive  research on human learning and teaching. Intl. Journal of  Artificial Intelligence in Education, 24(4), 470-497.   [13] Hmelo-Silver, C. E., & Pfeffer, M. G. (2004). Comparing  expert and novice understanding of a complex system from  the perspective of structures, behaviors, and  functions. Cognitive Science, 28(1), 127-138.   [14] Jivani, A. G. (2011). A comparative study of stemming  algorithms. Int. J. Comp. Tech. Appl, 2(6), 1930-1938.   [15] Khajah, M., Lindsey, R. V., & Mozer, M. C. (2016). How  deep is knowledge tracing Proc. of the 9th International  Conference on Educational Data Mining, 94-101.   [16] Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An  introduction to latent semantic analysis. Discourse  processes, 25(2-3), 259-284.   [17] Mimno, D., & McCallum, A. (2007). Expertise modeling for  matching papers with reviewers. Proc. 13th ACM SIGKDD   Conf. on Knowledge Discovery and Data Mining, 500-509.    [18] Pavlik Jr, P. I., Cen, H., & Koedinger, K. R. (2009).  Performance Factors Analysis--A New Alternative to  Knowledge Tracing. Proceedings of AIED 2009, 531-538.   [19] Piech, C., Bassen, J., Huang, J., Ganguli, S., Sahami, M.,  Guibas, L. J., & Sohl-Dickstein, J. (2015). Deep knowledge  tracing. In Advances in Neural Inf. Processing Sys. 505-513.   [20] Rayson, P. (2008). Wmatrix corpus analysis and comparison  tool. Lancaster University.   [21] Slater, S., Ocumpaugh, J., Baker, R., Scupelli, P., Inventado,  P.S., Heffernan, N. (2016) Semantic Features of Math  Problems: Relationships to Student Learning and  Engagement. Proceedings of the 9th International  Conference on Educational Data Mining., 223-230.   [22] Stamper, J. C., & Koedinger, K. R. (2011). Human-machine  student model discovery and improvement using DataShop.  International Conference on Artificial Intelligence in  Education. Springer Berlin Heidelberg, 353-360.   [23] Thai-Nghe, N., Horvth, T., & Schmidt-Thieme, L. (2010).  Factorization models for forecasting student performance.  Proceedings of Educational Data Mining 2011, 11-20.   [24] Walkington, C., Clinton, V., Ritter, S. N., & Nathan, M. J.  (2015). How readability and topic incidence relate to  performance on mathematics story problems in computer- based curricula. J. of Educational Psychology,107(4).      "}
{"index":{"_id":"48"}}
{"datatype":"inproceedings","key":"Hubbard:2017:ELT:3027385.3027390","author":"Hubbard, Ryan and Sipolins, Aldis and Zhou, Lin","title":"Enhancing Learning Through Virtual Reality and Neurofeedback: A First Step","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"398--403","numpages":"6","url":"http://doi.acm.org/10.1145/3027385.3027390","doi":"10.1145/3027385.3027390","acmid":"3027390","publisher":"ACM","address":"New York, NY, USA","keywords":"EEG, human-computer interaction, neurofeedback, virtual reality","Abstract":"Virtual reality presents exciting new prospects for the delivery of educational materials to students. By combining this technology with biological sensors, a student in a virtual educational environment can be monitored for physiological markers of engagement or more cognitive states of learning. With this information, the virtual reality environment can be adaptively altered to reflect the student's state, essentially creating a closed-loop feedback system. This paper explores these concepts, and presents preliminary data on a combined EEG-VR working memory experiment as a first step toward a broader implementation of an intelligent adaptive learning system. This first-pass neural time-series and oscillatory data suggest that while an EEG-based neurofeedback system is feasible, more work on removing artifacts and identifying relevant and important features will lead to higher prediction accuracy.","pdf":"Enhancing Learning Through Virtual Reality and  Neurofeedback: A First Step   Ryan Hubbard  University of Illinois, Urbana-  Champaign  405 N. Mathews Ave   Urbana, IL 61801  (661) 816-7368   rjhubba2@illinois.edu   Aldis Sipolins  IBM Thomas J. Watson Research   Center  1101 Kitchawan Rd   Yorktown Heights, NY 10598  (217) 819-9581   asipoli@us.ibm.com   Lin Zhou  IBM Watson Education   294 Route 100  Somers, NY 10589   (914) 766-1712   linzhou@us.ibm.com         ABSTRACT  Virtual reality presents exciting new prospects for the delivery of   educational materials to students. By combining this technology   with biological sensors, a student in a virtual educational   environment can be monitored for physiological markers of   engagement or more cognitive states of learning. With this   information, the virtual reality environment can be adaptively   altered to reflect the students state, essentially creating a closed-  loop feedback system. This paper explores these concepts, and   presents preliminary data on a combined EEG-VR working   memory experiment as a first step toward a broader   implementation of an intelligent adaptive learning system. This   first-pass neural time-series and oscillatory data suggest that while   an EEG-based neurofeedback system is feasible, more work on   removing artifacts and identifying relevant and important features   will lead to higher prediction accuracy.   Categories and Subject Descriptors   Applied computing~Interactive learning  environments    Applied computing~Biological   networks    Applied computing~Computer-assisted   instruction    Applied computing~Psychology   Keywords  Virtual reality; EEG; Neurofeedback; Human-computer  interaction   1. INTRODUCTION  An important and foundational concept of learning analytics is the   necessity of feedback or intervention to improve education and   learning.  Since the early days of learning theory, such as Kolbs   Experiential Learning Theory [1] and Banduras Social Learning   Theory [2], feedback has been considered to be an integral   element of learning. A large amount of modern empirical research   has focused on the usefulness and appropriateness of different   methods of feedback. In general, feedback seems to be more   effective when it focuses the individuals attention on task-related   information as opposed to self-related information or praise [3],   and when it is immediate as opposed to delayed, particularly when   more real-world paradigms are used [4, 5, 6]. Additionally, the   most successful feedback provides information on how to   improve, not just the correct answer, while remaining relatively   simple and easy for the learner to understand [7].   Campbell and Oblingers five-step model of learning analytics [8]   highlights the importance of tailoring interventions for learners in   the Predict, Act, and Refine stages. By collecting data about   individual learners, teachers can create personalized models that   predict success in a particular course, lesson, or even question.  If   the model reports that a student is at risk for failure, the teacher   can intervene to provide additional guidance to the student.    Finally, the result of this intervention, and continued collection of   data, can be used to refine and update the model for continued   successful prediction. These ideas of effectively utilizing data to   improve learning interventions are similarly expressed in other   learning analytics models as well [9, 10].   Recently, Clow [11] expanded on the five-step model by   introducing the Learning Analytics Cycle. Here, learners engage   in some form of education material, leading to collection of data   (which could be demographic, assessment-based, etc.) about the   learner.  These data are processed into metrics, outcome variables   which provide insight into learning or predict success. The   important last step is to use these metrics to intervene and alter the   learning process, effectively creating a closed-loop system; the   data output by the learner determines the intervention, which   affects the input.   Under this framework, learning will be most improved when   interventions are provided in real-time to the learner  as soon as   some predictive metric is obtained that identifies the individual   will fail, the system should intervene.  Here, we propose a system   to accomplish this goal that can provide rapid, informative   intervention prior to the subject even responding.  By measuring   electroencephalography (EEG) signals, we can track the   individuals learning state in real-time. Additionally, by providing   instruction in a virtual reality (VR) environment, we can not only   provide a more realistic learning environment, but also more   easily and adaptively alter the learning environment to intervene   based on the EEG signal, effectively closing the loop.    2. VIRTUAL REALITY  VR refers to technology that simulates a realistic three-  dimensional environment for the user to interact with. This is   typically and most successfully implemented as a headset, or        Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights   for components of this work owned by others than ACM must be   honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior   specific permission and/or a fee. Request permissions from   Permissions@acm.org.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada     2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00    DOI: http://dx.doi.org/10.1145/3027385.3027390   mailto:Permissions@acm.org   head-mounted display, with a screen that the user wears. The   digital environment is sent from the computer to the screen in the   headset. This allows the user to look around the 3D environment,   and also acts to reduce sensory input from the outside world.   Although interest and development of VR technology has been   present since the 1980s [12], the technological advancements of   the last several years have put a spotlight on VRs potential   applications and propelled the industry to new heights [13].   Importantly, many of these advancements, such as improved   graphical rendering and room-scale positional tracking, allow for   greater presence in the environment.  Presence, defined as the   phenomenological sense of actually being in a virtual environment   [14, 15], is important for educational VR applications.    Essentially, students will be more focused and attentive to   learning material, and less distracted by task-unrelated stimuli, if   they are deeply immersed in the virtual environment. This   immersion could enhance education in even advanced areas of   study  for example, a student studying marine biology could   explore virtual ocean environments and interact with simulated   marine life.   While the literature is currently somewhat limited, there has been   some research on VR applications geared toward training   individuals and improving learning [16]. VR has been shown to   be useful for learning certain visual-spatial tasks [17], and this   knowledge transfers to real world spatial navigation [18, 19].    Some preliminary work has also shown modest benefits on test   scores after VR training in geometry [20] and geography [21].    Additional work has shown real world benefits in surgery   performance after sessions of virtual surgery training [22, 23].    Lastly, a large-scale meta-analysis of virtual reality studies in   educational settings found that VR was overall effective at   improving learning outcomes [24]. Thus, while the evidence is   still somewhat limited, it is reasonable to believe that VR-based   educational systems give benefits to learning.   The additional benefit of a VR educational environment is it   allows for a rapid, automatic, and personalized feedback system.    Environments can be created that are tailored for each individual   based on personality factors, learning styles, and level of   knowledge about a subject. Additionally, the environment itself   can be altered in real-time based on the learners current   experience, attentional state or level of engagement, performance   on the task, or other cognitive or biological shifts that might cause   changes in behavior. There are a large number of specific   environmental changes that could be used, and which choice   would be optimal for improving learning would likely depend on   the particular task, as well as the individual. However, based on   research on effective feedback, these VR interventions likely   should rapidly direct attention to highlight important information   about the task at hand.   In order to provide these effective VR interventions, data that is   predictive of success must be collected from the learner.  Ideally,   multiple measures would be extracted in order to improve   accuracy, as a single measure is unlikely to be sufficient.    However, data based on behavioral output from the learner, such   as a response on a test, may not be informative as to the actual   internal mental state of the individual.  A better tool to measure   this is EEG.   3. ELECTROENCEPHALOGRAPHY  EEG is a non-invasive neuroimaging method that measures   electrical activity from the brain through sensors or electrodes   placed on the scalp [25]. The measured brain activity represents   the summed postsynaptic potentials (both excitatory and   inhibitory) of thousands of neurons whose firing is synchronous in   time [26, 27].  Given issues of volume conduction and spread of   electrical signals, the majority of EEG signals are likely generated   by cortical pyramidal neurons, although it remains possible that   subcortical structures do contribute [28]. Successful measurement   of EEG signals also depends on the spatial orientation of firing   cells  neurons that are parallel to each other and perpendicular to   the cortical surface will sum more effectively. Thus, it is   essentially impossible to determine exactly which structures of the   brain are responsible for the recorded signal on the scalp   (although modern computational methods can make reasonable   guesses and approximations [29]). Despite this drawback, EEG   allows for recording of brain activity with very high temporal   resolution, on the order of milliseconds. This is essential for   capturing rapid dynamics of brain activity including neural   oscillations, or rhythmic neural firing at different frequencies that   is critical for information processing [30]. This is also particularly   useful for feedback-related applications, as interventions can be   applied much more quickly.   EEG is also very useful for applied educational work because   behavioral output from the learner is not required to obtain a   signal  the brain will respond to stimuli whether or not a physical   response is made. Decades of cognitive and neuroscientific   research has demonstrated that EEG does in fact provide   informative signal that can be leveraged.  For example, induced   power in the alpha rhythm of the EEG  a neural oscillation in the   8-12 Hz range  is related to attention and visual processing of   information [31, 32]. Additionally, the phase of the ongoing alpha   rhythm is predictive of successful processing of visual stimuli [33,   34].  Thus, by measuring ongoing alpha power, we can have some   understanding of a learners attentional state, and immediately   modulate the virtual environment if attention is waning.    Additionally, we can track the phase of the alpha oscillation to   time the presentation of information and optimize its encoding.   EEG is not only useful for measuring broad state changes and low   level perceptual processing, but is also effective at measuring   higher level cognitive and mental processes. Predictive and   reliable EEG measures of successful episodic memory encoding   [35, 36], emotional operations [37], processing of semantics and   meaning [38], and second-language learning [39] have been   identified.  Thus, EEG is not simply another method of measuring   level of arousal; it potentially allows teachers to identify mental   states and cognitive processes in real-time, making it an ideal tool   for an educational feedback system.   4. PRELIMINARY DATA  We have described an idealized closed loop educational feedback   system combining VR and EEG. The actual utility of such a   system must be validated empirically. Additionally, combining   EEG and VR presents multiple technical hurdles; EEG   measurements must be sent in real-time to the VR environment   controller, and EEG data artifacts produced by head movements   and eyeblinks must be dealt with. Here, we present preliminary   data from 12 participants during an experiment designed as a first   pass to combine EEG and VR.   4.1 Experimental Design  The procedure was a visual working memory task developed with   the Unity3D game engine. The task was completed by all   participants both in VR and on in front of a computer screen with     a mouse. In a fairly simple virtual space, floating stimuli   (truncated icosahedrons) appeared in random locations from a pre-  defined set of possible locations. After appearing, the stimuli   would change to random colors for 1 sec, and then turn back to   gray for a delay of 1.1 sec. Following the delay, the floor of the   virtual environment would change to a color that one of the   stimuli had previously been. The participants then had to interact   with a stimulus to indicate which of the stimuli presented had   been that color. If correct, the stimulus exploded and the next trial   began; if incorrect, the stimulus shrunk to nothing and the next   trial began.   The task was adaptive, such that the number of stimuli presented   was dependent on the participants performance. At the start of   the experiment, 3 stimuli were presented. If the participant got a   trial correct, a counter would move up 1; if incorrect, down 1. If   the counter reached +3 or -3, the number of stimuli would   increase or decrease, respectively. Each participant completed 20   minutes of trials in the VR condition and 20 minutes in the non-  VR condition.   In the non-VR condition, participants were seated in front of a   computer monitor and interacted with stimuli by clicking a mouse.   In the VR condition, participants wore an HTC Vive VR headset   and interacted with stimuli using a handheld motion controller.    The Vive uses room-scale tracking, so participants could walk up   to stimuli and interact with them. In both conditions, participants   wore an InteraXon Muse EEG headband during the task to record   EEG data. The Muse is a consumer headset with 4 channels (2   frontal, 2 temporal), a 200 Hz sampling rate, and wireless   capabilities. Although a consumer headset may not provide as   clean of a signal as a medical grade EEG system, other work has   shown that mental states can still be classified with the Muse [40],   and demonstrating success with a consumer-grade headset   provides ecological validity, as real-life educational applications   will likely use consumer products.   Behaviorally, participants did not perform any better or worse in   VR compared to non-VR. Previous studies have found benefits   when performing tasks in VR, although these benefits are likely   somewhat task-dependent, and simpler, less complex tasks may   not lead to large improvements [41]. Thus, the simplicity of our   working memory task may have reduced the benefits of VR.    However, our primary goal was not to show large benefits of VR   usage  this experiment represents a first step towards combining   VR and EEG for neurofeedback work, and using a simple   experiment with a learning component before moving to a more   complex learning environment may allow us to identify   informative EEG features.   4.2 EEG Data  Our goal with the EEG data was to identify neural features that   differentiated trials in which participants made a correct response   from trials in which they didnt. Importantly, we examined data   during the encoding period of the task  while participants were   viewing the colored stimuli prior to the delay period.    Theoretically, this is when subjects were encoding information to   be retained in memory. If neural signals already differentiate   accurate and inaccurate choices at this time, then interventions   could be applied in real-time to lead subjects to successful   learning prior to a response even being made.   4.2.1 Event Related Potentials  We first examined event-related potentials (ERPs) in response to   the onset of the stimuli to be remembered [42]. ERPs are   measured by baseline correcting the EEG data for each individual   trial, and then averaging the trials together to create an average   response related to the stimulus. We used a baseline period of 100   ms prior to the onset of the stimuli, and filtered the data with a 16   Hz low-pass Butterworth filter to reduce noise. Plotted in Figures   1 & 2 are the grand average ERPs (averaged across subjects) of   correct vs. incorrect responses in the VR and non-VR conditions   for the temporal channels. Frontal channels were not plotted, as   the data were noisy and inconclusive.      Figure 1. ERPs from the non-VR condition during the   encoding period.  Amplitude change is plotted on the Y axis,   time (including baseline) is plotted in sec on the X axis.    Stimulus onset is at time 0. Blue lines are correct, red lines are   incorrect.      Figure 2. ERPs from the VR condition during the encoding   period.  Amplitude change is plotted in sec on the Y axis, time   (including baseline) is plotted on the X axis.  Stimulus onset is   at time 0. Blue lines are correct, red lines are incorrect.   Looking at the ERPs, it is clear that there are differences between   correct and incorrect responses during the time of encoding.   Interestingly, the large negative-going component 300 ms after   the onset of the stimulus is earlier in the VR condition, and the     differences between correct and incorrect are larger. A t-test on   the mean amplitudes between 250-400 ms across correct and   incorrect was trending toward significance (p = 0.061) in the right   temporal channel. It is unclear why the magnitude of this effect   would be larger in VR  while it is tempting to view it as a neural   correlate of immersion or spatial awareness in VR, caution must   be taken, as movement and eyeblink artifacts could contribute.   4.2.2 Time-Frequency Analysis  To examine stimulus related oscillatory activity, we performed   time-frequency analysis [43]. We computed the Fast Fourier   Transform (FFT) of 250 ms windows of the encoding period that   were tapered with a Hanning function. Each window had an 87%   overlap with the previous window. We also computed the FFT of   a 250 ms pre-stimulus baseline for each trial. After averaging   spectral data from each trial, we performed baseline correction by   Decibel conversion (10 * log(activity / baseline)). Thus, the   figures plotted display changes in power relative to baseline   across the encoding time period for the temporal channels.   Examining figures 3 & 4, we can see similarities between the VR   and non-VR conditions. First, there was an early decrease in alpha   (12 Hz) power from approximately 0-300 ms, followed by a later   increase in alpha power. Alpha effects were significant in the   early time window in the left channel (p = 0.0437) and trending   toward significance in the late time window (p = 0.0543) only in   the VR condition. Additionally, in both VR and non-VR, there   was a decrease in high beta (30-40 Hz) power from approximately   0-300 ms in correct responses relative to incorrect responses.   Average power was computed at approximately 28-33 Hz and 38-  43 Hz from roughly 0-270 ms in both channels, and these power   values were submitted to t-tests.  The lower range (28-33 Hz)   effect was significant for non-VR (p = 0.025) and VR (p = 0.044)   in the left channel only, while the higher range (38-43 Hz) effect   was significant only for VR in the right channel (p = 0.0428).   5. CONCLUSION  Here we have given a theoretical overview of a closed-loop   educational feedback system in which neurobiological signals   from the learner shape the virtual learning environment to   optimize learning. We also presented preliminary experimental   data that suggests such a predictive model is feasible, as we were   able to identify features in the EEG data that differentiated correct   and incorrect responses at the time of encoding. Future efforts will   attempt to use these features for machine learning classification of   correct vs. incorrect responses, with the eventual goal of having a   model that can accurately perform this discrimination in real-time   and intervene to improve performance. Statistically, time-  frequency responses were more robust, particularly in the VR   condition, suggesting these features may be more useful.    Important technical hurdles must still be overcome  the main   being removing artifacts from the EEG data. Eyeblinks and head   movements can create large fluctuations that mask the informative   signal. This problem is exacerbated when using a wireless   commercial headset, as the occasional signal dropout and   dropping of samples leads to even more noise. The Muse   presented an additional problem with the frontal channels  the   proximity of these channels to the reference, and to eye muscles,   resulted in highly noisy data that was mostly uninformative.   Additionally, with only a few electrodes, conventional methods   for artifact removal such as ICA [44] are mostly unsuccessful.    More advanced signal processing methods, such as adaptive   filtering [45], will be necessary to achieve high classification   accuracy, and will be critical for real-time implementation.   An important question is whether an EEG-VR intervention system   would be the most optimal system for enhancing learning.   Theoretically, interventions could occur before the participant   even makes an error, and if the participant is unaware of their   error, they may not learn as well.  In other words, errors can be   informative, and intervening before errors may not be the best   thing to do. We propose that there are multiple reasons why an   individual might make an error, including incorrect knowledge   about the problem, and lack of attention or encoding. If the   individual has incorrect knowledge, post-error learning might be   more useful; however, if the error is simply due to lack of   attention, a pre-error intervention might be more useful.  An EEG-  VR feedback system would allow for both types of interventions.         Figure 3. Time-frequency plots of the encoding period of the   non-VR condition. Frequency (Hz) is plotted on the Y axis,   time from onset of stimulus (ms) on the X axis, and color   represents relative power. Baselines not plotted.      Figure 4. Time-frequency plots of the encoding period of the   VR condition. Frequency (Hz) is plotted on the Y axis, time   from onset of stimulus (ms) on the X axis, and color represents   relative power. Baselines not plotted.     6. ACKNOWLEDGEMENTS  This work was conducted as part of IBM Watson Education and   IBM Research. We would like to thank the IBM colleagues for   their support: Payel Das, Patrick Watson and Dr. Chalapathy Neti.   7. REFERENCES  [1] Kolb, D.A. 1984. Experiential Learning: Experience as the   source of learning and development. Prentice Hall.   [2] Bandura, A., and Walters, R.H. 1977. Social Learning  Theory.   [3] Kluger, A.N., and DeNisi, A. 1996. The Effects of Feedback  Interventions on Performance: A Historical Review, a Meta-  Analysis, and a Preliminary Feedback Intervention Theory.   Psychological Bulletin. 119, 2 (1996), 254-284.   [4] Hattie, J., and Timperley, H. 2007. The Power of Feedback.  Review of Educational Research. 77, 1 (March 2007), 81-  112.   [5] Mory, E.H. 2003. Feedback research revisited. In D. H.  Jonassen (Ed.), Handbook of research for educational   communications and technology, New York: MacMillan   Library Reference.   [6] Thurlings, M., Vermeulen, M., Bastiaens, T., and Stijnen, S.  2013. Understanding feedback: A learning theory   perspective. Educational Research Review, 9 (2013), 1-15.   [7] Shute, V.J. 2008. Focus on Formative Feedback.  Review of  Educational Research, 78, 1 (2008), 153-189.   [8] Campbell, J.P. and Oblinger, D.G. 2007. Academic  Analytics. EDUCAUSE Quarterly, October (2007).   [9] Elias, T. 2011. Learning Analytics: Definitions, Processes  and Potential.   [10] Long, P. and Siemens, G. 2011. Penetrating the Fog:  Analytics in Learning and Education. Educause Review, 46,   5 (2011), 31-40.   [11] Clow, D. 2012. The Learning Analytics Cycle: Closing the  loop effectively. In Proceedings of the 2nd International   Conference on Learning Analytics and Knowledge - LAK   12, 2012, p. 134.   [12] Rheingold, H. 1991. Virtual Reality. New York: Summit.    [13] Natanson, E. 2016. Technology's Next Big Wave -- Virtual  Reality. Forbes.  Retrieved from http://www.forbes.com.   [14] Draper, J. V., Kaber, D. B. & Usher, J. M. 1998.  Telepresence. Human Factors, 40, (1998), 354375.   [15] Sanchez-Vives, M. V., & Slater, M. 2005. From presence to  consciousness through virtual reality. Nature Reviews   Neuroscience, 6(4), (2005), 332-339.   [16] Psotka, J. 1995. Immersive training systems: Virtual reality  and education and training. Instructional science, 23 (1995),   405-431.   [17] Regian, J. W., Shebilske, W., & Monk, J. 1992. A  preliminary empirical evaluation of virtual reality as an   instructional medium for visual-spatial tasks. Journal of   Communication, 42, 4 (1992), 136-149.    [18] Regian, L W., Shebilske, W., & Monk, J. 1993. VR as a  Training Tool: Transfer Effects. Unpublished manuscript,   Armstrong Laboratory, Brooks Air Force Base, Texas.   [19] Goldberg, S. 1994. Training dismounted soldiers in a  distributed interactive virtual environment. U. S. Army   Research Institute Newsletter, April 14, 9-12.   [20] Kaufmann, H., Schmalstieg, D., & Wagner, M. 2000.  Construct3D: a virtual reality application for mathematics   and geometry education. Education and information   technologies, 5, 4 (2000), 263-276.   [21] Virvou, M., Katsionis, G., & Manos, K. 2005. Combining  software games with education: evaluation of its educational   effectiveness. Educational Technology & Society, 8, 2   (2005), 54-65.   [22] Seymour, N. E., Gallagher, A. G., Roman, S. A., OBrien, M.  K., Bansal, V. K., Andersen, D. K., & Satava, R. M. 2002.   Virtual reality training improves operating room   performance: results of a randomized, double-blinded study.   Annals of surgery, 236, 4 (2002), 458-464.   [23] Lemole Jr, G. M., Banerjee, P. P., Luciano, C., Neckrysh, S.,  & Charbel, F. T. 2007. Virtual Reality in Neurosurgical   Education: PartTask Ventriculostomy Simulation with   Dynamic Visual and Haptic Feedback. Neurosurgery, 61, 1   (2007), 142-149.   [24] Merchant, Z., Goetz, E. T., Cifuentes, L., Keeney-Kennicutt,  W., & Davis, T. J. 2014. Effectiveness of virtual reality-  based instruction on students' learning outcomes in K-12 and   higher education: A meta-analysis. Computers & Education,   70 (2014), 29-40.   [25] Berger, H. 1934. ber das Elektrenkephalogramm des  Menschen. Archiv fr Psychiatrie und Nervenkrankheiten,   102, 1 (1934), 538-557.   [26] Cacioppo, J. T., Tassinary, L. G., & Berntson, G. (Eds.).  2007. Handbook of psychophysiology. Cambridge University   Press.   [27] Olejniczak, P. 2006. Neurophysiologic Basis of EEG.  Journal of Clinical Neurophysiology, 23, 3 (2006), 186-189.   [28] Tenke, C. E., Schroeder, C. E., Arezzo, J. C., & Vaughan Jr,  H. G. 1993. Interpretation of high-resolution current source   density profiles: a simulation of sublaminar contributions to   the visual evoked potential. Experimental Brain Research,   94, 2 (1993), 183-192.   [29] Sanei, S., & Chambers, J. A. 2013. EEG Source  Localization. In EEG Signal Processing, John Wiley & Sons.   [30] Buzsaki, G. 2006. Rhythms of the Brain. Oxford University  Press.   [31] Klimesch, W., Sauseng, P., & Hanslmayr, S. 2007. EEG  alpha oscillations: the inhibitiontiming hypothesis. Brain   research reviews, 53, 1 (2007), 63-88.   [32] Ray, W. J., & Cole, H. W. 1985. EEG Alpha Activity  Reflects Attentional Demands, and Beta Activity Reflects   Emotional and Cognitive Processes. Science, 228 (1985),   750-752.   [33] Busch, N. A., Dubois, J., & VanRullen, R. 2009. The Phase  of Ongoing EEG Oscillations Predicts Visual Perception. The   Journal of Neuroscience, 29, 24 (2009), 7869-7876.   [34] Mathewson, K. E., Gratton, G., Fabiani, M., Beck, D. M., &  Ro, T. 2009. To see or not to see: prestimulus  phase   predicts visual awareness. The Journal of Neuroscience, 29,   9 (2009), 2725-2732.      http://www.forbes.com/   [35] Werkle-Bergner, M., Muller, V., Li, S. C., & Lindenberger,  U. 2006. Cortical EEG correlates of successful memory   encoding: Implications for lifespan comparisons.   Neuroscience and Biobehavioral Reviews, 30 (2006), 839-  854.   [36] Friese, U., Kster, M., Hassler, U., Martens, U., Trujillo- Barreto, N., & Gruber, T. 2013. Successful memory   encoding is associated with increased cross-frequency   coupling between frontal theta and posterior gamma   oscillations in human scalp-recorded EEG. NeuroImage, 66   (2013), 642-647.   [37] Esslen, M., Pascual-Marqui, R. D., Hell, D., Kochi, K., &  Lehmann, D. 2004. Brain areas and time course of emotional   processing. Neuroimage, 21, 4 (2004), 1189-1203.   [38] Kutas, M., & Federmeier, K. D. 2011. Thirty years and  counting: Finding meaning in the N400 component of the   event related brain potential (ERP). Annual review of   psychology, 62 (2011), 621.   [39] Shestakova, A., Huotilainen, M., eponien, R., & Cheour,  M. 2003. Event-related potentials associated with second   language learning in children. Clinical Neurophysiology,   114, 8 (2003), 1507-1512.   [40] Bashivan, B., Rish, I., Heisig, S. 2015. Mental State  Recognition via Wearable EEG. In Proceedings of 5th NIPS   Workshop on Machine Learning and Interpretation in   Neuroimaging.   [41] Kozak, J. J., Hancock, P. A., Arthur, E. J., & Chrysler, S. T.  1993. Transfer of training from virtual reality. Ergonomics,   36, 7 (1993), 777-784.   [42] Luck, S. J. 2014. An introduction to the event-related  potential technique. MIT press.   [43] Cohen, M. X. 2014. Analyzing neural time series data:  theory and practice. MIT Press.   [44] Jung, T. P., Humphries, C., Lee, T. W., Makeig, S.,  McKeown, M. J., Iragui, V., & Sejnowski, T. J. 1998.   Extended ICA removes artifacts from   electroencephalographic recordings. Advances in neural   information processing systems, 894-900.   [45] Correa, A. G., Laciar, E., Patio, H. D., & Valentinuzzi, M.  E. 2007. Artifact removal from EEG signals using adaptive   filters in cascade. In Journal of Physics: Conference Series,   90, 1 (2007), p. 012081.       "}
{"index":{"_id":"49"}}
{"datatype":"inproceedings","key":"Huptych:2017:MRB:3027385.3027426","author":"Huptych, Michal and Bohuslavek, Michal and Hlosta, Martin and Zdrahal, Zdenek","title":"Measures for Recommendations Based on Past Students' Activity","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"404--408","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027426","doi":"10.1145/3027385.3027426","acmid":"3027426","publisher":"ACM","address":"New York, NY, USA","keywords":"effort, learning analytics, learning strategy, recommendation, relevance, student retention","Abstract":"This paper introduces two measures for the recommendation of study materials based on students' past study activity. We use records from the Virtual Learning Environment (VLE) and analyse the activity of previous students. We assume that the activity of past students represents patterns, which can be used as a basis for recommendations to current students. The measures we define are Relevance, for description of a supposed VLE activity derived from previous students of the course, and Effort, that represents the actual effort of individual current students. Based on these measures, we propose a composite measure, which we call Importance. We use data from the previous course presentations to evaluate of the consistency of students' behaviour. We use correlation of the defined measures Relevance and Average Effort to evaluate the behaviour of two different student cohorts and the Root Mean Square Error to measure the deviation of Average Effort and individual student Effort","pdf":"Measures for recommendations based on past students activity  Michal Huptych1,2 Michal Bohuslavek1, 3 Martin Hlosta1 Zdenek Zdrahal1, 2  Knowledge Media Institute 1 The Open University, Walton Hall  Milton Keynes, MK7 6AA, UK {michal.huptych; martin.hlosta;  z.zdrahal}@open.ac.uk  CIIRC, 2 Czech Technical University  Zikova street 1903/4 Prague, 166 36 Czech Republic  Faculty of Mechatronics, Informatics3 and Interdisciplinary Studies  Technical University of Liberec Studentska 1402/2, 461 17 Liberec 1  Czech Republic michal.bohuslavek@tul.cz  ABSTRACT This paper introduces two measures for the recommenda- tion of study materials based on students past study activ- ity. We use records from the Virtual Learning Environment (VLE) and analyse the activity of previous students. We as- sume that the activity of past students represents patterns, which can be used as a basis for recommendations to current students.  The measures we define are Relevance, for description of a supposed VLE activity derived from previous students of the course, and Effort, that represents the actual effort of individual current students. Based on these measures, we propose a composite measure, which we call Importance.  We use data from the previous course presentations to evaluate of the consistency of students behaviour. We use correlation of the defined measures Relevance and Average Effort to evaluate the behaviour of two different student cohorts and the Root Mean Square Error to measure the deviation of Average Effort and individual student Effort.  CCS Concepts Applied computingEducation; E-learning; Distance learning;  Keywords Learning strategy; Recommendation; Student Retention; Learn- ing Analytics; Relevance; Effort  1. INTRODUCTION Data and metadata generated by e-learning systems can  be fed back to various education-related tasks, such as the evaluation of learning materials and the design of new ma- terials [4], predictions of student performance [13][16], rec-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027426  ommendation of learning materials and the creation of per- sonalised study plans ([8][2][4][11]).  In order to improve student learning, it is necessary to know which learning activities lead the students towards success. In the case of an online environment with large amounts of materials, this might be difficult to obtain man- ually. However, there are several techniques that allows to process the data an automated way. The important is to specify strategy which might by use for description and rep- resentation of data.  Recommender systems provide information, items of in- terest or services to the user according to the users activ- ities and preferences. This paper presents a new approach to recommender design. Recommenders evaluate user be- haviour and preferences and offer the user the most appro- priate learning resource. There are different recommender techniques [1] [12] implemented in a number of recommender systems [7][15]. According to [12] these techniques can be divided into four categories:   Collaborative techniques construct recommenda- tions from the behaviour and results of similar learners. Similarity is usually calculated from the VLE activi- ties of the recommendation recipient and other learn- ers in the present or past courses. A detailed descrip- tion of collaborative recommenders can be found in [12][3][5][2].   Content-based techniques use for recommendation only information about the users and their histories [12]. Typical problem solving methods are Case Based Reasoning and Attribute-based techniques, which de- rive the recommendations from the learner profile [1][6] [18][12][14].   Matrix/tensor factorization techniques consist of decomposition of a tensor to factors. The recommen- dation calculates factorization of known tensor values, and use the product of factors to obtain the vector of unknown values. For details see [16].   Association rules are machine learning techniques for discovering dependence patterns in data. The rec- ommender mines rules from activities of learners in the past to recommend activities to the current learner. Examples of association rules used for recommenda- tion are in [10][17].    The approach presented in this paper draws from the col- laborative techniques and association rules. We evaluate the VLE activities of successful students in the previous presen- tation, compare it with the currently supported learner and recommend activities that should decrease the differences between the two. Some topics introduced earlier in the study plan are prerequisites for ones presented later, e.g. knowing the HTML language is necessary to understand the design of web applications. These dependencies are reflected in the learner behaviour and could be discovered from the measures introduced in this paper.  2. PROBLEM DESCRIPTION At the Open University (OU), courses (modules) take usu-  ally about 40 weeks and are offered to students in a number of consecutive years. Each module has a study plan which breaks the course content down into Blocks. Each Block presents a different topic taught in the course and is further divided into Parts (1 week long). Thus, Block 1 Part 1 refers to week 1, Block 1 part 2 to week 2, etc. The study plan usually does not significantly change between presentations. Study materials are provided in the Virtual Learning Envi- ronment (VLE) and therefore student clicks can be recorded. Each click has a semantic label called activity type, which indicates the kind of interaction with the VLE. Examples of activity types are forum, resource, ou-content or quiz. Clicks on different activity types have different information content; resource is a page with text in pdf and therefore one click provides access to all the underlying content. On the other hand, ou-content refers to the study materials repre- sented in usually highly structured HTML and the number of VLE accesses pretty well represents student effort. Key study materials in modules are represented as ou-content and for this reason we analyse clicks labeled as ou-content, both in the previous, already completed presentation and in the current one.  Each block in the study plan has associated expected study time. However, since student VLE interactions in the previous presentations are recorded, the real effort required for understanding each topic can be measured in terms of the average number of clicks of successful students on the corresponding web pages. We assume that the performance of students who passed the previous presentation well ap- proximates the effort required at present.  The problem addressed in this paper is how to use ou- content VLE activities of the previous presentation and VLE data collected from current students to design a person- alised study recommender that navigates students through the study plan.  Measuring of the time-on-task is not simple [9]. In our case the approximation by number of clicks is sufficient.  3. RECOMMENDATION STRATEGY The recommendation strategy is constructed from rele-  vance of the study material and learners activity. These concept are formally defined in the following sections.  3.1 Capturing study materials relevance Relevance is defined as a normalized difference of the aver-  age cumulative students activity a, measured by the cumu- lative number of clicks on a specific study activity, between two consecutive weeks i-1 and i :  R (w, a) =  w i=1 cp (i, a)  w1 i=1 cp (i, a)N  i=1 cp (i, a) , (1)  where cp (i, a) is the number of clicks for the activity a in week i,  w i=1 cp (i, a),  w1 i=1 cp (i, a) are cumulative clicks  from the beginning (week 1) to week w and w  1, respec- tively.  N i=1 cp (i, a) is the cumulative sum until the last  week N of the previous presentation. Henceforth:   Relevance is always non-negative, wa,R (w, a)  0,   sum of the Relevance for each activity over all weeks is 1, a,   w R (w, a) = 1,   Relevance of each activity is the same for all students.  An example of the cumulative clicks for 5 selected activ- ities is shown in Figure 1, the corresponding relevance is shown in Figure 2.  3.2 Capturing learners activity Further, we need a measure that can capture the activity  of the learner in the VLE, that can be related to relevance. Therefore, we create a measure Effort and define it as:  E (w, a) =  w i=1 cc (i, a)  w1 i=1 cc (i, a)N  i=1 cp (i, a) , (2)  where cc (i, a) is number of clicks for activity a in week i from current student,cp (i, a) is number of clicks for activity a in week i from previous presentation,  w1 i=1 cc,  w i=1 cc (i, a)  are the numbers of cumulative clicks for given activity from the beginning of the current presentation to week w 1 and w, respectively, and  N i=1 cp (i, a) is the number of cumula-  tive clicks until the last week of the previous presentation. Henceforth:   sum of the Effort for each activity over all weeks can reach one of the following eventuality:  a,  w  E (w, a) is    < 1, if N  i=1 cp (i, a)  > w  i=1 cc (i, a)  = 1, if N  i=1 cp (i, a)  = w  i=1 cc (i, a)  > 1, if N  i=1 cp (i, a)  < w  i=1 cc (i, a)  (3)   Effort is given for each student individually.   Average Effort is given as average of Effort over all students  Thus, the effort represents an approximation of the progress for the given activity for an individual student. An example of effort is shown in Figure 3.  Relevance and Effort, formalized by (1) and (2), capture our intuition of a transferring of the past experience (Rele- vance) to the behaviour of current student (Effort).    0  20  40  60  80  2014/10 2014/11 2014/12 2015/01 2015/02 2015/03 2015/04 2015/05 2015/06 2015/07  date  a v e ra  g e  c  u m  u la  ti v e  c  lic k s  activity name Block 1 Part 1 Block 1 Part 4 Block 2 Part 2 Block 3 Part 2 Block 4 Part 2  Figure 1: Average number of cumulative clicks in time  0.0  0.1  0.2  0.3  5 0 5 10 15 20 25 30 35 40  week  R e le  v a n c e  activity name Block 1 Part 1 Block 1 Part 4 Block 2 Part 2 Block 3 Part 2 Block 4 Part 2  Figure 2: Relevance derived from the cumulative clicks  3.3 Recommendation Thus, we propose a recommender strategy to output for  each activity a in week i its Importance as:  I (w, a) = R (w  1, a) E (w  1, a) , (4)  where R (w  1, a) and E (w  1, a) are appropriate Rele- vance and Effort for given activity in a previous week, re- spectively. Thus, the Importance represents a combination of information of the Relevance of some activity in the pre- vious week and Effort of the student for the given activity.  4. EVALUATION We can empirically evaluate similarity between students  behaviour for the current and previous presentation. We use 2014 presentation for computing the Relevance and 2015 as the presentation for retrieving the learners Effort.  In both presentations, we select only successful students. We disregard the failed/withdrawn students because the pre-  vious research [13] shows that VLE behaviour is the discrim- inative factor between successful and unsuccessful students. From the previous presentation we selected 1,062 students and from the current one 922 students. We focus only on the activity types for which we know that the repeated clicking is relevant, i.e. ou-content.  The Relevance and the Effort are both positive for all activities and weeks. If we use an Average Effort (over all students) in particular weeks, we can postulate that the Rel- evance and the Average Effort should be correlated. To measure the similarity, we use Pearsons correlation.  Figure 4 shows that the Relevance of the educational ac- tivities in the previous presentation is similar with the Effort in the current presentation across all the weeks for successful students. This means that a) the behaviour of the successful students does not change from the previous to the current presentation and b) the use of Effort value will recommend the activity which should allow the learner to achieve similar results as the successful students in the topics where they    0.0  0.1  0.2  0.3  5 0 5 10 15 20 25 30 35 40  week  E ff o rt  activity name Block 1 Part 1 Block 1 Part 4 Block 2 Part 2 Block 3 Part 2 Block 4 Part 2  Figure 3: Example of the Effort  Block 1 Part 1 Block 1 Part 2 Block 1 Part 3 Block 1 Part 4 Block 1 Part 6 Block 2 Part 1 Block 2 Part 2 Block 2 Part 4 Block 2 Part 5 Block 3 Part 1 Block 3 Part 2 Block 3 Part 3 Block 3 Part 4 Block 3 Part 5 Block 4 Part 1 Block 4 Part 2 Block 4 Part 3 Block 4 Part 4 Block 4 Part 5 Block 5 Part 1 Block 5 Part 2 Block 5 Part 3 Block 5 Part 4 Block 5 Part 5 Block 6 Part 1  B lo  c k  1   P a rt   1 B  lo c k  1   P a rt   2 B  lo c k  1   P a rt   3 B  lo c k  1   P a rt   4 B  lo c k  1   P a rt   6 B  lo c k  2   P a rt   1 B  lo c k  2   P a rt   2 B  lo c k  2   P a rt   4 B  lo c k  2   P a rt   5 B  lo c k  3   P a rt   1 B  lo c k  3   P a rt   2 B  lo c k  3   P a rt   3 B  lo c k  3   P a rt   4 B  lo c k  3   P a rt   5 B  lo c k  4   P a rt   1 B  lo c k  4   P a rt   2 B  lo c k  4   P a rt   3 B  lo c k  4   P a rt   4 B  lo c k  4   P a rt   5 B  lo c k  5   P a rt   1 B  lo c k  5   P a rt   2 B  lo c k  5   P a rt   3 B  lo c k  5   P a rt   4 B  lo c k  5   P a rt   5 B  lo c k  6   P a rt   1  Relevance  A v e ra  g e   E ff o rt  1.0  0.5  0.0  0.5  1.0 correlation  Figure 4: Correlation matrix for Relevance of previous presentation and Average Effort for current presenta- tion  are lagging behind. To show a deviation of the Average Effort and individual  Efforts we use the Root Mean Square Deviation (RMSD) (definition in [16]). The RMSD for the selected particular  activities is shown in Table 1. Dependencies between topics are shown in Figure 2. For  example, though the highest relevance of Block 1 Part 1 is in about week 1 of the presentation, the topic is obviously    Table 1: RMSD of average Effort and particular in- dividual Efforts  Activity name RMSD Activity name RMSD Block 1 Part 1 0.14 Block 3 Part 5 0.15 Block 1 Part 2 0.13 Block 4 Part 1 0.10 Block 1 Part 3 0.11 Block 4 Part 2 0.11 Block 1 Part 4 0.12 Block 4 Part 3 0.12 Block 1 Part 6 0.11 Block 4 Part 4 0.14 Block 2 Part 1 0.17 Block 4 Part 5 0.14 Block 2 Part 2 0.16 Block 5 Part 1 0.12 Block 2 Part 4 0.17 Block 5 Part 2 0.12 Block 2 Part 5 0.18 Block 5 Part 3 0.14 Block 3 Part 1 0.14 Block 5 Part 4 0.16 Block 3 Part 2 0.11 Block 5 Part 5 0.12 Block 3 Part 3 0.14 Block 6 Part 1 0.14 Block 3 Part 4 0.12    also relevant in week 7 and 8. Similar dependencies exist between other topics.  5. CONCLUSIONS AND FUTURE WORK In this work, we propose a novel strategy for personalized  study recommendation that utilises the information from the successful students in the previous presentation. We define two measures, Relevance and Effort, which describe a past students behaviour and current students effort, re- spectively. Further, we define the theoretical principle of the recommendation based on these two measures, which we call Importance.  We use the historical VLE activity for evaluation of our concept by correlating Relevance and Effort, which repre- sents consistency of students behaviour between both pre- sentations. The result shows a correlation (means  std = 0.94  0.05) between the activities of previous and current students. We interpret this finding as confirmation that the successful students have an important and significant pat- tern of learning.  Currently, we are enriching the OUAnalyse system with the proposed recommender and we are planning to evaluate its impact on students behaviour.  6. REFERENCES [1] G. Adomavicius and A. Tuzhilin. Toward the next  generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE Trans. on Knowl. and Data Eng., 17(6):734749, April 2005.  [2] A. R. Anaya, M. Luque, and M. Peinado. A visual recommender tool in a collaborative learning experience. Expert Systems with Applications, 45:248259, March 2016.  [3] J. Bobadilla, F. Serradilla, and A. Hernando. Collaborative filtering adapted to recommender systems of e-learning. Knowledge-Based Systems, 22(4):261265, May 2009.  [4] M.-I. Dascalu, C.-N. Bodea, M. N. Mihailescu, E. A. Tanase, and P. O. de Pablos. Educational recommender systems and their application in lifelong learning. Behaviour & Information Technology, 35(4):290297, January 2016.  [5] M.-I. Dascalu, C.-N. Bodea, A. Moldoveanu, A. Mohora, M. Lytras, and P. O. de Pablos. A recommender agent based on learning styles for better virtual collaborative learning experiences. Computers in Human Behavior, 45:243253, 2015.  [6] H. Drachsler, H. G. Hummel, and R. Koper. Personal recommender systems for learners in lifelong learning networks: the requirements, techniques and model. International Journal of Learning Technology, 3(4):404423, July 2008.  [7] H. Drachsler, K. Verbert, O. C. Santos, and N. Manouselis. Panorama of Recommender Systems to Support Learning, In Recommender Systems Handbook (eds: F .Ricci and L. Rokach and and S. Bracha). Springer US, Boston, MA, 2015.  [8] G. Durand, N. Belacel, and F. LaPlante. Graph theory based model for learning path recommendation. Information Sciences, 251:1021, December 2013.  [9] V. K. et al. Does time-on-task estimation matter implications on validity of learning analytics findings. Journal of Learning Analytics, 2(3):81101, February 2016.  [10] E. Garca, C. Romero, S. Ventura, and C. de Castro. An architecture for making recommendations to courseware authors using association rule mining and collaborative filtering. User Modeling and User-Adapted Interaction, 19(1):99132, February 2009.  [11] A. Garrido, L. Morales, and I. Serina. On the use of case-based planning for e-learning personalization. Expert Systems with Applications, 60:115, October 2016.  [12] A. Klasnja-Milicevic, M. Ivanovic, and A. Nanopoulos. Recommender systems in e-learning environments: a survey of the state-of-the-art and possible extensions. Artificial Intelligence Review, 44(4):571604, December 2015.  [13] J. Kuzilek, M. Hlosta, D. Herrmannova, Z. Zdrahal, and A. Wolff. Ou analyse: analysing at-risk students at the open university. Learning Analytics Review, LAK15-1:116, March 2015.  [14] J. Liu, P. Dolan, and E. R. Pedersen. Personalized news recommendation based on click behavior. In Proceedings of the 15th International Conference on Intelligent User Interfaces, pages 3140. ACM, February 2010.  [15] J. Lu, D. Wu, M. Mao, W. Wang, and G. Zhang. Recommender system application developments: A survey. Decision Support Systems, 74:1232, June 2015.  [16] N. Thai-Nghe, L. Drumond, A. Krohn-Grimberghe, and L. Schmidt-Thieme. Recommender system for predicting student performance. Procedia Computer Science, 1(2):28112819, 2010.  [17] F.-H. Wang and H.-M. Shao. Effective personalized recommendation based on time-framed navigation clustering and association mining. Expert Systems with Applications, 27(3):365377, October 2004.  [18] Y. J. Yang and C. Wu. An attribute-based ant colony system for adaptive learning object recommendation. Expert Systems with Applications, 36(2):30343047, March 2009.    "}
{"index":{"_id":"50"}}
{"datatype":"inproceedings","key":"Kopeinik:2017:SCL:3027385.3027421","author":"Kopeinik, Simone and Lex, Elisabeth and Seitlinger, Paul and Albert, Dietrich and Ley, Tobias","title":"Supporting Collaborative Learning with Tag Recommendations: A Real-world Study in an Inquiry-based Classroom Project","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"409--418","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027421","doi":"10.1145/3027385.3027421","acmid":"3027421","publisher":"ACM","address":"New York, NY, USA","keywords":"base-level learning equation, cognitive user models, inquiry-based learning, minerva, personalized tag recommendations, real-world testing, semantic stabilization, technology enhanced learning","Abstract":"In online social learning environments, tagging has demonstrated its potential to facilitate search, to improve recommendations and to foster reflection and learning.Studies have shown that shared understanding needs to be established in the group as a prerequisite for learning. We hypothesise that this can be fostered through tag recommendation strategies that contribute to semantic stabilization. In this study, we investigate the application of two tag recommenders that are inspired by models of human memory: (i) the base-level learning equation BLL and (ii) Minerva. BLL models the frequency and recency of tag use while Minerva is based on frequency of tag use and semantic context. We test the impact of both tag recommenders on semantic stabilization in an online study with 56 students completing a group-based inquiry learning project in school. We find that displaying tags from other group members contributes significantly to semantic stabilization in the group, as compared to a strategy where tags from the students' individual vocabularies are used. Testing for the accuracy of the different recommenders revealed that algorithms using frequency counts such as BLL performed better when individual tags were recommended. When group tags were recommended, the Minerva algorithm performed better. We conclude that tag recommenders, exposing learners to each other's tag choices by simulating search processes on learners' semantic memory structures, show potential to support semantic stabilization and thus, inquiry-based learning in groups.","pdf":"Supporting collaborative learning with tag recommendations: a real-world study in an inquiry-based  classroom project  Simone Kopeinik ISDS  Graz University of Technology Graz, Austria  simone.kopeinik@tugraz.at  Elisabeth Lex ISDS  Graz University of Technology Graz, Austria  elisabeth.lex@tugraz.at  Paul Seitlinger School of Educational  Sciences Tallinn University Tallinn, Estonia  paul.seitlinger@tlu.ee Dietrich Albert  ISDS Graz University of Technology  Graz, Austria dietrich.albert@tugraz.at  Tobias Ley School of Educational  Sciences Tallinn University Tallinn, Estonia tley@tlu.ee  ABSTRACT In online social learning environments, tagging has demon- strated its potential to facilitate search, to improve rec- ommendations and to foster reflection and learning.Studies have shown that shared understanding needs to be estab- lished in the group as a prerequisite for learning. We hy- pothesise that this can be fostered through tag recommen- dation strategies that contribute to semantic stabilization. In this study, we investigate the application of two tag rec- ommenders that are inspired by models of human memory: (i) the base-level learning equation BLL and (ii) Minerva. BLL models the frequency and recency of tag use while Min- erva is based on frequency of tag use and semantic context. We test the impact of both tag recommenders on semantic stabilization in an online study with 56 students complet- ing a group-based inquiry learning project in school. We find that displaying tags from other group members con- tributes significantly to semantic stabilization in the group, as compared to a strategy where tags from the students individual vocabularies are used. Testing for the accuracy of the different recommenders revealed that algorithms us- ing frequency counts such as BLL performed better when individual tags were recommended. When group tags were recommended, the Minerva algorithm performed better. We conclude that tag recommenders, exposing learners to each others tag choices by simulating search processes on learn- ers semantic memory structures, show potential to support semantic stabilization and thus, inquiry-based learning in groups.  LAK 17 March 1317, 2017,Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027421  CCS Concepts Information systems Social tagging systems; Rec- ommender systems; Applied computing  Collab- orative learning; Computing methodologies Cog- nitive science;  Keywords Semantic stabilization; Personalized tag recommendations; Cognitive user models; Base-level learning equation; Min- erva; Real-world testing; Inquiry-based learning; Technology enhanced learning  In open and social learning environments, students need to construct knowledge in a self-directed manner in a so- cial context. In inquiry-based learning (IBL), for example, students are encouraged to collect and retrieve information and create new content, which is continuously uploaded to the learning environment. In these settings, tagging has demonstrated its potential to enrich awareness and reflection of students. An empirical study conducted by Kuhn et al. [26] indicates that tagging supports learning in IBL by help- ing students organize, retrieve and reflect upon the content of learning resources they found on the Web (e.g., learning videos) or generated themselves (e.g., blog entries). Medi- ated by tagging, these activities become inherently social activities (e.g., [14]) as the tag vocabulary, on which a stu- dent draws to organize and reflect on resources, emerges not only from personal tag choices, but also from those of others. Students of an IBL setting can thus be expected to benefit from semantic stabilization (e.g., [40])  a phenomenon that becomes manifest in an increasing convergence in choosing tags for particular ranges of topics: the more stable the cur- rently evolved tag vocabulary is, the more helpful it should be to share own and exploit others search results (i.e., Web resources). And indeed, a study of Ley and Seitlinger [27] re- vealed that students tend to acquire more knowledge about domain concepts, if they act in groups that exhibit rela- tively higher levels of semantic stabilization. We therefore conclude that IBL can be supported by processes helping  rodkin Typewritten Text This work is licensed under a Creative Commons  Attribution-NonCommercial International 4.0 License.  https://creativecommons.org/licenses/by-nc/4.0/ https://creativecommons.org/licenses/by-nc/4.0/   students in achieving convergence in the naming of learning concepts and development of a more stable tag vocabulary. One strategy to drive such processes is to apply tag rec- ommendation mechanisms that stabilize tagging habits by displaying tags already used in the past (e.g., [12]).  Within technology enhanced learning (TEL) research, rec- ommendation mechanisms are part of adaptation or person- alization strategies that support students in their individual learning processes [10]. Recommendation mechanisms, ex- tract and draw on relevant data from learning traces, lever- aging learning analytics [11, 7]. This is one way of tackling the often criticized lack of support for the self-organization of learning content in TEL environments [3].  A tremendous number of recommendation approaches have been suggested in recent years [20]. However, research mainly focuses on the recommendation of learning resources, peers and activities [30, 10]. TEL research on tag recommenda- tion mechanisms and its potential for learning is still widely unattended [21]. Problem. The aim of this work is to test the impact of tag- ging recommendation mechanisms on semantic stabilization in an online IBL setting. With regard to results presented in Font et al. [12] it is fair to assume that an increased awareness of peer learners tag choices will promote the de- velopment of a common terminology. In particular, we are interested to measure the performance of tag recommenda- tions in two settings: personal (P), where students receive tag recommendations based on their personal tagging his- tory and collective (C), where students receive tag recom- mendations based on the collective tagging history of their learning group.  Additionally, our work is motivated by a more techni- cal stance: When selecting a proper tag recommendation strategy, TEL specific requirements need to be taken into account. For instance, in TEL scenarios, data is typically of a sparse nature [39]. Furthermore, sensitivity to an al- gorithms complexity is crucial when calculating real time recommendations on limited computing resources [31]. Approach and Methods. A very simple, though rela- tively effective, tag recommendation strategy is the Most Popular (MP) algorithm [19, 22]. We however assume that a frequency-based, computationally simple recommendation strategy may be even more successful, if it is grounded on a thorough understanding of how humans process informa- tion. Our hypothesis is that in online social learning environ- ments, semantic stabilization can be fostered by cognitively inspired tag recommendation approaches. Offline data stud- ies have indicated that the modelling of cognitive processes underlying tagging habits leads to an increased accuracy of recommendations [38]. However, offline data studies are lim- ited to evaluating the prediction of user behaviour. In our previous work [24, 22], we have intensively investigated the suitability of two tag recommendation approaches via offline studies [22]: the first of these, known as BLL implements the Base Level Learning Equation [1], which models the fre- quency and recency of past tag use. The second algorithm, known as Minerva [18, 36], incorporates tag use frequency as well as semantic context. Both approaches aim to imitate cognitive processes of retrieving words from memory.  In the present work, we study the performance of the algo- rithms in an online, real-world scenario to explore whether the promising results from offline data studies generalize to online environments. To this end, we carried out a field  study in which students used an online IBL environment in a realistic school context for a duration of about four weeks. Our aim is to investigate the effectiveness of the two cognitively inspired recommendation mechanisms BLL and Minerva that mimic students tagging behaviour, taking into account either temporal or semantic context. Contributions. Our contributions are twofold: First, we investigate the question of whether semantic stabilization  a socio-cognitive process supporting individual learning in an online IBL environment [27]  can be supported by tag recommendation mechanisms that have been developed and tested previously in offline studies on a variety of data sets.  Second, we systematically vary two variables underlying the design of these recommenders in order to derive more precise and practical design implications for specific learning settings. The first variable is the vocabulary, from which the algorithm selects the tags and which can either be the learners personal (P) or the collective vocabulary of the whole group of learners (C). The second variable is the type of information the algorithm takes into account to estimate the current probability of a tag being retrieved from the learners memory. While MP only considers a tags usage frequency (baseline), our two cognitively inspired algorithms extend this approach by the information of recency of usage (BLL) and the extent to which a tag matches the current (i.e., the resources) semantic context (Minerva).  The results indicate that the application of recommenders using collective tagging traces fosters semantic stabilization in collaborative learning settings. The consideration of fre- quency and semantic context further contributes to the ade- quacy of tagging recommendations. In respect to individual learning we find that a frequency and recency based ap- proach (BLL) performs best.  1. RELATED WORK At present, we identify three main lines of research related  to our work: tagging as a support in learning, semantic sta- bilization in social learning systems and tag recommendation approaches in the context of TEL.  1.1 Tagging as a Support in Learning Due to the growing quantity of learning resources and  learning data available in digital learning repositories and on the web [10], learners often struggle with the organiza- tion, the retrieval and even the awareness of relevant learn- ing content [9, 2].  Tagging, as a simple mechanism to annotate resources in- dividually or socially, has demonstrated its potential to facil- itate search, to improve recommendations and to foster re- flection and learning on the Web precisely as in technology- enhanced learning environments [41, 17]. For instance Kuhn et al. [25] investigated the effect of learning item annotation in the context of IBL and found that tagging encourages students to reflect upon retrieved learning contents. More- over, Bateman and Brusilovsky [3] argue that according to Blooms taxonomy of learning [6], learners engagement in the tagging process fosters the development of a metacogni- tive level of knowledge, and hypothesize that the evaluation of peer learners tags might even lead to a deeper level of learning.  Also, tagging in open and social learning settings can be applied as an alternative to the unpopular, since resource intensive, description of learning items through the adding    of metadata that is typically done by expert users [3]. Con- trary to indexing mechanisms with controlled vocabularies, tagging allows for unrestricted extension of verbalism: so- cial tagging systems are not bound to the use of predefined language or terminology, but its classification vocabulary grows with its users interactions. This entails advantages, such as the support of an adaptive level of granularity, but also challenges such as the lack of a coherent and useful tag vocabulary [32]. Along these lines, research [26, 27] indicates that students seek assistance in the tagging process, regard- ing two aspects: (a) the take up of the process and therefore, the finding of initial vocabulary and (b) the achievement of a semantically stable vocabulary amongst their learning peers.  1.2 Semantic Stabilization in Social Learning Systems  Individual users tagging of items shows great potential in the organization of knowledge within and across information systems [29]. However, the usefulness of such annotations is conditioned by the development of a shared terminology that leads to a meaningful description of resources [40]. The at- tainment of an implicit consensus on a collective vocabulary within a group, which is stable over time and in meaning, is called semantic stability [35]. In this work, we use the notion of semantic stabilization not to refer to a point in time, at which such consensus is reached and remains stable thereafter, but, we use it and a simple measure thereof (see Section 3.1) to merely characterize and compare the evolu- tion of convergence in tag choices of two groups of students over a short period of time (few weeks).  Fu et al. [15] shows that throughout the learning pro- cess (e.g., the exploration of knowledge) semantic structures of users in a social tagging environment assimilate. Thus, learners are influenced by the tagging behaviour of their peers. Other research assumes a mutual influence between learners internal knowledge representation and the tagging vocabulary that emerges in the social information system, in which they interact [13]. Ley et al. [27] investigates these dynamics and finds a positive influence of semantic stabi- lization on individual learning. Following this, we do not assume stabilization to be a prerequisite for learning but only that it provides some helpful structure for individual learning activities and is therefore conducive to individual learning gains [27].  1.3 Tag Recommendation Approaches in the Context of TEL  Despite the reported potential of tagging and students de- mand for tagging support, the study of tag recommendation mechanisms has been widely unattended in TEL research [21]. Noteworthy are some initial attempts that aim to fill this gap: Diaz et al. [8] investigated the automated tag- ging of learning objects utilizing a computationally expen- sive variant of Latent Dirichlet Allocation [5] and evaluated the tagging predictions in a user study. In Niemann et al. [33], an approach to automatically tag learning objects based on their usage context was introduced. It shows promising results towards the retrospective enhancement of learning object meta-data. However, their approach cannot be used in online settings as it is based on context information of resources that is extracted from user sessions. Kopeinik et al. [22] presents an offline data study comparing a varia- tion of tag recommendation strategies on six TEL data sets.  The present work builds upon this study, since it encom- passes algorithms that are applicable in runtime-sensitive online environments such as often found in educational set- tings. Moreover, their results have shown that simple rec- ommendation mechanisms based on the Base Level Learning Equation (BLL) and Most Popular (MP) outperform other state-of-the-art algorithms.  2. EXPERIMENTAL SETUP To test our hypothesis that in online social learning en-  vironments, semantic stabilization can be fostered by cog- nitively inspired tag recommendation strategies, we imple- mented a real-world evaluation in the context of high school biology lessons, engaging students in IBL projects. To this end, we used an online environment for open social inquiry- based learning. IBL itself is very well-suited to the purpose of a collaborative tagging study as throughout the learning process, students are constantly challenged to find, create, upload and share content. In the course of the study, four secondary school classes with students at the age of 15 to 17 used a dedicated social learning environment to work on their biology projects.  2.1 Procedure Prior to the first lesson, students and their parents were  presented with the goals and benefits of using IBL and the online learning environment, as well as with the aims of the study. Afterwards, parents and students were asked for their consent in written and verbal form, respectively. Through- out the study students participation was not obligatory, in either the platform or in tagging and did not contribute to their grading.  For the purpose of the study, students of each class were divided in two groups per class, which led to groups of 9 to 18 students, depending on class size. In the first two lessons, students were introduced to the online learning en- vironment (see Section 2.3) and the concepts of IBL. Then, each group used the virtual learning environment during at least eight school lessons over a period of four weeks or longer to complete an IBL project. The teacher provided each of the classes learning groups with similar learning content and learning tasks and acted in a supporting role. The variation between groups is constituted by the nature of tag recom- mendations. Tag recommendations of one group are based on individual users personal tag data, whereas the second groups tag recommender draw on the groups collective tag- ging traces. According to the group, tag recommendation strategies were randomly selected either from the personal or the collective pool of recommendation strategies, as illus- trated in Table 1. Each student was provided with a tablet computer available during class. Students were encouraged to use the tagging functionality when creating or upload- ing new content, and were also provided with information in verbal and written form, on how to do this.  2.2 Study Design We investigated the suitability of BLL and Minerva for  facing the challenges of real-world learning settings. The resulting data sample consists of N=56 students with an age ranging from 15 to 17 years. As summarized in Table 1, the independent variables formed a 2 (Vocabulary: Personal vs. Collective; between-subjects) by 3 (Algorithm: MP vs. BLL vs. Minerva; within-subjects) design. In addition we    Table 1: Students of each class were separated in two groups and consequently received either tag recom- mendations based on their personal tagging history (P) or based on the collective tagging traces of their inquiry group (C). Condition C was complemented by the mixed approach BLLU+ MPG.  Vocabulary Algorithm Personal (P) MPU BLLU MinervaU Collective (C) MPG BLLG MinervaG BLLU+ MPG  consider BLL+MP for the collective vocabulary condition. This recommendation approach is of particular interest, as in offline studies on TEL data sets it clearly outperformed remaining algorithms [22]. The dependent variables were se- mantic stabilization and recommender accuracy (see Section 3).  2.3 Environment The study was implemented in the collaborative online  learning environment weSPOT1. weSPOT is a European re- search project, and stands for Working Environment with Social and Personal Open Tools for IBL. In the course of the project, a theoretical framework and corresponding TEL tools for science learning and teaching have been developed. The tool set aims to support teachers in the application of IBL as a classroom activity [37].  The weSPOT platform, or more concretely, the weSPOT inquiry space guides students through the inquiry cycle, which models the scientific inquiry process in six phases: Question/Hypothesis, Operationalisation, Data Collection, Data Analysis, Interpretation/Discussion and Communica- tion. Each phase further includes dedicated activities as discussed in Protopsaltis et al. [34].  Figure 1 exhibits the weSPOT inquiry space that imple- ments the six IBL phases, providing individual tabs for each phase (1). The phase-tabs further include widgets (2) that enable the students to carry out activities relevant to a spe- cific phase. Reflection and support tools such as a learning analytics dashboard, an open user model and a recommen- dation interface are provided in the platforms side panel (3).  The weSPOT space supports collaborative learning in de- fined groups. Each student group is thus provided with a sub-environment that forms an inquiry space addressing a specified research interest. Teachers take on a supportive and administrating role. In the platform, they are provided with a configuration interface, for designing inquiry spaces by selecting phases (tabs) and activities (widgets) that suit the purpose of their students inquiry projects. Teachers also add students and initial learning content to the group environment.  While students work on their inquiry projects, they en- gage in activities that typically create content by, e.g., post- ing questions, starting or contributing to discussions or by uploading documents and pictures. These and other learn- ing activities are tracked, saved and fed into different user profiles, to be later used in learning analytic diagrams, to issue badges and to provide personalized recommendations of learning resources and tags. Technical Insights. The core of the weSPOT environment  1http://inquiry.wespot.net/  Figure 1: The collaborative online learning plat- form: weSPOT IBL space. (1) shows the IBL phases that are depicted as one tab each, (2) the widgets in one tab and (3) the side panel with external (sup- porting) tools and group information.  is an online platform which is based on elgg2. Elgg is an open source social networking engine that is extendable via plugins and follows a MVC (Model-View-Controller) pat- tern, which makes it convenient to extend. When a user enters content (e.g., question, hypothesis, file or discussion entry) to an inquiry, this happens through an input form which includes a tag view. The tag recommendation plu- gin is an extension of this tag viewand adaptively suggests tags to users. The tag view (thus also our tag recommen- dation functionality) is by default included in all plug-ins that allow users to create content, for instance in discus- sions, file uploads or blog entries. Figure 2 shows such an input form with our recommendation plug-in embodied as marked by the orange frame. Recommendations are calcu- lated in a backend web-service component, on the basis of the randomly selected recommendation strategy. Following common practice in social tagging systems, we set the num- ber of tag recommendations to five. However, due to the cold start of the user and group environment, fewer tags may be presented if fewer tags are available. Learners can either select from the suggested tags by clicking on the tag or they can manually enter their own tags. Tagging Interface. Figure 2 shows an extended version of the environments standard input form. The tag recom- mendation plugin that extends the form is marked with an orange frame.  Within this study, the annotation process consisted of two steps: firstly, the selection of semantic features (attributes) from a provided dropdown menu (1): the attributes were drawn from the inquirys domain model which has been pro- vided by the teacher. Further information on the domain model and related tools can be found in Bedek et al. [4]. Secondly, the assignment of tags: after the student closed the dropdown menu, tag recommendations (3) appeared just  2https://elgg.org/    Figure 2: The standard elgg input form extended by our tag recommendation plugin (marked with the orange frame). After choosing relevant semantic fea- tures, students can either select from recommended tags (3) by clicking on the selected item or enter their own tags in the text field (2).  below the tags input text field (2). Students could either select from these recommendations or add their own tags manually.  2.4 Learning Analytics Data Collection The data sets used in this study were collected on a dedi-  cated log data server, from which we extracted the eight in- quiry groups that participated in our experiment. All groups consisted of students attending a high school in Graz and worked on the projects in the course of biology classes, on altogether four different research topics. As one setting took place in the course of an extra-curricular specialisation, ten students participated twice in the experiment. The data was collected over a period of three school semesters (i.e., spring 2015 till summer 2016).  Although students were provided with initial instructions on the tagging interface and the tagging process itself, a relatively large number of students did not tag at all, or provided tags in unusual ways. Consequently, we manually pre-filtered the data sets by mainly excluding posts with tags in form of sentences or tags concatenated with special characters. Students with no remaining posts were also ex- cluded from the data sets, which led to the data samples given in Table 3a.  To evaluate semantic stability, which we measured in tag growth (TG), we selected the class which created the highest number of posts in both inquiry groups. We consider this sample as the most significant to our investigation. To mea- sure the recommendation accuracy (RA), we subsume all samples under the independent study variable Vocabulary. The resulting data set properties are presented in Table 3b.  3. EVALUATION MEASURES Our study design treats semantic stability and recommen-  dation accuracy as dependent variables. In this section, we give insight into the evaluation of both variables.  3.1 Semantic Stabilization As summarized in Wagner et al. [40], a multitude of met-  rics is available to evaluate semantic stabilization. Only few methods are yet suited for narrow folksonomies, where items are tagged only by the uploading user. Lin et al. [28] presents the Macro Tag Growth Method (MaTGM) that measures social vocabulary growth at a systemic level, look- ing at the social tagging system as a whole. In our setting, we consider each IBL group as an isolated social tagging sys- tem and thus, we apply MaTGM to compare the tag growth within these systems.  To that end, we first select the class that generated the most extensive tag data sets for both conditions (personal and collective) as representative groups. The selected data set is described in Table 2 as study TG. Then, for each group, we sort the posts (tag assignments) according to their timestamps, ending with the most recent item annotation. The tag growth after each post, is calculated as a value pair (tgi, f(tgi)), where tgi is the cumulative number of tags, and f(tgi) is the cumulative number of unique tags occurring in i posts.  3.2 Recommender Accuracy We evaluated the performance of the tag recommendation  algorithms MP, BLL and Minerva utilizing the performance metrics recall, precision and f-measure, which are commonly used in recommender system research [31]. When calculat- ing recall and precision, for each post, we determine the relation of tags recommended Tu,r to a user u for a resource r to the tags that the user assigned to a resource Tu,r. Recall (R) indicates how well the recommendation sup- ported the user, giving the relation between correctly rec- ommended tags (i.e. the subset of recommended tags, that the user assigned to the resource) and the set of tags the user needed to describe the resource.  R(Tu,r, Tu,r) = |Tu,r  Tu,r| |Tu,r|  (1)  Precision (P) is the proportion of tags that have been recommended correctly.  P (Tu,r, Tu,r) = |Tu,r  Tu,r| |Tu,r|  (2)  F-measure (F) combines recall and precision to their har- monic mean.  F = 2  (precision  recall) (precision + recall)  (3)  All metrics are averaged over the number of considered posts.  4. ALGORITHMS The applied recommendation mechanisms have been ex-  tensively investigated in offline experiments [24, 22] where they showed promising results when applied on social book- marking and TEL data sets. Notably, the cognitively in- spired mechanisms consistently outperformed state-of-the- art tag recommendation algorithms such as Collaborative Filtering, FolkRank and even graph based methods.    Table 2: |P | depicts the number of posts, |U | the number of users, |T | the number of tags, |Tunq| the number of unique tags, |ATu| the average number of tags per user, |APu| the average number of posts per user. Vocabulary refers to the data the tag recommendations were based on i.e., (P)ersonal or (C)ollective.  Research Topic Vocabulary |P | |U | |T | |Tunq| |ATu| |APu|  Soil ecosystems P 9 6 17 11 2.3 1.5 C 98 13 177 32 5.4 7.5  Biodiversity in cities P 8 4 19 9 2.3 2.0 C 35 14 75 24 3.9 2.5  Renewable resources P 6 5 29 22 4.6 1.2 C 12 8 34 19 4.1 1.5  Climate change P 65 6 232 85 16.8 10.8 C 83 10 297 86 16.4 8.3  (a) Properties of the preprocessed data sets extracted from eight inquiry groups.  Aspect Vocabulary |P | |U | |T | |Tunq| |ATu| |APu|  TG C 83 10 297 86 16.4 8.3 P 65 6 232 85 16.8 10.8  RA C 228 38 584 153 15.4 6.0 P 88 18 297 121 16.5 4.9  (b) Properties of the data sets taken into account for the investigations of two aspects: Tag growth (TG) and recommendation accuracy (RA).  4.1 Most Popular Tags The Most Popular approach (MP) is a simple mechanism  to rank tags according to their frequency of occurrence [19]. The algorithm is used as a baseline.  4.2 Base Level Learning Equation Tagging resources on the web can be understood as a very  basic form of communication, where people quickly retrieve word forms from their long-term memory [16], in order to provide textual labels for organizing their resources. In [23], we discuss and evaluate a personalized tag recommenda- tion mechanism that mimics retrieval from human memory. The mechanism implements equations developed within the ACT-R architecture [1], in particular, to model the acti- vation Ai and hence availability of elements in a persons declarative memory. Equation 4 comprises the base-level ac- tivation BLL and an associative component that represents semantic context. To model the semantic context, we look at the tags other users have assigned to the given resource, with Wj representing the frequency of appearance of a tagj and with Sji representing the normalized co-occurrence of tagi and tagj , as an estimate of the tags strength of associ- ation.  Ai = BLL +  j  WjSji (4)  With equation 5, we estimate how useful an item (tag) has been for an individual person in the past, with n determining the frequency of tag use in the past, and tj standing for recency, i.e. the time since a tag has been used for the jth  time. The parameter d models the power law function of forgetting and is in line with Anderson et al. [1] set to 0.5.  BLL = ln(  n j=1  tdj ) (5)  For the purpose of this study and taking into account data provided by the evaluation environment, the associative component cannot be calculated, as this component is based on tags other users have assigned to the very same content or item. weSPOT, however, is a narrow folksonomy (such as for instance Flickr), where content is generated and tagged only by one user. We thus make the assumption that BLL is the most accurate approach for our data set.  The most frequent tags of the users inquiry group are considered, however, in order to continue collecting context information (i.e. tags that are new to a user). This is imple- mented in an additional recommendation approach denoted by BLLU+ MPG.  4.3 Minerva The Minerva model aims to mimic a process of human  categorization as introduced and described in Seitlinger et al. [36]. It consists of a simple network model with an in- put, a hidden and an output layer. The input layer is a vector P of n features that describe the item to be tagged. Within this study students assigned semantic features to their resources, by selecting suitable attributes from a drop- down menu. These attributes were drawn from the learning groups domain model which was provided by the teacher and describes the learning topic of a group in form of a for- mal concept lattice. For further information on the domain model please see Bedek et al. [4].  The hidden layer stores feature vectors of all data set items in a matrix S, such that Sik is the activation of feature k in item i. Furthermore, in a tag matrix A, each data set item is associated with a tag vector Ai. Specifically, a tag activation value aij is defined as 1 if tag j was present in item i, and 0 otherwise. Taken the input vector as stimuli, the activation of single tags can be calculated. To this end we first compute the cosine similarity for the input feature vector P with each feature vector Si in our matrix, following equation 6:    Simi =  n k=1(Pk  Sik)n  k=1 P 2 k   n k=1 S  2 ik  (6)  where Pk and Sik are components of vector P and Si re- spectively. Finally, we calculate the activation value toutj of tag j as the weighted sum of tag activation values over all items in the data set.  toutj =  i  Simi  aij (7)  The output layer is a ranked list of tags, with a maximum of five suggestions.  5. RESULTS AND DISCUSSION This section presents the results of our evaluation. We  evaluate the suitability of the algorithms described earlier for supporting learners tagging processes. In line with the study design, all algorithms are applied in two modes: Per- sonal (P), where the recommendation strategy draws on a single users posting history, and collective (C), where the recommendation strategy draws on the prior posts of an en- tire group.  5.1 The Impact of Individual and Collabora- tive Tag Recommendations on Semantic Stabilization  The two plots illustrated in Figure 3 present the develop- ment of the tag vocabulary on a group level as described in Section 3.1. The graphs put side by side, the tag growth occurring in the collective group vocabulary condition (C) with the tag growth happening in the personal vocabulary condition (P), where students received their tag recommen- dations either based on collective tag traces or on personal tag traces, respectively. Figure 3a depicts the tag growth function according to the Macro Tag Growth Method and shows that while initially the vocabulary growth overlaps in both groups, group C starts to introduce less new vocab- ulary in relation to tags than group P. In other words, we can observe that students in the collective condition start to pick up the vocabulary of their peers faster. This result is even stronger when considering that a greater number of users contributed to the tagging data of the collective con- dition than to the data of the personal vocabulary condition (see Table 2). This indicates a positive effect of collective tag recommendations on semantic stabilization. Figure 3a provides additional insights into the timing of the process. We can observe that the two tag growth functions clearly diverge after about 40 added posts.  5.2 The Accuracy of Cognitive-Inspired Tag Recommendation Strategies in an Online Data Setting  This section presents the results of our evaluation study in respect to recommendation accuracy. Table 3 provides the number of observations (see column NT ) and accuracy estimates (R, P and F) for each recommender.  The table discloses the impact of the two variables algo- rithm and data set on performance: BLL appears to reach higher estimates than Minerva (relative to MP) under the personal vocabulary condition, with the opposite being true for the collective condition.  (a) Tag growth function according to the Macro Tag Growth Method.  (b) Number of unique tags accumulated with consecutive tag assignments.  Figure 3: The plots show the development of tag- ging vocabulary on a system (inquiry-based learn- ing group) level. The two line graphs depict the between-subject variables of the study, that distin- guish between the settings: collective (C) and per- sonal (P).  Table 3: Properties of the analysed data set, struc- tured by the applied algorithm. Data defines whether the algorithm was calculated on a users personal word trace P, an inquiry groups collective word traces C or a Mixed approach PC consider- ing both type of data. NT depicts the number of tagged resources, we derived from the online evalu- ation. The metrics recall, precision and f-measure are mean values and standard deviations of R@5, P@5 and F@5, respectively.  Algorithm NT P@5 R@5 F@5  P MP 30 0.26 (0.25) 0.44 (0.36) 0.31 (0.27) Minerva 36 0.38 (0.32) 0.53 (0.39) 0.41 (0.31) BLL 22 0.43 (0.28) 0.75 (0.33) 0.50 (0.26)  C MP 72 0.33 (0.21) 0.72 (0.36) 0.42 (0.23) BLL 62 0.31 (0.23) 0.67 (0.37) 0.39 (0.23) Minerva 31 0.38 (0.28) 0.73 (0.38) 0.46 (0.30)  PC BLL + MP 63 0.31 (0.21) 0.74 (0.38) 0.41 (0.25)    (a) Personal vocabulary condition: tag recommendations are based on a users personal tagging traces.  (b) Collective vocabulary condition: tag recommendations are based on the learning groups collective tagging traces.  Figure 4: Recall/Precision plots illustrating the accuracy of recommendation algorithms in the personal and the collective vocabulary condition. BLL applied in the personal setting performs best over all considered recommendation approaches. In the collective condition, best results can be achieved for BLL+MP and Minerva.  In line with this descriptive pattern, a 2 (Personal vs. Collective)  3 (MP vs. BLL vs. Minerva) ANOVA on F reveals no significant main effects - either for vocabu- lary, F (1, 44)=1.22, n.s., nor algorithm, F (2, 44)=2.35, n.s. - but a significant interaction between these two factors, F (2, 44) = 4.33, p < .05.  Results indicate that in the personal setting the BLL ap- proach, which mimics the activation of words in a persons memory as a function of frequency and recency, performs best. On the other hand BLL applied in the collective vo- cabulary condition performed very poorly (see also Figure 4).  Also, we can see that the recommender Minerva showed better performance in the collective, than in the personal vocabulary condition. While a model that categorizes ac- cording to semantic context should be able to depict both, personal and collective data, it is fair to assume that the size of the data set plays a crucial role. We believe the ap- proach will become more accurate with the growing extent of the data set. Hence, we draw two conclusions. Firstly, Minerva performs better on collective than on personal tag- ging traces, as the data set is likely to be more extensive. Secondly, the performance of the algorithm will enhance with the time of use.  This corroborates our expectations, as we can assume that students interests within a group differ but are individually relatively stable within the short period of a school project. The individual developments of the students within a topic can be further depicted with the introduction of recency, as implemented in BLLU .  A very interesting result constitutes the moderate perfor- mance of BLLU+ MPG, since it is contrary to results from offline TEL data studies such as presented in Kopeinik et al. [22] where the approach clearly outperforms remaining rec- ommendation strategies. This conforms with the assumed task difference between online and offline studies, according to which we either evaluate an support or a prediction task, respectively.  5.3 Data Sets If we look at Table 2, we can observe that the tagging  frequency varies greatly among the groups. Students that participated in the study used the environment in the course of biology lessons. However, the IBL project work did not contribute to their marking. Also, they were encouraged to tag, but there was no particular monitoring of this process taking place. Thus, some groups showed more motivation and participated more actively in the projects and within the environment than others.  Another aspect is that there is significantly less data avail- able for vocabulary condition P, where users tag recommen- dations were based on their individual tagging history. Due to the cold start problem, students in condition P had no ini- tial tag seeds provided but rather had to come up with their own personal tag traces to initiate the tag recommendation process. We believe the resulting lack of tagging support, played a crucial role when students did not tag their contri- butions or tagged their contributions in unusual ways (see Section 2.4). This is in line with previous findings (e.g., [26]) that underline the need for support students have in the tagging process.  6. CONCLUSION This paper presents a real-world evaluation investigating  the application of tag recommender approaches from two perspectives: First, by dividing students in two groups re- ceiving either tag recommendations based on personal or collective tag traces, we gain insights into the effect of col- lective tag recommendations on the semantic stabilization process of collective learning groups.  Second, we evaluate the performance of two tag recom- mender approaches that imitate human behaviour, in par- ticular the process of human categorization and the retrieval of words from memory. The algorithms, Minerva and base- level learning equation (BLL), as well as MP as a baseline, were applied as within-subject variables, either on the basis    of the collective or the personal tagging history. Our results demonstrate that selecting recommendations  from the collective vocabulary, i.e., exposing a learner to oth- ers tags, is much more effective to promote semantic sta- bilization than drawing from the personal vocabulary and thus, displaying only individual tags. Furthermore, the re- sults suggest that searching for relevant tags in the collec- tives vocabulary benefits strongly from considering usage frequency and semantic context, i.e., from a strategy im- plemented by Minerva. The information of recency, on the other hand, appears to show advantages when aiming to identify relevant tags within the personal vocabulary.  One practical design implication is thus that semantic sta- bilization within the setting of inquiry-based group learning can be supported well by recommenders that both draw on data of the whole collective and are sensitive to the semantic context of learners search results in order to estimate tag choice probabilities. In case of an individual learning setting, however, we suggest applying recommenders that focus on information about time and frequency of past tag choices to predict their current availability in a learners memory and hence, relevance for the current learning episode.  We are aware of the limited evaluation data which is a con- sequence of the selected real-world learning setting. Data in such learning environments is typically sparse, which has also been the reason for restricting the experiment to the three algorithm set-up. By contrast, results from offline data studies can compare a multitude of options. However, we argue that those results are limited in their reliability, as unlike real-world studies, offline data do not allow for in- vestigation of recommendation strategies ability to support users in their tasks, but solely evaluate the prediction of a users behaviour.  In future work we are planning to strengthen our argu- ment by introducing students learning as an additional de- pendent variable. This will allow for further investigation of the correlation between individuals learning progress and the development of a common terminology in their learning group.  7. ACKNOWLEDGMENTS This work is supported by the EU funded projects weSPOT  (Grant Agreement 318499), Learning Layers (Grant Agree- ment: 318209) and AFEL (Grant Agreement: 687916), the Austrian Science Fund (FWF) Projects MERITS (Grant No P25593-G22) and OMFix (Grant No P27709-G22), and the Know-Center. The Know-Center is funded within the Aus- trian COMET Program under the auspices of the Austrian Ministry of Transport, Innovation and Technology, the Aus- trian Ministry of Economics and Labor and by the State of Styria. We are also very grateful to Verein fur Bildung und Erziehung der Grazer Schulschwestern and particularly to the teacher Jurgen Mack for the great support in imple- menting the study in his lessons.  8. REFERENCES [1] J. R. Anderson and L. J. Schooler. Reflections of the  environment in memory. Psychological science, 2(6):396408, 1991.  [2] M. Anjorin, I. Dackiewicz, A. Fernandez, C. Rensing, et al. A framework for cross-platform graph-based recommendations for tel. In Proceedings of the 2nd  workshop on recommender systems in technology enhanced learning, pages 8388, 2012.  [3] S. Bateman, C. Brooks, G. McCalla, and P. Brusilovsky. Applying collaborative tagging to e-learning. Proceedings of ACM WWW, 3(4), 2007.  [4] M. A. Bedek, S. Kopeinik, B. Prunster, and D. Albert. Applying the formal concept analysis to introduce guidance in an inquiry-based learning environment. In Advanced Learning Technologies (ICALT), 2015 IEEE 15th International Conference on, pages 285289. IEEE, 2015.  [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:9931022, 2003.  [6] B. S. Bloom, M. D. Engelhart, E. J. Furst, W. H. Hill, and D. R. Krathwohl. Taxonomy of educational objectives, handbook 1: The cognitive domain, 1956.  [7] M. A. Chatti, A. L. Dyckhoff, U. Schroeder, and H. Thus. A reference model for learning analytics. International Journal of Technology Enhanced Learning, 4(5-6):318331, 2012.  [8] E. Diaz-Aviles, M. Fisichella, R. Kawase, W. Nejdl, and A. Stewart. Unsupervised auto-tagging for learning object enrichment. In Towards Ubiquitous Learning, pages 8396. Springer, 2011.  [9] H. Drachsler, H. Hummel, and R. Koper. Recommendations for learners are different: Applying memory-based recommender system techniques to lifelong learning. In TENC: Publications and Preprints. Paper presented at the SIRTEL workshop of EC-TEL 2007, volume 1. Keur der Wetenschap, 2007.  [10] H. Drachsler, K. Verbert, O. C. Santos, and N. Manouselis. Panorama of recommender systems to support learning. In Recommender systems handbook, pages 421451. Springer, 2015.  [11] E. Duval. Attention please!: learning analytics for visualization and recommendation. In Proceedings of the 1st International Conference on Learning Analytics and Knowledge, pages 917. ACM, 2011.  [12] F. Font, J. Serra, and X. Serra. Analysis of the impact of a tag recommendation system in a real-world folksonomy. ACM Trans. Intell. Syst. Technol., 7(1):6:16:27, Sept. 2015.  [13] W.-T. Fu. The microstructures of social tagging: a rational model. In Proceedings of the 2008 ACM conference on Computer supported cooperative work, pages 229238. ACM, 2008.  [14] W.-T. Fu and W. Dong. Collaborative indexing and knowledge exploration: A social learning model. IEEE Intelligent Systems, 27(1):3946, 2012.  [15] W.-T. Fu, T. G. Kannampallil, and R. Kang. A semantic imitation model of social tag choices. In Computational Science and Engineering, 2009. CSE09. International Conference on, volume 4, pages 6673. IEEE, 2009.  [16] H. Halpin, V. Robu, and H. Shepherd. The complex dynamics of collaborative tagging. In Proceedings of the 16th international conference on World Wide Web, pages 211220. ACM, 2007.  [17] P. Heymann, D. Ramage, and H. Garcia-Molina. Social tag prediction. In Proceedings of the 31st annual international ACM SIGIR conference on    Research and development in information retrieval, pages 531538. ACM, 2008.  [18] D. L. Hintzman. Minerva 2: A simulation model of human memory. Behavior Research Methods, Instruments, & Computers, 16(2):96101, 1984.  [19] R. Jaschke, L. Marinho, A. Hotho, L. Schmidt-Thieme, and G. Stumme. Tag recommendations in folksonomies. In Knowledge Discovery in Databases: PKDD 2007, pages 506514. Springer, 2007.  [20] M. K. Khribi, M. Jemni, and O. Nasraoui. Recommendation systems for personalized technology-enhanced learning. In Ubiquitous learning environments and technologies, pages 159180. Springer, 2015.  [21] A. Klasnja-Milicevic, M. Ivanovic, and A. Nanopoulos. Recommender systems in e-learning environments: a survey of the state-of-the-art and possible extensions. Artificial Intelligence Review, 44(4):571604, 2015.  [22] S. Kopeinik, D. Kowald, and E. Lex. Which algorithms suit which learning environments a comparative study of recommender systems in tel. In European Conference on Technology Enhanced Learning, pages 124138. Springer, 2016.  [23] D. Kowald, S. Kopeinik, P. Seitlinger, T. Ley, D. Albert, and C. Trattner. Refining frequency-based tag reuse predictions by means of time and semantic context. In Mining, Modeling, and RecommendingThings in Social Media, pages 5574. Springer, 2015.  [24] D. Kowald, P. Seitlinger, S. Kopeinik, T. Ley, and C. Trattner. Forgetting the words but remembering the meaning: Modeling forgetting in a verbal and semantic tag recommender. In Mining, Modeling, and RecommendingThings in Social Media, pages 7595. Springer, 2015.  [25] A. Kuhn, C. Cahill, C. Quintana, and S. Schmoll. Using tags to encourage reflection and annotation on data during nomadic inquiry. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 667670. ACM, 2011.  [26] A. Kuhn, B. McNally, S. Schmoll, C. Cahill, W.-T. Lo, C. Quintana, and I. Delen. How students find, evaluate and utilize peer-collected annotated multimedia data in science inquiry with zydeco. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 30613070. ACM, 2012.  [27] T. Ley and P. Seitlinger. Dynamics of human categorization in a collaborative tagging system: How social processes of semantic stabilization shape individual sensemaking. Computers in human behavior, 51:140151, 2015.  [28] N. Lin, D. Li, Y. Ding, B. He, Z. Qin, J. Tang, J. Li, and T. Dong. The dynamic features of delicious, flickr, and youtube. Journal of the American Society for Information Science and Technology, 63(1):139162, 2012.  [29] G. Macgregor and E. McCulloch. Collaborative tagging as a knowledge organisation and resource discovery tool. Library review, 55(5):291300, 2006.  [30] N. Manouselis, H. Drachsler, R. Vuorikari, H. Hummel, and R. Koper. Recommender systems in  technology enhanced learning. In Recommender systems handbook, pages 387415. Springer, 2011.  [31] L. B. Marinho, A. Hotho, R. Jaschke, A. Nanopoulos, S. Rendle, L. Schmidt-Thieme, G. Stumme, and P. Symeonidis. Recommender systems for social tagging systems. Springer Science & Business Media, 2012.  [32] A. Mathes. Folksonomies-cooperative classification and communication through shared metadata, 2004. Available at: http://www.adammathes.com/academic/computer- mediated-communication/folksonomies.html.  [33] K. Niemann. Automatic tagging of learning objects based on their usage in web portals. In Design for Teaching and Learning in a Networked World, pages 240253. Springer, 2015.  [34] A. Protopsaltis, P. Seitlinger, F. Chaimala, O. Firssova, S. Hetzner, K. Kikis-Papadakis, and P. Boytchev. Working environment with social and personal open tools for inquiry based learning: Pedagogic and diagnostic frameworks. The International Journal of Science, Mathematics and Technology Learning, 20(4):5163, 2014.  [35] R. Raffelsiefen. Semantic stability in derivationally related words. Amsterdam Studies in the Theory and History of Linguistic Science, 4:247268, 1998.  [36] P. Seitlinger, T. Ley, and D. Albert. An implicit-semantic tag recommendation mechanism for socio-semantic learning systems. In Open and Social Technologies for Networked Learning, pages 4146. Springer, 2013.  [37] M. Specht, M. Bedek, E. Duval, P. Held, A. Okada, K. Stevanov, E. Parodi, K. Kikis-Papadakis, and V. Strahovnik. Wespot: Inquiry based learning meets learning analytics. In 3rd international conference on e-Learning, pages 1520, 2012.  [38] C. Trattner, D. Kowald, P. Seitlinger, S. Kopeinik, and T. Ley. Modeling activation processes in human memory to predict the use of tags in social bookmarking systems. The Journal of Web Science, 2(1):116, 2015.  [39] K. Verbert, H. Drachsler, N. Manouselis, M. Wolpers, R. Vuorikari, and E. Duval. Dataset-driven research for improving recommender systems for learning. In Proceedings of the 1st International Conference on Learning Analytics and Knowledge, pages 4453. ACM, 2011.  [40] C. Wagner, P. Singer, M. Strohmaier, and B. Huberman. Semantic stability and implicit consensus in social tagging streams. IEEE Transactions on Computational Social Systems, 1(1):108120, 2014.  [41] Z. Xu, Y. Fu, J. Mao, and D. Su. Towards the semantic web: Collaborative tag suggestions. In Collaborative web tagging workshop at WWW2006, Edinburgh, Scotland, 2006.      "}
{"index":{"_id":"51"}}
{"datatype":"inproceedings","key":"Cross:2017:CHS:3027385.3027442","author":"Cross, Sebastian and Waters, Zak and Kitto, Kirsty and Zuccon, Guido","title":"Classifying Help Seeking Behaviour in Online Communities","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"419--423","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027442","doi":"10.1145/3027385.3027442","acmid":"3027442","publisher":"ACM","address":"New York, NY, USA","keywords":"content analysis, help seeking, machine learning, open data","Abstract":"While help seeking has been extensively studied using self report survey data and models, there is a lack of content analysis techniques that can be directly applied to classify help seeking behaviour. In this preliminary work we propose a coding scheme which is then applied to an open dataset that we have created by carefully selecting sub groups from two popular discussion sites (Reddit and StackExchange). We then explore the possibility for automatically classifying help seeking behaviour using machine learning models. A preliminary model provides good initial results, suggesting that it may indeed be possible to construct student support systems that build off of an accurate classifier.","pdf":"Classifying Help Seeking Behaviour in Online Communities Sebastian Cross, Zak Waters, Kirsty Kitto, Guido Zuccon  Queensland University of Technology (QUT) Brisbane, Australia  [kirsty.kitto,g.zuccon]@qut.edu.au  ABSTRACT While help seeking has been extensively studied using self report survey data and models, there is a lack of content analysis techniques that can be directly applied to classify help seeking behaviour. In this preliminary work we propose a coding scheme which is then applied to an open dataset that we have created by carefully se- lecting sub groups from two popular discussion sites (Reddit and StackExchange). We then explore the possibility for automatically classifying help seeking behaviour using machine learning models. A preliminary model provides good initial results, suggesting that it may indeed be possible to construct student support systems that build off of an accurate classifier.  CCS CONCEPTS Computing methodologies  Supervised learning by classifica- tion; Information systems  Personalization; Applied comput- ing  E-learning;  KEYWORDS help seeking; content analysis; machine learning; open data  ACM Reference format: Sebastian Cross, Zak Waters, Kirsty Kitto, Guido Zuccon. 2016. Classifying Help Seeking Behaviour in Online Communities. In Proceedings of LAK  17, , March 13 - 17, 2017, Vancouver, BC, Canada, 5 pages.  DOI: http://dx.doi.org/10.1145/3027385.3027442  1 INTRODUCTION Self-regulated learning (SRL) appears to be a key indicator of stu- dent success. SRL has been associated with improved academic outcomes, engagement and motivation, as well as with constructive autonomy [28]. Two decades of research has provided us with a wide range of definitions [21, 29] and models [3, 23, 27], which serve to highlight the importance of helping people to learn how to initiate a wide variety of meta-cognitive, cognitive, affective, and motivational behaviours to achieve their learning goals [15]. This has demon- strated that self regulated learners have an ability to plan, organize, self-instruct, and self-evaluate which is essential to achieving long term success [29].  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 17,  2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-4870-6/17/03. . . $$15.00 DOI: http://dx.doi.org/10.1145/3027385.3027442  SRL is particularly important in online learning environments, es- pecially as they scale up to Massive Open Online Courses (MOOCs) [15, 16]. Within this overarching context, help seeking forms an important part of the SRL process [1, 13]. Here, we briefly discuss help seeking before proposing a scheme for content analysis.  Help Seeking behaviour. Richardson et al. [24] defined help seeking as the tendency to seek help from instructors and friends when experiencing academic difficulties, but this definition misses an important further possibility; people can seek help from general communities of practice where they do not necessarily have an instructor or a friend. For example, learners frequently coalesce around topics of interest in sites such as StackExchange and Reddit, asking for help and often receiving it in a very timely manner. For this reason, we will adopt the definition provided by Kizilcec et al. [15], which extends this basic definition to include the possibility that a help seeker might consult external help and resources.  It becomes critically important that learners develop a sophis- ticated approach to seeking help when they are studying in self directed online learning environments; they must be able to self- diagnose as requiring help, and then understand how to find it. For decades collaborative learning environments have been proposed as a way to encourage learners to learn from each other, providing necessary help along the way [6, 25]. Today, the vast majority of Learning Management Systems (LMSs) feature discussion fora and other environments (e.g., instant messaging services) that enable students to seek help in a variety of ways. However, it is precisely the people who most need help who tend not to posses the skills necessary to find it in a timely manner. Students can lack under- standing of the topic [17, 18], not know how to ask a question [4], and some students suffer from help seeking threat. This can translate into a lack self-esteem, or a fear of social embarrassment [14], a problem that has been shown to disproportionately affect certain groups. Similarly, anxiety has been found to anti-correlate with appropriate help seeking behaviour [9].  We consider it important that Learning Analytics (LA) starts to develop methods for studying help seeking behaviour at scale. It is likely that recommendation systems can be built that would help to support a subset of students who have not yet learned to seek help effectively, or who suffer from help seeking threat. If categories of help seeking behaviour could be automatically detected in online fora then it would become possible to construct tools that could scaffold students towards more SRL patterns of behaviour. Even more interestingly, if we can automate the detection of help providing behaviour then students could potentially be rewarded for providing this invaluable service to their peers. Rewards do not have to relate just to summative assessment: sites such as StackExchange work on reputation, where the kudos of a help provider is increased as they provide more relevant help. (Although Howley has cautioned that up voting can be counter productive in an educational context [11].)    LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada, Sebastian Cross, Zak Waters, Kirsty Kitto, Guido Zuccon  This paper provides an exploratory step towards an overarching goal of automating the detection of help seeking and providing behaviour in online discussion transcripts.  Models of help seeking. A number of models of help seeking have been created but as Howley has observed [11], they tend to be very similar. One example was proposed by Gall [20] whose model of help seeking focuses on five specific steps: i) Awareness of need for help. ii) Decision to seek help. iii) Identification of potential helpers. iv) Employment of strategies to elicit help. v) Reaction to help seeking attempts. These steps are then implemented by the help seeker using different strategies. Non verbal strategies are indirect, and consist of the student placing themselves in the proximity of a likely help provider. Gall does not discuss the case, but in a modern online context, a non verbal strategy would involve lurking in a discussion forum, and passively waiting for a post that resolved the problem. Verbal strategies can be implemented in a wider range of formats. The direct help seeker might simply ask for help with a direct question. However, verbal help seekers may also apply indirect strategies, perhaps by implying that a task is hard, or that they cannot do something. They may also ask for information about a problem, rather than directly asking for assistance and expecting an answer. Sometimes a power relationship is leveraged, e.g., they might remind an instructor of an obligation to provide help.  Models such as these will prove essential in constructing a tool that can automate the provisioning of help to students who are identified as requiring assistance. However, we first require a way in which to understand help seeking behaviour per se.  Qualitative studies of help seeking. A number of different papers have studied help seeking using qualitative surveys. Usually a help seeking construct is defined and then studied together with other educational factors (e.g., there is a help seeking subscale a part of the MLSQ [22]). Help seeking behaviour is then often correlated with educational outcomes (e.g., GPA [24]). Intriguingly, the recent work of Kizilcec et al. [15] demonstrates a negative relation between help seeking behaviour and goal attainment in MOOCS, a result that appears to contradict the work of Richardson et al., which demonstrates a positive correlation for help seeking behaviour and GPA [24]. However, as these methods use self-reporting to establish their correlations their conclusions are fraught, and can be critically questioned [7]. It is important that we start to correlate survey data with behavioural data obtained from learning communities  are students actually seeking help in the way that they claim in surveys To answer these questions, it will be necessary to develop a coding scheme that can be used to classify help seeking behaviour in online discussion fora, code a variety of datasets and then attempt to correlate survey responses with actual patterns of behaviour.  Coding text for help seeking behaviour. Despite the apparent need to cross-correlate self report data with observed student behaviour, few content analysis schemes have been developed for the problem of classifying help seeking behaviour. One scheme exists for the classification of collaborative learning [5]. It is broken up into five main behaviour categories and associated sub categories: Planning (Group skills, Organising Work, Initiating Activities); Contributing (Help Giving, Feedback Giving, Exchanging Resources, Sharing Knowledge, Challenging Others and Explaining or Elaborating); Seeking Input (Help Seeking, Feedback Seeking and Advocating  Effort); Reflection/Monitoring (Monitoring Group Effort and Re- flecting on Medium); and Social Interaction (Social Interaction). This is a good first step, however, its roots in a schema for collabora- tive learning makes it difficult to apply to our scenario. Many of the behaviours in the contributing categories overlap which makes cod- ing unnecessarily complex. Furthermore, this scheme does code for achieving resolution (when an answer is provided to the help seeker). For this reason, we decided to develop a new specific scheme.  Automating the detection of help seeking behaviour. Q&A sys- tems are increasingly being used in education [8, 12, 26], but many rely on large amounts of curated or structured data. Moreover, they cannot tell whether the students require help in advance of a direct question, and so miss crucial potential intervention points. We adopt a different stance here. If it is possible to hand code a dataset for help seeking behaviour, then an obvious question presents; can we automate this coding process using Machine Learning As was discussed above, automating the detection of help seeking behaviour would enable the construction of tools that could help students to become more self directed in their learning.  2 APPROACH Rather than generating another closed dataset based upon real stu- dent data extracted from a LMS or MOOC, a core contribution of this work is the ongoing creation of an open dataset that can be used and extended by other research groups. We have started by carefully selecting a subset of online discussion fora from two al- ready released open datasets. Reddit (www.reddit.com) is a content aggregation and discussion website that is split up into a number of subsections covering a wide range of topics. These subsections are called SubReddits, and hundreds are generated daily by the Red- dit user base. The data examined from Reddit was obtained from www.kaggle.com and consists of all user generated activity from May 2015. The user base of Reddit is a mix between users who have made a name for themselves as well as users who make temporary accounts to remain anonymous, which potentially allows users to post more freely (and sometimes more abusively) than might be ex- pected in a more traditional online learning environment. We chose to use the /r/askhistorians SubReddit in this study, because it is a strong learning community offering a set of free flowing questions and answers that appear similar in structure to an inquiry based LMS discussion forum. This dataset was released under the API terms of Reddit allowing for non-commercial use.  Stack Exchange (www.stackexchange.com) is a question answer- ing (QA) platform that covers 160 topics, from coding and mathe- matics, to typography, language and science fiction. It is made up of many different communities, and these can contain sub communities of their own. The site has released a dataset which can be found at https://archive.org/details/stackexchange. This dataset consists of all user-contributed content (i.e. posts, comments, upvotes, downvotes, marked resolved, etc.) for the website. In this study we selected the english.stackexchange.com/ topic because (i) most of its members are identified and so we expect it to have better behaviour than some communities (ii) it contains few ideological arguments or humorous posts, and (iii) questions are frequently marked as resolved in this community, providing an extra corroborating data point. This dataset  www.stackexchange.com https://archive.org/details/stackexchange english.stackexchange.com/   Classifying Help Seeking Behaviour in Online Communities LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada,  Table 1: The qualitative coding scheme developed for labelling help seeking in online communities.  Main Label Sub-label Code Details  Help Seeking Direct Question DQ Direct question being asked. Indicative features: question marks; five ws (who, what, where, when, why); how; does. Example: Is there a pronoun I can use as a gender-neutral pronoun  Implied Help Seeking IS Validation seeking (I was wondering, ... am I wrong, ... Right) and Indirect questions. Example: Id love to find some area I could investigate on my own over the summer.  Help Providing Sharing Knowledge SK User sharing their knowledge on the current topic, can be a reply to a help seeking post or the elaboration of a previously given help providing post. Example: Singular they enjoys a long history of usage in English and can be used here: Each student should save their questions until the end.  Exchanging Resources ER Providing resources as part of an answer. Linking to websites, references, books... Example: Take a look at The Girls of Atomic City: The Untold Story of the Women Who Helped Win World War II.  Feedback or Validation Given FG Providing feedback or validation on a users work/response/opinion can be given even when validation is not sought. Example: Yes, that is correct.  Clarification Asking for Clarification AC Asking for clarification in reference to a post. Example: Can you please explain why you have no love for zir  Giving Clarification GC Providing clarification or elaboration in relation to a previous post. Example: I like zir but I reserve it (in my idiolect) for known persons of determinate gender who are neither male nor female.  Resolution Help Received HR An explicit textual indication that help was received. Example: Loved your answer :), Thanks  Social Interaction Unrelated USI Unrelated social interaction, random statement that is not related to a topic. Example: Youre so lucky - imagine, in German there is a female form for every profession and such (something like actor and actress), and we fight on the proper gender to use.  Abusive ASI Abusive, overly critical or inappropriate language and attitude. Example: You are an idiot.  Trolling TSI Unnecessary and intentionally provocative statements. Example: Hitler did nothing wrong.  Table 2: A comparison of the category counts of sentences in Stack Exchange and Reddit. Bracketed numbers denote percentages.  DQ IS SK ER FG AC GC HR USI ASI TSI  Reddit 197 (.0905) 35 (.0157) 1408 (.6475) 176 (.0817) 138 (.0642) 4 (.0018) 20 (.0092) 37 (.0175) 139 (.0646) 5 (.0023) 2 (.0009) Stack Exchange 474 (.1244) 86 (.0257) 2676 (.7023) 154 (.0404) 110 (.0288) 5 (.0013) 10 (.0026) 54 (.0141) 239 (.0627) 0 29(.0005)  is released under a CC BY-SA 3.0 licence. Only the posts made in May 2015 were used to maintain consistency across the datasets.  While not formal LMSs, we consider Reddit and Stack Exchange to be excellent proxies for online collaborative learning. Indeed, many students in the formal education system make use of one or the other of these two sites as a core component of their Personal Learning Networks [19] to seek help with formal coursework and generally find out information or debate issues. Thus, the decision to make use of data from these sites is not just pragmatic, such open fora play an increasingly critical role in learning, and it is important that we develop solutions for them too. Both datasets have different affordances. For example, Stack Exchange provides a more structured learning environment driven by direct questions and attempts to answer them. Additionally, reputation (in the form of up votes and down votes) can be awarded by any registered member for both questions and answers, and the original question asker can mark a specific answer as having solved their question. These features mean that Stack Exchange is very much geared towards high quality questions and answers. In contrast, the Reddit dataset provides a more untamed structure that lends itself to more social interaction and back and forth banter. Although posts can be marked up or down, users are not represented on the site with reputation scores, which creates a less punishing environment for low quality answers/replies.  The datasets were processed to allow for the labelling and classi- fication of the data. Both were split from paragraphs into sentences using NLTKs [2] nltk.tokenize function. No attempt was made  to correct for poor grammar or punctuation in an attempt to keep our approach as replicable as possible. After this process, we had extracted 2156 sentences from the Reddit dataset and 3810 sentences from the Stack Exchange dataset for further processing. This step involved removing all XML tags, along with all comments that were marked as deleted. For both datasets, all hyperlinks were con- verted to a standard indicator (URL) format using regex so that they were recognised accordingly during feature extraction. Reddit data preprocessing included an extra step where automated comments generated by bots, which tend to mimic help providing behaviour, were deleted.  2.1 Qualitative Coding We have created a coding scheme for help seeking and providing behaviour (see Table 1) that combines the initial work of Curtis and Lawson [5] and Gall [20], and extends it. Our scheme was developed with an underlying intent to code data at a sentence level rather than at a paragraph level, in an attempt to provide a more accurate representation of the behaviour being expressed. This can be very complex, with individual posts expressing many different sentiments. In order to encourage replicability between coders, our schema assumed no context (i.e. was based on the behaviour presented within specific sentences). While the majority of the current coding was completed by one person, a test of Inter-Rater Reliability (IRR) was carried out. Two coders worked to achieve an IRR Cohens kappa value of 0.766 on a test set of 200 sentences,    LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada, Sebastian Cross, Zak Waters, Kirsty Kitto, Guido Zuccon  after working to achieve full agreement on a previous subset of 600 classifications. Further incremental reliability tests will be performed to ensure dataset integrity as the dataset grows.  2.2 Exploratory Automation Two classifiers were used in an exploratory study aiming to automate the classification of help seeking and help providing behaviours within online discussions. We used (i) a Support Vector Machine (SVM), and (ii) a Random Forest (RF) model, as both perform well on feature rich high dimensional tasks. These models were implemented using the http://scikit-learn.org/ Python libraries. The features extracted for this classification task were primarily lexical; we implemented a basic approach utilising: (i) word N-grams; and (ii) simple question counts. We utilised groupings of words due to the hypothesis that similar HS behaviours may be associated with clusters of words and their contexts. One example of such a case could be the words is this, which might indicate the poster is asking a question or otherwise seeking help. Our second exploratory feature seeks to incorporate the amount of inquiry contained in a post by counting the usage of questions marks. We used two validation methods common for classification tasks: 10-fold cross validation; and a training/testing split (of 75% and 25% respectively), which was used in the generation of confusion matrices to enable further exploration of the results.  2.3 Results The breakdown of help seeking behaviour for our two online commu- nities is shown in Table 2. We admit to a certain amount of surprise that Stack Exchange and Reddit both appear to have a similar help seeking profile for the communities chosen. This similarity possibly arises from the specific communities chosen for preliminary analy- sis, and extending the dataset to other communities will be required before any conclusions can be drawn. It is unsurprising that few abu- sive and trolling behaviours were found in this dataset; as previously mentioned, both communities were chosen for their integrity and high moderation. Table 3 provides performance metrics for the two classification models, obtained using 10 fold cross validation. When we consider the simplicity of the feature set specified in Section 2.2, it is surprising that such high values were obtained.  The confusion matrix in Table 4 provides an explanation, showing an obvious over classification of SK behaviour and precision losses because of false positives. Table 5 shows the Gini importance (GI) value (the higher the GI, the more important in reducing classifi- cation uncertainty or miss classification ), for the 5 most prevalent features in the random forest model, for both the full dataset (FULL) and the most common categories. As expected, the most valued features are those that relate to under represented behaviours. For example, the url feature can often co-occur with a request for infor- mation (where someone points to a url as an example of something they are not sure about). Features such as yes often co-occur with FG behaviour, where a help provider agrees with a help seeker. Fi- nally, features such as does can often occur in DQ behaviour, at the start of a question. Some features are less obvious, for example the feature islam, which relates to the ER behaviour, and is due to a specificity of the dataset, where many of the comments discussed religion and users would provide Islamic text as sources.  Table 3: Performance metrics for 3 different datasets.  Precision Recall F-Value Accuracy  Reddit SVM 0.667 0.752 0.665 0.752 RF 0.728 0.739 0.677 0.742  Stack Exchange SVM 0.622 0.742 0.658 0.742 RF 0.765 0.787 0.762 0.791  Combined SVM 0.624 0.687 0.595 0.687 RF 0.725 0.742 0.698 0.751  Table 4: Confusion Matrix for Entire Dataset using RF. DQ IS SK ER FG AC GC HR USI ASI TSI Precision  DQ(164) 101 0 59 0 0 0 0 0 4 0 0 61.58 IS(39) 1 9 29 0 0 0 0 0 0 0 0 23.07 SK(1130) 16 4 1017 5 29 0 2 3 54 0 0 90 ER(55) 1 0 29 17 0 0 0 0 8 0 0 30.90 FG(20) 1 0 14 0 1 0 0 0 4 0 0 5 AC(2) 0 0 1 0 0 0 0 0 1 0 0 0 GC(0) 0 0 0 0 0 0 0 0 0 0 0 N/A HR(18) 0 0 2 0 0 0 0 15 1 0 0 83.33 USI(68) 2 0 36 0 1 0 0 0 29 0 0 42.64 ASI(0) 0 0 0 0 0 0 0 0 0 0 0 N/A TSI(0) 0 0 0 0 0 0 0 0 0 0 0 N/A  Recall 82.78 69.23 85.67 77.27 3.22 N/A 0 83.33 28.71 N/A N/A  2.4 Discussion At first glance, standard supervised learning techniques appear to perform surprisingly well in the identification of key help seeking and help providing behaviours using a very simple feature set. How- ever, it appears that both SVM and RF classifiers are fixating upon the Seeking Knowledge (SK) category. This is to be expected, as SK dominates these two datasets, a phenomenon that is to be expected given the affordances of the Stack Exchange and Reddit subcom- munities that we have chosen; both are very much sites set up for those who are seeking knowledge. This means that the precision and recall for SK is remarkably high, but only at the expense of low recall for many other categories of behaviour in the dataset.  This pattern of behaviour is also reflected in the dominant features of FULL vs SK shown in Table 5, which largely overlap. The RF appears to be the better performing algorithm with the current feature set. This may be attributed to the ability of RF models to withstand overfitting, by averaging the results of multiple decision trees.  3 FUTURE WORK The schema presented in Table 1 is preliminary and may need to be modified as it is applied to a wider range of online educational environments. This will be a priority for the future. Within the current schema, the distribution of data for both the Reddit and Stack Exchange datasets discussed in this paper shows a very di- rect approach to help seeking. We anticipate that more implied help seeking could be found in other online environments, but this remains to be tested in future work. We will also seek to compare our results with student survey data, enabling the explo- ration of actual help seeking behaviour as exhibited by students in an online forum when compared to their self-report data. We are surprised to find little work on this topic, and this paper is one small step towards that end goal. Our dataset is released un- der a CC BY-SA 4.0 license, and is currently available at https: //github.com/CognitiveEcosystemsLab/OpenEducationalData. It is essential that the LA community develops more open and shareable datasets for baseline comparisons between groups, and this work is a step in that direction. We will continue to develop and extend this dataset. It is currently hand coded for help seeking behaviour  http://scikit-learn.org/ https://github.com/CognitiveEcosystemsLab/OpenEducationalData https://github.com/CognitiveEcosystemsLab/OpenEducationalData   Classifying Help Seeking Behaviour in Online Communities LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada,  Table 5: Top 5 features and Gini importance for the full dataset (FULL), as well as the five most dominant categories in the schema.  Class Features (Gini importance)  FULL url (0.0413) thanks (0.0178) deleted (0.0106) thank (0.0096) does (0.0025) SK url (0.0385) thanks (0.0157) thank (0.0092) deleted (0.0091) welcome (0.0027) DQ does (0.0039) correct (0.0037) express (0.0026) vague (0.0025) enlighten (0.0025) ER url (0.1356) islam (0.0058) url did (0.0049) oxford university (0.0049) url url (0.0045) FG yes (0.0057) respond (0.0042) haven read (0.0039) didn actually (0.0039) good point (0.0037) USI deleted (0.0559) welcome (0.0124) elu (0.0095) congrats (0.0062) hi (0.0052)  according to the scheme discussed in Section 2.1, but will gradu- ally be extended with other educationally relevant coding schemas applied to the same data (e.g., Cognitive Presence [10]). As it is further extended and refined, we anticipate that this dataset could emerge as a baseline comparison data source for the LA community. We are also interested in more complex features of help seeking and providing behaviour, such as cognition indicators derived from text. It will be interesting to investigate how platform specific features such as post quality can be applied to ML under the lens of our framework. For example, what features correlate with meta-data such as up/down-votes (in the case of Reddit), and the marking of a post as resolving a question (StackExchange) of particular help seeking categories Extracting these features would enable them to be used in datasets that do not contain such metadata.  4 CONCLUSION While help seeking has attracted much interest over the decades, far less work has been completed on the content analysis of help seeking transcripts. This paper is a preliminary step in that direction, providing a coding scheme and exploring how it can be applied to the analysis of online community discourse. Two basic feature sets (n-grams and question counting) for automating the detection of this behaviour have proven to be of interest. Much work remains to be completed, but this paper has provided a proof of concept that help seeking behaviour can be automatically detected in online discourse. This is a promising development for the provisioning of personalised just-in-time solutions to students in online environments.  REFERENCES [1] Vincent Aleven, Ido Roll, Bruce M McLaren, and Kenneth R Koedinger. 2010.  Automated, unobtrusive, action-by-action assessment of self-regulation during learning with an intelligent tutoring system. Educational Psychologist 45, 4 (2010), 224233.  [2] Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python.  OReilly Media, Inc..  [3] Monique Boekaerts. 1997. Self-regulated learning: A new concept embraced by researchers, policy makers, educators, teachers, and students. Learning and instruction 7, 2 (1997), 161186.  [4] Ikseon Choi, Susan M. Land, and Alfred J. Turgeon. 2005. Scaffolding peer- questioning strategies to facilitate metacognition during online small group dis- cussion. Instructional Science 33 (2005), 483511. DOI:http://dx.doi.org/10. 1007/s11251-005-1277-4  [5] David D. Curtis and Michael. J. Lawson. 2001. Exploring collaborative online learning. Journal of Asynchronous Learning Networks 5, 1 (2001), 2134. DOI: http://dx.doi.org/10.1016/j.jcss.2007.08.004  [6] Pierre Dillenbourg. 1999. What do you mean by collaborative learning Collaborative-learning: Cognitive and Computational Approaches. 1 (1999), 119. DOI:http://dx.doi.org/10.1.1.167.4896  [7] Stewart I Donaldson and Elisa J Grant-Vallone. 2002. Understanding self-report bias in organizational behavior research. Journal of Business and Psychology 17, 2 (2002), 245260.  [8] Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy. 2006. An intelligent discussion-bot for answering student queries in threaded discussions. In Proceed- ings of the 11th international conference on Intelligent user interfaces. ACM, 171177.  [9] Adrian Furnham, Tomas Chamorro-Premuzic, and Fiona McDougall. 2002. Per- sonality, cognitive ability, and beliefs about intelligence as predictors of academic performance. Learning and Individual Differences 14 (2002), 4966. DOI: http://dx.doi.org/10.1016/j.lindif.2003.08.002  [10] D. Randy Garrison, Terry Anderson, and Walter Archer. 2001. Critical thinking, cognitive presence, and computer conferencing in distance education. American Journal of distance education 15, 1 (2001), 723.  [11] Iris Howley. 2015. Leveraging Educational Technology to Overcome Social Obstacles to Help Seeking. Ph.D. Dissertation. Carnegie Mellon University.  [12] Jason C Hung, Ching-Sheng Wang, Che-Yu Yang, Mao-Shuen Chiu, and George Yee. 2005. Applying word sense disambiguation to question answering system for e-learning. In 19th International Conference on Advanced Information Networking and Applications (AINA05) Volume 1 (AINA papers), Vol. 1. IEEE, 157162.  [13] Stuart Karabenick. 2001. Help seeking in large college classes: Who, why, and from whom. The annual meeting of the American Educational Research Association (2001).  [14] Stuart Karabenick. 2003. Seeking help in large college classes: A person-centered approach. Contemporary Educational Psychology 28 (2003), 3758. DOI:http: //dx.doi.org/10.1016/S0361-476X(02)00012-7 arXiv:S0361-476X(02)00012-7  [15] Rene Kizilcec, Mar Perez-Sanagustn, and Jorge Maldonado. 2017. Self-regulated learning strategies predict learner behavior and goal attainment in Massive Open Online Courses. Computers & Education 104 (2017), 1833.  [16] Allison Littlejohn, Nina Hood, Colin Milligan, and Paige Mustain. 2016. Learning in MOOCs: Motivations and self-regulated learning in MOOCs. The Internet and Higher Education 29 (2016), 4048.  [17] Hans Van Der Meij. 1990. Question Asking : To Know That You Do Not Know Is Not Enough. 82, 3 (1990), 505512.  [18] Naomi Miyake and Donald a. Norman. 1979. To ask a question, one must know enough to know what is not known. Journal of Verbal Learning and Verbal Behavior 18 (1979), 357364. DOI:http://dx.doi.org/10.1016/S0022-5371(79) 90200-7  [19] Jonathan Mott. 2010. Envisioning the post-LMS era: The open learning network. Educause Quarterly 33, 1 (2010), 19.  [20] Sharon Nelson Le-Gall. 1981. Help-seeking: An understudied problem-solving skill in children. Developmental Review 1 (1981), 224246. DOI:http://dx.doi. org/10.1016/0273-2297(81)90019-8 arXiv:0273-2297/81/030224-23  [21] Paul Pintrich. 1999. The role of motivation in promoting and sustaining self- regulated learning. International Journal of Educational Research 31, 6 (1999), 459  470. DOI:http://dx.doi.org/10.1016/S0883-0355(99)00015-4  [22] Paul Pintrich and Elisabeth De Groot. 1990. Motivational and self-regulated learning components of classroom academic performance. Journal of educational psychology 82, 1 (1990), 33.  [23] Minna Puustinen and Lea Pulkkinen. 2001. Models of self-regulated learning: A review. Scandinavian Journal of Educational Research 45, 3 (2001), 269286.  [24] Michelle Richardson, Charles Abraham, and Rod Bond. 2012. Psychological correlates of university students academic performance: a systematic review and meta-analysis. Psychological bulletin 138, 2 (2012), 353.  [25] Piet Van den Bossche, Wim Gijselaers, Mien Segers, and Paul A. Kirschner. 2006. Social and cognitive factors driving teamwork in collaborative learning environments team learning beliefs and behaviors. Small group research 37, 5 (2006), 490521.  [26] Chun-Chia Wang, Jason Hung, Che-Yu Yang, and Timothy Shih. 2006. An application of question answering system for collaborative learning. In 26th IEEE International Conference on Distributed Computing Systems Workshops (ICDCSW06). IEEE, 4949.  [27] P Hadwin Winne and Allyson F Hadwin. 1998. Studying as self-regulated learning. Metacognition in educational theory and practice 93 (1998), 2730.  [28] Barry J. Zimmerman. 2002. Becoming a Self-Regulated Learner: An Overview. Theory Into Practice 41, August (2002), 6470. DOI:http://dx.doi.org/10.1207/ s15430421tip4102  [29] Barry J. Zimmerman and Manuel Martinez-Pons. 1988. Construct validation of a strategy model of student self-regulated learning. Journal of Educational Psychol- ogy 80, January (1988), 284290. DOI:http://dx.doi.org/10.1037/0022-0663.80. 3.284  http://dx.doi.org/10.1007/s11251-005-1277-4 http://dx.doi.org/10.1007/s11251-005-1277-4 http://dx.doi.org/10.1016/j.jcss.2007.08.004 http://dx.doi.org/10.1.1.167.4896 http://dx.doi.org/10.1016/j.lindif.2003.08.002 http://dx.doi.org/10.1016/S0361-476X(02)00012-7 http://dx.doi.org/10.1016/S0361-476X(02)00012-7 http://dx.doi.org/10.1016/S0022-5371(79)90200-7 http://dx.doi.org/10.1016/S0022-5371(79)90200-7 http://dx.doi.org/10.1016/0273-2297(81)90019-8 http://dx.doi.org/10.1016/0273-2297(81)90019-8 http://dx.doi.org/10.1016/S0883-0355(99)00015-4 http://dx.doi.org/10.1207/s15430421tip4102 http://dx.doi.org/10.1207/s15430421tip4102 http://dx.doi.org/10.1037/0022-0663.80.3.284 http://dx.doi.org/10.1037/0022-0663.80.3.284   Abstract  1 Introduction  2 Approach  2.1 Qualitative Coding  2.2 Exploratory Automation  2.3 Results  2.4 Discussion   3 Future work  4 Conclusion  References   "}
{"index":{"_id":"52"}}
{"datatype":"inproceedings","key":"Corrin:2017:ULA:3027385.3027448","author":"Corrin, Linda and de Barba, Paula G. and Bakharia, Aneesha","title":"Using Learning Analytics to Explore Help-seeking Learner Profiles in MOOCs","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"424--428","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027448","doi":"10.1145/3027385.3027448","acmid":"3027448","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, help-seeking, learning analytics, learning design","Abstract":"In online learning environments, learners are often required to be more autonomous in their approach to learning. In scaled online learning environments, like Massive Open Online Courses (MOOCs), there are differences in the ability of learners to access teachers and peers to get help with their study than in more traditional educational environments. This exploratory study examines the help-seeking behaviour of learners across several MOOCs with different audiences and designs. Learning analytics techniques (e.g., dimension reduction with t-sne and clustering with affinity propagation) were applied to identify clusters and determine profiles of learners on the basis of their help-seeking behaviours. Five help-seeking learner profiles were identified which provide an insight into how learners' help-seeking behaviour relates to performance. The development of a more in-depth understanding of how learners seek help in large online learning environments is important to inform the way support for learners can be incorporated into the design and facilitation of online courses delivered at scale.","pdf":"Using learning analytics to explore help-seeking learner profiles in MOOCs  Linda Corrin University of Melbourne Level 6, 111 Barry Street  Melbourne, VIC 3010, Australia lcorrin@unimelb.edu.au  Paula G. de Barba University of Melbourne  Elisabeth Murdoch Building Melbourne, VIC 3010, Australia  paula.de@unimelb.edu.au  Aneesha Bakharia University of eensland  Learning Innovation Building St Lucia, QLD 4072, Australia  a.bakharia1@uq.edu.au  ABSTRACT In online learning environments, learners are oen required to be more autonomous in their approach to learning. In scaled on- line learning environments, like Massive Open Online Courses (MOOCs), there are dierences in the ability of learners to access teachers and peers to get help with their study than in more tradi- tional educational environments. is exploratory study examines the help-seeking behaviour of learners across several MOOCs with dierent audiences and designs. Learning analytics techniques (e.g., dimension reduction with t-sne and clustering with anity propa- gation) were applied to identify clusters and determine proles of learners on the basis of their help-seeking behaviours. Five help- seeking learner proles were identied which provide an insight into how learners help-seeking behaviour relates to performance. e development of a more in-depth understanding of how learners seek help in large online learning environments is important to inform the way support for learners can be incorporated into the design and facilitation of online courses delivered at scale.  CCS CONCEPTS Applied computing  E-learning;  KEYWORDS Learning analytics, MOOCs, Help-seeking, Learning design ACM Reference format: Linda Corrin, Paula G. de Barba, and Aneesha Bakharia. 2016. Using learning analytics to explore help-seeking learner proles in MOOCs. In Proceedings of LAK 17, Vancouver, BC, Canada, March 13-17, 2017, 5 pages. DOI: hp://dx.doi.org/10.1145/3027385.3027448  1 INTRODUCTION In online learning environments, learners are required to adopt a high level of autonomy to successfully complete their studies. In order to ensure that the design of online courses provides appropri- ate support for learners in such autonomous seings it is necessary to understand how and when learners seek help for their learning. e ability to seek help to resolve confusion and misunderstanding Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. LAK 17, Vancouver, BC, Canada  2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-4870-6/17/03. . .$$15.00 DOI: hp://dx.doi.org/10.1145/3027385.3027448  is especially important in Massive Open Online Courses (MOOCs) where learners access to teachers is limited due to the large number of learners and course activity. At the same time, many MOOCs provide opportunities for learners to seek help from their peers through tools such as discussion forums. What is still unclear is the extent to which this is taken up by learners for the purpose of geing help, and how this relates to other activities learners can undertake to get support for their learning. While learning analytics have been used extensively to explore learner behaviour paerns in MOOCs in relation to student performance and per- sistence, fewer studies have focused specically on the activities learners undertake in relation to the self-regulatory learning action of help-seeking.  In this paper we present the initial results of an exploratory study that uses learning analytics techniques to explore the proles of learners help-seeking behaviour in MOOCs. is paper makes two important contributions to the learning analytics literature: rstly, by comparing the proles of help-seeking behaviour across multiple MOOCs with dierent purposes (e.g., professional development vs. general interest) and designs; and secondly, by employing analysis techniques previously rarely used in learning analytics studies (e.g., t-sne and Anity Propagation) for clustering and to enable beer interpretation of prole groups. e outcomes of this study can help to identify ways that learners help-seeking can be supported and inform future investigation of specic elements of help-seeking behaviours in MOOCs.  2 CONTEXT Massive Open Online Courses (MOOCs) have become rmly ce- mented within the higher education landscape over the past eight years. e popularity of MOOCs, both in terms of learner de- mand for courses and the increasing involvement of universities in delivering them, provides opportunities for developing a greater understanding of how people learn in online environments. e scale of these courses allows unprecedented access to large sets of data on learner behaviour for researchers to explore educational issues including engagement [7], performance [6], and retention [1]. e application of learning analytics techniques to these large data sets oers the potential to uncover paerns and proles of learner behaviour that can inform beer learning support, feedback and course design.  In developing a beer understanding of learning in online en- vironments, researchers have used many dierent theoretical lens through which to explore this phenomenon. e approach adopted in this study relies on the lens of self-regulated learning. Self- regulated learners are active learners who plan, monitor, adapt and    LAK 17, March 13-17, 2017, Vancouver, BC, Canada L. Corrin et. al.  reect on their learning experience [16]. ey regulate their emo- tions, cognition, context and behaviour to achieve specic learning goals. Learners who display high levels of self-regulation have been associated with higher academic performance and persistence [4][10]. e nature of online learning environments oen requires greater levels of autonomy from learners than more traditional seings. As a result, learners need to be beer at self-regulating their learning to succeed [2].  A key element of self-regulated learning is the help-seeking behaviours learners adopt to support their learning processes. Help- seeking is the ability to seek out assistance from peers and teachers when learners encounter a challenge they nd too complex to solve [12]. Once they have identied the need for assistance, learners have to decide the best way to seek help in the given context.  is help-seeking process usually includes eight stages in which learners: determine whether there is a problem, determine whether help is needed, decide whether to seek help, decide on the type of help, decide on whom to ask, solicit help, obtain help, and process help received [12]. Learners can undertake information searches by consulting resources, such as a textbook, or have the option of help-seeking by consulting a human mediator, such as a peer. In online learning environments the concepts of information search and help-seeking overlap, as learners have the option to search for information that has been curated by their peers and teachers [17].  ere are several ways that learners can seek help within a MOOC environment. e most common is via the discussion forum where learners can post problems they are encountering to which peers and/or teachers can reply. e active use of discussion forums in MOOCs has been correlated to higher levels of performance in MOOCs in some studies (e.g., [19]). In relation to help-seeking be- haviours, a study by [9] found that clusters of dierent help-seeking behaviours had dierent course completion outcomes, with learn- ers who engaged primarily with sub-forums dedicated exclusively to discussion of the nal assignment more likely to fail the course. Other more passive forms of help-seeking behaviour in discussion forums include viewing and searching for posts on topics of interest. In addition, learners can seek help by pausing and searching for specic content in the instructional videos of the course.  Tools to support student help-seeking behaviour have been devel- oped and implemented in MOOCs. For example, the Data, Analytics and Learning MOOC (DALMOOC) incorporated a ick Helper tool designed to actively connect learners together to help to ensure learners who seek help through discussion forums receive a reply to their query [18].  Research on help-seeking in MOOCs has, to date, focused on students self-report data to measure how they actively ask other people for help [11] [13]. Alternatively, the research study pre- sented in this paper explores learners help-seeking behaviours in MOOCs through the use of learning analytics techniques to create learner proles. Such behaviours include not only asking others for help, but also indirect help-seeking behaviours, such as searching, subscribing and viewing forums or pausing and seeking specic locations in video. Recent research indicates the use of learning analytics (rather than self-report data) is a powerful method to in- vestigate students use of self-regulated learning skills [5]. Recent research also highlights the need for more studies to explore the instructional conditions, or learning designs, of online courses in  relation to learner usage proles [15] [3]. erefore this research was driven by the following research questions:  (1) What learner proles emerge from help-seeking behaviours displayed in MOOC environments  (2) What eect does the learning design have on dierent help-seeking behaviours in MOOCs in relation to learner performance  3 METHOD Four MOOCs were used in the analysis of learners help-seeking behaviour in this study. ese MOOCs were all developed by the University of Melbourne and delivered through the Coursera plat- form. Two of the MOOCs, Discrete Optimization and Assessment and Teaching of 21st Century Skills, represent MOOCs with a strong emphasis on professional development. Alternatively, the two other courses, e French Revolution and Animal Behaviour, were aimed more at a general interest audience. e cohorts of learners in- cluded in the study were taken from the 2015 oerings of each MOOC. e main design features of each MOOC are outlined in Table 1.  e sample included learners who achieved a grade of more than 4o% for the course. Unlike other studies of engagement proles in MOOCs that include all learners to be able to determine persistence, this study focused on the behaviour of those learners who partic- ipated in assessments throughout the course as a way to look at help-seeking behaviour. e sample size of learners for each MOOC used in the study was: Discrete Optimization (301), Assessment and teaching of 21st century skills (655), e French Revolution (825), and Animal Behaviour (658).  Data for each of the MOOCs was extracted from the Coursera course and clickstream databases. A feature matrix was then cre- ated which included 25 features which could represent help-seeking behaviours including active forum participation (i.e., the creation of forum posts, the creation of forum comments, forum thread sub- scription, forum post up voting, forum post down voting, forum tag creation and forum reputation points), passive forum partici- pation (i.e., forum post views, forum comment views and thread views), assessment (i.e., assignment, quiz and survey aempts, and the number of times a quiz result screen was viewed), search (i.e., search queries in discussion forums and search queries for over- all course), video interaction features (i.e., total videos accessed, number of times the play event was triggered, number of times the pause event was triggered, number of times the playback speed of a video was changed, number of times video seeking events were trig- gered and total video event interactions), total clickstream views, grade, whether the learner was enrolled in a signature track and notication subscription (i.e, email announcement subscription).  e feature matrix was scaled using min-max feature scaling prior to the performance of dimension reduction using t-distributed Stochastic Neighbor Embedding (t-SNE) [14]. e t-SNE dimension reduction technique provides a way to embed high-dimensional data (i.e., the learner feature matrix) into a lower dimensional space which can then be visualized in a scaer plot. e anity propaga- tion algorithm [8] was then used to nd the number of clusters and simultaneously group similar learners within a cluster. Traditional clustering algorithms like k-means, require the number of clusters    Using learning analytics to explore help-seeking learner profiles in MOOCs LAK 17, March 13-17, 2017, Vancouver, BC, Canada  Table 1: Design features of MOOCs  MOOC Description Duration Structure Discussion integration Animal Behaviour An exploration of how scientists study ani-  mal behaviour and the evolutionary forces of natural and sexual selection  8 weeks Sequential No direct instruction to ac- cess the discussion forums, but forums established for each week  Assessment and teaching of 21st century skills  An introduction to how 21st century skills can be taught and assessed in education  6 weeks Sequential Discussion prompts incorpo- rated into videos  e French Revolution An examination of the origins, course and outcomes of the French Revolution  6 weeks Sequential Weekly discussion prompts  Discrete Optimization An introduction to how to solve complex search problems with discrete optimiza- tion concepts and algorithms  8 weeks Open No direct instruction to ac- cess the discussion forums  to be specied as a parameter and additional techniques are re- quired to help determine the ideal number of clusters that exist in a dataset. Anity propagation was used within this study because its ability to automatically determine the number of clusters is useful in aiding exploratory data analysis and learner cluster interpreta- tion. Anity propagation also returns an exemplar for each cluster which in the context of this study represented the learner with the features that are most indicative of the cluster. e analysis of examplar students led to the initial development of the learner proles which were then validated by inspection of other learners in the cluster and feature statistics for the cluster. e resulting 2-D t-SNE scaer plot for the French Revolution MOOC is displayed in Figure 1, with each cluster labeled (and coloured).  Once the clusters had been generated, the learner exemplar and the cluster statistics (i.e., minimum, maximum and standard devia- tion for each feature) were analysed. Multiple clusters of MOOC participants were assigned to a learner prole based on forum activity, assessment performance, search and video interaction.  4 RESULTS AND DISCUSSION Five learner proles were identied using the clusters generated through the anity propagation technique. e learner proles and numbers of clusters associated with each prole across the four MOOCs are presented in Table 2.  e prole of low engagement included learners who made very lile use of the learning resources and discussion forums in the course. ese learners had low levels of performance on the assess- ments, but didnt seem to employ many help-seeking behaviours. In terms of dierences across the four MOOCs, the percentage of students who fell into this category from e French Revolution MOOC was higher than the other three courses. is could be at- tributed to the fact that this MOOC is designed as a general interest course which may impact the motivation of students to strive for high levels of engagement and performance.  e assessment-focused - low grades prole group included learn- ers who had multiple aempts at the assessment, but still received lower grades. ese learners didnt make use of the discussion fo- rum to seek help or watch many videos. Despite receiving feedback from multiple quizzes aempts, the performance of these students remained low. e exception in this prole group was the Discrete Optimization course. e grading and feedback for the assessment  in this course was automated around an algorithm and this was explained in detail to the students [20]. is awareness of the mean- ing of the feedback could account for the fact that learners were able to improve their grades on multiple aempts.  e passive engagement learner prole group represented the majority of learners across each of the courses. ese learners made moderate use of resources and forums resulting in a moderate grade for the course. While their use of the forums was higher than the two previous learner proles, there was still a low level of forum subscription and search, which represent more specic help-seeking behaviours in this context. An overwhelming majority of learners in Discrete Optimization fell within this learner prole. A large proportion of 21st Century Skills learners also fell within this prole which could be aributed to the fact that use of the discussion forums was integrated within the instructional videos, prompting learners to access and engage with the forums.  e active engagement prole includes learners who have high grades with a moderate number of assessment aempts. ese students have high levels of engagement with resources and videos, and participate actively in discussion forums. Like the passive en- gagement group they engage substantially in the discussion forums, but dont make use of the subscription and search functions.  e assessment-focused - high grades prole group had multiple aempts at assessments, resulting in higher grades. However, they did not engage with the discussion forums and made limited use of resources and videos. Interestingly, this prole group was found to exist in Animal Behaviour and 21st Century Skills, but not in the other two MOOCs. ese learners dont appear to engage in many help-seeking behaviours, yet still manage to perform very well overall. One possible explanation is that these learners have a high level of prior knowledge that allows them to perform well in assessments using, almost exclusively, feedback provided through quizzes to rene their knowledge in the context of the course.  is exploratory analysis has provided insights into the help- seeking behaviours of groups of students across dierent MOOCs. While dierences in MOOC design can possibly explain some dif- ferences in the proportions of help-seeking proles, further ex- ploration is needed to understand how these design factors have impacted learner behaviour in more depth. e analysis also un- covered several interesting observations in relation to help-seeking    LAK 17, March 13-17, 2017, Vancouver, BC, Canada L. Corrin et. al.  Table 2: Learner proles based on help-seeking behaviour  Prole Activity Forum Activity Animal Be- haviour (n = 658)  21st Cen- tury Skills (n = 655)  French Rev- olution (n = 825)  Discrete Optimization (n = 301)  Low engage- ment  Low grades, low pageviews, low video interaction, low, multiple grade aempts  No forum threads/post reads 19.3% 12.0% 34.6% 14.6%  Assessment- focused - low grades  Low grade, high as- sessment aempts, low pageviews, low video interaction  No forum thread/post reads 15.8% 23.3% 26.6% 0%  Passive engagement  Medium grade, moderate pageviews  Low forum thread subscrip- tion, low forum /course search, moderate forum thread/post reads  28.3% 34.6% 24.8% 67.1%  Active en- gagement  High grades, moderate as- sessment aempts, high pageviews  High forum participation, low forum thread subscrip- tion, low forum/course search  10.7% 10.8% 14.0% 18.3%  Assessment- focused - high grades  High grade, high as- sessment aempts, low pageviews, low video interaction  No forum activity, low forum thread subscription, low forum/course search  25.8% 19.0% 0% 0%  activities that could aid students, but are not commonly used, such as forum subscriptions and search features.  A benet of using the anity propagation technique over kmeans to identify the clusters of help-seeking learner behaviours is that it can generate learner exemplars. Further work on this data set will look at these exemplars using sequential and temporal analysis to explore these proles in more detail. For example, examining the sequence of how learners access, pause, replay and seek spe- cic locations in videos, which has been associated with achieving beer grades. It is expected that focusing on these more specic actions can help to inform the development of more targeted sup- port strategies and tools for learner help-seeking. e inuence of learner demographics within each prole is another avenue of further research we plan to pursue.  5 CONCLUSION is initial exploratory study has identied several interesting trends around the help-seeking behaviours of learners in MOOCs. Key help-seeking mechanisms within MOOC environments, such as discussion forums, still have low levels of engagement, with ac- tions such as search and subscription remaining very low. However, it was clear that engagement with the discussion forums, whether active or passive, generally resulted in higher performance in the MOOCs overall. e fact that many students are either not or only passively engaging with discussion forums indicates that design- ers of MOOCs need to explore other ways that support can be provided to learners. It was also important to note the impact of dierent learning designs in the distribution of help-seeking pro- les across the dierent MOOCs. For example, clearer explanations of assessment evaluation design (e.g., Discrete Optimization) and stronger integration of discussion tools into content delivery (21st  Figure 1: Learner clusters found by the Anity Propogation algorithm for the French Revolution MOOC.  Century Skills) resulted in dierent paerns of behaviour in relation to activities that could be viewed as help-seeking.  Developing beer strategies to assist learners with help-seeking in MOOCs is especially important in an environment where several MOOC providers are transitioning from cohort-based, timed deliv- ery of courses to on-demand courses that learners can take when they want and work through at their own pace. is change to how and when learners commence and progress through the course has    Using learning analytics to explore help-seeking learner profiles in MOOCs LAK 17, March 13-17, 2017, Vancouver, BC, Canada  implications for the ways that help can be sourced from peers. So if this contributes to the reduction in use or even disappearance of discussion forums as a key element of MOOCs, as predicted by some, what does this mean for help-seeking behaviour in MOOC environments  ese and many other questions about how help-seeking can be fostered and supported in MOOC environments remain. is short paper has presented the rst steps in the context of a broader study of how learners seek support for their learning. e use of learning analytics oers potential to explore this phenomenon in new and dierent ways. e methods adopted in this study have helped to gain a high-level view of the paerns of help-seeking behaviour across a number of MOOC designs. Further research is needed to explore some of these specic behaviours within the MOOC context in more detail, including the use of sequential and temporal techniques. In working to build and rene these help- seeking learner proles, it is also important to supplement the ndings derived from learning analytics techniques, such as the ones used here, with reections from learners about their help- seeking approaches. is will enable us to identify the strategies learners adopt to seek help that may fall between the gaps of tracked log events in MOOCs [21]. ese insights can then be used to inform strategies for the development of tools and resources to provide greater assistance and support to learners, regardless of the scale of the online learning environment.  ACKNOWLEDGMENTS e authors would like to acknowledge the support of the coordina- tors of the MOOCs included in this study: Professor Peter McPhee, Professor Raoul Mulder, Professor Patrick Grin and Professor Pascal Van Hentenryck. A Special Research Initiative of the Aus- tralian Research Council supported this research: ARC-SRI Science of Learning Research Centre (project number SRI20300015).  REFERENCES [1] Khaled M Alraimi, Hangjung Zo, and Andrew P Ciganek. 2015. Understanding  the MOOCs continuance: e role of openness and reputation. Computers & Education 80 (2015), 2838.  [2] Anthony R Artino. 2007. Self-regulated learning in online education: A review of the empirical literature. International Journal of Instructional Technology and Distance Learning 4, 6 (2007), 318.  [3] Aneesha Bakharia, Linda Corrin, Paula de Barba, Gregor Kennedy, Dragan Gasevic, Raoul Mulder, David Williams, Shane Dawson, and Lori Lockyer. 2016. A conceptual framework linking learning design with learning analytics. In Pro- ceedings of the Sixth International Conference on Learning Analytics & Knowledge. ACM, 329338.  [4] J Broadbent and WL Poon. 2015. Self-regulated learning strategies & academic achievement in online higher education learning environments: A systematic review. e Internet and Higher Education 27 (2015), 113.  [5] Moon-Heum Cho and Jin Soung Yoo. 2016. Exploring online students self- regulated learning with self-reported surveys and log les: a data mining ap- proach. Interactive Learning Environments (2016), 113.  [6] Carleton Corin, Linda Corrin, Paula de Barba, and Gregor Kennedy. 2014. Visual- izing paerns of student engagement and performance in MOOCs. In Proceedings of the fourth international conference on learning analytics and knowledge. ACM, 8392.  [7] Rebecca Ferguson and Doug Clow. 2015. Examining engagement: analysing learner subpopulations in massive open online courses (MOOCs). In Proceedings of the Fih International Conference on Learning Analytics And Knowledge. ACM, 5158.  [8] Brendan J Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. science 315, 5814 (2007), 972976.  [9] Nabeel Gillani, Rebecca Eynon, Michael Osborne, Isis Hjorth, and Stephen Roberts. 2014. Communication communities in MOOCs. arXiv preprint  arXiv:1403.4640 (2014). [10] Carolyn Hart. 2012. Factors associated with student persistence in an online  program of study: A review of the literature. Journal of Interactive Online Learning 11, 1 (2012), 1942.  [11] Nina Hood, Allison Lilejohn, and Colin Milligan. 2015. Context counts: How learners contexts inuence learning in a MOOC. Computers & Education 91 (2015), 8391.  [12] Stuart A Karabenick and Myron H Dembo. 2011. Understanding and facilitating self-regulated help seeking. New Directions for Teaching and Learning 2011, 126 (2011), 3343.  [13] Rene F Kizilcec, Mar Perez-Sanagustn, and Jorge J Maldonado. 2017. Self- regulated learning strategies predict learner behavior and goal aainment in Massive Open Online Courses. Computers & Education 104 (2017), 1833.  [14] Laurens van der Maaten and Georey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research 9, Nov (2008), 25792605.  [15] Negin Mirriahi, Daniyal Liaqat, Shane Dawson, and Dragan Gasevic. 2016. Uncov- ering student learning proles with a video annotation tool: reective learning with and without instructional norms. Educational Technology Research and Development 64, 6 (2016), 10831106.  [16] Paul R Pintrich. 2000. e role of goal orientation in self-regulated learning. Academic Press.  [17] Minna Puustinen and Jean-Francois Rouet. 2009. Learning with new technologies: Help seeking and information searching revisited. Computers & Education 53, 4 (2009), 10141019.  [18] Carolyn Penstein Rose, Oliver Ferschke, Gaurav Tomar, Diyi Yang, Iris Howley, Vincent Aleven, George Siemens, Mahew Crosslin, Dragan Gasevic, and Ryan Baker. 2015. Challenges and opportunities of dual-layer MOOCs: Reections from an edX deployment study. In Proceedings of the 11th International Conference on Computer Supported Collaborative Learning (CSCL 2015), Vol. 15. 848851.  [19] Shu-Fen Tseng, Yen-Wei Tsao, Liang-Chih Yu, Chien-Lung Chan, and K Robert Lai. 2016. Who will pass Analyzing learner behaviors in MOOCs. Research and Practice in Technology Enhanced Learning 11, 1 (2016), 8.  [20] Pascal Van Hentenryck and Carleton Corin. 2014. Teaching creative problem solving in a MOOC. In Proceedings of the 45th ACM technical symposium on Computer science education. ACM, 677682.  [21] George Veletsianos, Justin Reich, and Laura A Pasquini. 2016. e Life Between Big Data Log Events Learners Strategies to Overcome Challenges in MOOCs. AERA Open 2, 3 (2016), 110.    Abstract  1 Introduction  2 Context  3 Method  4 Results and Discussion  5 Conclusion  Acknowledgments  References   "}
{"index":{"_id":"53"}}
{"datatype":"inproceedings","key":"Ez-zaouia:2017:ETO:3027385.3027434","author":"Ez-zaouia, Mohamed and Lavou'e, Elise","title":"EMODA: A Tutor Oriented Multimodal and Contextual Emotional Dashboard","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"429--438","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027434","doi":"10.1145/3027385.3027434","acmid":"3027434","publisher":"ACM","address":"New York, NY, USA","keywords":"emotions, interactive visualizations, language training, learner monitoring, multimodal data, tutor dashboard","Abstract":"Learners' emotional state has proven to be a key factor for successful learning. Visualizing learners' emotions during synchronous on-line learning activities can help tutors in creating and maintaining socio-affective relationships with their learners. However, few dashboards offer emotional information on the learning activity. The current study focuses on synchronous interactions via a videoconferencing tool dedicated to foreign language training. We collected data on learners' emotions in real conditions during ten sessions (five sessions for two learners). We propose to adopt and combine different models of emotions (discrete and dimensional) and to use heterogeneous APIs for measuring learners' emotions from different data sources (audio, video, self-reporting and interaction traces). Based on a thorough data analysis, we propose an approach to combine different cues to infer information on learners' emotional states. We finally present the EMODA dashboard, an affective multimodal and contextual visual analytics dashboard, which allows the tutor to monitor learners' emotions and better understand their evolution during the synchronous learning activity.","pdf":"EMODA: a Tutor Oriented Multimodal and Contextual  Emotional Dashboard   Mohamed Ez-zaouia  Universit Blaise Pascal, Clermont-Ferrand   SpeakPlus, 56, bd Niels Bohr CS 5213269603  Villeurbanne, France   ezzaouia.mohamed@gmail.com   Elise Lavou  IAE Lyon, Universit Jean Moulin Lyon 3    LIRIS, UMR5205, CNRS   6 cours Albert Thomas, 69008 Lyon, France   elise.lavoue@univ-lyon3.fr     ABSTRACT  Learners emotional state has proven to be a key factor for  successful learning. Visualizing learners emotions during  synchronous on-line learning activities can help tutors in creating  and maintaining socio-affective relationships with their learners.  However, few dashboards offer emotional information on the  learning activity. The current study focuses on synchronous  interactions via a videoconferencing tool dedicated to foreign  language training. We collected data on learners emotions in real  conditions during ten sessions (five sessions for two learners). We  propose to adopt and combine different models of emotions  (discrete and dimensional) and to use heterogeneous APIs for  measuring learners emotions from different data sources (audio,  video, self-reporting and interaction traces). Based on a thorough  data analysis, we propose an approach to combine different cues  to infer information on learners emotional states. We finally  present the EMODA dashboard, an affective multimodal and  contextual visual analytics dashboard, which allows the tutor to  monitor learners emotions and better understand their evolution  during the synchronous learning activity.   CCS Concepts   Human-centered computing  Visualization   Visualization  application domains   Visual analytics  Applied computing  Education   Interactive learning environments;   Keywords  Emotions; Interactive visualizations; Learner monitoring; Tutor  dashboard; Language training; Multimodal data.   1. INTRODUCTION  Learning analytics (LA) is a fast-growing field with a strong  potential of making on-line learning environments competitive,  attractive and, more importantly, efficient and fit for purpose [1].  Several studies position the learner at the center of interest in  measuring performance in the learning environment ecosystem [2,  3]. At the same time, teachers lack visual insights that can help  them monitor learners during on-line learning activities, especially  with indicators on their emotions.    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.LAK '17, March 13 - 17, 2017, Vancouver, BC,  CanadaCopyright is held by the owner/author(s). Publication rights  licensed to ACM.ACM 978-1-4503-4870-6/17/03$15.00 DOI:  http://dx.doi.org/10.1145/3027385.3027434  Several recent studies demonstrate that emotions are an essential  factor in the learning process [4, 5] and, more precisely, that  positive emotions play a crucial role in fostering successful  learning [6, 7]. Therefore, considering learners' emotions can have  a huge impact on learning and achievement. Research into the  emotional dimension of learning environments has thus started to  attract more interest in the community (e.g. [8, 9, 10, 11]).   Consequently, to optimize the learning experience, it is important  for tutors to be aware of and understand the emotional  characteristics experienced by the learners.   The majority of the proposed dashboards considering learners  emotions are powered only by learners interaction traces stored  in the learning environment [12, 13]. Other dashboards are based  on a single channel [10, 11] or on self-reported data only [9].  However, several studies concerning the affective computing  domain, particularly emotion recognition, show that user behavior  has to be considered in a global and multimodal fashion (voice,  image, interaction, self-report, etc.). In this way, affective  multimodal systems are more accurate than unimodal approaches  [14, 15]. By considering multimodal data, the visual insights  proposed on dashboards can go beyond simple reporting visual  displays to attain a multimodal visual analytics tool combining  different cues with their associated contextual information. Such  rich visualizations can facilitate actionable feedback and decision- making for tutors in the online learning context.   The aim of our paper is to propose a multimodal and contextual  emotional dashboard called EMODA. The design of this  dashboard is based on a thorough analysis of learners emotional  data collected in real conditions. We more precisely answer the  following research questions:   1) How to collect data on learners emotions during online  learning sessions   2) How to exploit and combine different cues for inferring  learners emotions   3) How can learners inferred emotions be visualized by tutors to  facilitate actionable feedback    We conducted an experimental study using a real-time web  videoconferencing learning platform called SpeakPlus. This  solution aims at training learners in foreign language speaking,  based on online training sessions between a tutor and a learner.  We applied both dimensional and discrete models for measuring  learners emotions. We used heterogeneous APIs for emotion  classification based on different cues and in non-intrusive settings.  We provided an exploratory analysis comparing learners inferred  emotions and an approach for unifying different models of  emotions. Based on the results, we designed an affective  multimodal dashboard that presents the measured emotions with  their associated contextual information. These visualizations can     characteristics of the learners and, consequently, improve the  socio-affective relationship between them and their learners.   The paper is organized as follows. Section 2 presents the  theoretical and technological background. Section 3 details the  context and the experimental settings of our study. Section 4  focuses on emotional data collection and analysis. Section 5  presents the EMODA dashboard, a multimodal and contextual  emotional dashboard. The last section presents our conclusions  and future directions.   2. THEORETICAL AND  TECHNOLOGICAL BACKGROUND  This section gives an overview of research in learning analytics  and emotions. In the first part we present some recent affective  learning systems. The second part is dedicated to emotion  theories, emotion recognition and measurement methods.   2.1 Emotion and Learning  Over the last decade, researchers in the educational field have  showed huge interest in emotions. Recent studies underscore their  key role in learning, regulation processes and strong coupling to  motivation and achievement [5, 6, 8, 16]. Learning is more likely  to be successful if tutors, as well as learning platforms, help  minimize negative emotions (e.g. fear, frustration, stress) and  facilitate positive ones (e.g. happiness, enjoyment). This has led to  the increased importance of emotional data in online learning and  teaching environments. Designing learning environments that are  enjoyable, motivating and inspiring for learners is a key issue for  the LA community [17].     In a traditional learning environment, tutors can, more or less,  easily monitor and evaluate the emotions experienced by their  learners. However, in computer-mediated learning settings, this  might not be an easy task for tutors. Therefore, emotional  analytics can provide tutors with the opportunity to monitor and  better understand their learners emotions (e.g. [9]). The  development of such affective monitoring systems depends on the  measurement of learners emotional state. Methods for measuring  emotions can be either Objective/Subjective, Snapshot (before and  after learning)/Continuous and Qualitative/Quantitative [18].   Recent research has emerged on this topic by tracking learners'  emotions to promote awareness, decision-making and information  visualization. Ruiz et al. [9] propose an emotional dashboard to  support students' self-reflection. The authors opted particularly for  subjective methods to measure students emotions using a  questionnaire addressed to learners (before and after the learning  activity  snapshot type). Their results show that the students  emotions are correlated to their performance in class.  GhasemAghaei et al. [10] propose a dashboard to help and  support tutors in reflecting on students emotions experienced  during learning. The authors used objective and continuous data,  mainly images extracted from students webcams during the  learning activity, to infer students emotions using a dedicated  software. Happy et al. [11] propose a system that can estimate  both alertness and attention levels from ocular parameters and the  emotional state using objective data, mainly visual cues (video).   Most of the proposed dashboards in the educational domain are  rather unimodal-based (e.g. [9, 10, 19]). Some tools collect  subjective data from questionnaires that are easy to handle but  cannot detect emotional reactions at given times during the  learning session. Furthermore, unimodal data and subjective  questionnaires are not as accurate as global multimodal  approaches as explained in the next section. The latter introduces   emotion theories and the main tools for automatic emotion  recognition.   2.2 Emotion Recognition Theories  Emotions represent a complex psychological state of the person.  Researchers examining the development of our internal emotional  aspects have come to diverse opinions. This diversity leads to  different perspectives for emotions.   Discrete and dimensional are the two most important and widely  adopted emotion theories. Discrete emotions are a small set of  distinct emotions (e.g. anger, disgust, fear, happiness, sadness,  surprise), called universal or basic emotions, which constitute the  core of all humans emotions regardless of the socio-cultural  factors of the person [20]. Dimensional emotions are rather  structured in a dimensional space varying along certain  dimensions, such as the degree of the valence representing the  positiveness of the emotion felt (varying between positive and  negative) and the arousal that represents the physical response  and the intensity of that emotional manifestation (varying between  low and high) [21]. It should be noted that, while other  dimensions have been proposed (e.g. control [22]), valence and  arousal are still the most widely adopted dimensions. At the same  time, different studies have been derived, mapping one theory into  the other (e.g. [22]).   The interest in emotions has rapidly increased in human-computer  interaction applications, more commonly known as affective  computing [23]. Research in psychology, neuroscience and  computer science, especially Artificial Intelligence (AI), has been  combined to teach computers how to infer humans emotions.  Researchers rely on the intensity emitted (internally or  externally) by the emotion felt in order to infer the persons  emotions. Subjective emotions of a person can be collected  through self-reporting. Two other approaches are used to collect  objective measurements: perception-based measurements include  all the manifestations expressed by the person (facial, vocal,  gesture, textual, etc.) while physiological measurements include  all human body responses (heart-beat, blood pressure, brain  activity, etc.) [24].    The automatic emotion recognition (AER) process can be divided  into three main stages: feature extraction aims to filter, extract  and normalize the suitable features, feature reduction aims to  reduce the high dimensionality of the extracted features, and  classification aims to classify emotions using machine learning  techniques [25]. Our research follows these three AER steps to  feed the visualizations offered in our dashboard.   Three main features are used for automatic emotion recognition in  non-intrusive settings. Facial expression recognition can be based  on two techniques: a geometric model-based technique that relies  on distinctive facial features such as the position of nose, eyes,  mouth, and an appearance model-based technique that considers  the face as an array of intensity values (pixels). Thus, facial  expressions can be mapped to emotions through theories such as  those proposed by [20]. In terms of tools we can cite for example  FaceReader1 and Microsoft Emotion Service API2 that we adopted  in our study. Other automatic emotion recognition features are  based on voice (sound) and are attracting increasing interest with  researchers. The most commonly used features are the prosodic  (pitch, intensity and first formant frequency) [26] and the spectral  features, (Mel-frequency spectral coefficients (MFCC) [26]. In                                                                        1http://www.noldus.com  2https://www.microsoft.com/cognitive-services/en-us/emotion-api     terms of tools, we can cite for example OpenEAR [28] and  Beyond Verbal Emotion Service API3 that we chose for our study.  Textual cues are also considered. However, we do not consider  them since the learning activity we observe is not textual-oriented.    The most important current trend in emotion recognition is the  combination of different cues (visual, vocal, textual, etc.) for  inferring emotions. In fact, many studies (e.g. [14, 15]) show that  the multimodal approach for measuring peoples emotions is more  accurate than the unimodal approach. Different strategies exist for  data fusion. The first one, called feature level fusion, relies on  feature vectors for the different cues that are stacked together and  used as input to the classifier. The second strategy is called  decision level fusion; each cue is classified separately with its  classifier, and the result of each classifier is combined and used as  input for a final classifier. We opted for this second strategy.   In our study, the means of communication between the tutor and  the learner is a videoconferencing: audio-visual cues are thus the  main data channel. We propose to apply both dimensional and  discrete emotion theories using several APIs for emotion  recognition. We combine these data with self-reported data and  interaction traces. We thus aim to acquire rich data on learners  emotions to produce a multimodal and contextual emotional  dashboard for tutors.   3. CONTEXT AND METHODOLOGY  This section is dedicated to the description of the context and the  procedure of our study, mainly the learning environment and the  experimental settings.   3.1 SpeakPlus: a Web-based Learning  Environment for Foreign Language Training   SpeakPlus is a commercial Software as a Service (SaaS), a web- based environment dedicated to improving oral communication  skills in a foreign language. English, French and Spanish are  supported by the platform. The platform connects qualified tutors  and learners around the world in a way that the learning activity  focuses mainly on the goals set by the learner him/herself (e.g.  improving oral presentation skills, preparing for travel abroad).  The platform is designed in a modular fashion to be used as a  research tool by SpeakPlus researchers partners, mainly for  testing and evaluating new approaches and didactic models in the  educational domain.    The learning activity in SpeakPlus can be divided into two main  steps. The first step takes place before the learning session: at this  level, the tutor can create or customize learning materials  (activities) for each learning session by creating new materials or  reusing old ones from his/her library. By combining the materials,  the tutor can create a learning plan for each session. The learning  activity has a duration and can be associated with documents,  instructions for the learner or personal notes for the tutor. After  preparing the learning materials, the tutor can conduct a  synchronous learning session with a learner (see Figure 1). The  tutor and the learner communicate in real-time by means of a  dedicated videoconferencing technology optimized for online  learning. The tutor has the possibility of sharing the prepared  materials with the learner (e.g. pdf, word, image, audio, and  video) or of communicating via the chat (mainly to point out some  expressions, words, etc.).   3.2 Experiment Procedure and Participants  Our study is based on authentic and non-intrusive settings. In fact,                                                                       3http://www.beyondverbal.com/api   the data were collected using sensors already used in the platform  for the learning activity, namely a webcam and a microphone for  video and audio recording, respectively. The platform has been  slightly augmented to collect and process data (see section 4).   Two graduate female students (in their twenties) agreed to  participate in our pilot study. They were contacted after their  enrollment in the platform to train in French language speaking  for professional goals, more precisely for preparing a job  interview. One learner is from Latin-America and the other from  Asia. Each learner trained for five learning sessions with the same  tutor. The learners had one learning session every week. Each  session lasted 45 minutes. Data collection lasted around eight (8)  weeks as the students did not start their learning sessions at the  same time. We collected a total of 7 hours and 30 minutes of  audio/video data.     Figure 1. Live learning session on the SpeakPlus platform   4. EMOTIONAL DATA ACQUISITION  AND ANALYSIS  This section presents the first three stages of our research:  learners emotions data acquisition, data exploration, and data  analysis, that we applied to produce visualizations on the  proposed emotional dashboard for tutors.   4.1 Emotional Data Acquisition  Throughout this section we aim to respond to research questions  1) and 2) identified in the introduction. Therefore, different  trackable cues have been chosen to investigate how they can be  used to track learners emotions in the SpeakPlus learning  environment. We adopted a multimodal approach and identified  four data sources: audio, video, self-report, and interaction traces.  Audio and video are the primary communication channels in the  platform as the learning activity aims at improving oral skills via  videoconferencing technology. Self-reported data inform on  learners subjective emotions before and after the learning  session. Finally, interaction traces have been used to contextualize  the measured emotions and investigate into whether or not there  are any particular triggers behind learners emotions.   According to our multimodal approach, we used heterogeneous  cloud APIs for measuring emotions. These APIs were chosen to  apply both dimensional and discrete emotions models. The  Microsoft emotion recognition Service API (MS API) classifies  emotions based on facial expressions (video) according to a  discrete emotion model. The Beyond Verbal service API (BV  API) classifies emotions based on voice (audio), according to a  dimensional emotion model (arousal and valence).  Figure 2 shows the global architecture of the system we have built  to collect emotional data during learning sessions.  The learning  sessions between the tutor and the learner are recorded     (audiovisual). Records are stored along with the session in the  cloud (AWS S3). At the end of the session, audiovisual records  are downloaded from S3 and processed via the BV API and the  MS API. The resulting classified emotions are then stored in a  database, together with the interaction traces and self-reported  data, via a developed API for data collection. The stored data are  then processed and aggregated to produce visualizations. The  following subsections describe for each data source how the data  are collected and stored and how emotions are classified.     Figure 2. EMODA global architecture   4.1.1 Self-Reported Emotion Collection  We developed an interactive interface that enables learners to  express their emotion(s) before and after the learning session,  according to both dimensional and discrete emotions. The  dimensional interface is inspired from the MoodMap tool  proposed in [29]. Basically, it proposes a two-dimension (valence  and arousal) squared area (Figure 3-a). The learner can click on it  to express his/her valence and arousal levels. Four colors were  combined in a conic gradient form to represent the four levels:  positive and negative valence by green and red colors  respectively, and positive and negative arousal by blue and yellow  colors respectively. A mouse hover event has been added to track  and display the position of the mouse. This information is  reflected properly in a mirror bar chart at the bottom of the  squared area to guide the learner when reporting his/her two- dimensional emotion. Figure 3-b presents the interface that  enables learners to report their emotions in a discrete manner. A  slider for each emotion (neutral, happiness, surprise, sadness,  disgust, contempt, anger) is used to report its intensity on a  continuous scale from 1 to 100. An emoticon and a tooltip  displaying the emotion name (when the mouse is hovering the  slider) are used to distinguish between each emotion. The value of  the slider is tracked and displayed in a marker associated with the  slider and inside a bubble next to it.     Figure 3. Interface for self-reporting of emotions   In our study, learners were asked to express their emotions both at  the start and at the end of the learning session. This emotional   information is complementary to the data automatically collected  during the learning session. The collected self-reported data are  stored in a database as illustrated in Figure 2.   4.1.2 Facial Expression Acquisition and Analysis  The learning sessions are recorded in the SpeakPlus platform.  Each session may have one or more archive folder(s) depending  on whether or not there were any interruptions during the session.  Each archive folder contains two files: one for the learner  recorded stream and the other for the tutor recorded stream. An  algorithm based on the FFmpeg library (ffmpeg.org) was used to  concatenate the video streams from the archive folders when  needed. Once the video sequence has been constructed, emotions  are classified and stored using a simple algorithm that first uses  FFmpeg to extract frames (one frame per second) from the video  stream and then calls the MS API to classify the emotions  identified for each frame, before storing the result in a database.  More precisely, the MS API returns a face entry and its associated  emotion scores. An empty response indicates that no face was  detected. An emotion entry contains the fields listed in Table 1.   Table 1. Description of MS API response (& JSON example)  Field Description   faceRectangle Rectangle location of the face in the image.   scores  Emotion scores for each face in the image; (neutral,  happiness, surprise, sadness, disgust, contempt, fear,  anger).   application/JSON  [ {   faceRectangle : {  left : 68,  top : 97,  width : 64,  height : 97 },       scores : {         anger : 0.00300731952,         contempt : 5.14648448E-08,         disgust : 9.180124E-06,         fear : 0.0001912825,         happiness : 0.9875571,         neutral : 0.0009861537,         sadness : 1.889955E-05,         surprise : 0.008229999   } } ]   4.1.3 Audio Data Collection and Analysis  The same algorithm as for video was used to construct the sound  sequence from archive folders (only the learner channel). A call of  BV API then returns an array of time interval entries and their  associated emotion scores. An empty response indicates that no  emotions were detected in the audio file.    Table 2. Description of BV API response (&  JSON example)   Field Description   analysisSegments The array containing analysis segments.  Offset The offset of the segment from the beginning of   the sample being analyzed (in ms).  duration Duration of the analysis segment in the sample   being analyzed.  analysis Analysis object. Contains analysis values for the   segment. The content of the object is provided as  an example.    valence Valence object score (has value and group):  - Value: a value between 0 and 100.  - Group: positive, neutral or negative.   arousal Arousal object score (has value and group).  - Value: between 0 and 100.  - Group: low, neutral or high   application/JSON   analysisSegments : [  {   analysis : {                 Arousal : {  Group :  low ,  Value :  4.35  },                 Valence : {  Group :  positive ,  Value :  82.28  }                 duration : 37410,                 offset : 4576  } ]     An emotion entry contains the fields listed in Table 2. The  classified emotions are then stored in a database as illustrated in  Figure 2.   4.1.4 Interaction Trace Collection  We decided to track all actions performed by the tutor and the  learner on the SpeakPlus environment during the learning session.  This information can be used to contextualize the collected  emotional data. We list in Table 3 the events tracked (in real-time)  and stored in database using WebSocket.   Table 3. List and description of events collected in interaction  traces with the SpeakPlus environment   Field Description  action_name The name of the action (event) triggered   between the actors, it can be:   - SHARING_DOC: the tutor starts sharing a   document    - STOP_SHARING_DOC: the tutor stops   sharing a document    - ACTIVITY_TASK: an action sent to the   learner by the tutor   - FREE_TEXT: message chatting  - SHOW_DOC: the learner enables display   of the shared document  - HIDE_DOC: the learner disables display   of the shared document  action_content_type The type of event:   - PDF: Pdf document sharing  - AUDIO: Audio file sharing  - VIDEO: Video file sharing  - IMAGE: Image file sharing  - TEXT: Text action sent to the learner by   the tutor  - SHOW_HIDE_DOC: for the   SHOW_DOC or HIDE_DOC action  activity Activity Id  item Item Id in the activity (an activity might have   one or more items)  document Document Id (pdf, audio, video, etc.)  chat_message Message-Id   4.2 Emotional Data Exploration and Analysis  This part presents the data exploration and analysis stage of our  study. It addresses mainly research question 2), i.e. investigating  whether or not audio and video data are correlated. We do not  include self-reported information in the analysis because it is not  collected at the same time (before and after the session).  Throughout this section we answer more specifically the  following sub-questions: a) Is there a correlation between audio  and video data and b) What valuable emotional information  should be visualized   4.2.1 Emotional Data Comparison  Use of two different models for emotions (dimensional based on  audio and discrete based on video) was the most challenging part  of this comparison. Thus, to be able to compare these cues both  models have to be unified. Several studies have been conducted in  this context to convert discrete emotions into a dimensional model  (valence and arousal). We used the valence coordinates of the  discrete emotions (neutral, happiness, surprise, sadness, disgust,  contempt, fear, anger) as proposed in [22]. The coordinates  provided are on a scale from -100 to 100, from negative valence  to positive valence (neutral emotion has a valence equal to zero).  As described in section 3.1.3, audio valence is on a scale from 0 to  100 (as returned by the BV API). For instance, neutral valence is  equal to 50. Therefore, scale unification was needed between the   coordinates from [23] and the returned result from the VB API.  To this end, we used the formula (1) that maps a domain [a, b]  interval to a range interval [c, d] to scale VB interval from [0,  100] to [-100, 100].     =       (    )     +           (1)   As mentioned in 3.1.2, a successful call to the MS API returns the  emotion scores of the frames (images). By combining the  coordinates from [23] and the returned scores, we were able to  compute for each frame the corresponding value of the valence.  The weighted mean was used for this purpose, as illustrated in the  formula 2 :   345 67389: = 67389:  >   @@  {CDEFGHI,KHLLMCDNN,   NEGLGMND,NHOCDNN, OMNPENF,QRCFDSLF,  TDHG,HCPDG}       (2)   - scoreTGHSD[  : is the score of the emotion i   { neutral,   happiness, surprise, sadness, disgust, contempt, fear, anger }  in the result returned by the MS API for the frame n  (frameC)    - valenceM : the valence corresponding to the emotion i.   - valenceHcPTGHSD  [  : the weighted valence average (weighted  mean) of the frame n.   As described in 3.1.2, a call to the BV API returns an array of  time segment entries with their associated emotion scores (valence  and arousal). Thus, a simple algorithm has been implemented to  map the images extracted from video, every second, to its  corresponding audio time segments. Once the images have been  grouped, the valence is computed first for each image (with 2 ).  The average over images belonging to each time segment is then  computed.   4.2.2 Emotional Data Correlation  This section responds to the sub-question a). Once the two data  models were unified, the aim was to investigate whether there is  any correlation between both variables: audio and video valence.   There are several coefficients for measuring dependencies  between variables. Spearmans and Pearsons coefficients are  those most used in the literature. However, they can detect only  linear or non-linear monotonic correlations as stated in [30].  Conversely, the Maximal Information Coefficient (MIC) may be  more powerful as it can detect several associations between  variables, such as linearity, nonlinearity, asymmetry and even  non-functionality [30]. Table 4 presents the correlation  coefficients (MIC, Spearman, and Pearson coefficients) of audio  and video valence.    Table 4. Correlation coefficients between Audio / Video  valence - MIC vs. Pearson vs. Spearman    Coefficients MIC PEARSON-R SPEARMAN-R  Valence    Audio Vs Video 0.787 0.052 0.172   Regarding the MIC coefficient, the variables (audio and video  valence) are correlated (contrary to Pearson and Spearman). We  deduce that there is no clear linear correlation between the video  and audio variables, but there must be a correlation between the  peaks (either positive or negative) of both variables, which is a  non-functional correlation that the MIC can detect. At this stage,  an exploratory analysis using audio-video records was necessary     to further investigate and understand this non-functional  dependency between both cues.   4.2.3 Emotional Data Exploration  To explore audio and video valences, we draw a line chart for the  ten learning sessions. The result reveals an explicit dependency  between both valences. Figure 4 and Figure 5 are examples of one  learning session valence timeline. The x-axis represents the time- segment indices, while the y-axis represents the corresponding  valence score between -100 and 100. As annotated with blue color  rectangles, many apparent similarities and dissimilarities exist  between the peaks in audio and video valences.   Figure 4 presents some examples of dissimilarities comparing raw  audio and video data for one learning session. Table 5 presents the  associated interpretations and comments for the first four  segments.     Figure 4. Audio and video valence - examples of dissimilarities   Table 5. Interpretation of valence dissimilarities (audio/video)   Index Audio Video Comments   8 The correction of  a mistake by the  tutor triggers a  regret reaction in  the learner. Thus  valence  decreased.   Almost half of  the images have  been classified  as happiness.  Thus valence  increased.   Even if the learners  voice showed regret,  there was no sign of  this on his/her face.    13 A kind of aaaah  expressed by the  learner. Thus  valence  decreased.    Neutral emotion Audio might be a  better indicator for  some specific  emotional vocal  expressions.    55 The learners  voice was calm  (neutral).   Many images  have been  classified as  happiness.  Thus valence  increased.   The learner was  smiling, which was a  sign of happiness',  but a calm voice was  a sign of neutral' for  audio.   91 The learner was  speaking loudly  and confidently.  Thus valence  increased.    The majority of  images have  been classified  as neutral.  Thus valence  decreased.      Neutral was the  dominant emotion for  the video while the  audio indicated a  high level of valence  which is not reflected  on the face.   Similarly, Figure 5 presents some similarity examples comparing  raw audio and video data for the same learning session as above.  Table 6 presents the associated interpretations and comments for  the first two segments in the figure. The remainder of the  segments is more or less similar to the commented segments.     Figure 5. Audio and video valence - examples of similarities   The comparative analysis conducted in this section reveals some  interesting points. First, by adopting different models of emotions  and using heterogeneous APIs for emotion recognition, we were  able to identify many similarities and dissimilarities between the  measured emotions from both audio and video data. This result  underscores a dependency between these cues. Second, as  described in Table 5 and Table 6, audio data can reveal some  particular levels of valence that video data fail to detect and vice  versa. We deduce that both cues should be considered for  inferring learners emotions.   Table 6. Interpretation of valence similarities (audio/video)  Index Audio Video Comments   25 The learner was  obliged to  interrupt the  session in order to  talk to a member  of his/her family.     Neutral'  emotion   This proves that the  result might be more  accurate when both  cues are combined.   31 There was a joke  in this segment  that leads to  positive valence,  as well as a peak  in the audio   valence.   Only one image  has been  classified as  happiness.   This also proves that  the result might be  more accurate when  both cues are  combined.   4.2.4 Emotional Data Integration  This section responds to sub-question b) and tends to aggregate  data to create information for the visualizations offered in the  dashboard. The amount of data collected for each session is huge,  and only a subset of these data is likely to constitute an effective  emotional state. Hence, both dimensional and discrete emotions  need to be filtered and synthetized to keep only valuable  emotional reactions.    Concerning dimensional emotions (based on audio), we use  arousal (intensity of the emotion) to filter the valence of audio  data. A list of arousal thresholds (10, 20, 30, 40, 50) was defined.  When a threshold is selected, only the time-segment entries with  the highest corresponding arousal are considered of interest during  a learning session.    Concerning discrete emotions (based on video), time series  analysis was used to synthetize the collected emotions. First, the  set of emotions (neutral, happiness, surprise, sadness, disgust,  contempt, fear, anger) is divided into three categories: neutral,  positive (happiness, surprise) and negative (sadness, disgust,  contempt, fear, anger). Second, the scores of these emotions are  computed as the sum of the corresponding emotions for each  category and so, for each image (frame), (see (3) (4) (5)).  Third, once the scores have been computed for each image, we  identify the dominant type of emotion (neutral, positive, and  negative) as the most frequent over all the frames of the segment.   g9hi73j 67389:  =   67389:  :klmnop        (3)   qrs@i@49 67389: =  67389:  >  @  {KHLLMCDNN, NEGLGMND}          4    g953i@49 67389: = 67389:  >  @  {NHOCDNN, OMNPENF,QRCFDSLF,TDHG,HCPDG}          5    - scoreTGHSD[  : is the score of the emotion i   { neutral,   happiness, surprise, sadness, disgust, contempt, fear, anger }  in the result returned by the MS API for the frame n.     We then identified two patterns: the positive moments and the  negative moments (neutral is dropped as it may not be interesting  for learner monitoring). Thus, two measurements have been  defined, True positive and True negative, to track these  patterns. True positive detects the moments where the average of  positive points is highest or equal to a fixed threshold. True  negative is defined to detect moments where negative emotions  are defined as significant, i.e. the gap between the average of  negative points and the maximum score of the remaining  emotions is less than or equal to a fixed threshold. The detected  positive/negative moments for both dimensional and discrete  emotions are considered as the most important time-segments that  are more likely to represent effective emotional reactions during  the learning session, hence, they will be used to power our  visualizations described in the next section.   5. EMOTIONAL DATA VISUALIZATION  This section addresses our research question 3). We defined four  design principles (DPs) to build easily readable and  understandable visualizations: dp1) Unified design, dp2) Self- explanatory, dp3) Easy to use and dp4) Interactive design. By  adopting these principles, we aim to make the dashboard interface  intuitive and thus increase the learning curve.   5.1 Dashboard Overview  The emotional dashboard we propose is illustrated in Figure 9. It  is a tutor oriented web-based application. The tutor connects to  the dashboard by selecting a learner from his/her learners list.  The dashboard is then powered by the data for the selected  learner. The dashboard is designed so as to present information  from general-to-specific at different levels of abstraction: 1)  Global information about the learning session reflects in a  summarized manner the emotional state of the learner during the  learning session, combining both subjective and objective  emotions; 2) Timeline information integrates contextual  information to illustrate the learning session with the emotions  experienced by the learner along the learning activity; 3) Time- segment information is designed to provide more details about  each positive or negative emotion identified in the timelines.   The dashboard consists of a toolbar at the top and a body (see  Figure 9). The toolbar provides quick access to the filtering  functions. The tutor can select in the drop-down menu a learning  session corresponding to the selected learner. The tutor can also  select an arousal threshold in the drop-down menu. The toolbar is  also used to quickly show insights about learners emotions  during the session by displaying the number of positive and  negative emotions in the form of markers. The body of the  dashboard groups all the emotional visualizations related to the  selected session. When the session changes, all the visualizations  in the body are updated accordingly.   The top level of the body is designed to convey emotional insights  about the whole learning session. It is divided into two parts: the  left part presents the emotions expressed by the learner (self- reported) before and after the learning session, while the right part  presents the average of the emotions measured over the session.  The middle part of the interface provides details on a selected  time-segment. It is divided into two parts: the left part presents  contextual information related to the time-segment, in particular  learners facial expressions recorded during the learning session  and the audio record, while the right part displays quantified  information on associated emotions. The bottom part is composed  of three timelines, where the degree of detail increases from top to  bottom. On the top, bubbles represent the positive and negative   moments. In the middle, we placed contextual and emotional  information in a multiline-based chart. At the bottom, a segment- based chart presents the top three emotions for each time-segment.   Color is an important abstraction widely adopted in visual design  and particularly in emotional visualizations (e.g. [9, 10]). Hence,  we decided to define a color scheme for our visualizations. Figure  6 presents three sets of colors: the first for discrete emotions  (neutral, happiness, surprise, sadness, disgust, contempt, fear,  anger), the second for what we call markers: positive/negative  emotions and events (chatting and sharing documents between  tutor and learner) and the third for the audio/video valence.     Figure 6. Color Scheme   5.2 Global Information Level  The left part of the global level concerns the subjective  information expressed by the learner. As explained in section  3.1.1., learners can report their emotions immediately before the  beginning of the live learning session and just after its end. Figure  7 presents the implemented visualization of both dimensional  (valence and arousal) and discrete emotions at both times. This  representation facilitates the comparison between the two times  (before and after). Two different charts are used to visualize self- reported dimensional emotions and discrete emotions. A pie donut  chart is adopted to visualize dimensional emotions, where the  corresponding name and value (on a scale from 0 to 100) are  placed inside the pie chart. Chart color is green or red, depending  on whether the value of the emotion is positive or negative,  respectively. As the learner can choose several discrete emotions,  a light bar-based chart is adopted to represent them. Each bar  represents an emotion, and the width corresponds to the value of  the emotion as expressed by the learner on a scale from 0 to 100.  The emotion name and value are displayed on the top of each bar.     Figure 7. Self-reported emotion visualization     Figure 8. Session detail visualizations   The right part presents the average information of the inferred  emotions (using APIs). Three visualizations are proposed (see  Figure 8), ranging from the most general (on the left) to the most  detailed (on the right). The visualization in Figure 8-(a) is based  on a multi-arc pie donut chart. It presents the ratio of each type of  discrete emotion, corresponding to the abstraction of positive,  negative and neutral emotions as described in section 4.2   (emotional data integration). Each type is represented by a color  (red, green and blue);  the  percentage is  also  displayed  with       Figure 9. An overview of the EMODA dashboard. A positive marker is selected in the first timeline  three emoticons for positive, negative and neutral emotion. An  emoticon representing the dominant emotion is placed inside the  chart. This chart thus quickly conveys the dominant emotion set  as well as the percentage of each type. The second and third  segment detailed visualizations are a radar and bar-based chart  (Figure 8-(b) and Figure 8-(c)), respectively. Both visualizations  present the discrete emotion average data corresponding to the  learning session. Bars are colored according to the color scheme;  the y-axis represents the score of the emotion, which is also  displayed in a tooltip (dp4) when the mouse hovers the bar.    5.3 Timeline Level of the Learning Session  Timeline visualizations offer more explanatory (dp1)  representations of emotions as they incorporate the time notion of  the learning session. Our main timeline visualization is based on  the analysis described in part 4.2. As illustrated in Figure 10, a  specific timeline chart is implemented to combine both discrete  and dimensional emotions. We represent positive/negative  markers for positive and negative time-segments, respectively.  We choose a bubble shape to represent discrete emotion markers  and a star shape to represent dimensional emotions. The size of  the bubbles and stars depends on the computed score for each  emotion which is also represented with the height of the marker.     Figure 10. Positive/Negative markers visualization   The duration of each marker (time-segment) is added as a small  horizontal bar over the x-axis, which represents learning session  time in minutes. As mentioned above, a green color is chosen for  positive markers and a red color for negative ones. When the  session or arousal threshold changes (using drop-down menus in  the toolbar), the visualization is updated accordingly. Integrating  both markers (dimensional and discrete) in the same timeline   facilitates the comparison between the emotions from both audio  and video data sources.   Figure 11 presents a second timeline displaying only the top three  discrete emotions associated with each marker. The x-axis  represents the time in minutes, and each emotion is represented by  a horizontal bar with the associated color. The width of the bar  corresponds to the duration of the time-segment, while the  position of the bar in the y-axis corresponds to the emotion score  on a scale from 1 to 100. This visualization simplifies the  comparison between the top three emotions.      Figure 11. Top 3 discrete emotion visualization   The third timeline visualization (see Figure 12) allows the tutor to  monitor the discrete (top three) and the dimensional (audio/video  valence) emotions over time with the associated contextual  information (events). We collected two main events: chatting and  document sharing (pdf, audio, video or image). Emotions are  represented by lines with different colors. Events are added to the  top of the chart using bubbles as markers, with different icons to  distinguish events by their types. The duration of the event is  shown by a horizontal bar and two small icons (eye, eye-slash)  placed at both ends of the bar (showing the start and end of  document sharing). This setting not only allows better monitoring  of learners emotions but also correlation between emotions and  the events and activities that may have triggered these emotions.  For instance, we can observe on this chart that the learner was  greatly surprised at ~ 14 minutes while sharing an image.    By providing tutors with different levels of abstraction of learners'  emotions and contextualized information on these emotions, we  aim at helping them easily identify specific emotional moments  and reactions in the learning sessions (both positive and negative)  and explore the reasons (context) of such moments.       Figure 12. Contextual event and emotion visualization    5.4 Time Segment Detail Visualization  We provide tutors with details on emotional information  corresponding to each particular time-segment (marker). These  visualizations offer a detailed view of learners emotions at a  given moment selected in the timeline described above. The  dashboard is interactive (dp4): when the mouse hovers a marker in  the timeline. The detailed visualizations are updated accordingly  with the data related to the hovered marker.   To compare time-segment information and global information, the  same visualization presenting the average of the objective  emotional information was used and stacked vertically (see Figure  9). We also help tutors better understand quantified information  on learners emotions by showing either facial expressions and the  sound corresponding to each time-segment (see Figure 9). Images  are updated accordingly when the mouse hovers the markers (see  Figure 10). Likewise, the tutor can listen to the sound of a selected  time-segment by clicking on its associated marker. To know  whether or not the learner is talking, we implemented a bar-based  chart to display audio frequency with green or red colors  (depending on whether the selected marker is positive or  negative). Audio-visual contextual information is placed at the  same level as time-segment details visualizations.   5.5 Pilot Study on Tutors Perception  We conducted an initial study in order to evaluate the EMODA  dashboard and obtain feedback on how tutors perceive its  usability and utility as recommended in [31]. This pilot evaluation  was conducted with two tutors using the SpeakPlus platform. For  this purpose, the evaluation was conducted in three steps. The first  step consisted of a case study with 8 associated questions (e.g. At  what time did the learner feel a peak of fear during the learning  session). The second step consisted of 6 questions based on a 7- Likert Scale regarding global perception on the dashboard (e.g.  How do you find the display and organization of data on the  screen, To what extent do you think the dashboard will help  you as a tutor to improve the learning experience). The third step  consisted of 7 grid questions with 7-Likert scales for each  visualization (e.g. How do you find the learner image carousel  displayed in part (II-a)). We added to each question a text area  requesting more explanations.    The answers to the questionnaire associated with the case study  confirmed that the dashboard is easy to use, as the tutors were  able to answer all questions correctly without any help. Regarding  global perception, it was clear that our dashboard has a problem in  displaying timeline visualizations. Tutors state that reading is a  little bit difficult and Conversely, timeline visualizations were  not rapidly comprehensible... for me it would be better to keep  only one simplified visualization. This suggests that one  simplified visualization of the three timeline visualizations would  be easier to understand, for instance by adding a tooltip with the  mouse hover events to the marker timeline (Figure 10). This apart,  the perception of the dashboard was rather positive. From the last  part of the questionnaire, we note that the idea of combining  emotions and events was very interesting for both tutors. In fact,   they suggested further promoting this visualization: should be  better valued, the idea of combining events and emotions is  interesting, but the chart is not big its a good idea but should  be simplified. Therefore, more focus should be given to the  contextualization part as it provides essential understanding of the  emotions experienced by learners.   6. CONCLUSION AND FUTURE WORK  Our initial investigations prove the feasibility of using  heterogeneous APIs for emotion recognition in online learning  environments. By applying both dimensional and discrete emotion  models, we showed that we can detect different learners emotions  during such settings. This highlights the need for a multimodal  approach to emotion collection and recognition. We believe that  providing tutors with an awareness tool to visualize and track their  learners emotions will help them better maintain a sustainable  socio-affective relationship as well as adjust learning activity  materials accordingly. Furthermore, our study has a key side  benefit for the designers of the SpeakPlus platform. The feedback  on learners emotions can help them for instance monitor the  performance of the learning sessions and learners satisfaction.   In this study we opted for a post-processing (off-line) approach  for emotion tracking, analysis and visualization to build a first  prototype of our dashboard based on real data. Through an  iterative design process, the pilot study will help us to make the  dashboard evolve according to tutors feedbacks and needs. In  fact, the current architecture should be altered to offer a real-time  feedback to tutors in order to optimize the learning session and  make it more enjoyable.   Our study is learner-centric. As such, we also think it is important  to investigate whether there are correlations between the emotions  felt by the tutor and by the learner. Furthermore, our study focuses  on universal discrete emotions and dimensional emotions (arousal  and valence). Several recent studies (e.g. [6]) show that the  learning context is more concerned with domain-specific  emotions (e.g. confused, bored, frustrated, pride). Thus,  investigating such emotions or combining them with universal  ones might be a promising research issue for future work.   7. Acknowledgements  This research is conducted within the EmoViz project funded by  the Rgion Auvergne-Rhne-Alpes. We thank the SpeakPlus  (https://speakplus.fr) company for funding this research work. We  also thank the learners and tutors who participated to the study.   8. REFERENCES  [1] Siemens, G. and Long, P. Penetrating the Fog: Analytics in   Learning and Education. EDUCAUSE Review,  (September/October 2011).    [2] Wu, J. H., Tennyson, R. D. and Hsia, T. L. A study of  student satisfaction in a blended e-learning system  environment. Computers and Education, (August 2010).   [3] Sun, P. C., Tsai, R. J., Finger, G., Chen Y. Y. and Yeh,  Dowming. 2008. What drives a successful e-Learning An  empirical investigation of the critical factors influencing  learner satisfaction. Educational Psychology Review, (May  2008).   [4] Pekrun, R. The Control-Value Theory of Achievement  Emotions: Assumptions, Corollaries, and Implications for  Educational Research and Practice. Computers and  Education, (2006).     [5] Boekaerts, M. The crucial role of motivation and emotions in  classroom learning. In: Dumont, H., Istance, D., Benavides,  F. (Eds.) The nature of learning: Using research to inspire  practice. OECD, (2010).    [6] Pekrun, R., Goetz, T., Frenzel, A. C., Barchfeld, P. and  Perry, R. P. 2011. Measuring emotions in students learning  and performance: The Achievement Emotions Questionnaire  (AEQ). Contemporary Educational Psychology, (January  2011).    [7] D'Mello, S. K., Picard, R. W., and Graesser, A. C. Towards  an Affect-Sensitive AutoTutor. Special issue on Intelligent  Educational Systems  IEEE Intelligent Systems,  (July/August 2007).   [8] Mega, C., Ronconi, L., De, B. R. What makes a good  student How emotions, self-regulated learning, and  motivation contribute to academic achievement. Educational  Psychology, (Feb 2014).   [9] Ruiz, S., Charleer, S., Urretavizcaya, M., Klerkx, J., Isabel  Fernandez, C. I. and Duval, E. Supporting learning by  considering emotions: Tracking and Visualization. Sixth  International Conference on Learning Analytics and  Knowledge. Edinburgh, UK, (April 2016).   [10] GhasemAghaei, R., Arya, A. and Biddle, R. A Dashboard for  Affective E-learning: Data Visualization for Monitoring  Online Learner Emotions. Proceedings of EdMedia: World  Conference on Educational Media and Technology.  Vancouver, Ca., (Jun 28, 2016).   [11] Happy, S. L., Dasgupta, A., Patnaik and P., Routray.  Automated Alertness and Emotion Detection for Empathic  Feedback during e-Learning. IEEE Fifth International  Conference on Technology for Education (T4E). Edinburgh,  United Kingdom, (December 2013).   [12] Mazza, R. and V., Dimitrova. CourseVis: A Graphical  Student Monitoring Tool for Supporting Instructors in Web- Based Distance Courses. Human-Computer Studies, (2007).   [13] May, M., George, S. and Prvt, P. TrAVis to Enhance Self- Monitoring in Online Learning Supported by Computer- Mediated Communication Tools. International Journal of  Computer Information Systems and Industrial Management  Applications, (2011).   [14] Caridakis, G., Castellano, G., Kessous, L., Raouzaiou, A.,  Malatesta, L., Asteriadis, S. and Karpouzis, K. Multimodal  emotion recognition from expressive faces, body gestures  and speech. Artificial Intelligence and Innovations from  Theory to Applications, (2007).   [15] Pantic, M., Sebe, N., Cohn, J. and Huang, T. S. Affective  Multimodal Human-Computer Interaction. Proceedings of  the 13th annual ACM international conference on  Multimedia, (November 2005).   [16] Lavou E., Molinari G., Pri Y., Khezami S., Reflection-in- Action Markers for Reflection-on-Action in Computer- Supported Collaborative Learning Settings. Computers &  Education, (2015).   [17] Bouvier P., Sehaba K., Lavou E. A trace-based approach to  identifying users' engagement and qualifying their engaged- behaviours in interactive systems: Application to a social   game. User Modeling and User-Adaptated Interaction  (UMUAI), (2014).   [18] Afzal, S.and Robinson, P. Modelling Affect in Learning  Environments-Motivation and Methods. Proceedings of the  10th IEEE International Conference on Advanced Learning  Technologies, (July 2010).   [19] Michel, C., Lavou, E., George, S., & Ji, M. Supporting  Awareness and Self-Regulation In Project-Based Learning  through Personalized Dashboards. International Journal of  Technology Enhanced Learning. To be published, (2016.).   [20] Ekman, P. and Friesen, W. V. The Facial Action Coding  System: A Technique for the Measurement of Facial  Movement. Environmental psychology and nonverbal  behavior, (September 1976).   [21] Barrett L. F. and Russell, J. A. 1998. Independence and  Bipolarity in the Structure of Current Affect. Personality and  Social Psychology, (1998).   [22] Scherer, K. R., Fontaine, J. R. J. and Soriano, C. CoreGRID  and MiniGRID: Development and validation of two short  versions of the GRID instrument. In Fontaine, J. R. J.,  Scherer, K. R. and Soriano, C. (Eds.), Components of  Emotional Meaning. A sourcebook2. Oxford, UK: Oxford  University Press, (2013).   [23] Picard R. W. Affective Computing. MIT Media Lab.  Perceptual Computing Section. Technical Report, (1995).   [24]  Cernea, D. and Kerren, A. 2015. A survey of technologies  on the rise for emotion-enhanced interaction. Journal of  Visual Languages and Computing, (December 2015).   [25] Konar, A. and Chakraborty, A. Emotion Recognition: A  Pattern Analysis Approach. Wiley, (2015.).    [26] Lee, C. M. and Narayanan, S. S. Toward detecting emotions  in spoken dialogs. IEEE Transactions on Speech and Audio  Processing, (2005).   [27] Wu, C.H. and Liang, W. B. 2011. Emotion recognition of  affectives peech based on multiple classiers using acoustic  prosodic information and semantic labels. IEEE Transactions  on Affective Computing, (2011).   [28] Eyben, F., Wollmer, M. and Schuller, B. OpenEAR -  Introducing the Munich Open-Source Emotion and Affect  Recognition Toolkit. Affective Computing and Intelligent  Interaction and Workshops. (October 2009).   [29] Mora, S., Pelayo, V. R and Muller, L. Supporting Mood  Awareness in Collaborative Settings. 7th International  Conference on Collaborative Computing: Networking,  Applications and Worksharing, CollaborateCom. Orlando,  FL, USA, (January 2011).   [30] Posnett, D., Devanbu, P. and Filkov, V. MIC Check: A  Correlation Tactic for ESE Data. Proceedings of the 9th  IEEE Working Conference on Mining Software Repositories.  IEEE Press, Piscataway, NJ, USA, (2012).   [31] Lam, H., Bertini, E., Isenberg, P., Plaisant, C. and  Carpendale, S. Empirical Studies in Information  Visualization: Seven Scenarios. Vis. Comput. Graph. IEEE  Trans, (Sep. 2012).       "}
{"index":{"_id":"54"}}
{"datatype":"inproceedings","key":"Xu:2017:PAE:3027385.3027432","author":"Xu, Zhenhua and Woodruff, Earl","title":"Person-centered Approach to Explore Learner's Emotionality in Learning Within a 3D Narrative Game","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"439--443","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027432","doi":"10.1145/3027385.3027432","acmid":"3027432","publisher":"ACM","address":"New York, NY, USA","keywords":"affect, complex learning, educational video games, emotion, facial expression recognition, game-based learning, learner traits, scientific reasoning","Abstract":"Emotions form an integral part of our cognitive function. Past research has demonstrated conclusive associations between emotions and learning achievement [7, 26, 27]. This paper used a person-centered approach to explore students' (N  The results indicated a strong statistical relationship between three specific facial movements (i.e., outer brow raising, lip tightening and lip pressing), student self-regulatory learning strategy and learning performance. Outer brow raising (AU2) had strong predictive power when a student is confronted with obstacles and does not know how to proceed. Both lip tightening and pressing (AU23 and AU24) were predictive when a student engaged in a task that requires a deep level of incoming information processing and short memory activation. The findings also suggested a correlational relationship between student self-regulatory learning strategy use and neutral state. It is hoped that this study will provide empirical evidence for helping us develop a deeper understanding of the relationship between facial behavior and complex learning especially in educational games","pdf":"Person-Centered Approach to Explore Learners  Emotionality in Learning within a 3D Narrative Game   Zhenhua Xu  University of Toronto   252 Bloor St W, Toronto, ON., Canada, M5S 1V6  zhenh.xu@mail.utoronto.ca  Earl Woodruff  University of Toronto   252 Bloor St W, Toronto, ON., Canada, M5S 1V6  earl.woodruff@utoronto.ca        ABSTRACT  Emotions form an integral part of our cognitive function. Past  research has demonstrated conclusive associations between  emotions and learning achievement [7, 26, 27]. This paper used a  person-centered approach to explore students (N = 65) facial  behavior, emotions, learner traits and learning. An automatic facial  expression recognition system was used to detect both middle  school and university students real-time facial movements while  they learned scientific tasks in a 3D narrative video game.    The results indicated a strong statistical relationship between three  specific facial movements (i.e., outer brow raising, lip tightening  and lip pressing), student self-regulatory learning strategy and  learning performance. Outer brow raising (AU2) had strong  predictive power when a student is confronted with obstacles and  does not know how to proceed. Both lip tightening and pressing  (AU23 and AU24) were predictive when a student engaged in a  task that requires a deep level of incoming information processing  and short memory activation. The findings also suggested a  correlational relationship between student self-regulatory learning  strategy use and neutral state. It is hoped that this study will provide  empirical evidence for helping us develop a deeper understanding  of the relationship between facial behavior and complex learning  especially in educational games.    CCS Concepts   General and referenceEmpirical studies  Applied  computingInteractive learning environments   Keywords  Emotion; Affect; Facial expression recognition; Learner traits;  Game-based learning; Complex learning; Scientific reasoning;  Educational video games   1. INTRODUCTION  Emotion plays a critical role in complex learning. In recent years,              studies on the link between emotions and cognition in the field of  educational psychology [4, 25, 26], neuroscience [22] and artificial  intelligence education [e.g., 2, 5, 6, 7, 36] have made significant  contribution in helping us understand emotion and its role in  students learning processes [27, 34]. Consequently, we have  witnessed a variety of methods being used to assess students  affective experiences across different age groups and learning  platforms [2, 6, 3, 7]. In particular, recent advancements in  computer facial recognition technology within which a  combination of sophisticated algorithms and non-intrusive  webcams are used obtain reliable measures of student emotions in  real-time [14, 23, 35]. Nevertheless, there is a paucity of empirical  work exploring the role of emotion in complex learning with  educational video games. To our knowledge, there are two major  areas of research: one focusing on examining the difference in  learners emotional experience when learning from an educational  game in comparison to learning with intelligent tutoring systems  [e.g., 32], the other using self-report approach or computational  models to predict special emotions such as confusion and its  relationship to learning and performance.   Different from the aforementioned studies, this present study used  a person-centered approach to examine the direct relationship  between emotion, appraisal processes and learning outcomes by  using multi-channel data, which includes students real-time facial  data, questionnaires and computer log data. Person-centered  approach refers to placing the individual at the focal point of the  analysis because it enables us to investigate how psychological  constructs relate to emotions and learning outcomes. Specifically,  we asked the following research questions:  (1) What were the commonly occurring facial movements and  emotions during learning in an educational video game   (2) Could the frequency of the facial movements predict students  learning gains and to what extent did the facial behavior predict  learning  (3)  Is there a relationship between learner traits and emotions  and/or facial movements    1.1 A Description of 3D Game-Based  Learning Environment: CRYSTAL ISLAND   CRYSTAL ISLAND is a 3D game-based learning environment  designed to develop students problem solving, scientific  reasoning, and literacy skills (See Figure 1). In the game, a  participant is informed to play the role of a medical detective to  identify the epidemic that has spread amongst a group of scientists  on a remote island. The participant explores the virtual environment  from a first-person perspective: s/he is asked to engage in a series  of activities that involve generating hypotheses based on the clues  s/he gathered from virtual characters (i.e., patients, a lab technician,  a nurse and a chef), reading books and articles, viewing posters and   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions from Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada.  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4870-6/17/03$15.00  DOI: http://dx.doi.org/10.1145/3027385.3027432        running lab tests to identify the spreading diseases transmission  sources. At the end of gameplay, the participant must submit a  correct diagnosis. In CRYSTAL ISLAND, the reading material has  concept matrices used to assess participants understanding of  scientific concepts. The matrices are designed in a multiple-choice  format. When a participant finishes reading every book or research  paper, s/he has to compete the questions in the concept matrix.   2. THEORETICAL FRAMEWORK  In this study, we applied Ekman and Friesens theory of universal  facial expressions of emotion [11] and Scherers component  process model (CPM) [31] to guide our understanding of the  relationships between emotion, appraisal factors and learning.  Ekman and Friesen hypothesized that universals are to be found in  the relationship between distinctive movements of the facial  muscles and particular emotions [11]. They indicated that the  difference in facial behaviors is the function of the evoking events  rather than the reflection of the difference in facial muscles. Ekman  and Friesens study of facial expressions of emotions has lead them  to develop the Facial Affect Scoring Technique, which enabled  them to study deception, emotion recognition and in-depth facial  expression of emotions. In recent years, research has suggested that  facial expression is a useful approach to examine affect because  facial expression is ubiquitous, and it is convenient as facial  representation can be used to model learning-centered affective  states [18].   Component Process Model is the most comprehensive model of  emotion [1]. Scherer indicated that emotion is an emergent,  dynamic process based on an individuals subjective appraisal of  significant events [31]. In a nutshell, CPM demonstrates three  central notions about the emotion processes [32, 15] which are: 1)  People appraise events through sequentially processed evaluation  such as novelty, intrinsic pleasantness, goal conduciveness, et.al.  [15, 16]; 2) The effects of the appraisal evaluation, that is, how the  result of the appraisal evaluation changes the state of emotion and  ensures that it is adaptive to the event and;  3) The pattern of an  emotional response is the cumulative result of all appraisal induced  changes in the emotion components [16].    3. METHODOLOGY  3.1 Participants  Sixty-five students participated in this study. Forty-four  undergraduate students (77% females) were recruited from a  leading Canadian university (mean age of 22 years) and twenty-one  middle school and high school students (76% males, mean age of  13 years) were recruited from a sport club and Canadian public  schools. The majority of the undergraduate students (44%) were in  their fourth year of study representing a variety of disciplines. In  the teenage group, 61% of the students in grade seven and eight.   3.2 Measures/Materials   3.2.1 Survey Questionnaires  Two questionnaires were used to gather participants information:  1) Demographic information (e.g., age, gender, ethnicity, year of  school enrollment and prior knowledge on life science) was  obtained from all participants. 2) The 30 items from the Motivation  Strategies for Learning Questionnaires (MSLQ) [31, 9] were  modified and adapted to measure students motivation and self- regulated learning strategies using a 7-point Likert scale. In  addition, a content pre-test (19 items) was also administrated to all  participants. It was used to measure students understanding of  microbiology concepts.    3.2.2 Facial Data and CRYSTAL ISLAND Logs  Multi-channel process data were collected when participants  played CRYSTAL ISLAND: 1) software log files and; 2) real-time  participants facial data using the webcam video corpus (See Figure  2). The log-file data captured students interactions with the game  environment, including timestamp, action type, location, object,  and characters involved in the interaction. In this study, we only  focused on using reading task related variables for the data analysis.  These variables were treated as learning outcome variables.  Namely, they were: the number of books completed, the number of  questions answered incorrectly, the number of answers being  corrected and the number of attempts made in order to obtain the  correct answers before submitting to the system.     Students facial data was generated through an automatic facial  expression monitoring software (iMotions n.d.), which relies on  a video data stream of learners facial expressions to track their  affective states and fluctuations. This software draws on Ekman  and Friesens (1978) Facial Action Coding System (FACS) to  detect and track participants affective states through action units  (AUs) that are corresponding to individual facial motor muscles  [1]. The facial expression monitoring software analyzes the video  recording at 30 frames per second. It classifies seven basic  emotions (i.e., joy, anger, sadness, surprise, fear, contempt, disgust)  three learner-centered emotions (i.e., frustration, confusion and  neutral state) and 19 facial action units (AUs).    The video corpus used in the present study consisted of fifty-eight  video recordings with an average of 52,300 FACS-annotated video  frames per video recording. Both basic and learning-centered  emotions and facial action units were used for data analysis in this  paper.   3.3 Data Analysis   3.3.1 Analysis of Leaner Variables   Prior to data analysis, reliability tests were conducted to examine  the internal consistency of the three instruments (i.e., MSLQ, a  content pre-test and a scale measuring students prior knowledge).  The results suggested that the reliability of all the measures were  satisfied: Cronbachs alpha value for MSLQ motivation scale was  .80; the Cronbachs alpha value for MSLQ learning strategy scale  was 0.75. The Cronbach value for the content pre-test was .88. For  the prior knowledge measure, the Cronbachs alpha value was .81.   Following the reliability tests, six composite variables were created  based on the adapted 30 MSLQ items that pertained to self-efficacy,  goal orientation, task values and self-regulatory strategies on a 7- point Likert scale. One composite variable was also created based  on the scale that measures students prior knowledge.      3.3.2 Task Variables  Task variables selected for the data analysis were: 1) students  content pre-test scores and, 2) reading related variables extracted  from the CYRSTAL ISLAND video game log files including the  number of books completed by each student, the number of  questions in concept matrix answered incorrectly, the number of  attempts made to obtain the right answers and the frequency of  answers being corrected before submitting final answers to the  system.      3.3.3 Analysis of Facial Data  Students facial data selected for this study were the seven basic  emotions (i.e., joy, anger, sadness, surprise, fear, contempt,  disgust), the three learner-centered emotions (i.e., frustration,  confusion and neutral state) and the 19 facial action units (AUs).     For the purpose of this study, the frequency values for each emotion  and AU were computed and used for the data analysis. That is, the  percentage of each emotion and AU was registered as existing for  each participant and was used to answer the first two research  questions.   4. RESULTS  To answer the first research question, we ran descriptive statistics  to identify the occurring emotions and facial movement action units  in the gameplay. Following that, we applied R software to extract  the number of raw evidence output values that were beyond  threshold values, and then, we computed the values by dividing  them by the total number of output values for each participant. The  frequently occurring emotions and facial action units were  identified and they were: four basic emotions (joy, surprise, sad,  anger), three learning-centered emotions (confusion, frustration  and neutral) and eight facial action units such as outer brow raising  (AU2), brow lowering (AU4), upper lid raising (AU5), upper lip  raising (AU10), lip corner depressing (AU15), lip tightening  (AU23), lip pressing (AU24) and eyes closing (AU43).    Table 1: Selected sample images of commonly occurring facial  action units1   Facial Muscle  Action Units Description Example Image   AU2 Outer Brow Raiser     AU4 Brow Lowered     AU5 Upper Lid Raiser     AU23 Lip Tightener     AU24 Lip Pressor        Before answering the second research question, multiple  correlation analyses were conducted to explore the relationship  between emotions, facial movements and task variables. The  findings suggested that three basic emotions (e.g., surprise, fear and  anger) were associated with the number of books completed (r = - .30, p < .05; r = -.32, p < .05; r = -.30, p < .05). Two learning- centered emotions such as frustration and neutral were correlated  with the number of attempts students made to answer the questions  (r = .32, p < .05) and the content pre-test (r = .30, p < .05). Sad was  negatively associated with the content pre-test and student prior  knowledge level (r = -. 36, p < .001; r = -.30, p < .001, respectively).  In terms of the facial action units, upper lid raising (AU5) was  significantly associated with the number of questions in concept  matrix that were answered incorrectly (r = .57, p <.001) and the  number of attempts made to answer the questions (r = .71, p < .001).  Both lip tightening (AU23) and pressing (AU24) were also  significantly correlated with the number of attempts made to  answer the questions and the number of times corrected the wrong  answers (r = .50, p < .001; r = .46, p < .001; r = .40, p < .001; r =  36, p < .001, respectively). Lip puckering (AU18) was negatively  associated with students prior knowledge level (r = -.33, p < .05).  Eyes closing (AU43) was negatively correlated with the number of                                                                       1 The facial action unit (AU) descriptions and sample images were   adapted from The Facial Action Coding System (FACS) [13].   books completed, the number of incorrect answers and the number  of attempts students made to answer the questions.    Linear regression models were constructed to examine the  predictive effects of frequently occurring facial action units and  emotions on learning performance. The findings indicated that lip  tightening (AU23) and lip pressing (AU24) were strong predictors  of the number of books completed by each student (R2 = .13, F(2,  56) = 3.68, p <.05). Outer brow raising (AU2) was a strong  predictor of the number of questions in concept matrix that were  answered incorrectly (R2 = .11, F(1, 57) = 6.00, p <.05) and the  number of answers corrected before submitting the correct answers  to the system (R2 = .16, F(1,57) = 9.18, p < .05) and the number of  attempts students made to answer all the questions that were listed  in the concept matrix (R2 = .09, F(1, 57) = 4.82, p < .05). Both fear  and frustration were best predictors of the number of books  completed (R2 = .17, F(2, 56) = 5.32, p < .05).     To answer the third research question, correlation analyses were  conducted and the results indicated that disgust was statistically  significantly associated with self-efficacy (r = .34, p < .05). Anger  was significantly associated with intrinsic goal orientation.  Contempt, disgust and sad were correlated with extrinsic goal  orientation (r = .26, p < .05; r = .30, p < .05; r = .28, p <.05,  respectively). Sad was negatively correlated with students prior  knowledge level (r = -.30, p < .05).  Neutral emotional state was  correlated with self-regulatory learning strategies (r = .34, p < .05).  contempt was negatively correlated with both task value and  students prior knowledge level (r = -.34, p < .05; r = .40, p < .001,  respectively).   5. CONCLUSION  This study used an automated real-time facial data to address  empirical yet complementary questions about the relationship  between learner traits, facial behavior, emotions and learning  performance with a complex educational game.    The results highlighted the strong statistical relationship between  specific facial movements and learning outcomes. For instance,  outer brow raising (AU2) was a strong predictor of the number of  questions answered incorrectly in the concept matrix, the number  of attempts made to answer the questions and the number of times  the wrong answers being corrected before submitting the correct  answers to the system. In past research, AU2 was identified as  being associated with frustration, surprise and anxiety [6, 18].  According to Ekman, Friesen and Hager [13], the combination of  AU2 and AU4 was called fear brow. When an individual is  confronted with obstacles (e.g., consistently asked to correct the  wrong answers in the concept matrix) and does not know how to  proceed, s/he is more likely to experience negative emotions (e.g.,  frustration). We also found that lip tightening (AU23) and lip  pressing (AU24) were strong predictors of the number of books  completed by each student. However, lip tightening and lip  pressing can be interpreted as a state of deep concentration. As  students were working on the reading materials in CYSTAL  ISLAND and knowing that they have to answer the concept matrix  questions right after the completion of each reading, they were  more likely to engage in a deep level of incoming information  processing and activate short-term memory.   Furthermore, this study also investigated the relationship between  learner trait variables, emotion and facial expressions. One of the     interesting findings was the correlational relationship between  neutral state and students self-regulatory learning strategy use.  Past research suggested neutral state can be treated as a state of flow  or cognitive equilibrium. However, there remains much to discover  about the role of neural or confusion facial expressions in learning- centered emotions.   This study is the first of its kind to provide empirical evidence for  relations between facial movements, emotions, learner traits and  learning performance within a narrative video game environment,  using a person-centered approach. As researchers have suggested,  the automated facial action unit tracking allows for close  examination of persistent cognitive-affective states [18], it is hoped  that the present study will help us develop a deeper understanding  of the role of facial behavior in learning, and therefore, to help us  create more emotionally-adaptive learning environments.       Figure 1. Screenshot of CRYSTAL ISLAND interface:   Infirmary interface.            Figure 2. Real-time emotion classification of facial expression  while learning with CRYSTAL ISLAND using the automatic   facial expression monitoring software (iMotions n.d.)     6. ACKNOWLEDGMENTS  Our thanks to the North Carolina State University Center for  Educational Informatics (Dr. James Lester and his research team)  for allowing us to use the CRYSTAL ISLAND game-based  learning platform and for their technical support in the process of  data collection. We would also like to thank our research assistants  (Ruilin Li, Esther Zheng and Anna Chu) for helping us with the  initial data preparation.     7. REFERENCES  [1] Azevedo, R., Taub, M., Mudrick, N., Farnsworth, J. and   Martin, S. A. 2016. Interdisciplinary Research Methods Used  to Investigate Emotions with Advanced Learning  Technologies. In M. Zembylas and P.A. Schutz (Eds.),   Methodological Advances in Research on Emotion and  Education. doi:10.1007/978-3-319-29049-2_18   [2] Calvo, R. A. and DMello, S. 2010. Affect detection: An  interdisciplinary review of models, methods, and their  applications. IEEE Transactions on Affective Computing,  1(1), 18-37.   [3] Conati, C. and Maclaren, H. 2009. Empirically Building and  Evaluating a Probabilistic Model of User Affect. User  Modeling and User-Adapted Interaction, 19, 267-303.   [4] Csikszentmihalyi, M. 1990. Flow: The psychology of optimal  experience. New York, NY: HarperCollins.   [5] D' Mello, S. K. 2013. A Selective Meta-analysis on the  Relative Incidence of Discrete Affective States during  Learning with Technology, Journal of Educational  Psychology, 105(4), 1082-1099.   [6] DMello, S. and Graesser, A.C. 2010. Multimodal semi- automated affect detection from conversational cues, gross  body language, and facial features. User Modeling and User  Adapted Interaction, 20 (2), 147-187.   [7] DMello, S. and Graesser, A.C. 2011. The Half-Life of  Cognitive-Affective States during Complex Learning.  Cognition and Emotion, 25(7), 1299-1308.   [8] DMello, S. K., and Kory, J. 2015. A review and meta- analysis of multimodal affect detection systems, ACM  Computing Surveys, 47(3).   [9] Duncan, T. G. and McKeachie, W. J. 2005. The making of  the Motivated Strategies for Learning Questionnaire.  Educational Psychologist, 40(2), 117-128.   [10] Ekman, P. and Friesen, W.V. 1978. Facial Action Coding  System. Palo Alto: Consulting Psychologist Press.   [11] Ekman, P. 1970. Universal facial expressions of emotion.  California Mental Health Research Digest, 8(4).    [12] Ekman, P. 1999. Basic emotions. In T. Dalgleish and M.  Power (Eds.). Handbook of Cognition and Emotion. Sussex:  John Wiley & Sons.   [13] Ekman, P., Friesen, W. V. and Hager, J.C. 2002. Facial  Action Coding System: Investigators Guide. A Human Face.   [14] Geller, T. 2014. How do you feel Your computer knows.  Communications of the ACM, 57(1), 24-26.   [15] Gentsch, K., Grandjean, D. and Scherer, K. R. 2014.  Coherence explored between emotion components: Evidence  from event-related potentials and facial electromyography.  Biological Psychology, 98, 70-81.   [16] Gentsch, K., Grandjean, D. and Scherer, K. R. 2015.  Appraisals generate specific configurations of facial muscle  movements in a gambling task: Evidence for the component  process model of emotion. PLoS ONE, 10(8).  doi:10.1371/journal.pone.0135837   [17] Grafsgaard, J. F., Fulton, R. M., Boyer, K. E., Wiebe, E. N.  and Lester, J. C. 2012. Multimodal Analysis of the Implicit  Affective Channel in Computer-Mediated Textual  Communication. Proceedings of the 14th ACM international  conference on Multimodal interaction, 145-152.  doi>10.1145/2388676.2388708   [18] Grafsgaard, J. F., W. J. B., Boyer, K., Wiebe, E. N. and  Lester, J. C. 2013. Automatically recognizing facial  indicators of frustration: A learning-centric analysis. 2013     Humaine Association Conference on Affective Computing  and Intelligent Interaction, 159-165.  doi>10.1109/ACII.2013.33   [19] Grafsgaard, J., Boyer, E. K. and Lester, J. C. 2011.  Predicting Facial Indicators of Confusion with Hidden  Markov Models. Proceedings of the Fourth International  Conference on Affective Computing and Intelligent  Interaction, (Memphis, the United States, October 09  12,  2011). 97-106.   [20] Grandjean, D. and Scherer, K. R. 2008. Unpacking the  cognitive architecture of emotion processes. Emotion, 8, 341- 351.   [21] Harley, J. M. 2014. Measuring Emotions with an Agent- based Learning Environment (Doctoral dissertation).  Retrieved from McGill University Library.   [22] Immordino-Yang, M. H. and Damasio, A. 2007. We feel,  therefore we learn: The relevance of affective and social  neuroscience to education. Learning Landscapes, 5(1), 115- 131.   [23] Kodra, E., Senechal, T., McDuff, D. and Kaliouby, R. 2013.  From Dials to Facial Coding: Automated Detection of  Spontaneous Facial Expressions for Media Research.  Proceedings 2013 IEEE International Conference.    [24] Lester, J. C., McQuiggan, S. W., & Sabourin, J. L. (2011).  Affect recognition and expression in narrative-centered  learning environments. In R. A. Calvo & S. DMello (Eds.),  New perspectives on affect and learning technologies. New  York, NY: Springer.   [25] Mandler, G. 1975. Mind and Emotion. New York, NY:  Wiley.    [26] Pekrun, R., Elliot, A. J. and Maier, M. A. 2006. Achievement  goals and discrete achievement emotions: A theoretical  model and prospective test. Journal of Educational  Psychology, 98(3), 583-597.   [27] Pekrun, R. and Linnenbrink-Garcia, L. 2012. Academic  emotions and student engagement. In S. L. Christenson, A.  L. Reschly & C. Wylie (Eds.), Handbook of emotions in  education. New York, NY: Routledge.   [28] Pintrich, P. R. 2000. The role of goal orientation in self- regulated learning. In M. Boekaerts, P. R. Pintrich & M,  Zeidner (Eds.), Handbook of self regulation. New York:  Academic Press.   [29] Rodrigo, M. M. T. and Baker, R. S. J. D. 2011. Comparing  learners affect while using an intelligent tutor and an  educational game. Research and Practice in Technology  Enhanced Learning. 6 (1), 4366.   [30] Sabourin, J. L. and Lester, J. C. 2014. Affect and  engagement in game-based learning environments. IEEE  Transactions on Affective Computing, 5(1), 45-56.    [31] Scherer, K. R. 2009. The dynamic architecture of emotion:  Evidence for the component process model. Cognition and  Emotion, 23(7). 1307-1351.   [32] Scherer, K. R. 2000. Emotions as episodes of subsystem  synchronization driven by nonlinear appraisal processes. In  M. D. Lewis, & I. Granic (Eds.), Emotion, Development, and  Self-organization: Dynamic Systems Approaches to  Emotional Development. New York: Cambridge University  Press.   [33] Scherer, K. R. 2009. The dynamic architecture of emotion:  Evidence for the component process model. Cognition and  Emotion, 23 (7), 1307-1351.   [34] Sabourin, J. L. and Lester, J. C. 2014. Affect and  engagement in game-based learning environments. IEEE  Transactions on Affective Computing, 5(1), 45-56.   [35] Terzis, V., Moridis, C. N. and Economides, A. A. 2013.  Measuring instant emotions based on facial expressions  during computer-based assessment. Journal of Personal and  Ubiquitous Computing, 17(1), 43-52. doi>10.1007/s00779- 011-0477-y   [36] Whitehill, J., Bartlett, M. and Movellan, J. 2008. Automatic  facial expression recognition for intelligent tutoring systems.  2008 IEEE Computer Society Conference on Computer  Vision and Pattern Recognition Workshop, 1-6. doi:  10.1109/CVPRW.2008.4563182           "}
{"index":{"_id":"55"}}
{"datatype":"inproceedings","key":"Azevedo:2017:UDV:3027385.3027440","author":"Azevedo, Roger and Millar, Garrett C. and Taub, Michelle and Mudrick, Nicholas V. and Bradbury, Amanda E. and Price, Megan J.","title":"Using Data Visualizations to Foster Emotion Regulation During Self-regulated Learning with Advanced Learning Technologies: A Conceptual Framework","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"444--448","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027440","doi":"10.1145/3027385.3027440","acmid":"3027440","publisher":"ACM","address":"New York, NY, USA","keywords":"advanced learning technologies, emotion regulation, emotions","Abstract":"Emotions play a critical role during learning and problem solving with advanced learning technologies (ALTs). Despite their importance, relatively few attempts have been made to understand learners' emotional monitoring and regulation by using data visualizations of their own (and others') cognitive, affective, metacognitive, and motivational (CAMM) self-regulated learning (SRL) processes to potentially foster their emotion regulation (ER). We present a theoretically based and empirically driven conceptual framework that addresses ER by proposing the use of visualizations of one's own and others' CAMM SRL multichannel data to facilitate learners' monitoring and regulation of emotions during learning with ALTs. We use an example with eye-tracking data to illustrate the mapping between theoretical assumptions, ER strategies, and the types of data visualizations that can enhance learners' ER, including key processes such as emotion flexibility, emotion adaptivity, and emotion efficacy. We conclude with future directions leading to a systematic interdisciplinary research agenda that addresses outstanding ER-related issues by integrating models, theories, methods, and analytical techniques for the cognitive, learning, and affective sciences; human- computer interaction (HCI); data visualization; big data; data mining; and SRL.","pdf":"Using Data Visualizations to Foster Emotion Regulation  during Self-Regulated Learning with Advanced Learning   Technologies: A Conceptual Framework     Roger Azevedo, Garrett C. Millar, Michelle Taub, Nicholas V. Mudrick,   Amanda E. Bradbury, & Megan J. Price   North Carolina State University  Department of Psychology   2310 Stinson Drive, 640 Poe Hall  Raleigh, NC 27695   {razeved, gcmillar; mtaub; nvmudric; aebradbu; mjprice3} @ncsu.edu   ABSTRACT  Emotions play a critical role during learning and problem solving  with advanced learning technologies (ALTs). Despite their im- portance, relatively few attempts have been made to understand  learners emotional monitoring and regulation by using data visu- alizations of their own (and others) cognitive, affective, meta- cognitive, and motivational (CAMM) self-regulated learning  (SRL) processes to potentially foster their emotion regulation  (ER). We present a theoretically based and empirically driven  conceptual framework that addresses ER by proposing the use of  visualizations of ones own and others CAMM SRL multichannel  data to facilitate learners monitoring and regulation of emotions  during learning with ALTs. We use an example with eye-tracking  data to illustrate the mapping between theoretical assumptions,  ER strategies, and the types of data visualizations that can en- hance learners ER, including key processes such as emotion flex- ibility, emotion adaptivity, and emotion efficacy. We conclude  with future directions leading to a systematic interdisciplinary  research agenda that addresses outstanding ER-related issues by  integrating models, theories, methods, and analytical techniques  for the cognitive, learning, and affective sciences; human computer interaction (HCI); data visualization; big data; data min- ing; and SRL.   CCS Concepts  Human-centered computing    Human-centered computing- Visualization     Human computer interaction (HCI)     Visuali- zation application domains     Visual analytics   Keywords  Emotions; Emotion regulation; Advanced learning technologies;   Data visualizations; Self-regulated learning; Process data    1. INTRODUCTION  Understanding and reasoning about cognitive, affective, metacog- nitive, and motivational (CAMM) self-regulated learning (SRL)  processes to foster emotion regulation (ER) with advanced learn- ing technologies (ALTs) by using data visualizations is key to  advancing conceptual, theoretical, methodological, analytical, and  educational issues currently plaguing the learning, cognitive, edu- cational, and computational sciences [1][2]. For example, data  visualizations (e.g., static histogram displaying the frequency of  learning strategies used over time, videos of eye-gaze behavior  during learning with a hypermedia system, etc.) allow researchers,  teachers, and learners to visualize, illustrate, conceptualize, mod- el, and understand complex mechanisms and processes, such as  metacognitive monitoring and control, accuracy of metacognitive  judgments, learning trajectories, the cyclical nature of SRL, emo- tion flexibility, emotion adaptivity, and emotion efficacy.    Data visualizations of CAMM SRL processes are important be- cause they can be used by researchers, learners, teachers, trainers,  designers, administrators, and policy-makers for various purposes.  These purposes include the following: (1) articulating complex  conceptual issues associated with ER (e.g., emotion flexibility);  (2) illustrating the dynamics of monitoring and control processes;  (3) reasoning about the importance of key cognitive and metacog- nitive processes and inferring subsequent behaviors; (4) generat- ing hypotheses about underlying SRL mechanisms and their im- pact on learning; (5) developing teaching and training tools to  enhance learners emotions, self-regulation, and ER and teachers  ability to monitor and regulate learners emotions and SRL; and  (6) developing sophisticated learner and teacher dashboards to  trigger and support instructional decision-making that include  interface elements (e.g., open learner models [OLMs] representing  metacognitive accuracy) to provide learner data that might foster  changes in SRL behaviors.    Despite these and other potential uses of data visualizations, many  conceptual, theoretical, methodological, and analytical questions  remain unanswered. For example, while the majority of research  has focused on data visualizations designed for learners to keep  track of their task- or domain-specific knowledge and skills by  using OLMs [3] and teacher dashboards focus on student assess- ments [4], relatively little attention has been paid to learners   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be hon- ored. Abstracting with credit is permitted. To copy otherwise, or repub- lish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions  from Permissions@acm.org.    LAK '17, March 13-17, 2017, Vancouver, BC, Canada    2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027440     emotions experienced during learning with ALTs (e.g., intelligent  tutoring systems, multimedia, hypermedia, simulations, serious  games, tangible computing, augmented reality). Emotions are  critical for learning, problem solving, and performing tasks across  domains, and if left unchecked can have a deleterious impact on  learning (e.g., frustration transitioning to boredom and disen- gagement [5]). As such, our paper focuses on the neglected area  of ER with the goal of presenting, discussing, and debating the  significance of visualizing CAMM SRL process data such as met- acognitive judgments, deployment of cognitive strategies, fluctua- tions in affective states, changes in self-efficacy, the nature of  temporality, the granularity of time scales, levels of abstraction,  static versus dynamic representations, and converging evidence  from multimodal multichannel data. This is accomplished by us- ing various methodological tools including eye-tracking data,  concurrent and retrospective think-alouds, classroom discourse,  facial expressions of emotions, log files, recordings from physio- logical sensors, screen recordings, verbal and nonverbal human artificial agent interactions, and classroom videos to foster learn- ers ER with ALTs (see [6]). In sum, by addressing ER with these  visualizations we argue that they can advance the fields of emo- tion and ER, SRL, humancomputer interaction, data visualiza- tions, big data, data mining, personalized instruction, and ALTs.     2. SRL, EMOTIONS, AND ALTS  Fundamentally, SRL involves the temporal deployment of  CAMM processes during complex learning with ALTs, such as  MetaTutor, Bettys Brain, CRYSTAL ISLAND, nSTUDY, and  AutoTutor [7][12]. SRL plays a central role in learning, reason- ing, and problem solving in complex domains with  ALTs [13][14]. CAMM processes can be measured using trace  methods including log files, eye tracking, physiological sensors,  and facial expression data [1],[2][15][16]. Despite the emerging  evidence on the effectiveness of multichannel trace data to meas- ure emotions [6], no systematic program of research focuses on  ER using data visualizations of CAMM processes.    ER is vital to successful learning about complex topics across  domains with ALTs. Contrary to the wealth of empirical research  about the fundamental role of ER found in the affective, clinical,  social, and neurosciences, there is a paucity of research on ER in  the educational, learning, and cognitive sciences (see [17][19]).  Emotions play a critical role in learning, and without a systematic,  empirically driven program of research aimed at understanding  the monitoring and regulatory processes underlying ER, there is a  lack of fundamental understanding of the affective SRL processes  that impact learning, problem solving, and conceptual understand- ing, especially in STEM domains with ALTs   We argue that advances in ER are needed and researchers should  focus on two specific areas. First, they should collect rich traces  of temporally unfolding emotions along with other SRL processes  (e.g., metacognitive judgments, use of effective cognitive strate- gies) during learning with ALTs to understand the nature of these  processes and how their characteristics (e.g., antecedents, dura- tion, sequence, parallel/serial deployment) are related to changing  internal and external task demands, and are predictive of perfor- mance (e.g., on embedded science assessments), ER (e.g., emo- tion flexibility, emotion efficacy, and emotion adaptivity), and  learning and transfer (e.g., immediate and delayed posttests). Se- cond, researchers should focus on scaffolding ER strategies in real  time to provide students with optimal scaffolding that supports   emotions, ER, and other SRL processes (e.g., feedback from ped- agogical agent [PA]  confusion  prolonged fixation on PA   frustration  agent deploys situational modification strategy by  suggesting learner select a new learning goal  ). In terms of  our emerging conceptual framework, we begin with the presenta- tion of Table 1, which lists several multimodal multichannel affect  data sources, including both online trace and self-report data,  typically collected by researchers. We argue that collection of  these data sources is critical in ascribing data visualizations of  self- and other CAMM SRL processes to foster learners ER.  Table 1: Sample multimodal multichannel affect data used to   examine emotion regulation      Data Sources Affective Indicator/Component   Screen re- cordings  (video and  audio)    Context (mouse movements, facial ex- pressions, interface elements, learner agent dialogue, etc.)     Emotion-eliciting event (PA scaffolding,  complexity of multimedia content, inter- face changes)   Concurrent  think-alouds    Self-reports of emotions verbalized either  during or following learning, problem  solving, and performance    Eye tracking  Repeated number of fixations on areas of  interest (AOIs)    Sequential patterns of fixations   Revisits to AOIs   Saccades, smooth pursuit eye movements   (e.g., overall gaze behavior)  Log files  Context     Task performance    Time spent on various aspects of the sys-  tem (e.g., time spent on text, diagram,  planning)    Sequence of events during interaction  with ALT (e.g., planning  setting goals   reading  using strategies  regulat- ing emotions complying with PA )   Facial ex- pressions of  emotions    Automated evidence scores of learner- centered emotions (e.g., frustration, con- fusion, boredom) and basic emotions  (e.g., joy, fear, anger, sadness, surprise)     Automated evidence scores of action  units (AUs; e.g., AUs 1, 4, 14, 28, 30)     Manually coded scores of basic and  learner-centered emotions, and AUs    Micro-expressions    Physiological  sensors     Skin conductance responses    Electrodermal activity over time (tonic   vs. phasic)   Self-report  question- naires (e.g.,  AEQ, EV,  ERQ,  PAUSe)    Perceptions of current emotions (EV)    Perceptions of ability to cognitively reap-  praise and suppress expressions (ERQ)    Perceptions of the utility of affect and   emotions (PAUSe)    Perceptions of academic achievement   emotions (AEQ)      Despite the potential benefits of using data visualizations of  CAMM SRL to foster learners ER, humans find themselves un- der the growing pressure of communicating or simply making  sense out of increasingly more abstract data. Moreover, humans  struggle with abstracting meaning from large amounts of complex  data, especially when the physiological sensors and instruments  used to collect the process data grow more advanced. Visualiza- tion techniques that are validated as effective for one particular  type of problem or context or learner might not be well suited for  another. As such, it is difficult to design a data visualization  framework that can both keep up with the technology used to  collect online trace data as well as apply design components  across multiple domains and different types of learners. For ex- ample, presenting physiological data (e.g., high levels of arousal  from electrodermal activity, overall gaze behavior; see Table 1) in  real time poses serious challenges for learners interpretation,  comprehension, and inference generation (critical for emotion  monitoring and regulation) by potentially imposing extraneous  cognitive load and inducing negative emotions (e.g., frustration  and anger). Cognitive load theory asserts people have limited  cognitive capacity, which can lead to decreased interpretation,  comprehension, and inferences made during visualization- centered learning and problem-solving tasks. In addition, repre- sentations depicting processes (e.g., facial expressions of confu- sion, high levels of arousal from electrodermal activity) in real  time call for accurate and rapid detection, discrimination, and  comprehension of relevant information. As such, it is necessary to  design an interface that minimizes extraneous cognitive load ex- perienced by learners when attempting to comprehend and reason  about CAMM SRL visualizations presented in real time during  the process of ER (e.g., attending to the CAMM SRL data visuali- zations, making inferences about the antecedents that are por- trayed in the data, selecting the relevant ER strategy necessary to  address the experienced emotions, using the ER strategy, as- sessing the adequacy of strategy use, etc.). In the next section, we  present an initial theoretical framework on ER with ALTs.   3. FRAMEWORK: SUPPORTING EMO- TION REGULATION AND SRL THROUGH  CAMM SRL DATA VISUALIZATIONS   Our conceptual framework incorporates Gross model of emotion  regulation [17] that posits an emotion can be regulated at five  points in the emotion generation process: (1) selection of the sit- uation, (2) modification of the situation, (3) deployment of atten- tion, (4) cognitive change, and (5) modulation of responses. Alt- hough regulation strategies can beand often areused in com- bination, the heuristic value of this framework arises from its  ability to simplify a complex problem space and direct attention to  each of the separate families of ER [17]. The model makes several  assumptions: (1) emotions involve loosely coupled changes in  subjective experience, behavior, and peripheral physiology; (2)  emotions unfold over time; (3) emotions can be either helpful or  harmful, depending on the context; and (4) emotions differ based  on duration, intensity, and quality. Next, we describe the four ER  strategies and corresponding scaffolding strategies that can be  enacted by ALTs to support ER. Situation selection is not includ- ed in our taxonomy since we assume that using an ALT to learn,  solve problems, and so forth precludes one from being able to  select a (new) situation. Our framework addresses the four main  ER strategies delineated in Grosss model, and integrates elements   of Scherers component process model [19], DMello and  Graessers affect dynamics model, and extensive research on  emotions in ALTs including theoretical assumptions of SRL and  related empirical evidence on CAMM SRL and ALTs [6][7][13].   First, situation modification refers to taking actions that directly  alter a situation to change its emotional impact. Examples include  not complying with an PAs feedback, selecting a new learning  goal, and electing to take an embedded assessment to control cog- nition following an accurate metacognitive judgment, decrease an  emerging negative emotion (e.g., frustration), and increase persis- tence and interest in the overall learning activity (motivational  processes not covered in our taxonomy). Second, attentional de- ployment refers to directing ones attention with the goal of influ- encing ones emotional response. A common strategy involves  redirecting attention within a given situation (e.g., from an emo- tion-eliciting interaction with complex representations of infor- mation) or shifting attention away from the present situation alto- gether (e.g., averting gaze behavior from a PA to search for new  informational sources). This strategy involves change in ones  gaze and/or shifts in ones internal focus (i.e., attention) on differ- ent interface elements in ALTs and can be elicited by the individ- ual learner, proposed by the PA, or scaffolded by visualizations.  The third ER strategy is cognitive change and refers to modifying  ones appraisal of a situation to alter its emotional impact. Cogni- tive change can be applied to an external situation (e.g., my per- formance in this study is not directly relevant to my academic  major but it is a chance for me to learn more about science), to  an internal situation (e.g., My racing heart is not a sign of anxie- ty; it means my body is preparing for understanding this complex  diagram), or to alter how one thinks about ones capacity to  manage situational demands using self-regulatory skills (e.g.,  Although learning about this content feels overwhelming, I know  I can handle it using my existing knowledge, skills and the sup- port provided by the system). The last ER strategy discussed is  response modulation, which refers to directly influencing experi- ential, behavioral, or physiological components of the emotional  response after the emotion is well developed. For example, we  envision learners self-initiating prompts when frustration arises,  such as smiling when receiving negative feedback or a low score  on a quiz, or managing boredom by watching a video of a related  topic. In summary, we argue for a theoretical amalgamation of the  component process model, model of affect dynamics, and a model  of ER to provide a comprehensive foundation for a conceptual  framework that aims to foster ER using data visualization from  CAMM SRL process data. Our approach integrates these leading  models, theories, and frameworks along with contemporary re- search on dashboards and OLMs.  Supporting learners ER during SRL with ALTs by providing  their own and others CAMM SRL data visualizations is innova- tive and has the potential to significantly impact SRL and the  educational effectiveness of ALTs. In addition to the theoretical  and empirical literature briefly presented above, we make a few  additional assumptions in proposing our conceptual framework.    3.1 Core Assumptions  First, multimodal multichannel CAMM SRL data are relevant (for  ER) and therefore allow learners to accurately monitor and regu- late their emotions. We emphasize that the relevancy of CAMM  SRL data will vary based on individual differences, ones ability  to accurately monitor and regulate CAMM processes, internal and     contextual conditions, and so forth. Second, the relevance of  CAMM SRL data visualizations must be synchronized accurately  and coordinated with other visualizations for maximum potential  of facilitating monitoring and regulation of ones emotions. Third,  presentation of multichannel data should not exceed human cogni- tive architecture constraints (e.g., limited attention, not exceed  WM capacity, cognitive load), must adhere to SRL assumptions  (e.g., learners have the potential to monitor and regulate their  cognitive, affective, metacognitive, and motivational processes),  should not compete for learners attention, should be easy to pro- cess (i.e., not become a dual task), and should be actionable (i.e.,  providing data with which learners can control their emotions).  Fourth, the coordination and timely presentation of various multi- channel data need to be carefully considered based on the learn- ers immediate needs (e.g., deal with negative affect that ex- ceeds a time threshold and is negatively impacting the learners  level of cognitive engagement) as well as their learning history  (typically modeled in a student or user model). This assumption is  based on several types of data collected from the learner (e.g., self  reports, online trace data; see Table 1 for a sample) at different  times (e.g., prior to learning, during learning, and following learn- ing) and across time spans (e.g., several minutes, one hour, one  day, one month, one semester, etc.). Fifth, CAMM processes are  represented in ways that include a number of representations for  each individual process and all CAMM SRL processes collective- ly (e.g., histogram displaying frequency of learning strategies  used vs. histogram of frequency of facial expressions of emotions  and corresponding histogram of the effectiveness of learners use  of specific ER strategies), representation type (e.g., static vs. dy- namic or both), ease of understanding and interpretability (e.g.,  balance between simple to understand and abstract representation  that might require further cognitive processing and reflection on  ones affective reactions), and configuration (e.g., if more than  one data visualization is shown then how will it be presented on  the interface to maximize selective attention, processing, interpre- tation, and understanding vis--vis SRL behaviors, learning, and  problem solving). Sixth, it is critical to account for the roles of  CAMM SRL processes both individually and together when con- sidering the above-mentioned assumptions; however, in this paper  we focus on the emotional aspects. Seventh, we assume that all  learners (of all ages, levels of expertise) have the potential to ac- curately monitor and effectively regulate their CAMM processes.  However, there might be individual-, task-, and context-specific  variables, factors, and processes that could impede the successful  monitoring and regulation of CAMM SRL processes. Eighth, our  framework emphasizes the strategic use and presentation of  CAMM SRL visualizations designed to facilitate learners accu- rate monitoring and regulation of their emotions. Lastly, data  visualizations of CAMM processes are important because emotion  monitoring and regulation can stem (i.e., the antecedents) from  cognitive, metacognitive, and affective processes. We  acknowledge the importance of motivational processes; however,  we do not discuss them here due to space limitations.   3.2 Data Visualizations to Foster ER from  CAMM SRL Data: An Example  Due to space limitations, we present a brief example from our  eye-tracking data to illustrate our conceptual framework that in- cludes the CAMM SRL processes, multimodal multichannel data  sources, sample data representations to be used as the data visual- izations to foster ER, and a preliminary description of some scaf-  folding techniques that will allow learners to engage in accurate  monitoring and effective regulation of their emotions (based on  [2][6][7][9]).    Figure 1 illustrates the use of a heat map to show a learner that a  potential source of frustration is their lack of attention to the intel- ligent virtual human as the learner tended to allocate the most  attention to the last paragraph followed by the first and second  paragraphs and the diagram. This figure exemplifies how the rep- resentation of gaze behavior can be used by a PA (during scaf- folding or ER training) to explain (or verbally walkthrough) how a  learners attentional deployment might be indicative of an ineffec- tive strategy when trying to learn from multimedia materials be- cause it is not indicative of effective selection, organization, and  integration of text and diagram (see [20]). Additionally, the speed  of attentional deployment is indicative of the poor metacognitive  monitoring needed to accurately assess the relevancy of the mate- rials, and the relatively few fixations on the humans face, which  failed to capitalize on the agents nonverbal facial cues related to  the relevancy of the materials vis--vis the science question (green  box at the top of the image). It is important to highlight that this  example relates to the attentional deployment ER strategy pro- posed by Gross [17].      Figure 1: A complex visualization of CAMM SRL data.   Lastly, Figure 1 also delineates the integration of several CAMM  SRL data (with the exception of EDA and emotion data) that can  be used to provide elaborate adaptive scaffolding. This adaptive  ER scaffolding technique explicitly illustrates critical variables  such as time spent on different (relevant and irrelevant) areas of  interest (AOIs), the time spent on and accuracy of use of specific  cognitive strategies (e.g., coordinating informational sources), and  the accuracy of specific metacognitive judgments (e.g., content  evaluation; CE) during learning. Implicit connections between  cognitive strategies and metacognitive judgments (e.g., coordinat- ing informational sources is associated with CE) are symbolically  represented on the same colored bar. CEs are metacognitive  judgments related to a representations relevancy given a learning  goal. The size of the CE bars is representative of the accuracy of  metacognitive judgments, whereas the size of the coordinating  informational sources bar represents the amount of time spent  using multiple sources of information (e.g., text, diagram, PA  expression). This approach allows for ease of interpretation as  well as integration of cognitive and metacognitive monitoring     processes, and can also be made explicit by having a human or  artificial agent do a verbal (cognitive) walkthrough of this entire  figure at some point during ER scaffolding. Lastly, we emphasize  that these CAMM SRL processes and ER scaffolding techniques  presented in our framework need to be experimentally tested prior  to adoption in ALTs to foster successful ER.    4. FUTURE DIRECTIONS  Emotions play a critical role during learning and problem solving  with ALTs. We argue that despite their importance, relatively few  attempts have been made to understand and foster learners emo- tional monitoring and regulation by using data visualizations of  their own (and others) CAMM SRL processes, since they can  potentially foster ER during learning with ALTs. In this paper, we  presented a theoretically based and empirically driven conceptual  framework that addresses ER by proposing the use of ones own  and others CAMM SRL data visualizations (e.g., eye-movement  behaviors, facial expressions of emotions, physiological arousal)  to facilitate learners monitoring and regulation of their emotions.  We proposed several examples of ER strategies based on the  presentation of multimodal multichannel data to illustrate the  mapping between theoretical assumptions, ER strategies, and the  types of data visualizations that can be used to enhance learners  ER. We expect this line of work to lead to interdisciplinary efforts  addressing the use of CAMM SRL data visualizations to foster ER  and lead to optimal and successful learning with ALTs. In doing  so, we expect advances in the educational, learning, cognitive,  affective, social, engineering, and computational sciences to con- tribute immensely to the myriad of issues presented in our paper.  The plethora of conceptual, theoretical, methodological, and ana- lytical challenges (e.g., identifying robust behavioral signatures of  emotion flexibility, emotion adaptivity, and emotion efficacy)  from multimodal multichannel data will impact contemporary  research in the cognitive, learning, and affective sciences; HCI;  data visualization; big data; data mining; OLMs; and SRL.   4.1.1.1.1 Acknowledgements  This paper was supported by funding from the National Science  Foundation (DRL#1431552) and the Social Sciences and Humani- ties Research Council of Canada (SSHRC 895-2011-1006).    REFERENCES  [1] Azevedo, R., Taub, M., & Mudrick, N. (2015). Technologies   supporting self-regulated learning. In M. Spector et al.  (Eds.), The SAGE encyclopedia of educational technolo- gy (pp. 731734). Thousand Oaks, CA: SAGE.   [2] Azevedo, R., Taub, M., Mudrick, N., Farnsworth, J., & Mar- tin, S. A. (2016). Interdisciplinary research methods used to  investigate emotions with advanced learning technologies. In  M. Zembylas & P. Schutz (Eds.), Methodological advances  in research on emotion and education (pp. 231243). Am- sterdam, The Netherlands: Springer.   [3] Bull, S., & Kay, J. (2016). SMILI: A framework for interfac- es to learning data in open learner models, learning analytics  and related fields. IJAIED, 26, 293331.   [4] Verbert, K., Govaerts, S., Duval, E., Santos, J. L., Assche, F.,  Parra, G., & Klerkx, J. (2013). Learning dashboards: An  overview and future research opportunities. Personal and  Ubiquitous Computing, 18, 14991514.    [5] DMello, S. K. (2013). A selective meta-analysis on the rela- tive incidence of discrete affective states during learning with  technology. J. Educ. Psychol., 105, 10821099.   [6] Azevedo, R. (2015). Defining and measuring engagement  and learning in science: Conceptual, theoretical, methodolog- ical, and analytical issues. Educ. Psychol., 50, 8494.    [7] Azevedo, R., Taub, M., & Mudrick, N. V. (in press). Using  multi-channel trace data to infer and foster self-regulated  learning between humans and advanced learning technolo- gies. In D. Schunk & J. A. Greene (Eds.), Handbook of self- regulation of learning and performance (2nd ed.). New York,  NY: Routledge.   [8] Biswas, G., Segedy, J. R., & Bunchongchit, K. (2016). From  design to implementation to practiceA learning by teach- ing system: Bettys Brain. IJAIED, 26, 350364.   [9] Taub, M., Mudrick, N. V., Azevedo, R., Millar, G. C., Rowe,  J., & Lester, J. (in press). Using multi-channel data with mul- ti-level modeling to assess in-game performance during  gameplay with CRYSTAL ISLAND. Computers in Human  Behavior   [10] Graesser, A. C., & McNamara, D. S. (2010). Self-regulated  learning in learning environments with pedagogical agents  that interact in natural language. Educ. Psychol., 45, 234 244.   [11] Sabourin, J., & Lester, J. (2014). Affect and engagement in  game-based learning environments. IEEE Transactions on  Affective Computing, 5, 4556.    [12] Winne, P. H., & Hadwin, A. F. (2013). nStudy: Tracing and  supporting self-regulated learning in the Internet. In R.  Azevedo & V. Aleven (Eds.), International handbook of  metacognition and learning technologies (pp. 293310).  Amsterdam, The Netherlands: Springer.   [13] Azevedo, R., & Aleven, V. (Eds.). (2013). International  handbook of metacognition and learning technologies. Am- sterdam, The Netherlands: Springer.    [14] Winne, P. H., & Azevedo, R. (2014). Metacognition. In R. K.  Sawyer (Ed.), The Cambridge handbook of the learning sci- ences (2nd ed., pp. 6387). Cambridge, England: Cambridge  University Press.   [15] DMello, S., & Graesser, A. (2012). AutoTutor and Affective  AutoTutor: Learning by talking with cognitively and emo- tionally intelligent computers that talk back. ACM Transac- tions on Interactive Intelligent Systems, 2, 139.   [16] Harley, J. M., Bouchet, F., Hussain, S., Azevedo, R., & Cal- vo, R. (2015). A multi-componential analysis of emotions  during complex learning with an intelligent multi-agent sys- tem. Computers in Human Behavior, 48, 615625.   [17] Gross, J. J. (2015). Emotion regulation: Current status and  future prospects. Psychological Inquiry, 26, 126.    [18] Pekrun, R., & Linnenbrink-Garcia, L. (Eds.). (2014). Interna- tional handbook of emotions in education. New York, NY:  Routledge.   [19] Scherer, K. R. (2009). The dynamic architecture of emotion:  Evidence for the component process model. Cogn. Emot., 7,  13071351.   [20] Mayer, R. (Ed.). (2014). The Cambridge handbook of multi- media learning (2nd ed.). Cambridge, England: Cambridge  University Press.        "}
{"index":{"_id":"56"}}
{"datatype":"inproceedings","key":"Aguerrebere:2017:SDL:3027385.3027444","author":"Aguerrebere, Cecilia and Cobo, Crist'obal and Gomez, Marcela and Mateu, Mat'ias","title":"Strategies for Data and Learning Analytics Informed National Education Policies: The Case of Uruguay","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"449--453","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027444","doi":"10.1145/3027385.3027444","acmid":"3027444","publisher":"ACM","address":"New York, NY, USA","keywords":"big data, education policies, plan ceibal, technology enhanced learning","Abstract":"This work provides an overview of an education and technology monitoring system developed at Plan Ceibal, a nationwide initiative created to enable technology enhanced learning in Uruguay. Plan Ceibal currently offers one-to-one access to technology and connectivity to every student and teacher (from primary and secondary education) as well as a comprehensive set of educational software platforms. All these resources generate massive amounts of data about the progress and style of students learning. This work introduces the conceptual framework, design and preliminary results of the Big Data Center for learning analytics currently being developed at Plan Ceibal. This initiative is focused on exploiting these datasets and conducting advanced analytics to support the educational system. To this aim, a 360 degrees profile will be built including information characterizing the user's online behavior as well as a set of technology enhanced learning factors. These profiles will be studied both at user (e.g., student or teacher) and larger scale levels (e.g., per school or school system), addressing both the need of understanding how technology is being used for learning as well as to provide accurate feedback to support evidence based educational policies.","pdf":"Strategies for Data and Learning Analytics Informed National Education Policies: the Case of Uruguay  Cecilia Aguerrebere Plan Ceibal  Av. Italia 6201, CP 11500 Montevideo, Uruguay  caguerrebere@ceibal.edu.uy  Cristbal Cobo Plan Ceibal  Av. Italia 6201, CP 11500 Montevideo, Uruguay  ccobo@ceibal.edu.uy  Marcela Gomez Plan Ceibal  Av. Italia 6201, CP 11500 Montevideo, Uruguay  migomez@ceibal.edu.uy Matas Mateu  Plan Ceibal Av. Italia 6201, CP 11500  Montevideo, Uruguay mmateu@ceibal.edu.uy  ABSTRACT This work provides an overview of an education and tech- nology monitoring system developed at Plan Ceibal, a na- tionwide initiative created to enable technology enhanced learning in Uruguay. Plan Ceibal currently offers one-to-one access to technology and connectivity to every student and teacher (from primary and secondary education) as well as a comprehensive set of educational software platforms. All these resources generate massive amounts of data about the progress and style of students learning. This work introduces the conceptual framework, design and preliminary results of the Big Data Center for learning analytics currently being developed at Plan Ceibal. This initiative is focused on ex- ploiting these datasets and conducting advanced analytics to support the educational system. To this aim, a 360 de- grees profile will be built including information characteriz- ing the users online behavior as well as a set of technology enhanced learning factors. These profiles will be studied both at user (e.g., student or teacher) and larger scale levels (e.g., per school or school system), addressing both the need of understanding how technology is being used for learning as well as to provide accurate feedback to support evidence based educational policies.  CCS Concepts Social and professional topics Computing education; Student assessment;  Keywords big data; Plan Ceibal; education policies; technology en- hanced learning  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027444  1. INTRODUCTION Plan Ceibal1 is a national policy programme that has been  implemented over the last nine years in Uruguay. At its very beginning, the government agency created for such purposes, promoted equal opportunities by providing a laptop and In- ternet access to every child and teacher. After distributing over 700.000 laptops and deploying Internet connections for schools and communities, reaching 85% of the students in the country, Plan Ceibal has made considerable progress to- wards enabling higher levels of social inclusion and equity by reducing the digital divide in all socio-economic contexts.  With a reduced digital divide, Plan Ceibal is now focused on providing the public educational system with an inte- grated set of tools, technologies, methodologies and digi- tal strategies to enhance the learning and teaching process. Plan Ceibal currently offers a set of educational software platforms for teaching, learning, training, hosting, exchang- ing and creating information. Virtual learning environments (VLE) at Plan Ceibal allow real-time interaction between students and their teachers and peers through a variety of resources and exercises, discussions or instant messag- ing. These VLEs generate massive amounts of data on the progress and style of students learning. In addition to this, after nine years of providing connectivity, Plan Ceibal has a wide range of data coming from the use of devices (tablets, laptops) and schools WiFi networks. Strategic use of data in the education sector is crucial to inform the teaching and learning process, to reduce achievement gaps and to increase the quality of public education [3].  Today Plan Ceibal is building a business intelligence sys- tem, developing preliminary experiences in educational data mining. However, until now, these databases have been col- lected and processed just for management and operational needs, without analysing multi-platform online users be- havior. In order to expand these experiences and conduct advanced analytics of the educational system, a Big Data Centre for Learning Analytics is being planned [2]. This will build the foundation for future research which can further analyze how technology is being used and how it can be used for teaching and learning practices. This involves new chal- lenges such as the development of new tools, techniques, and  1www.ceibal.edu.uy, http://www.fundacionceibal.edu.uy/en    360 User Profile Variables  Access  variables  Upload traffic Download traffic Number of connections Device type  Use variables                                Educational Taxonomy      Recreational of traffic         Social networking                               Others   Content creation Content consumption  User's  sociodemographic features  Age Gender School sociocultural context School Location  Learning Management System  Comments posted Submissions Active users Files uploaded  Ceibal's Math Adaptive Platform  Exercises completed Topic Success rate Time of usage Autonomous work  School's features  Deployment of Ceibal  intervention policies  for technology  incorporation  Learning Indicators   Engagement Performance  Figure 1: Main variables to be included in the 360  profile.  peoples skills; resolving data concerns such as how the data is captured, analyzed and disseminated protecting ethical, legal and societal concerns. In this regard, several efforts are being made by the organization in order to guarantee the quality and integrity of the data management, ensur- ing anonymity and following ethical and legal guidelines not only in accordance to the Uruguayan legislation but also in agreement with international recommendations [9].  One of the goals of this work is to build a comprehensive user online profile, hereafter referred to as 360 user pro- file. This profile, combined with statistical modelling tech- niques, can help: (i) identify online patterns on aspects such as learning styles, content creation pathways, adaptive con- tents services; (ii) predict learning behaviors; (iii) measure students engagement or retention. Also within the scope of the 360 profile, but at a larger scale, this meta-index can provide useful information of the school system as a whole. This higher level of data is expected to provide ac- curate information for decision-makers regarding how tools (e.g., devices, infrastructure) and services (e.g., connectivity, educational software) are used at a national scale. It is ex- pected that the integration of variables, aggregated or disag- gregated, can contribute to understand either general trends or individual based behavior, helping Plan Ceibal make bet- ter informed decisions on future educational strategies.  This article is organized as follows. Section 2 introduces the 360 user profile and proposes some of the fundamen- tal questions that motivate its creation. Sections 3 and 4 provide examples of the analysis based on the this profile. Finally, a summary of this work is presented in Section 5.  2. TOWARDS A 360 USER PROFILE The 360 user profile is the result of integrating key vari-  ables that describe a wide variety of aspects related to the online behavior of educators and learners. This profile in- cludes aspects such as: network usage (e.g., what kind of application does the learner prefer, is the learner active in social networks, is the learner active in the educational platforms), educational platforms usage (e.g., if the learner uses the educational platforms, what kind of usage does she/he do number of exercises completed, number of cor- rect exercises, number of required hints, books consulta- tions, frequency of usage, usage outside the school, proactive or teacher suggested usage, participation in discussion fo- rums, individual or collaborative work), sociocultural back- ground, school context, among others. Figure 1 presents a summary of the main variables included in the 360 user profile.  This profile will be the main input underpinning the anal- ysis in future studies, both at user (e.g., student or teacher) and larger scale levels (e.g., per school or school system), ad- dressing both the need to understand the influence of tech- nology on the learning experience as well as to provide ac- curate feedback to support evidence based national policies.  When the 360 profile integrates data at a general level (school system) the information analyzed aims to provide large-scale insights to monitor and understand how and when the system is being used. What are the demands and main uses of the educational community at a national level. On the other hand, the individual-based level aims to offer a comprehensive (multi-dimensional) perspective of the user and his educational practices. Although groups of individu- als with similar behaviour can suggest patterns, the aim is to provide information that could be useful for learners and educators when monitoring the benefits and challenges of the learning experience, targeting questions such as: What student actions are associated with better performance, sat- isfaction, engagement, learning progress What features of an online learning environment lead to better performance learning When are students falling behind in the course When a student should be referred to a counselor for help  In order to illustrate how the 360 profile information is analyzed at these two levels, we provide in the following sections examples of these complementary approaches: 1) A global monitoring system which explores key trends that describe how the network is used by the school system at an aggregated level, longitudinal evolution of the traffic, among others. 2) Measuring a set of variables to understand in- dividuals performance when completing exercises with an intelligent tutoring system for mathematics (e.g., frequency, type and number of exercises completed, type of devices used, use of help, etc).  3. A GLOBAL MONITORING OF THE SCHOOLS SYSTEM  Network usage data forms a central part of the Big Data project at Plan Ceibal. To this aim, a Global Monitoring System (GMS), inspired in [6], is being developed. This GMS will enable the visualization of the network usage, which is an essential input to: (i) assess for what purposes Plan Ceibals network resources are being used, (ii) capture the dynamics of network usage to understand the existing trends and help adjust Plan Ceibals supply based on the users needs, (iii) study how the different family of devices are being used, (iv) provide network usage data at user level to build part of the key components of the 360 profile (e.g., most used resources, most visited sites and platforms, etc). The development of the GMS consists in four main phases, detailed in the Gantt diagram shown in Figure 2.  3.1 Data Sources The GMS combines access and use variables. Access vari-  ables include upload and download traffic (per site, user, or application), number of connections (per site, user, or ser- vice), and multiple types of devices (laptops, tablets, smart- phones, others). Use variables cover the taxonomy of traffic (pedagogical or educational, recreational or entertainment, among others), origin and destination of traffic (school, home, national, or international), and content production versus consumption. The access variables show which online appli-    2015 2016 2017  Q4 Q1 Q2 Q3 Q4 Q1 Q2  100% completeConceptual framework  100% completeDesign  75% completeImplementation  0% completeEvaluation  Figure 2: Planning for the Global Monitoring Sys- tem project.  40M  30M  20M  10M  0M 1 2 3 4 5 6 7 8 9 10 11 12  A ve  ra g e  D  o w  n lo  a d  p  e r   S it e  [ b p s ]  YEAR  2  2  2  2  2  100M  80M  60M  40M  20M  0M T1 T2 T3 T4  A ve  ra g e  D  o w  n lo  a d  p  e r   S it e  [ b p s ]  YEAR  2011  2012  2013  2014  2015  YEAR  2011  2012  2013  2014  2015  Figure 3: Traffic demand evolution of Plan Ceibal network from February 2011 to December 2015, ac- cumulated by quarters (left) and by month (right).  cations and devices are consuming more network resources (i.e., bandwidth, wifi connections), while the use variables il- lustrate the types of use (e.g., educational, recreational, type of devices, etc) therefore suggesting the means and motiva- tions for getting online.  3.2 Preliminary results  3.2.1 Traffic and connections The conceptual framework for the definition of global ac-  cess variables was presented in [7] and an initial analysis of results is introduced in [12]. The latter focuses on the question: what was the evolution of the aggregate demand of Internet access in the Plan Ceibal network between 2011 and 2015 and what is the projection for 2016 to 2019  The considered variables are download and upload inter- net traffic and simultaneous connections. These variables were collected hourly everyday, from February 2011 until December 2015. The analysis was conducted with the value of these variables at busy hour2. The number of potential users in the network was 625.000, of whom about 120.000 were simultaneously connected during busy hour [4].  Figure 3 presents two charts: the first one, aggregated in quarters, shows a consistent increase in traffic demand from quarter to quarter. In the second chart, the aggrega- tion is done by month, showing a dramatic decrease during the summer break (December to February), and a moder- ate decrease during the winter break (July). Figure 4 shows the evolution of traffic per site and simultaneous connec- tions during the period 2011 to 2015, considering primary and secondary schools. It is interesting to note that, start- ing in 2014, the number of connections in secondary schools has grown dramatically, surpassing those in primary schools. This phenomenon correlates in time with the introduction of the Bring Your Own Device Policy in secondary schools (users are now allowed to connect to Plan Ceibals network their personal devices), which may explain the effect.  260 minute period of the day during which the maximum total traffic load of the network occurs.  2011 2012 2013 2014 2015 2016  Year  0M  10M  20M  30M  40M  50M  D o w  n lo  a d s  p  e r  S  it e  [ b p s ]  Primary Schools  Secondary Schools  2011 2012 2013 2014 2015 2016  Year  0  100  200  300  400  500  600  700  800  900  A v e ra  g e  C  o n n e c ti o n s  p  e r  S  it e  g  Primary Schools  Secondary Schools  Figure 4: Traffic demand (left) and number of conexions (right) evolution of Plan Ceibal network from February 2011 to December 2015 for primary and secondary schools.  From these preliminary results we observe that Plan Ceibals internet download traffic has grown 13 times between 2011 and 2015. CISCO VNI reported that global Internet down- load traffic has grown five times in the same period [6]. In other words, Plan Ceibals internet consumption has grown about 2.5 times faster than the global Internet. At the same time, download traffic has doubled every 18 months during the period between 2011 and 2015, while upload traffic also doubled every 16 months in the same period. Simultaneous connections per site have doubled every 12 months between 2011 and 2015. Based on these observations, and consider- ing that aspects such as the bandwith capacity, the number of devices and the number of active users are expected to increase, the hypothesis is that the network will continue expanding.  4. TECHNOLOGY ENHANCED LEARNING ASSESSMENT  The 360 user profile will include variables describing socio- cultural aspects (e.g., school socio-cultural level), school char- acteristics (e.g., full-time, part-time, special education, geo- graphical area, internet connectivity), information concern- ing the use of educational platforms (e.g., number of users, activities on the platform, type and intensity of use), among others. Furthermore, meta-indexes will be built, from the analysis and combination of these variables, to study more general concepts such as students school engagement and performance.  Student performance is a major point of interest and has therefore been studied from several perspectives. One such approach is to model student learning curves using cogni- tive models such as Bayesian Knowledge Tracing [8], Per- formance Factor Analysis [15], Additive Factor Models [5], among others, studying individual or collaborative work [13]. Another perspective is to follow a more classical data min- ing approach using classical classification or regression tech- niques to predict grade scores from various features, such as previous scores, students demographics, extra-curricular ac- tivities, high school background, social interaction network, psychometric factors, among others [17]. Furthermore, there exists an extensive bibliography on relationship mining fo- cused on finding relationships between performance and var- ious features, such as the ones previously listed for perfor- mance prediction [10, 11, 18, 16, 1].  As an integral part of the creation of the 360 user pro- file, we will study performance from different perspectives, as well as the effects of technology enhanced learning on performance outcomes. In the following section we present a particular case study of performance which focuses on stu-    Table 1: Adaptive platform for mathematics (PAM) users and number of performed activities from June 2013 to September 2016.  Total Growth  Year Users Activities Users Activities  20133 51.025 4.341.127 - - 2014 91.685 8.442.303 80% 94% 2015 113.617 31.818.345 24% 277% 20164 120.349 27.346.793 15%5 19%  dents performance on an intelligent tutoring system (ITS) for mathematics learning.  4.1 Performance in an intelligent tutoring for mathematics learning: a case study  We propose to study students performance on an ITS that has been incorporated into the mathematics curricula through the Plan Ceibal network since 2013. This ITS im- plements an adaptive platform for mathematics (PAM, for its acronym in Spanish), which proposes series of exercises and suggests particular areas of improvement depending on the users mathematical skills. Table 1 presents a summary of global usage indicators of this platform. So far this year, 27 million exercises have been made in the platform, 63% of which were performed by primary school students, 25% by secondary school students and the rest being completed by other beneficiaries of Plan Ceibal. It is important to remark that a very heterogeneous use of the platform is observed, as 20% of the total users completed 75% of the total exercises.  By combining data from different sources, including log- files of the users activity in PAM, we aim at studying the effect of various factors on students performance and an- swering fundamental questions such as: the more exercises the better What role does frequency of exercises com- pleted play here Do proactive and autonomous students outperform the rest Do schools receiving supportive Plan Ceibal policies have an increased average performance than the rest To what extent the engagement of the teacher with the platform affects the performance of the student and if so under what circumstances Is higher performance in the platform related to particular patterns of network usage  4.1.1 Data Sources The considered dataset consists of 120.000 students in  1760 schools. The variables to be studied can be classi- fied into five main categories: socio-cultural (e.g., schools socioeconomic context), geographical (e.g., urban or rural school), educational (e.g., the deployment of Ceibal inter- vention policies, such as the presence of support teachers to promote technology incorporation at school), technology (e.g., type of device used by the user) and user-specific (e.g., whether the user has a proactive attitude towards using PAM or not). Table 2 shows a detailed description of some of the considered variables. The performance indicator is here defined as the total number of correct exercises com- pleted by the student in PAM in a given time period, for instance half or a complete school year.  3Data from June to December. 4Data from January to September. 5Growth corresponding to September 2015 to September 2016.  4.1.2 Methodology In order to gain further insight into the effect of certain  factors into students performance in PAM, we will evalu- ate possible correspondences between the variables listed in Table 2 and performance. First, the Pearson correlation between each variable and the performance outcomes will be computed to find possible linear dependences between these variables. Second, following a classical data mining approach, we will conduct various regression methods for performance prediction from the considered variables. More precisely, we will assess the prediction capacity of common classification methods such as decision trees, Naive Bayes, SVM and random forests, previously shown to be power- ful for educational data mining applications [14, 19]. This evaluation will show the capacity of this set of variables to predict the performance outcomes.  5. SUMMARY AND FUTURE WORK This work provides an overview of a unique national-scale  education and technology monitoring system developed in Uruguay. Different efforts are currently conducted to adopt new metrics to better understand and support users needs in Uruguays educational system.  The overall goal of the Big Data Centre for Learning An- alytics at Plan Ceibal is to build a 360 degrees user profile at a school or at users level (the later can include students or educators). Approaching these two levels is considered useful to fulfil the information needs of different communi- ties (e.g., educators, learners, school leaders, policy makers, technology developers, researchers, etc.)  Today, the demand of Plan Ceibals resources is facing a phase of expansion. This is considered a significant op- portunity to improve the analysis process in order to better answer the needs of the different stockholders linked with this education and technology national initiative.  Acknowledging that building a multi-dimensional profile demands the selection and integration of key indicators, in this article the authors provide a framework of variables and information sources to consider when building these two stages of the 360 profile.  Noteworthy, the principal goal is to elaborate a meta- index, which provides relevant and accurate data of the teaching and/or learning experiences. In other words, if this analysis does not offer tools either to support the work of educators while they plan their teaching, or to provide rele- vant data for the learners regarding their performance (i.e., to support self-directed autonomous learning), then the pro- posed implementation will be suboptimal at best.  As follow, some of the forthcoming challenges that we an- ticipate during the creation of the meta-index (360 degrees profile) are highlighted:  1. Although Plan Ceibal manages national scale educa- tional information, it is considered strategic to begin with the analysis at two levels (schools and users). The idea is to design mechanisms to compare large scale and individual- based analysis, whose outcomes will be relevant for different stakeholders.  2. The broader the scope of data available to include in the process the higher the responsibility of adopting proper standards to ensure the quality and integrity of the data management, protecting the identity and privacy of users (i.e., privacy-by-design) or following appropriate ethical and    Table 2: Variables used for performance prediction.  Description Category  Schools socioeconomic context Socio-cultural Schools location (e.g., urban, suburban, rural) Geographical School type (e.g., regular, critical socioeconomic context, special care needed) Educational Deployment of Ceibal supportive policies Educational Teacher engagement with platform adoption in class (measured by the number of Ceibal courses completed by the teacher) Educational Device type (laptop, tablet, pc, smartphone) Technology Total number of exercises completed User-specific Autonomous work (total number of exercises started by user choice, as opposed to those indicated by the teacher) User-specific Completion of exercises suggested by the platform in the adaptive learning process User-specific Time or Location when the exercises were completed (e.g., school time, leisure time, school hours, off school hours) User-specific Frequency in which exercises were completed User-specific Network usage profile User-specific  data privacy guidelines (such as DELICATE [9]). 3. To include educators and school leaders in an early  stage of the learning analytic process. In order to ensure their participation but also to improve the design of the an- alytics with continuous iterations and inputs from the dif- ferent communities involved.  6. ACKNOWLEDGMENTS The authors would like to thank Plan Ceibal and Fun-  dacion Ceibal for supporting this study, as well as German Capdehourat and Enrique Lev for their valuable contribu- tions.  7. REFERENCES [1] L. K. Allen and D. S. McNamara. You are your words:  Modeling students vocabulary knowledge with natural language processing tools. In 8th Int. Conf. on Educational Data Mining, pages 258265, 2015.  [2] M. Bailon, M. Carballo, C. Cobo, S. Magnone, C. Marconi, M. Mateu, and H. Susunday. How can Plan Ceibal Land into the Age of Big Data In 4th Int. Conf. on Data Analytics, pages 126129, 2015.  [3] P. Bergman. Technology adoption in education: Usage, spillovers and student achievement. In 2015 Fall Conference: The Golden Age of Evidence-Based Policy. Appam, 2015.  [4] G. Capdehourat, G. Marn, and A. Rodrguez. Plan Ceibals wireless network for the implementation of the 1:1 educational model. Latin America Networking Conf., pages 9:19:4, 2014.  [5] H. Cen, K. Koedinger, and B. Junker. Learning factors analysisa general method for cognitive model evaluation and improvement. In Int. Conf. on Intelligent Tutoring Systems, pages 164175, 2006.  [6] Cisco. Cisco visual networking index: Forecast and methodology. Technical report, Cisco, 2014.  [7] C. Cobo and M. Mateu. A conceptual framework for analysis and visualization of uruguayan internet for education. To appear in Interactions, ACM, 2016.  [8] A. T. Corbett and J. R. Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction, 4(4):253278, 1994.  [9] H. Drachsler and W. Greller. Privacy and analytics: Its a delicate issue a checklist for trusted learning analytics. In In 6th Int. Conf. on Learning Analytics & Knowledge, pages 8998, 2016.  [10] S. Fancsali. Causal discovery with models: behavior, affect, and learning in cognitive tutor algebra. In In 7th Int. Conf. on Educational Data Mining, pages 2835, 2014.  [11] C. Forsyth, A. Graesser, P. Pavlik Jr, K. Millis, and B. Samei. Discovering theoretically grounded predictors of deep vs. shallow level learning. In 7th Int. Conf. on Educational Data Mining, pages 229232, 2014.  [12] M. Mateu. Plan Ceibal 2016: Future scenarios of technology and education. Unpublished Masters Thesis, 2016.  [13] J. K. Olsen, V. Aleven, and N. Rummel. Predicting student performance in a collaborative learning environment. In 8th Int. Conf. on Educational Data Mining, pages 211217, 2015.  [14] Z. A. Pardos, R. Baker, M. San Pedro, S. M. Gowda, and S. M. Gowda. Affective states and state tests: investigating how affect and engagement during the school year predict end-of-year learning outcomes. Journal of Learning Analytics, 1(1):107128, 2014.  [15] P. I. Pavlik, H. Cen, and K. R. Koedinger. Performance factors analysis a new alternative to knowledge tracing. In Artificial Intelligence in Education: Building Learning Systems That Care: From Knowledge Representation to Affective Modelling, pages 531538, 2009.  [16] M. O. Z. San Pedro, E. L. Snow, R. S. Baker, D. S. McNamara, and N. T. Heffernan. Exploring dynamical assessments of affect, behavior, and cognition and math state test achievement. In 8th Int. Conf. on Educational Data Mining, pages 8592, 2015.  [17] A. M. Shahiri, W. Husain, et al. A review on predicting students performance using data mining techniques. Procedia Computer Science, 72:414422, 2015.  [18] E. Snow, L. Varner, and D. McNamara. Tracking choices: Computational analysis of learning trajectories. In In 7th Int. Conf. on Educational Data Mining, pages 316319, 2014.  [19] P. Strecht, L. Cruz, C. Soares, J. a. Mendes-Moreira, and R. Abreu. A comparative study of classification and regression algorithms for modelling students academic performance. In 8th Int. Conf. on Educational Data Mining, pages 392395, 2015.    "}
{"index":{"_id":"57"}}
{"datatype":"inproceedings","key":"Davis:2017:FSC:3027385.3027411","author":"Davis, Dan and Jivet, Ioana and Kizilcec, Ren'e F. and Chen, Guanliang and Hauff, Claudia and Houben, Geert-Jan","title":"Follow the Successful Crowd: Raising MOOC Completion Rates Through Social Comparison at Scale","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"454--463","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027411","doi":"10.1145/3027385.3027411","acmid":"3027411","publisher":"ACM","address":"New York, NY, USA","keywords":"cultural differences, feedback, framing, learning analytics, massive open online course, social comparison","Abstract":"Social comparison theory asserts that we establish our social and personal worth by comparing ourselves to others. In in-person learning environments, social comparison offers students critical feedback on how to behave and be successful. By contrast, online learning environments afford fewer social cues to facilitate social comparison. Can increased availability of such cues promote effective self-regulatory behavior and achievement in Massive Open Online Courses (MOOCs)? We developed a personalized feedback system that facilitates social comparison with previously successful learners based on an interactive visualization of multiple behavioral indicators. Across four randomized controlled trials in MOOCs (overall N ","pdf":"Follow the Successful Crowd: Raising MOOC Completion Rates through Social Comparison at Scale  Dan Davis Delft University of Technology  Delft, the Netherlands d.j.davis@tudelft.nl  Ioana Jivet Open University of the Netherlands  Heerlen, the Netherlands ioana.jivet@ou.nl  Ren F. Kizilcec Stanford University Stanford, CA, USA  kizilcec@stanford.edu Guanliang Chen  Delft University of Technology Delft, the Netherlands  guanliang.chen@tudelft.nl  Claudia Hauff Delft University of Technology  Delft, the Netherlands c.hauff@tudelft.nl  Geert-Jan Houben Delft University of Technology  Delft, the Netherlands g.j.p.m.houben@tudelft.nl  ABSTRACT Social comparison theory asserts that we establish our so- cial and personal worth by comparing ourselves to others. In in-person learning environments, social comparison offers students critical feedback on how to behave and be success- ful. By contrast, online learning environments afford fewer social cues to facilitate social comparison. Can increased availability of such cues promote effective self-regulatory be- havior and achievement in Massive Open Online Courses (MOOCs) We developed a personalized feedback system that facilitates social comparison with previously successful learners based on an interactive visualization of multiple be- havioral indicators. Across four randomized controlled trials in MOOCs (overall N = 33, 726), we find: (1) the availabil- ity of social comparison cues significantly increases comple- tion rates, (2) this type of feedback benefits highly educated learners, and (3) learners cultural context plays a significant role in their course engagement and achievement.  CCS Concepts Applied computing  Collaborative learning;  Keywords Learning Analytics; Massive Open Online Course; Feedback; Social Comparison; Framing; Cultural Differences  This work is co-funded by the Erasmus+ Programme of the European Union. Project: STELA 62167-EPP-1-2015-BE- EPPKA3-PI-FORWARD. The authors research is supported by the Leiden-Delft- Erasmus Centre for Education and Learning. Work performed while at Delft University of Technology The authors research is supported by the Extension School of the Delft University of Technology.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 17, March 13 - 17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4870-6/17/03. . . $15.00  DOI: http://dx.doi.org/10.1145/3027385.3027411  1. INTRODUCTION A mechanism for increasing access to higher education  content, Massive Open Online Courses (MOOCs) have af- forded millions of people worldwide the opportunity to learn for little or no cost. To achieve this unprecedented scale, MOOCs provide the same material to all learners, no matter what background, motivation, and skill set they possess. Yet this approach falls short of leveraging the technical possibil- ities of contemporary educational resources to offer learners personalized support, such as giving guidance to learners who are less adept at regulating their learning process over several weeks to achieve mastery. Low course completion rates (typically between 5-10%) highlight the need for ad- ditional support in MOOCs. While many learners have no intention to complete MOOCs and instead use them to fulfill alternative needs (e.g., to refresh their memory of a specific topic or to meet new people), the majority of learners who are motivated and committed to complete the course still fail to achieve their goal [25, 26]. Most learners report that they could not find the time to keep up with the course, a challenge that is related to insufficient self-regulatory abili- ties [44, 23]. Self-regulated learning (SRL; i.e., the ability to plan, monitor, and actively control ones learning process) is associated with a higher likelihood of achieving personal course goals in MOOCs, including course completion [24, 29]. However, the current design of MOOCs does not sup- port learners to engage in SRL [34]. In particular, most MOOC platforms do not provide learners with personalized feedback beyond grades [7], and thus, learners may not know if their engagement in the course is conducive to achieving their learning goals.  We propose a technological solution that facilitates social comparison to help learners regulate their learning behavior to support course completion. According to social compar- ison theory [8], people establish their social and personal worth by comparing themselves to others. Offering learn- ers the opportunity to compare their behavior with that of their peers promotes increased student achievement in formal learning environments [2, 15, 36]. Students in in- person classrooms can easily identify role models and regu- larly monitor these role models behavior and compare it to their own. However, this affordance of social comparison is missing in most online classrooms. Instead, online learners need to be self-directed and regulate their learning process  http://dx.doi.org/10.1145/3027385.3027411   independently with sparse social and normative signals. In addition to evaluating the impact of providing learners  with personalized feedback, we further examined the po- tential of adjusting the framing of the feedback to match learners cultural context. Framing feedback in a way that is consistent with the norms and achievement-based motiva- tion of learners cultural context is expected to support in- ternalization and behavior change. Prior work has observed differences in the way learners from different countries and cultures interact with MOOCs [11, 23, 30]. We define cul- tural context based on two established country-level cultural dimensions: individualism by Hofstede et al. [14] and tight- ness by Gelfand et al. [9].  We explore the extent to which insights from the social comparison and cultural psychology literature can be trans- lated to support learners in MOOCs. We evaluate how to offer feedback based on social comparison in an online learn- ing environment. To this end, we design, develop, and em- pirically evaluate a personalized and scalable feedback sys- tem that presents MOOC learners with a visual comparison of their behavior to that of their successful peers, that is, those who completed the course in the past. We deployed the system in four edX1 MOOCs offered by the Delft University of Technology with a total of N = 33, 726 learners. In each deployment we drew on research findings across multiple do- mains including learning analytics, educational psychology, and social & cultural psychology to inform the design on both the feedback we provide (i.e. the behavioural metrics shown to the learners) and how the feedback is framed (e.g., individualistic- or collectivist- oriented framing).  Our work extends prior research by testing a theory-informed technological solution in a large and diverse population (i.e., MOOC learners) for a prolonged period of time. These are our main findings:   Personalized social-comparison feedback increases course completion rates.   Only highly educated learners benefit from this kind of feedback.   Course engagement and achievement varies by cultural context: learners in countries with a loose culture outperform those in countries with a tight culture.  2. BACKGROUND In this section we provide the theoretical and empirical  underpinnings to our work which facilitates social compar- isons with personalized feedback. We discuss (i) previous studies on incorporating feedback in online learning, (ii) the theory of social comparison and its application to learning, and (iii) past research on the impact of learners cultural context on learning behavior.  Feedback. Providing feedback is one of the most effective teaching  strategies to improve student achievement [12]. Given the scale of MOOCs it is impossible for a teacher or teaching assistant to personally monitor and attend to each learners unique needs. Therefore, up to this point, the majority of feedback solutions developed for MOOCs and other online learning environments have been for the course instructor, typically in the form of a dashboard representing aggregated learner data [27, 33, 43].  1https://www.edx.org/  While teacher-facing feedback systems can provide key in- sights for improving the course experience, they are unlikely to address the issue that many learners feel lost and isolated in MOOCs [21]. Personalized feedback promises to pro- mote effective SRL behavior by facilitating self-monitoring of learning processes [18]. One of the most important lines of research which aims to provide learners with personalized feedback is that of Open Learner Models (OLM), an educa- tional interface that gives learners insight into their current knowledge state and activity patterns, which are typically unavailable to them [3]. By allowing learners to visualize and reflect on their own learning and achievements, OLMs have been proven to work as powerful meta-cognitive feedback tools that impact learners use of SRL strategies [4, 10]. We designed the Feedback System informed by prior work on the design of accessible, understandable, and scrutable [20] learner models [5, 19].  There has been little progress in developing and deploying personalized feedback for large-scale MOOC environments, and most work focuses on supporting teachers [40]. In the present research, instead of presenting aggregate data for all learners in a course, we addresses the challenge of delivering individualized, targeted feedback to each learner based on her behavior in the course relative to her peers behavior to facilitate social comparison. The present research con- tributes an empirically evaluated scalable and personalized feedback intervention to the literature on learning analytics. Recent studies have begun to run controlled experiments [37], but most feedback system evaluations thus far explain the design, development, and implementation considerations without rigorously testing whether the added support con- tributes to behavior change or learning gains [43].  Social Comparison. The feedback learners receive through the Feedback Sys-  tem is grounded in social comparison theory, initially pro- posed by Festinger [8]. The theory posits that, guided by a drive to continuously improve, people evaluate their abilities through comparison to others when they are lacking objec- tive means of comparison. It has received empirical valida- tion and found application in various domains, including marketing, health psychology, interpersonal relationships, and also in education [6, 28]. In one study, social com- parison was used to improve the Web search behavior of novice users [1]. The authors found that showing non-expert searchers visual indicators of the search behaviors of expert searchers resulted in closer alignment with effective behav- ior and, therefore, more successful search task completion among novices.  Social comparison is an inherent phenomenon in tradi- tional classroom environments because of both the visibility and accessibility of similar peers [28]. Multiple studies have demonstrated that comparing oneself to self-selected peers who perform slightly better has a beneficial effect on mid- dle school students grades [2, 15]. Forced comparisons also have a beneficial effect on performance when the target of comparison is performing slightly better than the learner, although no effects were found when there was a big perfor- mance gap between two sides [16].  In the context of a small online learning platform (N = 55), Papanikolaou [36] investigated students attitudes to- wards viewing the learner model of others. Her results showed that when learners compare their behavior to that of a de- sired one, they are then motivated to recognize and adapt  https://www.edx.org/   their learning strategies. She suggests that the desired state should be generated based on real data coming from peers who are worth following. We build on this insight by considering MOOC graduates of previous editions as the basis for creating a role model.  Guerra et al. [10] integrated social comparison features in the form of peer and class progress in the design of an in- telligent interface for a learning management system to pro- vide additional motivation and navigation support. This ap- proach showed a positive effect on engagement and efficiency in two studies (N = 89), but no significant effects on learner performance in terms of final grades or learning gains. On the other hand, Rogers et al. [38] investigated discourage- ment by peer excellence in a MOOC setting and concluded that learners who are exposed to examples of excellent peer achievements risked feeling less capable of performing at the level of those peers. The Feedback System is different in that it shows the behavior patterns of the average complet- ing learner, so as not to risk discouragement.  The present research adds to the literature on social com- parison in the online learning environments by investigating the effects of forced comparison of learners performance and engagement in a MOOC setting. With the Feedback Sys- tem, MOOC learners can visualize their behavior compared to that of successful learners, offering them a model against which they can evaluate their own study habits.  Culture. MOOC learners come from all over the world and cover  a profoundly wide range of cultural contexts. Prior MOOC research has observed a learners culture as affecting behav- ior within the course. For example, Liu et al. [30] explored patterns in MOOC learner behavior in relation to Hofst- edes cultural dimensions [14]. The authors clustered coun- tries based on similarity across four cultural dimensions and found significant variation in learner behavior between the clusters. Moreover, Kizilcec et al. [22] found in two ran- domized experiments in MOOCs that the effect of a self- regulation intervention depended on learners cultural con- text (N = 17, 963): only learners in individualist countries benefited from the brief writing activity. Thus, prior work supports the hypothesis that cultural factors shape learner behavior in MOOCs. We examine two country-level cultural dimensions: individualism [14] and tightness [9].  Hofstedes dimension of individualism-collectivism char- acterizes cultural variation around the world. Cultures high in individualism are those which emphasize the individual as an independent actor with loose social relations. Cul- tures high in collectivism are characterized by tightly-knit social relations and shared responsibility for the collective well-being [14]. Gelfand et al. conceived an index that ranks countries by their cultural tightness: tight cultures are those withstrong norms and a low tolerance of deviant behavior, and conversely, cultures of low tightness (or loose cultures) are those with weak social norms and a high tolerance of deviant behavior [9]. The present study attempts to adapt feedback to learners cultural context so that it resonates with the learner, facilitates internalization of the feedback, and promotes positive behavior change.  Prior work suggests that cultural differences shape peo- ples regulatory focus, whether they are motivated by push- ing for success (promotion) or by avoiding failure (preven- tion) [13, 31]. Members of individualist and tight cultures focus more on promotion, while members of collectivist and  loose cultures focus more on prevention [31, 35]. We apply this insight in the design of our feedback framing messages to appeal to learners in different cultural contexts.  3. MOOC OVERVIEW For our experiments, we employed our personalized Feed-  back System to learners across four MOOCsall of them re-runs (i.e. not in their first edition)provided by the Delft University of Technology on the edX platform:  WaterX The Drinking Water Treatment MOOC teaches tech- nologies for drinking water treatment. Its second edition ran between 12 January and 29 March 2016. It is a seven- week course with 63 instructional videos and 42 summa- tive quiz questions. A total of 10,943 learners registered for the course. To complete the course, learners had to gain at least 60% of all scores (i.e. passing threshold pass = 60%).  UrbanX The Urban Sewage Treatment MOOC learners are taught how to design and manage solutions for urban sewage. The second edition of the seven-week course ran between 12 April and 20 June 2016 with 8,137 learners. There are 272 summative quiz questions (pass = 60%) and 71 videos.  BusinessX Responsible Innovation: Ethics, Safety and Tech- nology teaches learners how to deal with risks and ethi- cal questions arising from new technologies. 2,352 learn- ers registered to the second edition which ran between 11 April and 14 June 2016. The course has 79 summative quiz questions (pass = 59%) and 54 videos.  CalcX Pre-university Calculus is the only MOOC in our list that targets beginning Bachelor students and was de- signed as a refreshment course before entering higher edu- cation. The third iteration of this course ran from 28 June 2016 through 27 September 2016 with 12,294 learners, 85 videos, and 327 summative quiz questions (pass = 60%).  We found the WaterX, UrbanX, and BusinessX MOOCs to attract a similar population of learners: two thirds of the enrolled learners were male, the median age was 28, and the majority of learners held a BSc or MSc degree. The learner population in the CalcX course was instead targeted at high- school students who were about to enter university. While the gender balance was consistent with other MOOCs (30% female), the median age was only 25, and the most common education level was a high school diploma (45%).  For each learner, we collected all available edX log traces such as the learners clicks, views, dwell time on the edX platform, and their provided answers to the quiz questions.  4. APPROACH In Section 4.1 we first introduce the research questions  driving our work before detailing the design of our Feedback System which was deployed in different instantiations across the four MOOCs just described.  4.1 Research Questions The first Research Question and Hypotheses are based  primarily on the social comparison literature in the context of education and learning environments:  RQ1 Does providing personalized social comparison feed- back increase learner achievement and self-regulatory behavior in MOOCs    H1.1 In line with previous findings [1, 36], we expect that providing learners a comparison of their own be- havior to that of previously successful peers will in- crease learner achievement (measured in terms of completing/passing the course) and engagement (ac- tivity levels within the course environment).  H1.2 Learners will change the aspects of their behavior that the Feedback System makes them aware of.  H1.3 Certain feedback metrics (and combinations of met- rics) will be more effective than others in leading to desirable changes in student behavior.  Based on prior work which has shown that learners from different cultural contexts learn and behave differently in MOOCs [11, 23, 24, 30], we explore:  RQ2 Which learners benefit most from the Feedback Sys- tem  We also examine the differences in learning behavior accord- ing to learners cultural context. We expected the effects of the feedback to depend on learners cultural context in terms of individualism and tightness, and moreover, that matching the framing of feedback to learners culture to be beneficial:  RQ3 Does feedback framed in line with a learners cul- tural context lead to increased achievement and self- regulatory behavior compared to a culturally mis- matched framing  H3.1 Learners from individualist cultures will show more engagement than those from collectivist cultures with the individual-promotional framing, while learners from collectivist cultures will show more engagement with the collectivist-prevention framing.  H3.2 Learners from tight cultures will show more engage- ment than those from loose cultures with the collectivist- prevention framing, while learners from loose cul- tures will show more engagement with the individual- promotional framing.  4.2 Feedback System Design Recall, that our design rationale of the Feedback Sys-  tem (presented as the Learning Tracker to learners in the courses, cf. Figure 1) is to provide learners feedback about their own behavior that enables them to make well-informed decisions about their learning strategies going forward [45] as a result of increased self-awareness. The Feedback Sys- tem can be thought of as a mirror with which learners can view and react to their own, previously-invisible behavior. Since SRL skills are generalizable, the design should be ag- nostic to the content of each specific MOOC the feedback system is deployed in. We identified three key criteria for our system design:   Traceable: we can only provide feedback on behavior we can extract and derive from edXs log traces2;   Scrutable [20]: afford learners the ability to intuitively understand and explore the information presented;  2edX provides fine-grained log traces of each learners clicks & views, provided answers to assignments, forum interac- tions, etc.   Actionable: learners should be able to take action and change their behavior based on what they learn from the presented feedback.  After surveying the literature on learner model visualiza- tions, we settled on employing a single spider chart to visual- ize six metrics of learners behavior in relation to that of their successful peers, as shown in Figure 1. The spider charts key benefits include: (i) a single, embodied representation of multiple metrics, (ii) numerous indicators displayed in a small space, (iii) a simple representation of metricsdata is shown as single points along radial straight lines, and (iv) easily comparableinformation is represented as differently colored areas that can be layered [39].  In all four courses, the experimental conditions were not made explicitly known to the learners; the Feedback System appeared seamlessly integrated with the rest of the course materials.  We operationalized previously successful students, orrole models, as learners who earned a passing grade in the previ- ous edition of the MOOC (note that this setup requires that subsequent editions of the same MOOC have few changes). We updated the Feedback System every week (based on the learners activities on the platform in all weeks leading up to the current one) so that the learners could see an up-to- date representation of their activities as compared to that of the role models. The learners behaviors in the courses were tracked by the standard edX tracking log system.  Table 1: Overview of feedback metrics and alter- ations presented to learners in each MOOC. A  in- dicates the presence of the metric/alteration.  W a t e r X  U r b a n X  B u s i n e s s X  C a l c X  Feedback metrics Quiz submission timeliness (days)     Time on the platform (in hours)  Time watching videos (in hours)  Number of videos accessed  Number of quiz questions attempted     Proportion of time spent on videos while on the platform (in %)    Average time on the platform per week (in hours)    Number of revisited video lectures  Number of forum visits  Number of forum contributions  % of time spent on quizzes  Number of sessions per week   Mean session length (in minutes)   Mean time between sessions (in hours)   % of time-on-task - time spent on video-lecture, quiz or forum pages    Alterations Interactive visualization    Planning ahead    Feedback framing   In each MOOC, the Feedback System was placed in the Weekly Introduction unit of each course week so that it    would be readily available and immediately visible to learn- ers upon entering the new course week, enabling them to reflect on their SRL behavior so far. With the exception of the Feedback System, all learners received the same course materials, independent of the experimental condition.  Feedback metrics. Table 1 shows an overview of the feedback metrics given  to learners in each MOOC. At the end of each week in the course, the metrics were computed based on the log traces of all weeks prior. These metrics were chosen based on the following criteria: relevance to self-regulated learning, clar- ity/intuitiveness to the learner, and availability in the log data. For each metric, all values of previously successful learners were sorted and the top 5% and bottom 5% of values were discarded to remove outliers. The mean of the remain- ing values was computed, yielding a single value per metric  we consider this mean to be indicative of the tendency of the whole successful group of learners. We operationalize sessions as strings of activity with less than an hour gap between two events. As shown in Table 1, we used different feedback metrics in different MOOCs to explore the impact of the choice of metrics (H1.2 and H1.3).  Feedback System Alterations. Apart from the different metrics, we also explored three  refinements of the Feedback System:  1. Planning ahead: in WaterX the learners only received feed- back about their behavior up to now and how it compares to that of successful learners. In this alteration (in UrbanX and BusinessX), we also provide the learner with a visual- ization of the role models behavior (labelled as Average graduate this week in Figure 1) in the upcoming week, enabling learners to plan ahead instead of only reflect.  2. Interactive visualization: instead of a static feedback im- age (as provided in WaterX), in this alteration, we provide learners with an interactive visualization they can explore, i.e. mouse over the metrics to reveal exact numbers and comparisons (cf. Figure 1), and toggle on/off the metrics of the average successful learner for the upcoming week.  3. Cultural framing : in the first three MOOCs, the Feedback System provides no written interpretation of the visualiza- tion; instead learners are left to draw their own conclu- sions. In CalcX we additionally provide an explanatory text (as shown in Figure 1) that offers a clear interpreta- tion of the learners on-trackness.  4.3 Studies In each MOOC, we deployed a variation of the Feedback  System. Table 1 summarizes the feedback metrics and varia- tions deployed. For random assignment, we used a between- subjects design, where learners were assigned to either the control or a treatment condition and remained in this con- dition throughout the study. Table 2 shows a breakdown of the number of learners assigned to each condition for each MOOC. To gather baseline data from the first two weeks of each course, we released the Feedback System in the treat- ment conditions in the third week in each experiment. As noted before, the Feedback System is then updated on a weekly basis to reflect the updated learner activity data.  In the control condition across all experiments, learners did not receive the Feedback System. However, the edX platform offers a very basic form of learner feedback: a  Framing  Feedback Metrics  Planning Ahead  Interactive  Figure 1: The Feedback System as shown in the individualistic-promotional condition in CalcX, anno- tated for clarity.  Table 2: Overview of the number of learners enrolled and assigned to the control and treatment groups respectively. The number of active learners (having spent at least 5 minutes in the course platform) is in the parentheses beneath.  WaterX UrbanX BusinessX CalcX  Enrolled 10,943 (2,519)  8,137 (1,517)  2,352 (324)  12,294 (3,415)  Control Group 5,460  (1,268) 4,038 (771)  1,184 (164)  4,142 (1,150)  Treatment Group 1 5,483  (1,251) 4,099 (746)  1,168 (160)  4,087 (1,147)  Treatment Group 2    4,065  (1,118)  learner can visit her progress page and view the number of points scored so far in the course. This progress page is available to all learners, independent of their condition as- signment. In the treatment condition, learners received the Feedback System in addition to edXs progress page.  In all but one study there is only one treatment condition. In CalcX, we had two treatment conditions, one for each culture-specific framing of the explanatory feedback text:   CalcX treatment 1 received text with an individualistic promotion-focused framing;   CalcX treatment 2 received text with a collectivist prevention-focused framing.  We determined each learners cultural context based on the IP address used to access the course relying on Max- minds GeoIP lookup database3, as not all learners self- report their nationality. For learners with more than one IP address used, we consider the first one they used to ac- cess the course as their country.  3http://www.maxmind.com  http://www.maxmind.com   We developed a strong manipulation of the culture-specific framing by drawing on the cultural difference in (1) indi- vidualistic vs. collectivist appeals (collectivist cultures see the self embedded in a relational network, while the self- concept is more independent in individualist cultures), and (2) prevention- vs. promotion-focus (the prevention of neg- ative outcomes is emphasized over the promotion of positive outcomes in collectivist cultures, and vice versa for individ- ualist cultures) [13, 32, 35]. We designed two texts for each treatment group: one for learners who are on track (char- acterized by exhibiting similar behavior to that of the role model learners) and one for learners who are behind (char- acterized by exhibiting less course engagement compared to the role model learners). The texts (four overall) and how those texts align with a particular framing are shown in Ta- ble 3. The learners were evaluated as on-track or behind based on their on-trackness score, OT . The on-trackness score quantifies the similarity between a learners behavior and that of the previously successful learners: we normalize each metric to a value in the range [0, 10] (chosen for con- venience to work well in the spider chart setup) and then compute the difference, di, between the learners score on metric mi and the previously successful learners average score on mi. If di  1 mi, i = {1, .., 6} the learner is classified as behind, otherwise she is on-track  this is a very conservative classification, the learner has to have a lower engagement level on every single metric before she is considered as being behind.  The study design and all analyses conducted as part of the CalcX experiment4 were pre-registered through the Open Science Framework, vetted, and approved to meet the re- quirements of the Center for Open Science Preregistration Challenge5. All manuscripts, data, and scripts used for anal- ysis are available at: http://osf.io/ys6au.  4.4 Measures & Method of Analysis The primary outcome variable that we targeted with the  design of our Feedback System is course completion, which indicates that a learner achieved the required minimum pass- ing score on all summative quiz questions and thus earned a certificate. Course completion demonstrates sustained com- mitment to the course and mastery over the course material. The Feedback System is designed to support this type of sustained commitment and learning, even if individual stu- dent intentions may vary. The secondary outcome is to pro- mote SRL and meta-cognitive awareness. While many SRL processes are meta-cognitive and remain unobserved, it is possible to infer some of them based on learners logged ac- tions with the course materials [17, 24, 41, 42]; for example, goal-setting & planning, time management, self-monitoring, and social comparison.  For non-binary measures, to test if differences between experimental conditions are statistically significant, we used the non-parametric Kruskal-Wallis test, because these mea- sures were not normally distributed and exhibited unequal variances across conditions. For binary measures, we tested differences in proportion using a 2 test. We present the results of each test by each groups mean and median along with the 2 value, degrees of freedom, and level of statistical  4We pre-registered this experiment because it was the fourth and final study of the present research and included an added manipulated variable in the cultural framing. 5https://cos.io/prereg/  significance. Due to the commonly high levels of attrition in MOOCs (65%-74% of learners never returned to the course after enrolling in one of our four MOOCs), the subsequent analyses only consider data generated by active learners. We define active learners as those having spent at least five min- utes on the course platform. See Table 2 for the breakdown of registered vs. active learners per MOOC.  5. RESULTS We present our findings for the Research Questions out-  lined in Section 4.1. We discuss the impact of the Feedback System on course completion and engagement in Sections 5.1 & 5.2, heterogeneous treatment effects of the Feedback System in Section 5.3, and lastly in Section 5.4, we compare the effects for different cultural framings of the feedback.  5.1 Course Completion We hypothesized that the Feedback System will increase  learner achievement in terms of course completion (H1.1). Table 4 shows the completion rates in all conditions for the first three experiments. The completion rate is consistently higher in the treatment condition than in the control con- dition in all experiments. Pooling across experiments, we observed an increase in the completion rate from 15.5% to 18.9% (2 = 5.87, p = 0.008). Thus, regarding hypothesis H1.1, we conclude:  The Feedback System significantly increases course completion rates in MOOCs.  In the fourth experiment, which tested two treatment con- ditions with different cultural framings against the control of not providing the Feedback System, we also observed higher completion rates in the treatment conditions (Ta- ble 5). However, this difference was not statistically sig- nificant (ps > 0.25). However, the overall completion rate in the CalcX course was extremely low (1.7%). This sug- gests that the sample is drawn from a population of less committed learners and that potential effects could be ob- fuscated by high levels of unexplained variance in completion outcomes. Another contributing factor to this rift between CalcX and the other three courses is the fact that CalcX was self-paced (content released all-at-once), whereas the others were instructor-paced (content released weekly), thus pro- viding less structure/support to the learners.  Moreover, we hypothesized that showing certain combina- tions of feedback metrics will better promote positive changes in behavior than others (H1.3). We explored this by chang- ing the (combination of) metrics in each of the four itera- tions of the Feedback System (see Table 1). Given that the course completion rates increased across all four iterations each with a different combination of feedback metrics (with two of the six metricsquiz submission timeliness (how far ahead of the deadline responses were submitted) and quiz questions attemptedwere present in all four) we conclude:  Each combination of metrics shown to the learners pro- duced increases in completion.  5.2 Engagement In light of the positive effect of the Feedback System on  course completion, we next evaluated specific changes in learner behavior corresponding to the behavioral metrics that were visualized in the Feedback System (H1.1). These  http://osf.io/ys6au https://cos.io/prereg/   Table 3: Overview of the supplementary texts the treatment groups received in CalcX, depending on their performance in the course so far (either on track or behind). The alignment of the words and phrases with the intended framing is highlighted. Sentences prefixed by  are directly addressed at the individual (individualistic framing). Best viewed in color.  Treatment Group 1 Treatment Group 2  ( individualistic promotional framing) ( collectivist prevention framing)  On track Looks like youre right on track to achieve your  goal! Keep taking advantage of the exciting new  topics each week. Always push yourself to  be successful .  Looks like youre keeping up with the course  for now ! Were doing our best to introduce you  to exciting new topics each week. Please  dont let us down now .  Behind Looks like youre a bit behind in achieving your  goal!  Work harder to take advantage of the ex-  citing new topics each week. Always push yourself  to be successful .  Looks like youre a bit behind in the course right  now! Were doing our best to introduce you  to exciting new topics each week. Please  dont let us down now .  Table 4: Course completion rates across the first three studies among the active learners. Overall, the difference in completion rate between the groups is statistically significant (p = 0.008).  Condition N # Pass Pass Rate  WaterX Control 1,268 160 12.6% Treatment 1,251 188 15.0%  UrbanX Control 771 136 17.6% Treatment 746 165 22.1%  BusinessX Control 164 46 28.0% Treatment 160 54 33.8%  Overall Control 2,203 342 15.5% Treatment 2,157 407 18.9%  Table 5: Course completion rates in the CalcX course among active learners. A binomial test of indepen- dent proportions revealed no statistically significant differences between the three conditions.  Condition N # Pass Pass Rate  Control 1,150 45 3.91% Indiv.-Promotion 1,147 62 5.41% Collect.-Prevention 1,118 51 4.56%  metrics, which varied across experiments, were most likely to be directly affected through social comparison. Table 6 shows the results of Kruskal-Wallis tests6 comparing the var- ious feedback metrics between the treatment and control groups in study to test H1.2. A common thread across the three experiments was that of the Feedback System in- creased the number of summative quiz questions that learn- ers submitted, which directly promotes course completion.  Looking at each feedback metric individually in Table 6, we observe 15 out of 18 times an improvement from control  6While the Kruskal-Wallis test measures the difference be- tween rank orders, the median values are often zero, so in the table we show the mean for better context.  Table 6: Results of the Kruskal-Wallis tests for the behavior metrics (feedback metrics) provided in the Feedback System for WaterX, UrbanX, and BusinessX. Statistically significant differences are in bold.  Metric Ctrl Treat. 2 p x x  W a t e r X  quiz questions attempted 4.2 4.7 4.46 0.04 videos accessed 7.0 7.0 0.01 0.94 time on platform (hours) 4.5 4.6 0.17 0.68 time watching videos (hours) 0.8 0.8 0.04 0.85 ratio video/total time (%) 25.0 25.0 0.11 0.75 submission timeliness (days) 27.9 31.3 4.20 0.04  U r b a n X  quiz questions attempted 5.7 6.6 3.16 0.08 sessions per week 3.8 4.0 2.11 0.15 avg. session length (minutes) 8.1 8.2 0.18 0.67 time between sessions (hours) 117.0 120.0 0.29 0.59 forum visits 2.7 3.0 2.88 0.09 submission timeliness (days) 28.3 32.0 3.27 0.07  B u s i n e s s X  quiz questions attempted 21.4 25.3 3.97 0.05 sessions per week 0.5 0.7 4.89 0.02 avg. session length (minutes) 32.8 46.7 8.42 0.00 time between sessions (hours) 95.9 92.2 1.17 0.28 time-on-task (%) 64.3 67.5 0.32 0.57 submission timeliness (days) 19.9 21.4 1.12 0.29  to treatment condition7; three times no change is observed. The treatment condition does not lead to a worse effect in any feedback metric. While only a handful of these differ- ences are statistically significant, this consistency lends itself to some explanatory power over the statistically significant increases in course completion rates: while on an individual level, only some metrics show significant increases as a re- sult of the Feedback System, on a macro levelthat which accounts for a learners overall activity in the coursewe infer that these small increases in engagement all effectively coalesce into a boost in desirable behavior that leads to in- creased completion rates. We draw the following conclusion:  The Feedback System causes desirable changes in learner engagement.  7A high time between sessions score is not better per se, but it indicates a desirable high-spacing learning routine    Table 7 shows the results of the same analysis on the en- gagement metrics across the three conditions in CalcX; the results are less consistent.  In H1.2, we hypothesize that learners change aspects of their behavior that are reflected back to them in the Feed- back System. Since there is no consistency among signifi- cant increases in the provided behavior metrics, we conclude:  Learners do not change specific behaviors based on what metrics are shown in the Feedback System.  5.3 Who benefited from the feedback Going beyond average treatment effects of the Feedback  System, we now evaluate heterogeneous treatment effects, that is, how the feedback affects different groups of learn- ers (RQ2). Specifically, we focus on heterogeneity by prior education level, as this might determine learners ability to use the information provided in the Feedback System. We gather learners prior education levels from their edX user profile; learners who do not report their education level are omitted from this analysis. We define high prior education learners as those with a Bachelors, Masters, or PhD degree, and low prior education learners as those with any degree below Bachelors. Table 8 compares the average final grades in the control and treatment conditions of the first three courses separately for high vs. low prior education learners.  In WaterX, UrbanX and BusinessX we observed a consis- tent increase in final grades for highly educated learners, but not for less educated learners. However, this pattern did not replicate in the CalcX course, as education level did moderate the effect on grades (p = 0.82)8. Nevertheless, the results for CalcX are harder to interpret due to the rel- atively low completion rate in this course. Moreover, CalcX stands out in that a majority of low prior education learners were enrolled in this course, while the WaterX, UrbanX and BusinessX courses had a majority of high prior education learners. Based on these analyses, we conclude that:  The Feedback System only helps to improve the achievement (final grade) of learners who are already highly educated.  This finding suggests three possibilities: (i) the Feedback System is too complex for people falling in the low prior ed- ucation category to understand, (ii) highly educated learn- ers are better able to synthesize the information offered by the Feedback System and translate it into positive behav- ior as they are already experienced learners (with at least some SRL skills), and/or (iii) less educated learners are not concerned with obtaining a certificate, but rather focus on knowledge acquisition.  5.4 Framing Feedback to Cultural Contexts In the CalcX course, we tested H3.1 and H3.2 about  supplementing the Feedback System with culture-specific feedback. As before, we evaluated each hypothesis both in terms of learner achievement and engagement. All pre- registered analyses for this experiment are reported in Sec- tion 5.4.1. Additional exploratory analyses are reported in Sections 5.4.3 and 5.4.2. 8Once more we report CalcX separately due to the overall difference in completion rate compared to WaterX, Busi- nessX and UrbanX as shown in Tables 4 & 5.  5.4.1 Pre-registered: Completion & Engagement We compared the completion rate and six behavioral mea-  sures (the ones shown in the Feedback System) between the treatment and control conditions separately by learners cultural context. To address H3.1, we segmented learners into three groups of individualismhigh, balanced, and low individualismand compared completion rates of learners in high vs. low individualism cultures in each condition. There was no significant increase in completion rates for ei- ther feedback framing, neither for learners in low individual- ism cultures nor for those in high individualism cultures (all p > 0.12). Likewise, we tested for treatment effects in con- texts defined by cultural tightness (H3.2) and also found no significant increase in completion rates (all p > 0.29). Results for learner engagement were also not significant (cf Table 7). Finally, we tested the moderating role of educa- tion level, as in the prior experiments (RQ2), but found no evidence in support of moderation (2 = 0.40, p = 0.82). We thus conclude that:  Supplementing the Feedback System with feedback framing tailored to cultural tendencies of individualism and tightness does not increase learners course achieve- ment or engagement.  5.4.2 Increased Active  Threshold From the exceptionally low completion rate of CalcX, we  gathered that a high proportion of uncommitted learners rendered the data set noisy. Whereas the WaterX, UrbanX, and BusinessX experiments yielded a consistent main ef- fect on course completion, this effect was not detectable in the CalcX experiment. To focus our analysis in CalcX on more committed learners, we imposed a stricter threshold foractive learners. Considering only learners who accessed the course platform for at least an average of 1hr/week, we proceeded by analyzing data for highly active learners (n = 658). This threshold is reasonable given the amount of course content per week (between 68 hours). Moreover, the overall completion rate in this sample was 15.65%, a similar rate as in the other experiments.  Among highly active learners, we find that the individ- ualist framing increased completion rates regardless of a learners own cultural context from 12.8% in the control con- dition to 19.9%, a 7.1 percentage point increase (t = 2.02, p = 0.04). Moreover, we find that the effect of the indi- vidualistic framing was especially large for learners in tight cultures, effectively tripling the completion rate from 12.1% in the control condition to 36.3% (t = 2.07, p = 0.04). We therefore conclude that:  The individualist framing was most effective in increas- ing course completion rates overall, and especially for learners in tight cultures.  The effect of the individualist framing is surprising in terms of its large magnitude and cultural heterogeneity. We expected the individualist framing to resonate in loose rather than tight cultures. Perhaps the individualist framing is more congruent in an environment where learners tend to be anonymous and socially isolated. Learners in tight cultures were also more likely to benefit as there course performance was generally lower, as discussed next.    Table 7: Results of the Kruskal-Wallis tests for CalcX. Statistically significant differences indicated in bold.  Metric Ctrl Treat.1 2 p Ctrl Treat.2 2 p Treat.1 Treat.2 2 p x x x x x x  avg. time/week (minutes) 31.9 33.4 1.38 0.24 31.9 33.8 0.11 0.74 33.4 33.8 2.16 0.14 revisited lectures 3.44 3.62 0.59 0.44 3.44 3.42 0.35 0.55 3.62 3.42 1.88 0.17 forum posts 0.34 0.53 0.03 0.86 0.34 0.36 4.01 0.05 0.53 0.36 3.31 0.06 quiz questions attempted 31.3 32.4 1.52 0.22 31.3 33.5 0.09 0.77 32.4 33.5 2.24 0.13 time on quizzes (%) 37.0 34.0 5.08 0.02 37.0 36.4 0.15 0.70 34.0 36.4 3.30 0.07 submission timeliness (days) 47.48 45.70 1.31 0.25 47.48 46.71 0.84 0.36 45.70 46.71 0.04 0.83  Table 8: Mean final grades (out of a possible 100 points) grouped by prior education levels. The Prior Education column indicates the highest de- gree the learner has earned; N is the sample size; and p shows the result of a Kruskall-Wallis test. Significant values are in bold.  Course Prior N Ctrl Treat. p Education x x  WaterX High 2,006 13.2 15.7 0.15 Low 788 11.8 11.5 0.16  UrbanX High 1,337 17.4 21.3 0.04 Low 438 16.4 14.0 0.66  BusinessX High 299 23.7 29.1 0.04 Low 92 21.4 22.8 0.78  OVR High 3,642 16.3 19.5 <0.01 Low 1,318 14.4 13.6 0.36  5.4.3 Lower Achievement in Tight Cultures In the preceding analyses, we observed a notable cultural  difference along the tightness dimension. Pooling across ex- perimental conditions in the CalcX course, we found for ev- ery metric (cf. Table 1) with the exception of number of fo- rum posts that learners in tight cultures exhibit significantly higher levels of achievement and engagement than those in loose cultures (ps 0.02). We repeated the analysis for the other three courses and found the same cultural differences. We therefore conclude:  Learners from countries with low cultural tightness sig- nificantly outperform their peers from countries of high cultural tightness in terms of both engagement (all p- values p0.1) and achievement (p0.02).  This cultural difference in performance could arise from the nature of the MOOC learning experience. MOOCs pro- vide significant latitude for different levels of commitment and engagement; in fact, learners can come and go as they please at no cost. This may especially appeal to loose cul- tures, where there are few strongly-enforced rules and high tolerance for deviation. In contrast, traditional classroom environments with strict attendance and performance poli- cies would align more with the ideals of tight cultures. Al- ternatively, the current finding may reflect structural differ- ences that are associated with both tightness and perfor- mance, such as infrastructure and education levels.  6. CONCLUSION This research tested the effect of providing online learners  with personalized feedback in four large-scale randomized  controlled experiments in MOOCs. The Feedback System was designed to promote learners awareness of both their own SRL behavior and that of their successful peers through social comparison. It significantly increased course comple- tion rates across different courses. The combination of be- havior metrics that was shown to learners in the Feedback System did not determine the significance of the effect on course completion, highlighting a need for further research on the optimal set of metrics to show. Moreover, we discov- ered that the Feedback System primarily benefited highly educated learners, although the system was envisioned to support those who struggle with self-regulation. This sug- gests a new challenge for MOOC researchers and designers to make targeted interventions that support learners who are less educated and need more support.  As online courses can be culturally diverse learning envi- ronments, we investigated how the Feedback System could be adapted to resonate with learners from different back- grounds. Our pre-registered analyses yielded no significant effects of changing the cultural framing of the feedback. In exploratory analyses, however, we found strong benefits of framing feedback with an individualistic and promotion fo- cus. This insight warrants further research to establish its generalizability. Aside from our intervention, we found that learners from loose cultures consistently outperformed learn- ers tight cultures in terms of course engagement and final grades. In light of the two sources of heterogeneity we iden- tified, future MOOC interventions may be strengthened by personalization based on learners prior education level and cultural context.  In future work, we plan test a different feedback interface design that presents a set of different personas that learn- ers can identify with, such as person who works a bit every day and one who works a lot over the weekend. We will also evaluate new approaches for feedback messages to bet- ter support learners with different cultural and educational backgrounds.  7. REFERENCES [1] S. Bateman, J. Teevan, and R. W. White. The search  dashboard: how reflection and comparison impact search behavior. In CHI 12, pages 17851794, 2012.  [2] H. Blanton, B. P. Buunk, F. X. Gibbons, and H. Kuyper. When better-than-others compare upward: Choice of comparison and comparative evaluation as independent predictors of academic performance. Journal of personality and social psychology, 76(3):420, 1999.  [3] S. Bull and J. Kay. Open learner models. In Advances in intelligent tutoring systems, pages 301322. Springer, 2010.  [4] S. Bull and J. Kay. Open learner models as drivers for metacognitive processes. In International handbook of metacognition and learning technologies, pages 349365. Springer, 2013.  [5] R. Cook and J. Kay. The Justified User Model: A Viewable, Explained User Model. In UM 94, pages 145150, 1994.    [6] J. Crabtree and A. Rutland. Self-evaluation and social comparison amongst adolescents with learning difficulties. Journal of Community & Applied Social Psychology, 11(5):347359, 2001.  [7] D. Davis, G. Chen, I. Jivet, C. Hauff, and G. J. Houben. Encouraging Metacognition & Self-Regulation in MOOCs through Increased Learner Feedback. In Workshop on Learning Analytics for Learners, 2016.  [8] L. Festinger. A theory of social comparison processes. Human relations, 7(2):117140, 1954.  [9] M. J. Gelfand, J. L. Raver, L. Nishii, L. M. Leslie, J. Lun, B. C. Lim, L. Duan, A. Almaliach, S. Ang, J. Arnadottir, et al. Differences between tight and loose cultures: A 33-nation study. Science, 332(6033):11001104, 2011.  [10] J. Guerra, R. Hosseini, S. Somyurek, and P. Brusilovsky. An Intelligent Interface for Learning Content: Combining an Open Learner Model and Social Comparison to Support Self-Regulated Learning and Engagement. In IUI 16, pages 152163, 2016.  [11] P. J. Guo and K. Reinecke. Demographic differences in how students navigate through MOOCs. In L@S 14, pages 2130, 2014.  [12] J. Hattie. Visible learning: A synthesis of over 800 meta-analyses relating to achievement. Routledge, 2008.  [13] E. T. Higgins. Promotion and prevention: Regulatory focus as a motivational principle. Advances in experimental social psychology, 30:146, 1998.  [14] G. Hofstede, G. J. Hofstede, and M. Minkov. Cultures and organizations: Software of the mind. 1991.  [15] P. Huguet, F. Dumas, J. M. Monteil, and N. Genestoux. Social comparison choices in the classroom: Further evidence for students upward comparison tendency and its beneficial impact on performance. European journal of social psychology, 31(5):557578, 2001.  [16] P. Huguet, M. P. Galvaing, J. M. Monteil, and F. Dumas. Social presence effects in the stroop task: further evidence for an attentional view of social facilitation. Journal of personality and social psychology, 77(5):1011, 1999.  [17] Y. Jo, G. Tomar, O. Ferschke, C. P. Rose, and D. Gasevic. Expediting Support for Social Learning with Behavior Modeling. In EDM 16, pages 400405, 2016.  [18] D. F. Kauffman. Self-regulated learning in web-based environments: Instructional tools designed to facilitate cognitive strategy use, metacognitive processing, and motivational beliefs. Journal of educational computing research, 30(1-2):139161, 2004.  [19] J. Kay. Learner know thyself: Student models to give learner control and responsibility. In ICCE 97, pages 1724, 1997.  [20] J. Kay. Stereotypes, student models and scrutability. In International Conference on Intelligent Tutoring Systems, pages 1930, 2000.  [21] H. Khalil and M. Ebner. Moocs completion rates and possible methods to improve retention-a literature review. In World Conference on Educational Multimedia, Hypermedia and Telecommunications, number 1, pages 13051313, 2014.  [22] R. F. Kizilcec and G. L. Cohen. An 8-minute self-regulation intervention improves educational attainment at scale in individualist but not collectivist cultures. Working Paper, 2017.  [23] R. F. Kizilcec and S. Halawa. Attrition and achievement gaps in online learning. In L@S 15, pages 5766, 2015.  [24] R. F. Kizilcec, M. Perez-Sanagustn, and J. J. Maldonado. Self-regulated learning strategies predict learner behavior and goal attainment in massive open online courses. Computers & Education, pages 1833, 2016.  [25] R. F. Kizilcec, C. Piech, and E. Schneider. Deconstructing disengagement: analyzing learner subpopulations in massive open online courses. In LAK 13, pages 170179. ACM, 2013.  [26] R. F. Kizilcec and E. Schneider. Motivation As a Lens to Understand Online Learners: Toward Data-Driven Design with the OLEI Scale. TOCHI, 22(2):6:16:24, 2015.  [27] M. Leon, R. Cobos, K. Dickens, S. White, and H. Davis. Visualising the MOOC experience: a dynamic MOOC dashboard built through institutional collaboration. In EMOOCs 16, pages 461470, 2016.  [28] J. M. Levine. Social comparison and education. pages 329, 1983.  [29] A. Littlejohn, N. Hood, C. Milligan, and P. Mustain. Learning in moocs: Motivations and self-regulated learning in moocs. The Internet and Higher Education, 29:4048, 2016.  [30] Z. Liu, R. Brown, C. F. Lynch, T. Barnes, R. Baker, Y. Bergner, and D. McNamara. Mooc learner behaviors by country and culture; an exploratory analysis. In EDM 16, pages 127134, 2016.  [31] P. Lockwood, D. Dolderman, P. Sadler, and E. Gerchak. Feeling better about doing worse: social comparisons within romantic relationships. Journal of personality and social psychology, 87(1):80, 2004.  [32] P. Lockwood, T. C. Marshall, and P. Sadler. Promoting success or preventing failure: Cultural differences in motivation by positive and negative role models. Personality and Social Psychology Bulletin, 31(3):379392, 2005.  [33] R. M. Maldonado, J. Kay, K. Yacef, and B. Schwendimann. An interactive teachers dashboard for monitoring groups in a multi-tabletop learning environment. In ITS 12, pages 482492, 2012.  [34] A. Margaryan, M. Bianco, and A. Littlejohn. Instructional quality of MOOCs. Computers & Education, 80:7783, 2015.  [35] H. R. Markus and S. Kitayama. Culture and the self: Implications for cognition, emotion, and motivation. Psychological review, 98(2):224, 1991.  [36] K. A. Papanikolaou. Constructing interpretative views of learners interaction behavior in an open learner model. IEEE Transactions on Learning Technologies, 8(2):201214, 2015.  [37] Y. Park and I.-H. Jo. Development of the learning analytics dashboard to support students learning performance. J. UCS, 21(1):110133, 2015.  [38] T. Rogers and A. Feller. Discouraged by Peer Excellence: Exposure to Exemplary Peer Performance Causes Quitting. Psychological Science, 27(3):365374, 2016.  [39] M. J. Saary. Radar plots: a useful way for presenting multivariate health care data. Journal of clinical epidemiology, 61(4):311317, 2008.  [40] B. A. Schwendimann, M. J. Rodrguez-Triana, A. Vozniuk, L. P. Prieto, M. S. Boroujeni, A. Holzer, D. Gillet, and P. Dillenbourg. Understanding learning at a glance: An overview of learning dashboard studies. In LAK 16, pages 532533, 2016.  [41] M. Siadaty, D. Gasevic, and M. Hatala. Measuring the impact of technological scaffolding interventions on micro-level processes of self-regulated workplace learning. Computers in Human Behavior, 59:469482, 2016.  [42] M. Siadaty, D. Gasevic, and M. Hatala. Trace-based micro-analytic measurement of self-regulated learning processes. Journal of Learning Analytics, 3(1):183214, 2016.  [43] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. L. Santos. Learning analytics dashboard applications. American Behavioral Scientist, 57(10):15001509, 2013.  [44] S. Zheng, M. B. Rosson, P. C. Shih, and J. M. Carroll. Understanding student motivation, behaviors and perceptions in moocs. In CSCW 15, pages 18821895, 2015.  [45] B. J. Zimmerman. Becoming a self-regulated learner: An overview. Theory into practice, 41(2):6470, 2002.    Introduction  Background  MOOC Overview  Approach  Research Questions  Feedback System Design  Studies  Measures & Method of Analysis   Results  Course Completion  Engagement  Who benefited from the feedback  Framing Feedback to Cultural Contexts  Pre-registered: Completion & Engagement  Increased ``Active  Threshold  Lower Achievement in Tight Cultures    Conclusion  References   "}
{"index":{"_id":"58"}}
{"datatype":"inproceedings","key":"Yeomans:2017:PPI:3027385.3027416","author":"Yeomans, Michael and Reich, Justin","title":"Planning Prompts Increase and Forecast Course Completion in Massive Open Online Courses","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"464--473","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027416","doi":"10.1145/3027385.3027416","acmid":"3027416","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, decision-making, goal pursuit, learning analytics, motivation, natural language processing","Abstract":"Among all of the learners in Massive Open Online Courses (MOOCs) who intend to complete a course, the majority fail to do so. This intention-action gap is found in many domains of human experience, and research in similar goal pursuit domains suggests that plan-making is a cheap and effective nudge to encourage follow-through. In a natural field experiment in three HarvardX courses, some students received open-ended planning prompts at the beginning of a course. These prompts increased course completion by 29%, and payment for certificates by 40%. This effect was largest for students enrolled in traditional schools. Furthermore, the contents of students' plans could predict which students were least likely to succeed - in particular, students whose plans focused on specific times were unlikely to complete the course. Our results suggest that planning prompts can help learners adopted productive frames of mind at the outset of a learning goal that encourage and forecast student success.","pdf":"Planning Prompts Increase and Forecast Course Completion in  Massive Open Online Courses  ABSTRACT  Among all of the learners in Massive Open Online Courses  (MOOCs) who intend to complete a course, the majority fail to do  so. This intention-action gap is found in many domains of human  experience, and research in similar goal pursuit domains suggests  that plan-making is a cheap and effective nudge to encourage  follow-through. In a natural field experiment in three HarvardX  courses, some students received open-ended planning prompts at  the beginning of a  course. These prompts increased course  completion by 29%, and payment for certificates by 40%. This  effect was largest for students enrolled in traditional schools.  Furthermore, the contents of students plans could predict which  students were least likely to succeed - in particular, students  whose plans focused on specific times were unlikely to complete  the course. Our results suggest that planning prompts can help  learners adopted productive frames of mind at the outset of a  learning goal that encourage and forecast student success.   CCS Concepts   Applied computing~Psychology    Applied  computing~Distance learning   General Terms  Decision-Making, Goal Pursuit, Natural Language Processing   Keywords  MOOCS; Learning Analytics; Motivation    Copyright Information   1. INTRODUCTION  The human mind has an incredible capacity to look into the future,  set goals, and plan for action [1]. But in many essential domains,  this goal-setting often outpaces goal-achieving. While the link  between intention and action is strong, often it is not as strong as  we would like.  The intention-action gap is one of the distinctive features of open  online education [10]. Since the first Massive Open Online  Courses (MOOCs) were created at Stanford in 2011, over 35  million students have enrolled in one of 4,200 courses offered by  over five hundred universities worldwide [42]. These courses  offer access to university-level instruction from elite institutions  to anyone in the world, free of charge, and they hope to transform  the production function of higher education [4]. But despite this  growing interest and attention, it is still true that the vast majority  of students who have enrolled in MOOCs do not finish.    More importantly, the majority of students with a stated intention  to complete a MOOC do not finish. Among students enrolled in  HarvardX and MITx courses, who declared at enrollment that they  intended to finish their course, only 22% did so [16, 37]. In many  domains, research suggests that nudges - simple, inexpensive  psychological supports - can be used to help people address their  intention-action gaps [46]. To what extent can these same  approaches be deployed to help students achieve their stated  goals  In the current research we address this question in a natural field  experiment, by prompting some MOOC students to plan their  course participation in advance. Planning prompts encourage goal  pursuers to elaborate on their implementation strategies while  their intentions are still vivid [12, 39]. This intervention has been  successful in other domains where the benefits of follow-through  lie far in the future, but where goal pursuit is easily derailed in the  present [29, 30]. MOOCs are minimally structured by design, so  they may be especially vulnerable to obstacles during  implementation [6]. This diagnosis implies that MOOCs might be  particularly responsive to planning prompts.  Voluntary online courses provide a compelling new setting in  which to test theories of planning in goal pursuit. MOOCs require  persistence over multiple assignments, months apart, testing the  effects of planning over a much longer time scale than in previous  research. The online platform also provides exact measurements  of planners subsequent activities, and allows us capture the full  text of thousands of student plans, creating a corpus of student  natural language that can be parsed and evaluated to determine  which kinds of plans were most likely to be successful. These data  provide a novel and comprehensive view of how planning can  increase follow-through, in online education and in other domains  where people struggle with persistence, and firms struggle with  retention.   Michael Yeomans  Harvard University   Cambridge, MA  yeomans@fas.harvard.edu   Justin Reich  Massachusetts Institute of Technology   Cambridge, MA  jreich@mit.edu  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027416   mailto:jreich@mit.edu   2. BACKGROUND LITERATURE  This research draws from two disparate bodies of work, for which  we now provide a brief review. First, we describe how planning  prompts have been used to increase follow-through in other  domains, and how MOOCs provide a novel extension of that  literature. Second, we describe the budding literature on online  education, with a particular focus on how MOOCs might serve as  a model of goal pursuit, which motivates our formal hypotheses.   2.1. Planning Prompts  Planning is central to social psychological models of goal pursuit,  as the path by which current intentions can be translated into  future action [1]. Planning works because even when intentions  are strong, many long-term goals fail from a lack of  implementation [12]. These implementation factors are often  overlooked when people forecast their future goal pursuit [25, 28,  36].  The theoretical motivation for planning prompts, then, is to spur  attention to these implementation factors while intentions are  strong [39]. Elaborating on implementation can help plans last  when intentions fade and are overcome by forgetfulness [40] and  procrastination [33]. Recent field applications have shown that  planning prompts are a low-cost way to encourage follow-through  in social beneficial domains like voter turnout, flu shots, and  colonoscopies [29, 30, 31].  Online education offers a unique opportunity to study the effect of  planning prompts on goal pursuit. MOOCs require sustained effort  over months, in contrast to goals pursued within a single lab  session, or behaviors (like a doctors visit) which involve a single  plan, enacted once in the future. Additionally, all goal relevant  behavior is passively and automatically tracked by the server logs  of the MOOC platform, which precludes common measurement  problems like attrition, demand effects, social pressure, or self- report bias, which compromise plan-making results from the lab.  Finally, participation in MOOCs is endogenous - students choose  to enroll of their own accord. By contrast, lab experiments rely on  extrinsic rewards for participation, such as course credit or money.  And previous field settings simply cold-called (or cold-mailed)  participants, so it was not clear whether plan-making encouraged  follow-through, or was simply an effective persuasion technique.  All participants in this research enrolled in these courses of their  own accord, and students explicitly reported their intentions, so  that we could test intentions as a potential moderator [43]. Put  simply: we could be reasonably sure our students were there to  learn.   2.2. Goal Pursuit in MOOCs  Our research was conducted on EdX, a MOOC platform that has  served more than ten million students. MOOCs like these provide  new kinds of educational opportunities by making elite college  courses widely available for free or minimal cost. But their  ultimate value to higher education institutions remains unclear. In  particular, the consistently low completion rates - typically less  than 10% of enrolled students - call into question their  effectiveness [10]. One valid response is to point out that many  students only intend to browse, rather than finish the required  assignments. Indeed, browsers account for a large percentage of  so-called drop outs, whether assessed using tracking data [2, 19,  27] or by asking students to declare their intentions upfront [16,  37]. But evidence still shows that even when students explicitly  intend to finish, follow-through failures are common.    These results have prompted two responses. One response is to  consider whether psychologically-informed interventions might  be used to align the design of MOOCs with their students goals  and increase course completion. However, this nascent literature  has mainly produced null results [20, 23, 50]. One encouraging  result shows that participation badges can increase forum  participation [2], which is consistent with the hypothesis that a  lack of structure is at least partly responsible for drop-outs [6].  These results lead us to the prediction that planning prompts  might be a particularly effective way to nudge student follow- through.  Hypothesis 1: Prompting MOOC students to plan their efforts at  the start of a course will increase completion rates.  A second response to low MOOC completion rates has focused on  modeling the heterogeneity in course completion, to better  understand why some students do not follow through on their  goals. Forecasting from demographics has been shown previously  [21], but that exercise is intrinsically limited - demographics  cannot be manipulated by interventions, and provide a sparse  understanding of how students life situation affects their ability to  follow through on their goal.  Some understanding into course dynamics can gained from course  activity logs [2, 24, 48, 53]. But is unclear whether these results  meet the epistemological definition of forecasting - that is, do  activity logs anticipate drop-outs in advance, or merely reveal  them as they happen A similar concern arises from forecasts  based on in-class discussion boards - posts may simply reveal  which students are encountering difficulties, rather than  anticipating potential difficulties [52, 54].  In contrast, planning prompts might forecast students engagement  at the moment they start the course. This timing is essential in  practice, because most drop outs occur in the first week of the  class [10, 16]. This means that targeted interventions will be most  successful if they can be deployed early, while students attention  is still piqued. Furthermore, open-ended plans might provide a  richer forecast of students behavior and their expected obstacles.  This might also lay the groundwork for personalized learning,  allowing the course platform to adapt to students individual plans  for goal pursuit [9, 26, 41].  Hypothesis 2: Course completion can be forecasted from the  content of MOOC students plans at the start the course.   3. METHODS   3.1. Experimental Setting  This research was conducted in three online courses created by  HarvardX, teaching Business, Chemistry and Political Science.  Each course was taught by a Harvard professor, and paralleled an  existing course at the University. The course material consisted of  video lectures, assigned readings, and discussion boards, and  chapters of the course were doled out in sequence over 2-3  months. Grades were determined by a combination of quizzes,  peer-assessed written assignments, and self-assessed participation.  Participation was free, though students were given an option to  pay for a verified certificate, where assessments were remotely  proctored and their identity was confirmed.   These are reported as Study 1 (Business) and Study  2 (Chemistry and Political Science), because some elements of  the survey design in Study 1 were modified before the roll-out in  Study 2. Our preregistration describes the methods in Study 2, but  was posted before the outcome of Study 1 could be observed, and  we follow the same analyses throughout. For completeness, we     report the results of analyses in both studies, individually and  pooled. We also report how we determined our sample size, all  data exclusions, all conditions, and all measures in every study.  Additionally, our data, code, materials and preregistered analysis  plan are available at https://osf.io/mky8n/.   3.2. Data Collection  The MOOC platform records every action that every student takes  on the course platform, including enrollment, verification, and  grades. This ensures that course progress can be tracked  accurately, exhaustively, and without any effort or awareness on  the part of the student. This also produces many possible outcome  measures from a MOOC, and we discuss several in this research.  However, our pre-registered analysis plan focused on only one  primary outcome: whether or not students completed enough of  the coursework to earn a certificate in their class. This requires  earning a grade above a certain threshold (between 70-80%,  depending on the class), and if they achieve the threshold before  the final date, they are deemed to have certified in the class.  3.2.1. Course Grades  To achieve a certificate the class, students completed assignments  and quizzes at the end of each chapter in the courses, as well as  some peer-grading (Study 1) and some participation credit (Study  2). Students could track some of their progress on the website, but  final grades and certificates were not handed out until after the  class had closed. The distribution of grades was bimodal - 49% of  students in our sample had a grade of zero, while 16% earned a  passing grade, and the rest were scattered in between.  3.2.2. Certificate Verification  During the first half of each course, all students had the option of  paying ($50-100 USD) for a verified certificate,  that also  involved a formal identity check. By the end of the course, 6% of  students in our sample were verified. However, 3.1% verified  when they enrolled, before they saw the planning prompts. The  other 2.9% upgraded their account during the course, and after the  planning prompts. After pre-registration, we determined that  verification rates were an outcome of increasing interest to  MOOC stakeholders, and investigated this outcome in an  exploratory manner. Specifically, we were encouraged by new  datasets that distinguished two kinds of verifications in our  sample: pre-course enrollments, as a pre-treatment covariate; and  in-course upgrades, as a post-treatment outcome.  3.2.3. Pre-Course Survey   Every class run by HarvardX has a pre-course survey embedded  as the first chapter of the course. The pre-course survey is  optional, and has no effect on grades but it is encouraged as a  way to get to know our students. The majority of enrolled  students did not attempt the pre-course survey. However, the vast  majority of those students did not complete much else. These  results are typical [37], and suggest that the pre-course survey  provides good coverage of the students who actually participate in  the course.  The pre-course survey has two purposes in this research. First, the  survey contained our planning prompt treatments. Second, the  pre-course survey also collected information about demographic  and behavioral covariates, as part of a standard battery of survey  questions included in all HarvardX classes. These answers were  used to define our exclusion criteria, and allowed us to test for  treatment effect heterogeneity.   3.3. Population of Interest   We decided ex ante to analyze only a subset of the 60,778 students  who enrolled in these three courses. All of our primary analyses  below follow these pre-registered exclusion criteria.  Most enrolled students were excluded simply because they did not  participate in the class after enrollment. Our cut-off rule was to  include anyone who completed enough of the pre-course survey to  be assigned to a treatment, regardless of whether or not they  actually wrote a plan (i.e. intent to treat). We also removed anyone  who did not report that they were fluent in written English,  because the planning prompt was intended to be a natural  language task for students. We also expected that the vast majority  of students would start the class in the first month, and planned to  drop all late enrollees in our primary analyses.  3.3.1. Pre-course Intentions   Intentions were self-declared in the pre-course survey, as an  option in a non-binding multiple-choice question. The exact text  was as follows (emphasis added):   People register for HarvardX courses for different reasons.  Which of the following best describes you  Here to browse the materials, but not planning on completing any  course activities (watching videos, reading text, answering  problems, etc.).  Planning on completing some course activities, but not planning  on earning a certificate.  Planning on completing enough course activities to earn a  certificate.  Have not decided whether I will complete any course activities.  Students responses from our experiment are plotted in Figure 1.  The majority of students (57%) intended to certify, indicating high  interest. And most students who earned a certificate intended to do  so (83%). But intentions alone were not enough, as among those  students who intended to certify, only a minority completed  enough work to achieve that goal (16%).   3.4. Planning Prompt Intervention   All pre-course surveys included a single randomized factor, which  was the presence or absence of a planning prompt. However, the  protocol varied slightly between the two studies.  3.4.1. Study 1   Figure 1. Course Intentions and Completion Rates    Students were randomized between two different conditions -  planning or control. The two surveys were exactly the same,  except that students in the planning condition received a planning  prompt. The control condition had no additional materials. This  prompt asked students to describe any specific plans they made to  engage course content and complete assignments on time.  However, students were not explicitly encouraged to make extra  plans, or told about the benefits of planning.   Below the prompt, two open-ended text boxes were provided, into  which students could type their plans. Students were free to leave  the boxes blank if they did not want to engage. In fact, 14% of  students left the boxes blank. All of the analyses below estimate  intent to treat, which includes students who did not write  anything, or who quit the pre-course survey after being assigned  to treatment (but not those who quit before, who were excluded).  3.4.2. Study 2  Students were randomly assigned to one of three conditions -  control, simple-planning, or planning-plus. The control condition  was identical to Study 1 (i.e. no planning prompt). The simple- planning condition was virtually identical to the planning  condition in Study 1, however, there were some minor wording  changes to help clarify the instructions, and the number of text  boxes was increased to accommodate additional planning  instructions. The exact text of the prompt was as follows:   We want to know about what plans you have made to  complete this course. In the space below, write down some  of your plans to learn. For example, try to specify:   a) When and where do you plan to spend time engaging  the course content b) What specific steps you will take to ensure you  complete the required course work   c) How will you respond to obstacles that you might  encounter during the course  Please use some of the boxes below to describe the plans  you are making for this course. (Note: You don't have to  fill every box; just use the different boxes to separate the  distinct plans you have).  The planning-plus condition was identical to the simple-planning  condition, with two exceptions. First, at the top of the page,  students were explicitly told that planning was a useful strategy to  increase follow-through. Second, after typing in their plans, the  next page on the survey displayed to students the text of their  plans in a list labeled your plans for this course. Students were  encouraged to write their plans down and stick to them during the  course.   4. EXPERIMENTAL RESULTS   4.1. Descriptive Statistics  Our samples are far more diverse than any brick-and-mortar  school, but this diversity is quite typical for MOOCs. Basic  demographics for all three courses are given in Table 1. Students  come from a range of ages, educational backgrounds, and current  vocations, and a majority are not based in the United States.  Students in Study 1 were more likely to be older, working, and  already have a degree, while students in Study 2 were younger  and more likely to be currently in university. Overall, the vast  majority of students (75%) had previously enrolled in a MOOC,  suggesting they should be familiar with the domain. We also   confirmed that the random assignment was successful across all  observables (i.e. p>0.05 for all balance tests in both studies).   4.2. Average Treatment Effects  4.2.1. Course Completion.   Across both studies, we find a consistent and robust effect of  planning - students prompted to write out their plans at the  beginning of the course had a higher follow-through rate  (M=17.7%, 95% CI=[15.6%, 19.8%]) than those who were not  prompted write out their plans (M=13.8%, 95% CI=[11.3%, 16.3%]; 2(1)=5.2, p=.023). The robustness of these results are  confirmed in a series of logistic regressions in Table 2. The results  imply that planning prompts increased course completion by 29%   compared to the control condition. For comparison, this effect size   A B C D  All Plans 0.609 (0.353)* 0.252   (0.139)* 0.302   (0.130)**  Simple Plans 0.190 (0.161)  Plans Plus 0.312 (0.158)** Courses Study 1 Study 2 Study 2 ALL  Course Effects NO YES YES YES  N 293 1760 1760 2053  pseudo R2 .013 .008 .008 .009  Table 2. Effects of Treatment on Course Completion  Healthcare Biochemistry Government  Age 36.5 (11.7) 30.0 (13.5) 35.0 (14.6)  % Female 61.8% 50.6% 55.3%  Lives in USA 31.7% 42.9% 51.6%  Country HDI .801 (.147) .824 (.134) .847 (.120) Full-Time  Employed 64.5% 34.5% 49.8%  Part-Time  Employed 17.4% 16.0% 14.8%  Concurrent  Student 22.6% 49.7% 33.1%  Bachelors  Degree 84.3% 49.9% 56.0%  Advanced  Degree 51.9% 24.2% 26.5%  MOOCs  Enrolled 3.9 (3.9) 4.3 (4.4) 4.3 (4.3)  MOOCs  Completed 2.6 (3.5) 2.8 (3.9) 3.2 (4.0)  Pre-Course  Enrollment 36.9% 59.7% 76.3%  Pre-Course  Verification 3.1% 1.8% 4.6%  Table 1. Descriptive statistics    is similar to the difference between students who have enrolled in  (and completed) one MOOC before, and students who had never  enrolled in a MOOC.  We conducted two additional exploratory robustness checks,  reported in the full paper. First, we expand our sample to include  the (surprisingly numerous) people who signed up in the later  months of the course and still intended to complete the course.  Second, we expand our sample again, to include people who did  not intend to complete the course. This also provided a conceptual  replication of Sheeran and colleagues [43], who report a  moderation of the plan-making effect by initial intentions. In both  analyses, we found that (i) the effect of planning was robust in a  broader sample; (ii) both intentions and sign-up times had direct  main effects on course completion; and (iii) neither of these  variables moderated the effect of planning on course completion.  4.2.1. Verification Rates.  We also decided to test whether planning prompts affected  students willingness to pay to upgrade to a verified certificate  during the course, reported in Table 3. We indeed find that these  upgrades were more common among students who saw a planning  prompt (M=3.6%, 95% CI=[2.6%, 4.6%]) than those in the control  condition (M=1.8%, 95% CI=[0.8%, 2.8%]; 2(1)=4.41, p=.036).  Added to verifications at enrollment, this implies that planning  prompts increased the total verification rate by 40%, from 4.8% in  control (95% CI=[3.3%, 6.3%]) to 6.7% in the planning  conditions (95% CI=[5.4%, 8.0%]); 2(1)=2.70, p=.100).   The causal mechanism between course progress and verification is  unclear - some students may verify as a proactive commitment  device, but others may simply wait to upgrade until after they are  sure they will complete the course. The average time between the  pre-course surveys and verification upgrades was similar among  students who did not receive a planning prompt (M=15 days,  SD=42 days) and those who did (M=19 days, SD=46 days). Either  way, this measure provides new evidence that plan-making has a  causal impact on real-stakes commitments to online education.   4.2.1. Simple Planning vs. Planning Plus.  In Study 2, participants were randomly assigned to one of two  different planning prompts. Students in the the planning-plus  condition did not write longer plans (M=30.0 words, 95%  CI=[28.8, 31.2]) than students in the simple-planning condition  (M=28.9 words, 95% CI=[27.7, 30.1]; t(1161)=0.6, p=.518), or  spend longer time writing, on average (plans: M=137s, 95%  CI=[131,142]; plans plus: M=148s, 95% CI=[143, 154];   t(1161)=1.4, p=.141). Furthermore, certification rates in the  planning-plus condition (M=18.8%, 95% CI=[17.2%, 20.4%])  were only slightly larger than in the simple-planning condition  (M=16.8%, 95% CI=[15.3%, 18.3%]; 2(1)=0.7, p=.404), while  the difference in verification rates was in the opposite direction  (planning-plus: M=2.9%, 95% CI=[2.2%, 3.6%]; simple- planning: M=5.0%, 95% CI=[4.1%, 5.9%]; 2(1)=2.6, p=.104). If  there is a true difference between these conditions, it is too small  to detect in our data, so we collapse across these two planning  conditions throughout our analyses.   4.3. Treatment Effect Heterogeneity  MOOCs attract a large and diverse student body, and it is natural  to wonder whether the effects of planning prompts are stronger  among certain subgroups of students than others. But many pre- treatment covariates could plausibly moderate the treatment effect  and we did not preregister any, so our analysis follows a  procedure to correct for multiple comparisons [14]. Specifically,  we constructed 13 separate logistic regressions, each of which  tested a single interaction between the treatment effect and one of  the covariates listed in Table 1, after controlling for course fixed  effects and covariate main effects. The p-values from those 13  interaction terms were then corrected to account for the expected  false discovery rate [5].  This analysis finds that only one of the covariates - current  enrollment in a brick-and-mortar school - significantly moderated  the effect on course completion. That is, MOOC students who  were also at a traditional school were more likely to benefit from  the planning prompt (interaction term: =1.27, SE=0.37;  z(1514)=3.5, raw p<.001; corrected p=.043). The next strongest  moderator, age, is highly correlated with school enrollment, and  not significant after this correction. The regression coefficients  imply that planning increased completion rates from 13.6% to  19.4% among those not enrolled in school, and from 12.5% to  25.5% among students who were concurrently enrolled in school.  Though exploratory, this result is consistent with the diagnosis  that follow-through in MOOCs is rare because of a lack of  structure. That is, plan-making seems to be more effective when it  is supported by a structured learning environment in students  lives.   5.  NATURAL LANGUAGE FORECASTING  In this section we explore our second hypothesis. That is, could  the planning prompts also be used to forecast student  achievement Like most text data, the content of the plans are  unstructured and high-dimensional, which poses problems for  traditional analytic approaches [13, 17, 32].    5.1. Length of Course Plans  Although the planning prompts were optional, 87% of the students  in our sample wrote sincere plans (i.e. more than two words). Of  those who did write something, the average word count was 33.4  words (SD=28.4). The length of students plans was, at best, a  weak predictor of their likelihood of completing the course (=. 004, SE=.002, z(1319)=1.6, p=.156).  But the content of the plans  was rich, and we parsed them to build a more sophisticated  forecasting model.   5.2. Forecasting Course Completion  We use standard natural language tools to extract from each  students planning document a set of feature counts - essentially,  tallies of concepts and phrases that are commonly mentioned.   E F G  All Plans 0.627 (0.319)** 0.627   (0.319)**  Simple Plans 0.877 (0.34)***  Plans Plus 0.308 (0.373) Courses Study 2 Study 2 ALL  Course Effects YES YES YES  N 1705 1705 1989  pseudo R2 .010 .017 .045  Table 3. Effects of Treatment on Verification Upgrades    These counts were then processed by an algorithm to determine  the most distinctive features of successful plans.  5.2.1. NLP Forecasting Model  The documents were first processed manually, in two ways. Every  text was spell-checked with software assistance. To resolve synonymy,  we then created a simple word substitution algorithm - for example,  each day, every day, per day and daily were all replaced with the  word daily. These procedures were unsupervised - that is, they were  performed without any knowledge of the students treatment condition or  certificate status.  The resulting documents were then processed automatically, by  converting to lowercase; expanding contractions; removing punctuation;  removing common function words (stopwords); and stemming the  remaining words using the standard Porter stemmer.  The remaining  word stems were then grouped into ngrams - groups of two or three  sequential word stems. To focus on the most common features, ngrams  which appeared in less than 1% of all documents were excluded.  This  process reduced the documents to a feature count matrix, in  which each document (i.e. each student) was assigned a row, while  each ngram feature was assigned a column, and the value of each  cell represented the number of times that ngram appeared in that  document. In addition to the ngram counts, we calculated two  summary linguistic features: the raw word count, as well as a  binary indicator of which prompts were left blank.  This process produces a high-dimensional set of feature counts  which must be regularized in some form to avoid over-fitting. We  use a common method, the LASSO, implemented using the  glmnet package [15, 49]. This algorithm estimates a logistic  regression with a constraint on the total absolute size of the  coefficients. The size of that constraint is determined empirically,  by calculating out-of-sample error via cross-validation within the  training set.    This algorithm reduces most coefficients in the regression to  exactly zero, leaving a smaller set with non-zero coefficients in  the model. The model would then be used to predict the likelihood  that new documents (not included in training) were written by  students who would go on to complete their course. In essence,  they were forecasts of the students likelihood of success, which  could be compared to forecasts based on other data, and forecasts  made by the students themselves. All forecasting models also  included course fixed effects, so that they would learn differences  between students, not between classes.   Forecaster accuracy was primarily measured using the area under  the curve metric (AUC). This tests calculates the probability that  the prediction for a randomly-chosen certified student will be  higher than the prediction for a randomly-chosen drop-out  student. This metric is appealing when, as in our case, the  outcomes are unbalanced (i.e. more people dropped out than  certified), because it captures the relative accuracy of predictions  across students. However, we are not concerned with the absolute  accuracy of forecasting a single students outcome correctly. This  simulates a common decision-making margin - for example, if a  course administrator has to allocate costly interventions among  their students.   5.2.2. Study 1 NLP Results  As an initial test of our hypothesis, we use a strict hold-out  procedure. The students from Study 1 served as test data (N=156).  To enrich the training data for this test, the sample included all  students in Study 2 who intended to complete the course  (N=1,792). Table 4 shows the selected language features and their  assigned coefficients in the model. These forecasts proved   successful in anticipating students follow-through (AUC=.659,  95% CI=[.548,.771]; Mann-Whitney U=1186, p=.009). Because  no data from Study 1 were included in training, this result implies  that the markers of successful plans are not course-specific.  5.2.3. Study 2 NLP Results  To test out-of-sample accuracy using in-course data, we used a  nested cross-validation procedure [45, 51]. Specifically, the  dataset was split into 20 folds, and predictions for each fold were  made using a model trained and tuned on the other 19 folds. We  used two techniques to smooth out instability caused by the cross- validation. First, fold assignment was stratified, to balance the  course composition and certification rate in every fold.  Additionally, the entire cross-validation procedure was repeated  10 times, and the prediction for every student was an average over  those 10 out-of-sample predictions. The language features were  once again predictive of course completion among the students  who had signed up in the first month and intended to complete the  course (AUC=.579, 95% CI=[.537,.622]; Mann-Whitney  U=83238, p<.001).  5.2.4. Study 2 Benchmark Predictions  In Study 2, we sought two other responses from students that  could be used to benchmark the results of our NLP model. First,  we asked students to directly estimate the probability that they  will finish enough of the course to earn a certificate. The average  student forecast (83.5%) was far more optimistic than the actual  completion rate (16.7%). However, these predictions were still a  valid signal of course completion (AUC=.597, 95% CI=[. 555,.639]), because students who completed the course had given  higher estimates (M=87.7%, 95% CI=[85.8,89.6]) than those who  did not complete (M=82.9%, 95% CI=[81.9,83.9], t(1157)=4.1,  p<.001). This prediction provides a useful benchmark, and shows  that natural language forecasting can approximate students' own  insights into their expected success.  We also asked students about their grit, using an eight-item survey  instrument designed to measure resilience in goal pursuit [11].  This metric also produced similar forecasting accuracy to the  natural language forecast (AUC=.577, 95% CI=[.546,.631]).   Feature Coefficient % of Plans  work.cours -0.0897 2.4  free.time -0.0496 5.5  plan.studi -0.0296 4  time.day -0.0287 1  home.will -0.0249 2.1  onehour.daili -0.0193 3.3  discuss.board 0.0004 2.8  plan.engag 0.0006 1.3  will.engag 0.0410 1.2  complet.work 0.0498 1.1  [word count] 0.0642 -  hour.week 0.0667 2.4 cours.home 0.0834 1.5  watch.lectur 0.2087 4.3  Table 4. NLP Features Selected by the Forecasting Model    However, the natural language forecast has two advantages over  the grit scale. First, the grit scale forecasts did not improve the fit  of the prediction model, beyond a baseline model of students self- predictions (2(1,N=1161)=0.32, p=.572), whereas the natural  language forecasts did explain additional variance in course  outcomes (2(1,N=1161)=7.46, p=.006). Second, the grit scale  only provides a point estimate of students expected success.  Natural language forecasts, on the other hand, can provide a much  richer model of students follow-through constraints, by revealing  the contents of which plans were most (or least) likely to succeed.   5.3. Contents of Course Plans  What, then, were the differences between successful and  unsuccessful plans The feature set in Table 4 is sparse, in part  because of the modest sample size for training. Additionally, the  coefficients from a lasso regression can be hard to interpret,  because the regularization path only selects features that add  unique variance as the model becomes more complex. In effect,  the selected features are taken out of of their context, by choosing  only one feature among many that co-occur together, and which  all map onto a common concept.   5.3.1. Topic Modeling  To better understand that mapping, we turn to Latent Dirichlet  Allocation as a quantitative model of the planning text [7, 38].  This algorithm clusters words into topics based on their co- occurrence, which exploits the very feature that makes the Lasso  coefficients opaque. To estimate the topic structure, we pooled all  the written plans that were longer than ten words (N=1007). We  used a variant of the standard unsupervised LDA algorithm which  incorporates a deterministic algorithm for initializing the anchor  word of each topic [3]. There are no hard-and-fast rules for   choosing the number of topics, so the researcher must choose the  an appropriate level of granularity for their own research.  Informed by some reasonable guidelines [47], we chose to fit a  15-topic model in this paper, though our basic conclusions are  robust across a range of reasonable topic quantities.  After the model was estimated, we calculated the log-normal  prevalence of every topic in every document, as well as how that  prevalence correlated with both course completion rates and  document length (after controlling for differences courses). These  two correlations for every topic are plotted against one another in  Figure 2 (with units expressed in terms of standardized regression  coefficients). The letter labels are sized to scale with total topic  prevalence. The legend indicates the top five key words of each  topic (by FREX, see [38]). Additionally, the topics that are most  distinctive of course completion are presented as word clouds in  Figure 3. Qualitatively, the most stark pattern in the topic model  was the divide between context plans, which focus on the time  and location for learning, versus action plans, which focus on  the materials and methods of learning. We followed up on these  observations to quantify them using more structured analyses.  5.3.2. Time Plans  Time was a first-order concern of most students in our  experiments -  89% of students who wrote any plan mentioned  time at least once, as defined by a pre-written dictionary of time- related words [34]. However, time focus was not necessarily  helpful to goal pursuit. In fact, in a logistic regression over all  non-blank texts (with course fixed effects), we found that the  proportion of time words predicted that a given plan was less  likely to succeed (=-0.223, SE=0.091, z(1131)=2.4, p=.015).  That is, plans that focus on time were less likely to succeed than  other plans.   Figure 2. Planning Topics and Course Completion Rates    Of course, there is a range of possible time plans. In particular,  they can also be subdivided using a dictionary of concreteness  ratings, along a range from concrete (e.g. day, month,  afternoon) to abstract (e.g. sometime, future, soon; see  [45]). We calculated the average concreteness of just the words  from the time list, following their procedure exactly (i.e. by  dropping texts that did not include at least four words from their  list). The students who used more concrete time words were less  likely to complete the course than students who used more  abstract time words (=-0.168, SE=0.098, z(720)=1.7, p=.086).  Concrete time-based plans were less likely to succeed than  abstract time-based plans.   These estimates are not causal. Choice of plans is endogenous to  contextual factors, like other time pressures, that also affect course  completion. But these diagnostic results are still important for  understanding goal pursuit. Specific time-and-place plans were  most successful in previous field experiments on planning  prompts [39]. Our results provide stronger theoretical support for  the mechanism behind the causal effects of specific plans. That is,  specific plan-making may have been most beneficial for those  students who would have focused on time in their open-ended  planning prompts. Our work suggests a way to diagnose  responsiveness to particular interventions, and future experiments  should investigate this hypothesis.   6.  GENERAL  DISCUSSION   Students enrolling in MOOCs often have ambitious intentions and  high expectations that they will follow through on their goal. But  most learners who intend to complete a MOOC fail to do so. In  this paper we present results from an intervention that is targeted   at the follow-through problem. In a field experiment, we asked  some students to describe their personal plans for completing the  course. They were prompted at the beginning of the course, when  intentions were strong, but follow-through was uncertain.   These planning prompts had two benefits. First, planning  increased follow-through. We estimate that completion rates  among students prompted to make a plan were 29% higher than  those who were not prompted to make plans, on average.  Additionally, students who planned were 40% more likely to pay  for a verified certificate during the course. These effect on  completion was almost twice as large among people concurrently  enrolled in a traditional school. Planning prompts provided  psychological scaffolding for students frame of mind, and paid  dividends weeks and months later in terms of greater persistence  and completion.   Our results show a second benefit from planning prompts: the text  of students plans could be used to forecast their success. Natural  language processing algorithms could parse the plans and forecast  course completion as well as the students own predictions,  finding predictive features in the text that students do not see for  themselves. Students who make plans and succeed are more likely  to write about how they will engage with the course, while  students who make plans and fail are more likely to write about  the concrete steps of when and where they will engage the course.  These results add to mounting evidence from the field for the  effect of planning prompts [39]. Ours is the first natural field  experiment to show an effect of planning on a long-term goal that  requires many actions over time, rather than a single action at one  point in the future. The open-ended text data also provide a unique  window into the psychology of planning. While previous field   Figure 3. Distinctive words from selected planning topics that predict course completion    experiments in planning have focused on single events where the  scope of possible plans is limited, MOOCs are complex goals  with many strategies and obstacles. A common task for platforms  is plan recognition - that is, anticipating a users goals from their  behavior [8,18]. However, common plan recognition strategies  presuppose knowledge of the range of possible plans, whereas in  many domains we may want to learn the plans from the data. Our  work shows that many MOOC students are willing to report their  plans voluntarily at enrollment, and that these plans can be  modeled to understand their implementation intentions.   This modeling is important because in an online environment, the  mixture - and dosage - of different interventions can be  personalized to individual students, based on their needs and  likelihood of dropping out. Typically, these prediction problems  have been approached using in-course activity data (e.g. [48, 53]  though see [21,22]). However, activity data often cannot provide  enough lead time for a course designer to intervene before the  drop-out is inevitable. Furthermore, MOOC data has shown that  most drop-outs occur early in the course [2, 16, 35]. Planning  prompts are given to students early, when they have not yet  dropped out and their exposure to interventions will be high.   Our results show that planning prompts do not just forecast  follow-through failures - they can identify the nature of the  impending obstacles. This can provide deeper guidance into the  appropriate mid-course intervention. To take one example,  Coursera recently introduced a calendar app in all of their  courses for students to allocate time for future activities. Our  results suggest that the treatment effects of this app could be  heterogenous, and that an optimal course design policy would  push this app into a more prominent place for students whose  plans reveal time constraints.  Finally, these results provide context for low completion rates in  MOOCs. It is clear that a substantial fraction of students do not  achieve the goals they set at the start of the course [2,16,19,37].  However, our research adds to other results that suggest the  relative lack of structure in current MOOCs makes it hard for the  less diligent students to stick to their goals [6]. As MOOCs  become a more established feature in the educational landscape,  there will be even more demand for choice architecture to better  align students behavior with their intentions. Planning prompts  can be one important response to that demand, and future work  should focus on other ways to encourage follow-through. We now  have initial evidence that these approaches are effective in  MOOCs, and they should be investigated in other forms of online  and hybrid learning as well.    7. ACKNOWLEDGEMENTS  This work would not have been possible without the close  assistance of Glenn Lopez, Rebecca Petersen, Marshall Thomas,  Shilpa Adnani, Zofia Gajdos, Zachary Davis, Heather Sternshein,  and Dustin Tingley, and we are grateful for helpful comments  from Julia Minson, Todd Rogers, Jacob Whitehill, Joseph  Williams and Sendhil Mullainathan.  8. REFERENCES  [1] Ajzen, I. (1991). The theory of planned behavior. Org Behav   Hum Dec, 50(2), 179-211.   [2] Anderson, A., Huttenlocher, D., Kleinberg, J., & Leskovec, J.  (2014). Engaging with massive online courses. In Proc 23rd  Internl Conf on World Wide Web, 687-698.   [3] Arora, S., Ge, R., Halpern, Y., Mimno, D., Moitra, A.,  Sontag, D., Wu, Y. & Zhu, M. (2013). A Practical Algorithm   for Topic Modeling with Provable Guarantees. In Proc 30th  Inter'l Conf Machine Learning, 280-288.   [4] Barber, M., Donnelly, K., Rizvi, S., & Summers, L. (2013).  An avalanche is coming. Higher Education and the  revolution ahead, 73.   [5] Benjamini, Y., & Hochberg, Y. (1995). Controlling the false  discovery rate: a practical and powerful approach to multiple  testing. J Royal Stat Soc B Met, 289-300.   [6] Banerjee, A. V., & Duflo, E. (2014). (Dis) organization and  Success in an Economics MOOC. Am Econ Rev, 104(5),  514-518.   [7] Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent  dirichlet allocation. the J Machine Learning Res, 3,  993-1022.   [8] Carberry, S. (2001). Techniques for plan recognition. User  Modeling and User-Adapted Interaction, 11(1-2), 31-48.   [9] Chen, C. M. (2008). Intelligent web-based learning system  with personalized learning path guidance. Computers &  Education, 51(2), 787-814.   [10] Clow, D. (2013). MOOCs and the funnel of participation. In  Proc 3rd Interl Conf on Learning Analytics and Knowledge,  185-189.   [11] Duckworth, A. L., & Quinn, P. D. (2009). Development and  validation of the Short Grit Scale (GRITS). J Personality  Assess, 91(2), 166-174.   [12] Gollwitzer, P. M., & Sheeran, P. (2006). Implementation  intentions and goal achievement: A metaanalysis of effects  and processes. Adv Exp Soc Psychol, 38, 69-119.   [13] Grimmer, J., & Stewart, B. M. (2013). Text as Data: The  Promise and Pitfalls of Automatic Content Analysis Methods  for Political Texts. Polit Anal, 21(3), 298-313.   [14] Fink, G., McConnell, M., & Vollmer, S. (2014). Testing for  heterogeneous treatment effects in experimental data: false  discovery risks and correction procedures. J Dev  Effectiveness, 6(1), 44-57.   [15] Friedman, J., Hastie, T., & Tibshirani, R. (2010).  Regularization paths for generalized linear models via  coordinate descent. J Stat software, 33(1), 1.   [16] Ho, A. D., Chuang, I., Reich, J., Coleman, C. A., Whitehill,  J., Northcutt, C. G., ... & Petersen, R. (2015). HarvardX and  MITx: Two Years of Open Online Courses Fall 2012-Summer  2014.   [17] Jurafsky, D. & Martin, J. (2009). Speech and natural  language processing: An introduction to natural language  processing, computational linguistics, and speech  recognition. MIT Press.   [18] Kautz, H. A., & Allen, J. F. (1986). Generalized Plan  Recognition. AAAI 86(3237), 5.   [19] Kizilcec, R. F., Piech, C., & Schneider, E. (2013).  Deconstructing disengagement: analyzing learner  subpopulations in massive open online courses. In Proc 3rd   Inter'l Conf on Learning Analytics and Knowledge, 170-179.   [20] Kizilcec, R. F., Schneider, E., Cohen, G. L., & McFarland, D.  A. (2014). Encouraging forum participation in online courses  with collectivist, individualist and neutral motivational     framings. Experiences and best practices in and around  MOOCs, 17.   [21] Kizilcec, R. F., & Halawa, S. (2015). Attrition and  achievement gaps in online learning. In Proc 2nd ACM Conf  on Learning@ Scale, 57-66.   [22] Kizilcec, R. F., & Schneider, E. (2015). Motivation as a lens  to understand online learners: Toward data-driven design  with the OLEI scale. ACM Transactions on Computer- Human Interaction,22(2), 6.   [23] Kizilcec, R. F., Prez-Sanagustn, M., & Maldonado, J. J.  (2016, April). Recommending Self-Regulated Learning  Strategies Does Not Improve Performance in a MOOC. In  Proc 3rd ACM Conf on Learning@ Scale, 101-104.   [24] Kloft, M., Stiehler, F., Zheng, Z., & Pinkwart, N. (2014).  Predicting MOOC dropout over weeks using machine  learning methods. Empirical Methods on Natural Language  Processing, 60.    [25] Koehler, D. J., & Poon, C. S. (2006). Self-predictions  overweight strength of current intentions. J Exp Soc Psychol,  42(4), 517-524.   [26] Koller, D. (2011). Death knell for the lecture: Technology as  a passport to personalized education. New York Times, Dec 5,  5.   [27] Koller, D., Ng, A., Do, C., & Chen, Z. (2013). Retention and  intention in massive open online courses: In depth. Educause  Rev, 48(3), 62-63.   [28] Kruger, J., & Evans, M. (2004). If you don't want to be late,  enumerate: Unpacking reduces the planning fallacy. J Exp  Soc Psychol, 40(5), 586-598.   [29] Milkman, K. L., Beshears, J., Choi, J. J., Laibson, D., &  Madrian, B. C. (2011). Using implementation intentions  prompts to enhance influenza vaccination rates. Proc Natl  Acad Sci USA, 108(26), 10415-10420.   [30] Milkman, K.L., Beshears, J., J.J. Choi, D. Laibson, and B.C.  Madrian (2013). Planning prompts as a means of increasing  preventive screening rates. Prev Med, 56, 92-93.   [31] Nickerson, D.W. & Rogers, T. (2010). Do you have a voting  plan Implementation intentions, voter turnout, and organic  plan making. Psychol Sci, 21(2), 194-199.    [32] OConnor, B., Bamman, D., & Smith, N. A. (2011).  Computational text analysis for social science: Model  assumptions and complexity. Public Health, 41(42), 43-50.    [33] O'Donoghue, T. & Rabin, M. (1999). Doing it Now or Later.  Am Econ Rev, 89 (1), 103-124.   [34] Pennebaker, J. W., Booth, R. J., & Francis, M. E. (2007).  Linguistic inquiry and word count: LIWC [Computer  software]. Austin, TX: liwc. net.    [35] Perna, L. W., Ruby, A., Boruch, R. F., Wang, N., Scull, J.,  Ahmad, S., & Evans, C. (2014). Moving through MOOCs  understanding the progression of users in Massive Open  Online Courses. Educ Res, 0013189X14562423.   [36] Poon, C. S., Koehler, D. J., & Buehler, R. (2014). On the  psychology of self-prediction: Consideration of situational  barriers to intended actions. Judgm Decis Making, 9(3), 207.   [37] Reich, J. (2014). MOOC completion and retention in the  context of student intent. EDUCAUSE Review Online.   [38] Roberts, M. E., Stewart, B. M., Tingley, D., Lucas, C., Leder Luis, J., Gadarian, S. K., Albertson, B. & Rand, D. G. (2014).  Structural Topic Models for OpenEnded Survey Responses.  Am J Polit Sci, 58(4), 1064-1082.   [39] Rogers, T., Milkman, K., John, L., & Norton, M. I. (2016).  Making the best-laid plans better: how plan making increases  follow-through. Behav Sci & Pol, In Press.   [40] Schacter, D.L. (1999). The seven sins of memory: Insights  from psychology and cognitive neuroscience. Am Psychol,  54, 182-203.    [41] Self, J. (1998). The defining characteristics of intelligent  tutoring systems research: ITSs care, precisely. Interl J Artif  Intell in Educ, 10, 350-364.   [42] Shah, D. (2015). By The Numbers: MOOCS in 2015 - Class  Central's MOOC Report. Retrieved June 05, 2016, from  https://www.class-central.com/report/moocs-2015-stats/   [43] Sheeran, P., Webb, T. L., & Gollwitzer, P. M. (2005). The  interplay between goal intentions and implementation  intentions. Pers Soc Psych B, 31(1), 87-98.   [44] Snefjella, B., & Kuperman, V. (2015). Concreteness and  psychological distance in natural language use. Psychol Sci,  26(9), 1449-1460.   [45] Stone, M. (1974). Cross-validatory choice and assessment of  statistical predictions. J Royal Stat Soc B Met, 111-147.   [46] Sunstein, C. R., & Thaler, R. (2008). Nudge. The politics of  libertarian paternalism. New Haven.   [47] Taddy, M. (2012). On Estimation and Selection for Topic  Models. In 15th Interl Conf on Artif Intel and Stat,  1184-1193.   [48] Taylor, C., Veeramachaneni, K., & O'Reilly, U. M. (2014).  Likely to stop predicting stop-out in massive open online  courses. arXiv preprint arXiv:1408.3382.   [49] Tibshirani, R. (1996). Regression shrinkage and selection via  the lasso.  J Royal Stat Soc B Met, 267-288.   [50] Tomkin, J. H., & Charlevoix, D. (2014, March). Do  professors matter: Using an a/b test to evaluate the impact  of instructor involvement on MOOC student outcomes. In  Proc 1st ACM Conf Learning@Scale, 71-78   [51] Varma, S., & Simon, R. (2006). Bias in error estimation  when using cross-validation for model selection. BMC  Bioinformatics, 7(1), 91.   [52] Wen, M., Yang, D., & Rose, C. (2014). Sentiment Analysis in  MOOC Discussion Forums: What does it tell us In Proc 7th  Intl Conf Educ Data Mining, 130-137   [53] Whitehill, J., Williams, J. J., Lopez, G., Coleman, C. A., &  Reich, J. (2015). Beyond prediction: First steps towards  automatic intervention in MOOC student stopout. Proc 8th  Intl Conf Educ Data Mining, 171-179.   [54] Yang, D., Wen, M., Howley, I., Kraut, R., & Rose, C. (2015,  March). Exploring the effect of confusion in discussion  forums of massive open online courses. In Proc 2nd ACM  Conference on Learning@Scale, 121-130.    "}
{"index":{"_id":"59"}}
{"datatype":"inproceedings","key":"Dawson:2017:PIE:3027385.3027405","author":"Dawson, Shane and Jovanovic, Jelena and Gavsevi'c, Dragan and Pardo, Abelardo","title":"From Prediction to Impact: Evaluation of a Learning Analytics Retention Program","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"474--478","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027405","doi":"10.1145/3027385.3027405","acmid":"3027405","publisher":"ACM","address":"New York, NY, USA","keywords":"early alert systems, learning analytics, mixed-effects model, predictive models, student retention","Abstract":"Learning analytics research has often been touted as a means to address concerns regarding student retention outcomes. However, few research studies to date, have examined the impact of the implemented intervention strategies designed to address such retention challenges. Moreover, the methodological rigor of some of the existing studies has been challenged. This study evaluates the impact of a pilot retention program. The study contrasts the findings obtained by the use of different methods for analysis of the effect of the intervention. The pilot study was undertaken between 2012 and 2014 resulting in a combined enrolment of 11,160 students. A model to predict attrition was developed, drawing on data from student information system, learning management system interactions, and assessment. The predictive model identified some 1868 students as academically at-risk. Early interventions were implemented involving learning and remediation support. Common statistical methods demonstrated a positive association between the intervention and student retention. However, the effect size was low. The use of more advanced statistical methods, specifically mixed-effect methods explained higher variability in the data (over 99%), yet found the intervention had no effect on the retention outcomes. The study demonstrates that more data about individual differences is required to not only explain retention but to also develop more effective intervention approaches.","pdf":"From prediction to impact:   Evaluation of a learning analytics retention program   Shane Dawson  University of South   Australia  Adelaide, Australia   shane.dawson@  unisa.edu.au   Jelena Jovanovic  University of Belgrade   Serbia  jeljov @fon.rs   Dragan Gaevi   The University of   Edinburgh  UK   dgasevic@acm.org   Abelardo Pardo   The University of Sydney   Australia  Abelardo.Pardo@   sydney.edu.au      ABSTRACT  Learning analytics research has often been touted as a means to  address concerns regarding student retention outcomes. However,  few research studies to date, have examined the impact of the  implemented intervention strategies designed to address such  retention challenges. Moreover, the methodological rigor of some  of the existing studies has been challenged. This study evaluates  the impact of a pilot retention program. The study contrasts the  findings obtained by the use of different methods for analysis of  the effect of the intervention. The pilot study was undertaken  between 2012 and 2014 resulting in a combined enrolment of  11,160 students. A model to predict attrition was developed,  drawing on data from student information system, learning  management system interactions, and assessment. The predictive  model identified some 1868 students as academically at-risk.  Early interventions were implemented involving learning and  remediation support. Common statistical methods demonstrated a  positive association between the intervention and student  retention. However, the effect size was low. The use of more  advanced statistical methods, specifically mixed-effect methods  explained higher variability in the data (over 99%), yet found the  intervention had no effect on the retention outcomes. The study  demonstrates that more data about individual differences is  required to not only explain retention but to also develop more  effective intervention approaches.   CCS Concepts  J.1 [Administrative Data Processing]: Education; K.3.0  [Computer Uses in Education]: General   Keywords  Student retention; learning analytics; mixed-effects model; early  alert systems; predictive models   1. INTRODUCTION  There has been an abundance of learning analytics research  investigating the development of predictive models and early alert  systems to address concerns regarding declining student retention  outcomes in higher education. The underpinning drivers for such   work are valid and well-grounded. Despite numerous university  and government policy changes, increased accountability  measures, and changes in the teaching practice [16; 30], there  continues to be a very real and growing concern regarding student  retention in post-secondary school settings. Clearly, improving  retention is a complex challenge. The capacity to accurately  predict students that may or may not continue in their course of  study comprises only one small piece of a larger puzzle.    This present study examined the impact of a pilot retention  program implemented in a large metropolitan University setting.  The retention program involved the development of a predictive  model to identify students that were deemed as at-risk of  academic success and retention. The designed retention strategy  involved implementing a series of campaigns (calls) in order to  contact students and provide learning and remediation support.  The present study specifically evaluates the impact of the contact  and support interventions on student retention outcomes.  Furthermore, the study highlights the need for critically evaluating  the methods employed to assess the impact on retention outcomes.   2. PREDICTIVE MODELS  Much of the learning analytics research to date has centered on  the development of models to predict student academic success. In  so doing, this work has effectively scaffolded the development of  early alert systems that aim to provide a form of intelligence to  course instructors that in-turn are able to enact timely learning  support interventions. A key example in this work is the Purdue  Signals early alert system. Campbell [1; 7] established a  predictive model based on student prior grades, demographic  details and engagement with the Universitys learning  management system (LMS). The Signals early alert system uses  traffic light signals (red, amber, and green) as a metaphor for  predicting student academic success [2]. While predictive  analytics at this time was not a new concept for education, the  Purdue Signals was novel in its attempt to bring real-time  actionable intelligence into teaching practice.    More recently, we have witnessed a wealth of new commercial  and proprietary early alert systems. For instance, Brightspace has  developed Insights; BlackBoard has established BB analytics and  X-Ray analytics while research and reports on in-house developed  products such as E2 Coach [32], Course Signals [1], and Student  explorer [19] have been regularly presented at learning analytics  conferences. Without doubt, the continuing evolution of machine  learning and data-mining approaches are increasingly leading to  more and more sophisticated applications that are targeted  towards supporting student academic success [3]. However, as  noted by Jayaprakash et al., [14] models do not influence course  completion and retention rates without being combined with   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org. LAK '17,  March 13-17, 2017, Vancouver, BC, Canada   2017 ACM. ISBN 978-1- 4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027405      effective intervention strategies (p8.). Further work is required to  determine how such analytics can be effectively used to optimize  student learning. For instance, the identification of the types and  timing of interventions that will have optimal impact on student  learning outcomes remain largely unknown. To date the predictive  analytic models have been generally evaluated against the degree  of accuracy the model predicts the identified outcome; e.g.,  student academic success in terms of pass or fail. For example,  Romero, et al., [27; 29] provided a comprehensive overview  detailing the process for mining LMS user-interaction data to  establish a prediction of student grades. Barber and Sharkey [4]  developed a model to identify students at-risk of academic success  based on data from the LMS, finances and student information  system. Wolff, et al., [31] added a further dimension by  establishing a model based on changes in student LMS user- behavior. Other work in this area has focused on developing  models from specific LMS tools such as discussion forums [21;  22; 28].   Even within this research there are further refinements required to  aid replicability of findings and ensure theoretical and  methodological assumptions are well considered. For instance,  Gasevic, et al., [10] demonstrated the impact of alternate learning  contexts on the performance of an established model for  predicting student academic success. The authors noted that the  types of tools used, alongside the differing modes and diversity of  assessment practices heavily influence the accuracy of the  identified model. Similarly, Kovanovic, et al., [18] revealed the  vast array of differing approaches to defining student time on-task  in a LMS within the analytics research literature. The authors  empirically confirmed that how time on-task is calculated and  subsequently incorporated as a key variable in any developed  model strongly influences the performance of the predictive  model. Kovanovic, et al., [17; 18] stressed the importance for  future learning analytics research to clearly note the underpinning  assumptions and definitions related to how key variables are  defined and calculated when developing predictive models to  better aid generalization and replicability of the findings.    Research evaluating the impact of early alert systems on student  learning has been less forthcoming. However, there are some  notable examples. Arnold & Pistilli [2] reported the impact the  Purdue Course Signals early alert system has had on student  academic performance and retention. The results highlight the  significant positive association between the use of Course Signals  and student academic performance. However, subsequent  discussions related to the work have contested the statistical  approach undertaken and cast some doubt on the noted impact of  the introduction of Course Signals [8]. Wright, et al., [32]  described the student support tool E2 Coach developed at the  University of Michigan. Where prior, alert systems have been  designed to predict pass/ fail or specific academic performance,  the E2 Coach incorporated a Better Than or Worse Than  Expected value. The E2 Coach is very much targeted at providing  meaningful personalized automated support communications to  aid student learning in first year Physics. Wright, et al., [32] noted  that use of the tool was positively associated with a Better Than  Expected score. Simply put, high users of the tool performed  better. Again the results of the early alert system are promising yet  require further investigation to ensure that the positive findings  are attributed to the tool and the associated learning support  process in lieu of other mitigating or confounding factors. This  research begins to illustrate the high level of complexity involved  when attempting to identify how such early alert interventions can  be meaningfully evaluated.   3. PILOT RETENTION PROGRAM  3.1 Theoretical foundation  The pilot retention program presented in this study was grounded  in the student retention and development literature [16]. This body  of research argues that a students level of self-efficacy,  resilience, sense of connection and study motivation all play a role  in shaping their academic outcomes. The relationship between  these cognitive and affective dimensions and actual outcomes is  recursive: successful academic outcomes will in turn strengthen a  students cognitive and affective dispositions [6]. However,  academic failure can negatively impact on these dispositions,  possibly perpetuating poor performance. The design of the  retention program recognized this duality. The pilot retention  program sought to heighten a students chance for academic  success through the proactive provision of targeted advising and,  where required, referral to counseling, learning, disability or other  specialist support services. The program campaign calls informed  students about these services in the university that were available  to them in the belief that uptake of these services would improve  student success outcomes. By advising of support options, and  encouraging students to access these, the program both respected,  and sought to develop, student autonomy and agency while also  equipping them with resources and knowledge that had the  potential to enhance their future persistence and resilience in  study at university.    While the pilot program primarily targeted student success and  retention outcomes, it also provided an important pastoral care  benefit. By contacting students identified to be academically at  risk, the program was engaging with individuals that have a  heightened propensity and vulnerability to be socially and  academically disengaged from the institution [15; 33], or have  their academic experiences mediated by a variety of differing and  often extenuating personal factors [11; 33]. These cohorts of  students often possess poorly developed help-seeking behaviors  that can lead to further isolation [20; 24]. By reaching out to  students through campaign calls, the program circumvented the  need for students to initiate their help-seeking process.  Additionally, the contact serves as a symbolic function that  reinforces to the student that she/he is an important member of the  university community.    3.2 Approach  The pilot program was introduced post the establishment of a  University model predicting student performance and retention.  The predictive model was developed using an integrated  classification and association rule mining algorithm (CBA). The  model was tested for accuracy using historical data sets including  basic student demographic and LMS activity variables. In  summary, student engagement activity with the institutions LMS,  as well as prior study location (secondary school), parental level  of education and distance from university were negatively  associated with performance and retention. The model contributed  to the development of a set of potential triggers for providing an  alert for a learning support intervention. The interventions  consisted of multiple campaigns (calls) over the semester of study  to maximize opportunities to reach students. The triggers were  grouped according to three main categories: LMS engagement,  attendance in class, or academic (referring to grades, or other  academic outcomes such as failing to submit an assignment).   The pilot program was introduced into 17 first-year university on- campus courses involving disciplines such as math, accounting,  health, and computer science. The diversity of courses ensured all     faculties were represented in the pilot program. The combined  course enrolment represented some 11160 students. Within these  courses, 1868 students of the total enrolment were identified as  vulnerable to poor academic outcomes and an attempt was made  to contact the students. Within each campaign, two attempts were  made to contact a student identified as at-risk. The mode of  contact was via phone calls undertaken by trained personnel. Over  the duration of the pilot program some 1271 students were  successfully contacted, representing 68% of all students identified  as at risk. All data related to the student cases were combined with  individual academic data, including, course outcome (e.g. pass or  fail, and GPA equivalence), and retention data (defined as student  enrolment in the following semester).    3.3 Analyses  Student retention (Retention) was the dependent variable for all  analyses undertaken. Retention is defined as a binary variable  indicating if a student continued in their future studies or  alternatively ceased university study or, at minimum did not enroll  in the following semester. The independent variables included: i)  basic student demographics: Age (numeric), Gender (binary), and  International (international/ domestic), ii) students' academic load  (AcademicLoad), a binary variable indicating if a student had full  or partial academic load; iii) Contacted, a binary variable  indicating if a student responded to an intervention attempt (i.e.,  he/she was successfully contacted); iv) variables related to the  timing of an intervention attempt: StudyPeriod, a binary variable,  as the intervention program took place in semester 1 or 2, and  Campaign, nominal variable indicating in which of the 4  campaigns of a semester a particular intervention attempt was  made. A student may have been exposed to several intervention  attempts (in case of 23.23% of students, an intervention was  attempted more than once). All students were uniquely identified  by their ids (StudentID).    To examine the association between the intervention and the  retention outcome we started with statistical methods  'traditionally' used for this type of task, namely Chi Square test of  association and logistic regression. This was followed by  generalized (namely, logistic) mixed-effect models  statistical  models that include a combination of fixed and random effects,  and allow for assessing the influence of the fixed effects on the  outcome variable after accounting for any other variables (random  effects) that might have affected the outcome but could not be  controlled [13]. First, we built 'null' mixed-models, with random  effects only (as baseline), and then several 'full' mixed-models  with both fixed and random effects. A comparison of the 'null'  model with the 'full' models allowed us to determine whether the  intervention could predict the retention outcome above and  beyond the random effects. Akaike Information Criterion (AIC),  Log Likelihood (LL) and a likelihood ratio test were used to  assess the fitness of the mixed models. To estimate the variance  explained by the mixed-models, we used a pseudo R2 method  suggested by Nakagawa and Schielzeth [23]. In particular, we  computed marginal R2 (!! ), which estimates the variance  explained by the fixed effects only, and conditional R2 (!!) that  estimates the variance explained by the entire model (both fixed  and random effects). All the analyses were done in R. The lme4 R  package [5] was used for building the mixed-models. Statistical  significance (alpha) was set to 0.05.   4. RESULTS  Chi Square test of association indicated a significant association  between the intervention (Contacted variable) and the retention   outcome (Retention): ! = 6.3757,  = 0.0116, though with a  very small effect size (Crammer's V=0.052).   Logistic regression with Retention as the outcome variable and  Contacted as the only predictor variable showed that the  intervention (Contacted) was a significant predictor of retention  outcomes ( = 0.2718,  = 0.01). After exponentiating the  coefficient (  = 1.3124), we found that the odds of  contacted students keeping with the studies were 31.24% higher  than for those were not contacted. However, the explanatory  power of this model was very low, R2=0.005.     The first logistic mixed-effect model was a null model (MN) with  StudentID as the only random effect. This model proved to have  exceptional explanatory power explaining 99.23% of the  variability in Retention (i.e., !! = 0.9923). This result made it  clear that the variability in the retention originated in the student- specific features. Accordingly, the next step was to examine full  models with StudentID as the random effect and student-related  variables as fixed effects, and see if at least part of the student- specific variability could be attributed to those fixed effects.    The first full logistic mixed-model (MF1) with the Contacted  variable as the only fixed effect and StudentID as the random  effect was not significantly different than the baseline model  (MN), ! = 0.5605,  = 0.454. This suggested that the  intervention (Contacted) did not have a significant effect on the  Retention, when controlling for student-specific features, which  was confirmed by the model estimates for the fixed effect  ( = 0.2769,  = 0.452).    The second full logistic mixed-model (MF2) included all student- related variables (Age, Gender, International, AcademicLoad,  Contacted) as fixed effects and StudentID as the random effect.  As MF1, this model was also no different than the baseline (MN),  ! = 1.6294,  = .8977. This finding indicated that none of the  available student-related variables had a significant effect on the  Retention, which was confirmed by the MF2 estimates for the fixed  effects, none of which was significant. The model still explained  99.22% of variability in the Retention variable (i.e., !! = 0.9922), but that was originating almost exclusively from the  random effect due to low value of marginal R2 (!! = 0.0001).  This further means that there are other student-specific variables  (that we do not have data about) that influence students' retention  outcome.   With the third full logistic mixed-model (MF3) we aimed to  examine if some elements of the timing of the intervention  program affected the retention. Therefore, MF3 included  StudyPeriod and Campaign as fixed effects and StudentID as the  only random effect. Initially, we included interactions between  StudyPeriod and Contacted and Campaign and Contacted in MF3,  but the model failed to converge. These interactions were,  subsequently, examined in separate mixed-models, but were  insignificant. The MF3 model proved to be better than the baseline  (MN), ! = 44.287,  < 0.001, suggesting that at least one of the  fixed effects was significant. Indeed, StudyPeriod proved  significant ( = 2.5918,  < 0.001), indicating that, other  things being equal, the odds of keeping with the studies in case  the intervention program took place in study period 5 were 92.5%  lower than when it took place in study period 2 (exp() = 0.075).  However, the explanatory power of the StudyPeriod effect in  comparison to the power of student-idiosyncratic features  (unknown to us at this moment) is very small: marginal R2,  !! = 0.0023, while the R  2 for the overall MF3 model, !! = 0.9940.     5. DISCUSSION  At LAK2016, Clow and colleagues [9] held a novel workshop  aptly titled LAK Failathon. The workshop was held in the spirit  of learning from failed learning analytic projects. As the  researchers noted: most papers in the learning analytics literature  report success or, at least, read as if they are reporting success.  This is almost certainly not because learning analytics research  and activity are always successful. Generally, we report our  successes widely, but keep our failures to ourselves. (p. 509).  This paper embraces the same spirit in reporting our learnings  from a less than successful learning analytics pilot project.    This paper outlined the evaluation of a pilot program that aimed to  address concerns over declining student retention in core first year  university courses. The findings illustrated that rigorous  methodological processes are required to ensure that reports of  impact are accurately represented. This is well demonstrated in  the paper through the employment of increasingly sophisticated  statistical approaches adopted to evaluate the overall impact of the  retention interventions. The initial findings using Chi square tests  revealed a significant association between the intervention and  retention outcomes. A similarly optimistic finding was observed  when performing a logistic regression with retention as the  outcome variable. The results indicated that contacted students  were approximately 31% more likely to be retained in their  studies over non-contacted students. Including additional student  attributes, age and international student status were also  significant variables. At this level of analysis, the interventions  would appear to be effective in achieving their desired outcome.    However, as dark clouds are harbingers of stormy days so too  were the outcomes of the logistic mixed-effect model. The results  were in stark contrast to the previous statistical analyses. In  essence the model demonstrated that the call campaigns were less  than successful and that the student-specific features, in lieu of the  actual implemented interventions, explained the variance. The  model was more robust and offered an almost perfect explanation  of the variance in the data. Hence, the model outcomes strongly  suggest that any improvement in retention cannot be attributed to  the pilot program.    This paper provides two important findings for future learning  analytics work evaluating early alert systems and the associated  intervention strategies. First, and as noted above, there is a need  for rigorous statistical approaches to validate any claims made  about impact of early alert systems and interventions. The paucity  of learning analytics research evaluating the impact of support  interventions can be explained in part, by the methodological  constraints associated with these forms of research. For example,  in most education settings the evaluation of any intervention  cannot be established through randomized control trials or control  groups. Hence, the limitations and constraints places a greater  emphasis on applying strong statistical methods. Second, further  data related to student individual differences such as motivation,  employment, prior studies and self-efficacy are required for  interventions to have impact.    Pardo and colleagues [25; 26] have previously argued that any  learning analytics feedback that is designed to improve student  learning must be personalized, instructional and timely. While this  advice reflects much of the assessment literature [12] the  complexity lies in the ability for educators to do this at scale and  at the optimal learning phase for each student. As such, Pardo et  al., [26] noted that there are significant challenges for educators in  developing sufficient knowledge of the data analyses, presentation   of results as well as the sense-making processes to appropriately  interpret such findings and translate them into meaningful  feedback processes. For the majority of teaching staff, the  capacity to fully appreciate and understand all steps in the  learning analytics chain to best enact personalized intervention  processes is by and large a complex and difficult undertaking.    To address these issues information related to student motivations,  career and learning goals can be requested at the commencement  of study. For instance, simply asking students via an online form  their reasons for undertaking the study, desired grade and career  and learning motivations would provide a deeper understanding of  the individual student and lead to richer personalization of learner  feedback. As a short example, a capable final year student  undertaking an elective course may only be seeking a pass grade.  In contrast other students may be seeking a high distinction to  maintain a high overall grade point average. The level of study,  motivation and timing of feedback that would be provided to  students in these examples would significantly differ. The work  by Wright, et al., [32] with the E2Coach is somewhat  representative of such a model. That is the integration of student  psychological dimensions with analytics automation processes.  However, the level of work required to code per course, and  instructor is exceedingly high and for broader adoption an  unlikely occurrence. Requesting student input and demonstrating  how this input can aid personalized feedback for students could be  a more scalable option to achieve similar outcomes.    6. CONCLUSION  This paper sought to derive productive learnings from a failed  pilot program. The study initially identified that the intervention  process was significant and positively associated with student  retention. However, while the introduction of more rigorous  statistics provided a greater explanatory power they also presented  a bleaker outcome. The study highlights the need to revise the  intervention process in order to sharpen its effectiveness. As noted  in the paper  even with marginal impact, the intervention process  still plays an important role in developing student community and  raising awareness of the available support services.   7. REFERENCES  [1] Arnold, K.E., 2010. Signals: Applying Academic Analytics.   In EDUCAUSE Quarterly Magazine EDUCAUSE.  [2] Arnold, K.E. and Pistilli, M., 2012. Course Signals at   Purdue: Using Learning Analytics to Increase Student  Success In Proceedings of the 2nd International Conference  on Learning Analytics and Knowledge (Vancouver, British  Columbia, Canada2012), ACM, 267-270.   [3] Baker, R.S.J.D. and Siemens, G., 2014. Educational data  mining and learning analytics. In Cambridge Handbook of  the Learning Sciences (2nd Edition), K. Sawyer Ed.  Cambridge University, New York.   [4] Barber, R. and Sharkey, M., 2012. Course correction: Using  analytics to predict course success. In Proceedings of the  2nd International Conference on Learning Analytics and  Knowledge (LAK12) (Vancouver, Canada2012), ACM,  New York, USA, 259-262. DOI=  http://dx.doi.org/10.1145/2330601.2330664.   [5] Bates, D., Maechler, M., Bolker, B., and Walker, S., 2014.  lme4: Linear mixed-effects models using Eigen and S4. R  package version 1, 7.   [6] Brooman, S. and Darwent, S., 2014. Measuring the  beginning: a quantitative study of the transition to higher  education. Studies in Higher Education 39, 9, 1523-1541.     [7] Campbell, J., 2007. Utilizing student data within the course  management system to determine undergraduate student  academic success: An exploratory study Purdue University.   [8] Caulfield, M., 2013. What the Course Signals  Kerfuffle  is  About, and What it Means to You. Educause,  http://www.educause.edu/blogs/mcaulfield/what-course- signals-kerfuffle-about-and-what-it-means-you.   [9] Clow, D., Ferguson, R., Macfadyen, L., Prinsloo, P., and  Slade, S., 2016. LAK Failathon. In Sixth International  Conference on Learning Analytics & Knowledge, S.  Dawson, H. Drachsler and C. Rose Eds. ACM, Edinburgh,  Scotland, 509-511. DOI=  http://dx.doi.org/10.1145/2883851.2883918.   [10] Gaevi, D., Dawson, S., Rogers, T., and Gasevic, D., 2016.  Learning analytics should not promote one size fits all: The  effects of instructional conditions in predicting academic  success. The Internet and Higher Education 28, 68-84.  DOI=  http://dx.doi.org/http://dx.doi.org/10.1016/j.iheduc.2015.10. 002.   [11] Gilardi, S. and Guglielmetti, C., 2011. University life of  non-traditional students: Engagement styles and impact on  attrition. The Journal of Higher Education 82, 1, 33-53.   [12] Hattie, J. and Timperley, H., 2007. The power of feedback.  Review of Educational Research 77, 1, 81-112.   [13] Hayes, A.F., 2006. A Primer on Multilevel Modeling.  Human Communication Research 32, 4, 385410. DOI=  http://dx.doi.org/https://doi.org/10.1111/j.1468- 2958.2006.00281.x.   [14] Jayaprakash, S.M., Moody, E.W., Laura, E., Regan, J.R.,  and Baron, J.D., 2014. Early Alert of Academically At-Risk  Students: An Open Source Analytics Initiative Journal of  Learning Analytics 1, 1, 6-47.   [15] Kahu, E.R., 2013. Framing student engagement in higher  education. Studies in Higher Education 38, 5, 758-773.   [16] Kift, S., Nelson, K., and Clarke, J., 2010. Transition  Pedagogy: A third generation approach to FYE - A case  study of policy and practice for the higher education sector.  The International Journal of the First Year in Higher  Education 1, 1, 1-20.   [17] Kovanovic, V., Gasevic, D., Dawson, S., Joksimovic, S.,  Baker, R.S., and Hatala, M., 2015. Does Time-on-task  Estimation Matter Implications on Validity of Learning  Analytics Findings. Journal of Learning Analytics 2, 3, 81- 110. DOI=  http://dx.doi.org/http://dx.doi.org/10.18608/jla.2015.23.6.   [18] Kovanovic, V., Gasevic, D., Dawson, S., Joksimovic, S.,  Baker, R.S., and Hatala, M., 2015. Penetrating the black  box of time-on-task estimation. In Proceedings of the Fifth  International Conference on Learning Analytics And  Knowledge ACM, Poughkeepsie, New York, 184-193.  DOI= http://dx.doi.org/10.1145/2723576.2723623.   [19] Krumm, A.E., Waddington, R.J., Teasley, S.D., and Lonn,  S., 2014. A Learning Management System-Based Early  Warning System for Academic Advising in Undergraduate  Engineering. In Learning Analytics: From Research to  Practice,, J.A. Larusson and B. White Eds. Springer  Science+Business Media, New York, USA, 103-119.   [20] Laidlaw, A., Mclellan, J., and Ozakinci, G., 2015.  Understanding undergraduate student perceptions of mental  health, mental well-being and help-seeking behaviour.  Studies in Higher Education Published online, 1-13. DOI=  http://dx.doi.org/10.1080/03075079.2015.1026890.   [21] Macfadyen, L. and Dawson, S., 2010. Mining LMS data to  develop an early warning system for educators: A proof  of concept. Computers & Education 54, 2, 588-599.   [22] Morris, L.V., Finnegan, C., and Wu, S., 2005. Tracking  student behavior, persistence, and achievement in online  courses. Internet and Higher Education 8, 3, 221-231.   [23] Nakagawa, S. and Schielzeth, H., 2013. A general and  simple method for obtaining R2 from generalized linear  mixed-effects models. . Methods in Ecology and Evolution  4, 2, 133-142.   [24] Nichols, M., 2010. Student perceptions of support services  and the influence of targeted interventions on retention in  distance education. Distance Education 31, 1, 93-113.   [25] Pardo, A. and Dawson, S., 2016. Learning Analytics. How  can Data be used to Improve Learning Practice . In  Measuring and visualising learning in the information-rich  classroom, P. Reimann, S. Bull, M. Kickmeier-Rust, R.  Vatrapu and B. Wasson Eds. Routledge, USA, 41-55.   [26] Pardo, A., Mirriahi, N., Martinez-Maldonado, R.,  Jovanovic, J., Dawson, S., and Gaevi, D., 2016.  Generating actionable predictive models of academic  performance. In Proceedings of the Sixth International  Conference on Learning Analytics & Knowledge  (Edinburgh, UK2016), ACM, New York, USA, 474-478.   [27] Romero, C., Espejo, P.G., Zafra, A., Romero, J.R., and  Ventura, S., 2013. Web usage mining for predicting final  marks of students that use Moodle courses. Comput. Appl.  Eng. Educ 21, 135-146. DOI=  http://dx.doi.org/10.1002/cae.20456.   [28] Romero, C., Lpez, M.-I., Luna, J.-M., and Ventura, S.,  2013. Predicting students' final performance from  participation in on-line discussion forums. Computers &  Education 68(10//), 458-472. DOI=  http://dx.doi.org/http://dx.doi.org/10.1016/j.compedu.2013. 06.009.   [29] Romero, C. and Ventura, S., 2010. Educational Data  Mining: A Review of the State of the Art. IEEE  Transactions on Systems, Man and Cubernetics- PartC:  Applications and Reviews 40, 6, 601-618.   [30] Siemens, G., Dawson, S., and Lynch, G., 2013. Improving  the Productivity of the Higher Education Sector: Policy and  Strategy for Systems-Level Deployment of Learning  Analytics. Society for Learning Analytics Research; and the  Australian Government Office for Learning and Teaching.   [31] Wolff, A., Zdrahal, Z., Nikolov, A., and Pantucek, M.,  2013. Improving retention: predicting at-risk students by  analysing clicking behaviour in a virtual learning  environment. In Proceedings of the Proceedings of the  Third International Conference on Learning Analytics and  Knowledge (Leuven, Belgium2013), ACM, New York,  USA, 145-149. DOI=  http://dx.doi.org/10.1145/2460296.2460324.   [32] Wright, M.C., Mckay, T., Hershock, C., Miller, K., and  Tritz, J., 2014. Better Than Expected: Using Learning  Analytics to Promote Student Success in Gateway Science.  Change: The Magazine of Higher Learning 46, 1, 28-34.  DOI= http://dx.doi.org/10.1080/00091383.2014.867209.   [33] Zaitseva, E., Milsom, C., and Stewart, M., 2013.  Connecting the dots: Using concept maps for interpreting  student satisfaction. Quality in Higher Education 19, 2,  225-247.           "}
{"index":{"_id":"60"}}
{"datatype":"inproceedings","key":"Ocumpaugh:2017:GCR:3027385.3027435","author":"Ocumpaugh, Jaclyn and Baker, Ryan S. and San Pedro, Maria O. C. Z. and Hawn, M. Aaron and Heffernan, Cristina and Heffernan, Neil and Slater, Stefan A.","title":"Guidance Counselor Reports of the ASSISTments College Prediction Model (ACPM)","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"479--488","numpages":"10","url":"http://doi.acm.org/10.1145/3027385.3027435","doi":"10.1145/3027385.3027435","acmid":"3027435","publisher":"ACM","address":"New York, NY, USA","keywords":"college attendance, guidance counselors, intelligent tutoring systems, predictive analytics, stakeholder reports, student engagement","Abstract":"Advances in the learning analytics community have created opportunities to deliver early warnings that alert teachers and instructors when a student is at risk of not meeting academic goals [6], [71]. Alert systems have also been developed for school district leaders [33] and for academic advisors in higher education [39], but other professionals in the K-12 system, namely guidance counselors, have not been widely served by these systems. In this study, we use college enrollment models created for the ASSISTments learning system [55] to develop reports that target the needs of these professionals, who often work directly with students, but usually not in classroom settings. These reports are designed to facilitate guidance counselors' efforts to help students to set long term academic and career goals. As such, they provide the calculated likelihood that a student will attend college (the ASSISTments College Prediction Model or ACPM), alongside student engagement and learning measures. Using design principles from risk communication research and student feedback theories to inform a co-design process, we developed reports that can inform guidance counselor efforts to support student achievement.","pdf":"Guidance Counselor Reports of the ASSISTments College  Prediction Model (ACPM)   Jaclyn Ocumpaugh,   Graduate School of Education   University of Pennsylvania  3700 Walnut St., Phil., PA 19104 USA   +1 (215) 573-2990  jlocumpaugh@gmail.com   M. Aaron Hawn  Teachers College, Columbia University   525 W. 120th St.  New York, NY 10027 USA  mah102@tc.columbia.edu   Ryan S. Baker  Graduate School of Education   University of Pennsylvania  3700 Walnut St., Phil., PA 19104 USA   +1 (215) 573-2990  ryanshaunbaker@gmail.com   Cristina Heffernan  Department of Computer Science  Worcester Polytechnic Institute   100 Institute Rd.  Worcester, MA 01609 USA   ch@wpi.edu    Stefan A. Slater  Graduate School of Education   University of Pennsylvania  3700 Walnut St., Phil., PA 19104 USA   Maria O.C.Z. San Pedro  ACT, Inc.   2122 ACT Circle  Iowa City, IA 52245 USA   speetsp@gmail.com   Neil Heffernan  Department of Computer Science  Worcester Polytechnic Institute   100 Institute Rd.  Worcester, MA 01609 USA   nth@wpi.edu   ABSTRACT Advances in the learning analytics community have created  opportunities to deliver early warnings that alert teachers and  instructors when a student is at risk of not meeting academic goals  [6], [71]. Alert systems have also been developed for school  district leaders [33] and for academic advisors in higher education  [39], but other professionals in the K-12 system, namely guidance  counselors, have not been widely served by these systems. In this  study, we use college enrollment models created for the  ASSISTments learning system [55] to develop reports that target  the needs of these professionals, who often work directly with  students, but usually not in classroom settings. These reports are  designed to facilitate guidance counselors efforts to help students  to set long term academic and career goals. As such, they provide  the calculated likelihood that a student will attend college (the  ASSISTments College Prediction Model or ACPM), alongside  student engagement and learning measures. Using design  principles from risk communication research and student feedback  theories to inform a co-design process, we developed reports that  can inform guidance counselor efforts to support student  achievement.    CCS Concepts Applied computing  Computer-managed instruction   Keywords  Intelligent tutoring systems, stakeholder reports, predictive  analytics, guidance counselors, college attendance, student  engagement.   1. INTRODUCTION Learning analytics, which has long provided tools for modeling  knowledge states (e.g., [15]), has now advanced to the point that  real-time engagement indicators (e.g., affective states [48]) and  long-term outcomes (e.g., predictions of college attendance [55],  [56]) are also becoming common measures. In addition to driving  basic research, these models have proven to have a wide range of  practical applications. They have been embedded in automated  personalization approaches [10], [7], [20] and used to generate  reports for both students (e.g., [7], [8], [45], [71]) and education  professionals (e.g., [23], [6], [39]). However, as the learning  analytics community provides more sophisticated measures,  understanding how best to communicate these findings to a wide  range of audiences is of increasing importance.    In this paper, we use a co-design process [51] to develop an early  warning system for school guidance counselors using data from  student interactions with ASSISTments, an intelligent tutor for  middle school mathematics [53]. These reports leverage the  extensive development of cross-validated student models already  available to Learning Analytics researchers who are studying  ASSISTments data. Specifically, they use of models of student  engagement and learning (knowledge, gaming the system,  carelessness, off-task behavior, boredom, confusion, frustration,  and engaged concentration). These models, which were further  refined to ensure population validity [48], have been used to  predict state standardized exams [49], college attendance [55],  college major [56], and the selectivity of the college attended [57].  There has been relatively little work, however, to provide data on  these types of fine-grained models to school personnel, who could  use them in data-driven decision-making. In this paper, we discuss  our efforts to use these models to provide guidance counselors,   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4870-6/17/03$15.00 DOI: http://dx.doi.org/10.1145/3027385.3027435    who are responsible for a wide array of educational decisions that  impact students lives, with learning and engagement information  that might not otherwise be available in a typical students file.   Given the large body of research demonstrating that students  trajectories towards college enrollment and success begin years  before the college application process [38], there is considerable  potential to improve outcomes through an early warning system  that targets students in the middle school years. However,  research on student feedback suggests that this type of  information must be presented carefully [32], [69]. Students need  assistance interpreting the feedback (both in terms of interpreting  the individual constructs, and in terms of understanding the  correlational nature of these models), and they need help setting  concrete goals based on this information.   Guidance counselors, who are often over-burdened in their job  duties, could benefit from richer data about student engagement  with specific domains such as mathematics, which would help  them to better support students in preparing for college or for  setting alternative career goals. For this reason, in conjunction  with four middle school guidance counselors, we have developed  two reports based on [55]s college enrollment prediction model  (referred to in this paper as the ASSISTments College Prediction  Model or ACPM). The first is the Individual Forecast Report,  which provides each students likelihood of enrolling in college  (as predicted by the ACPM) as well as information about which  indicators of learning and engagement (the features used to  generate the ACPM) are most heavily contributing to each  students prediction. The second is the Group Summary of Lowest  Performing Factors, which allows guidance counselors to take a  group of students (e.g., all those with a low chance of attending  college or all those in a particular class) and to identify which  learning and engagement factors are most in need of interventions  for these students.   2. BACKGROUND  2.1 ASSISTments  ASSISTments is an intelligent tutoring system designed to assess  students mathematics knowledge while it assists in learning with  automated scaffolding and hint messages [53]. ASSISTments is a  useful context to conduct this type of research, as it already  provides a wide variety of reports on individual students and at  the classroom level. Currently, these reports are largely geared  towards helping teachers address specific learning objectives (e.g.,  has a student mastered a specific skill) or towards supporting  grading and grade-book management goals (e.g., automatically  tallying correctness and assignment completion). However,  ASSISTments also has reports in place for parents on homework  completion and performance [12]. While ASSISTments has not  yet offered reports to guidance counselors, it has the infrastructure  in place to do so.    2.2 Guidance Counselors  Guidance counselors provide advice on academic, career, college  readiness, and other competencies to students, as part of the  school professional community [2]. In the US, these positions  were initially created solely to provide support the college  application process, but their roles have changed substantially  over the years (see reviews in [36], [41], [54]).    Today, 24 of 50 US states mandate that schools provide guidance  counselors [3], however guidance counselors often have to  support students while dealing with extremely high counselor to  student ratios (e.g., 1:800) [2]. Moreover, in schools where they   are present, counselors are tasked with a wide range of data- driven jobs, such as (a) helping principals to identify/resolve  student needs, (b) advocating for students during meetings that  involve future academic or professional plans, (c) providing  individual and small/group services that support social  development and learning, (d) counseling students with behavior  problems, (e) providing individual students with academic  program planning, and (f) collaborating with teachers in order to  develop effective classroom management strategies [2]. As  guidance counselors are increasingly expected to take on more  administrative roles [36], they are overburdened with clerical  activities and tasks outside their core role, including attendance  monitoring, hall monitoring, data entry, and many other support  activities for which schools are understaffed [13], [50], [11], [28].    An early warning system that provides learning and engagement  data could supplement data from parents and teachers and student  self-report data that busy guidance counselors typically rely upon.  The use of such a system could significantly improve  opportunities to identify the students who are most in need of  services, ensure that students are being appropriately challenged,  develop programs that address student difficulties in dealing with  confusion, frustration, or boredom, identify teachers who are need  of support, and otherwise ensure that students are being given  adequate opportunities to prepare for college.   3. COLLEGE ENROLLMENT  PREDICTION MODEL  Our reports to guidance counselors are built from San Pedro et  al.s ACPM model [55] that infers college enrollment. The ACPM  uses a discovery with models approach, where one model is used  as a component in another model (see review in [26]).    Specifically, the ACPM was developed by applying models of  student engagement and learning to log files of 3,747 students  who had used ASSISTments while they were in middle school  during the 2004-05 to 2006-07 school years. In this longitudinal  study [55], these models were then used to predict which students  enrolled in college several years later, using data from the  National Student Clearinghouse, which maintains records on all  U.S. college students (http://www.studentclearinghouse.org/).    The final model for college attendance was developed using  logistic regression and a backwards elimination procedure that  removed non-significant features. The resulting model included  six weighted features:   log   =  .351  1.145  + 1.119           +  .698   +  .261      + .217                              + .169    Readers should note that student knowledge, the feature weighted  most heavily, causes three features to change their direction in this  model (namely carelessness, confusion, and boredom) relative to  their direction when considered individually [55]. For example, as  student knowledge increases, carelessness becomes positively  correlated with college attendance even though it is negatively  associated with college attendance when considered individually.  Under standard 5-fold cross validation (at the student level) this  model achieved an A of 0.686 and a Kappa of 0.247 [55],  indicating that it can generalize reasonably well to new students.     4. CO-DESIGN & DESIGN PRINCIPLES   In order to develop a report that could effectively communicate  the college enrollment predictions and the reasons why a  prediction is made for specific students to busy guidance  counselors, we used a modified co-design process. In traditional  co-design, practitioners are included throughout the design  process [51]. In this case, we worked with counselors to determine  what kind of data would be most useful, but chose to present them  with several preliminary design options, rather than to hold design  meetings with them where the team started from scratch.    This modification to the typical co-design process was motivated  by two primary concerns. First, there is a large literature on the  communication of risk which suggests that the simpler designs  typically preferred by end users are sometimes inadequate for  communicating both the risk involved and the certainty of the  model [5]. Second, because this project involved guidance  counselors, a group whose schedules are regularly overtaxed, we  were reluctant to take more of their time in designing than we  absolutely had to. Thus, we sought to utilize design principles  from previous research on risk communication and educational  feedback to create first designs, presenting potential  representations of the predictions to the guidance counselors who  participated in the design process with us. In this way, we were  able to leverage the benefits of the co-design process while also  ensuring that we did not waste our collaborators time by asking  them to reinvent principles that were already well established in  the literature.    4.1 Risk Communication Principles  Risk-communication research influenced both the initial and final  designs of our reports. In particular, we consulted well-known  research on how different forms of data presentation are  interpreted by both experts and novices (cf. [4], [29], [61]), since  guidance counselors may have varying levels of training in data  analysis. Furthermore, it has long been known that even highly  trained professionals can interpret information differently when it  is presented in different ways or using different scales (e.g., [59]).     Given the potential risks involved when presenting long-term  predictions about students, these concerns were given serious  consideration during our initial design process. As such, our initial  designs drew on several principles that are common in the  information design literature (e.g., [35]), including many related  to data visualization (to be discussed in greater detail in Section  6). As the design process evolved, other common design  principles from the risk communication literature were also  incorporated into our designs, many of which were related to the  graphic presentation of the material (e.g., RC#1a-e, Table 1,  below).    Many of these principles are best understood in context, and will  be discussed more thoroughly in the following sections. However,  RC2, which deals with labels, deserves further consideration, as  does RC4 (provide baseline risks). These principles will be  defined in this section in order to facilitate the more thorough  discussions to follow.               Table 1. Design Principles from Risk Communication (RC)   Design Principles Source  RC1 Visual Characteristics    a Bar graphs encourage comparisons, but  are not optimal for proportional data   [29], [35], [64]   b Tables inhibit interpretations of precision  [35]   c Pie charts can communicate part-whole  relationships clearly, but only if  displaying a small number of categories    [61], [64]   d Keep scales equal and in the same  direction   [30], [5], [64]   e Related constructs should have matching  styles/colors (and unrelated constructs  should contrast)   [42], [61]   f Duplicate information  [61]    RC2 Labeling Characteristics   a Rely on cultural metaphors to reduce   working memory demands  [25], [61], [62],  [66]   b Frame labels to encourage fail-soft  interventions   [21], [67]   RC3 Demonstrate interactions [35]   RC4 Provide baseline risks [4], [45]   RC5 Do not exaggerate precision of  predictions   [35]      4.1.1 Cultural Metaphors (RC2a)  Work in the visual representation of information suggests that  users can only process a limited amount of new information,  leading many to suggest that designers rely on common cultural  paradigms, or metaphors, to aid working memory when presenting  data [25]. Examples from the literature that were used in this  study include traffic light coding schemes (green is good, red is  bad, and yellow urges caution; also see [6]) and the common  English metaphor of up is good and down is bad.    4.1.2 Framing Labels to Encourage Fail-Soft  Interventions (RC2b)  Research on attribute framing effects explores the degree to which  equivalent information, when presented with either positive or  negative reference points, biases peoples judgments [38]. For  example, [67] found that when people were presented with  mathematically identical hypothetical scenarios, they made riskier  choices when the odds were framed negatively (33% of people  will not die) than they did when the same information was framed  positively (33% of people will be saved).   Similarly, framing research has also examined the effects that  labels have on the behavior of those in positions of authority,  demonstrating that people are more likely to issue punishments to  people whose evaluations are framed negatively than they are to  the same people when the evaluations are framed positively [21].  Since guidance counselors are part of the authority structure in a  school, and since they often face high ratios of students to  guidance counselors, where even the most empathetic counselors  may be unable to maintain close relationships with every student  in their school, this research was especially relevant. Therefore,  we worked to ensure that our reports were framed in a manner that  would be most likely to result in fail-soft interventions (e.g. [43])  rather than punitive responses.      4.1.3 Provide Baseline Risks (RC4)  Research suggests that when data is presented without context, it  is more difficult for people to understand the risk (e.g., [4]). That  is, merely telling people that a student is unlikely to attend college  or that the student is struggling with a particular engagement  measure is not particularly useful unless it is clear how common  this issue is.    4.2 Student Feedback Principles  While studies suggests that student feedback is most effective  when it comes from a trusted source [73] like a guidance  counselor, other research suggests that careful framing of the  feedback is also important. Meta-analysis of the research on  feedback interventions (FI) has shown that while they can be  highly effective, in over one third of cases they actually reduce  performance [32]. This is perhaps not surprising since the  characteristics of feedback (e.g., amount, frequency, type, and  specificity) are known to interact with both student characteristics  (e.g., prior knowledge, self-efficacy, motivation, etc.) and task  characteristics (e.g., high cognitive load) in determining  effectiveness (see review in [72]).   Research on the on the effects of framing feedback messages  demonstrates that positively framed feedback is most effective  and suggests these effects are enhanced when performance  feedback is paired with interventions that help students to produce  concrete goals [32]. Experimental research has shown that  positively-framed feedback results both in higher self-efficacy and  in improved performance compared to feedback that only lists  problems [69]. Students who receive positively framed feedback  are more likely to self-select increasingly challenging tasks [34],  possibly because it allows them to set goals, while those who  received negative feedback were more likely to show avoidance  behavior, where students work to minimize opportunities for  negative feedback by any means, sometimes including avoiding  the academic task altogether [34]. Meanwhile, negatively-framed  feedback is thought to add to students cognitive load, by  requiring them to manage their self-concept while performing  challenging tasks [68].   Findings such as these have led researchers to advocate for  sustainable feedback principles [44]. In line with research  investigating the development of growth mindsets (e.g., [14], [22],  [63]), these researchers argue that feedback should evaluate the  task performance rather than the student. They also recommend  that evaluations take place immediately after a relevant task,  especially when delivering high-stakes predictions.    Table 2 Student Feedback Principles   Design Principles Source  SF1 Focus evaluations   on the task rather  than the learner.   Sustainable Feedback Theory   (e.g., [44]); Growth Mindset (e.g., [14],  [22])   SF2 Frame evaluations  positively.   Feedback Intervention Theory   (e.g., [27], [32], [68])   SF3 Facilitate the setting  of concrete goals.   Sustainable Feedback Theory (e.g.,  [44]); Growth Mindset (e.g., [14], [22]),  Feedback Intervention Theory (e.g.,  [32])      4.3 Guidance Counselors Design Priorities  Four guidance counselors participated in the design process,  providing feedback on the kind of data that would be useful to  them and the ways in which it should be presented. As discussed  above, we used a modified co-design process, leveraging both the   expertise that could be produced through participatory design with  end-users (guidance counselors) and design principles that reflect  effective strategies already established in the research literature.  This approach allowed us to ensure both that designs were  perceived as useful and that the designs lent themselves to the  most accurate interpretations possible.   During the co-design process, we explained the ACPMs features  (knowledge, correctness, carelessness, confusion, boredom,  number of first actions) and solicited advice about the kind of  information that was most likely to be useful when providing  guidance counselor services. While one counselor suggested that  she would only want information about students who were on the  cusp of not making it to college (those assessed as having a 40- 60% chance of attending college, according to this model), others  were interested in having information about all students. As one  counselor explained, the first thing he would do would be to find  the students he knew best, particularly those who were his top  performers, in order to better understand the meaning of the data.   As the design process evolved and counselors viewed prototypes,  many of their preferences reflected design principles outlined in  previous research. These included risk communication principles,  such as keeping scales in the same direction (e.g., [35]) and  duplicating graphics with alternative means of presentation (e.g.,  [35], [19]).    Guidance counselor design priorities also reflected research on the  framing of interventions. They expressed concerns that echo  longstanding admonitions about prematurely labeling a student  (e.g., [18], [24], [47], [1]), stressing the importance of framing the  model predictions for individual students in ways that reflect the  identification of opportunities rather than the creation of static  identities of underachievement (e.g., SF1: labeling with reference  to performance and behavior on specific tasks or situations rather  than labeling the student more generally, as recommended by  growth mindset research). Two counselors explicitly said that  using negative labels would be detrimental during discussions  with students, either because it would disrupt students ability to  focus or because students would pull away from someone who  was criticizing them (cf., [68]). A third counselor explained that  generic coaching (in his example Oh come on, you can do  better!) was ineffective. Positive labels, he said, would assist in  setting tangible and consistent goals that could be celebrated with  the student upon achievement, in line with sustainable feedback  theories [44].    Thus, we worked with the guidance counselors to select  positively-framed audience-appropriate labels for the measures of  learning and engagement that were used to predict college  attendance (knowledge, correctness, carelessness, confusion,  boredom, number of first actions). However, antonyms (often  corresponding to the original published label for the construct)  were kept in parentheses in order to help disambiguate what each  label meant. During this process, it was determined that while the  concept of correctness (one of the predictor variables in the  model) did not provide actionable information to the guidance  counselors, the concept of gaming the system, (not in the model,  but relatively strongly negatively correlated with college  attendance) did provide actionable information that was not  available in other parts of the model.    Table 3 shows the resulting labels, which were constructed in line  with SF1 to focus on performance and behavior in specific tasks  or situations, rather than the learner (e.g., proficiency on tested  skills rather than a proficient student, meticulousness/carelessness     rather than a meticulous/careless student, etc.) The resulting  report designs are discussed in greater detail in the next section.    Table 3. New Predictor Labels   Original Re-Labeled   Knowledge Proficiency on Tested Skills   (Low Proficiency)  Carelessness Meticulousness   (Carelessness)  Confusion Adequate Help Seeking   (Confusion)  Boredom Interest Level   (Boredom)  Number of 1st Actions High Practice   (Low Practice)  Gaming the System Sincere Effort   (Gaming the System)     5. REPORT DESIGNS & FORMALIZED  PRINCIPLES   Guidance counselors who participated in our process wanted  reports for two different purposes. Thus we developed two  reports: (1) the Individual Forecast Reports, which facilitate  individual interventions, such as one-on-one meetings to develop  personalized goals and (2) the Group Summary of Lowest  Performing Factor, which facilitate larger group interventions,  such working with teachers to identify areas of improvement that  an entire class could strive for.   5.1 Individual Forecast Report  Guidance counselors preferred that the ACPMs prediction (the  probability that a given student would attend college) be presented  alongside factors contributing to that students prediction. One  concern with this approach is that the ACPM is not guaranteed to  be causal and a variety of other factors will influence a specific  students college trajectory (e.g., [40], [58], [52]), but the  counselors stressed that part of their responsibilities are helping to  set goals that improve learning and engagement regardless of a  students desire to attend college. It also raised concerns because  of the complexities involved with communicating a logistic  regression model to someone who is not familiar with that  algorithm (or with advanced data analysis in general).    Longstanding research shows that tables are the best presentation  method when individual values (rather than comparisons across  subjects) are important (e.g., [29]). Tables also allow data from  multiple sources to be presented simultaneously, providing  baseline measures that can help contextualize each students  prediction (RC4). However, in order to present our data in  accordance with these and other design principles, including those  that caution against over-representing model precision (RC5), the  data first had to undergo several conversions.    These conversions will be explained in the remainder of section  5.1, while the evaluation of design principles that apply to the  Individual Forecast Report will be discussed in section 5.3.  However, it is worth summarizing the overall design of this report  (shown in Figure 1), which includes nine columns. In addition to   (1) each students name (shown here with alphabetized  pseudonyms), (2) their individual College Forecast (CF), and (3)  each students Lowest Performing Factor in the ACPM, there are  five columns showing normalized performance for the features  that comprise the ACPM. As summarized above (in Table 3),   these include (4) Proficiency on Tested Skills (formerly  Knowledge), (5) High Practice (formerly Number of 1st Actions),  (6) Meticulousness (formerly Slip or Carelessness), (7) Interest  Levels (formerly Boredom), and (8) Adequate Help Seeking  (formerly Confusion). Finally, the last column shows (9) Sincere  Effort (formerly Gaming the System), which was not included in  the ACPM but was individually correlated with college  enrollment.     Figure 1. Individual Forecast Report   5.1.1 College Forecast (CF)  The first conversion involved the confidence interval generated by  the ACPM. For each student, this value was converted into a  percentage and grouped into five ranges (0-20%, 21-40%, 41- 60%, 61-80%, or 81-100% chance of attending college), known as  the College Forecast (CF). These predictions can be seen in the  second column (after each students name) in Figure 1, where  they are also color-coded.   The decision to use these groupings was made in order to  minimize comparisons between individual students and to avoid  over-representing the precision of the model (e.g., [35]). That is,  the difference between a student who is forecasted to have a 63%  chance of attending college and a student forecasted to have 65%     chance of attending college is likely to be inconsequential and  well-within the models margin of error. By providing a more  abstract label than was used to generate the report, we seek to  direct the users focus to more meaningful differences.   5.1.2 Lowest Performing Factor (LPF)  In the third column of the Individual Forecast Report, each  students the lowest performing factor (LPF) is also identified.  This is the factor that is most negatively contributing to each  students probability of attending college. As Figure 1 shows, the  labels used in this column matches the column labels for the  learning and engagement factors in the following five columns  (RC1e).    5.1.3 Learning and Engagement Factors   Next, we sought to communicate the degree to which each  students risk of not attending college is increased by each of the  learning and engagement factors.    Because the varying scales and coefficients for each factor in the  ACPM could make interpretation challenging (RC1d), we  calculated what we will refer to as contribution weightings normalized values that reflect the weighting of each feature in the  ACPM for a specific students prediction. Specifically, we took  the value of the feature for that student, multiplied it by the weight  in the logistic regression equation for that feature, and ran it  through a logistic transformation (as was done in the original  logistic regression equation). These contribution weightings  therefore range from 0 (least contributing to specific students  prediction of college attendance) to +1 (most contributing to  specific students prediction of college attendance).    This process was conducted for the 5 features in [55]s model (the  ACPM). For gaming the system, which was not included in the  ACPM, but which was shown to correlate with lower rates of  college enrollment [55], we used a mathematically equivalent  process, simply computing the value of a single-feature logistic  regression model based on gaming the system for this student,  also resulting in a -1 to +1 rating for each student.    As with the CF, normalized scores for each of the learning and  engagement features were grouped into three ranges. Table 4  explicates the traffic-light color scheme (RC2a) and associated  labels (RC1f) used in Figure 1.   Table 4. Representation of Contribution Weightings in the  Individual Forecast Report   Contribution  Weightings Color Label Interpretation  0 to 0.33 red - intervention needed   0.33 to 0.66 yellow avg. intervention could help  0.66 to 1 green + no intervention needed      This coding scheme matches that of a related construct (the  reporting of each students CF), where data was grouped into five  ranges rather than three, supplementing it with comparable colors  (RC1e). (Recall that the CF used five categories: green for  students with over an 80% chance of attending college, light- green for students in the 60-80% range of attending college,  yellow for students in the 40-60% range, orange for students in  the 20-40% range, and red for students with under a 20% chance  of attending college.)    5.2 Group Summary of Lowest Performing  Factor  In order to assist guidance counselors in developing interventions  for groups of students and/or in providing pedagogical advice to  teachers, we developed a second report, the Group Summary of  Lowest Performing Factor. This allows guidance counselors who  have identified a particular group of interest (e.g., those in the 40- 60% prediction range or those in a given classroom) to determine  which factor or factors are most in need of interventions for that  group.    These reports are designed to provide baseline information on  selected groups of students (RC4). In doing so, these reports serve  to contextualize reports on individual students, demonstrating how  common it is for other students in the school or sub-population to  be struggling with a particular predictor variable. As with the  individual reports, it is important that these reports encourage  interventions which are fail-soft (e.g., unlikely to be harmful if not  relevant to every student in the group [43]).    In order to help our guidance counselor to quickly identify the  most pressing needs of students in a given range, we used pie  charts to show which of the model features were most negatively  influencing college predictions, which we called the Lowest  Performing Factor (LPF). Figure 2 shows the aggregated  information for students in the 40-60% prediction range at one  school.   Figure 2. Lowest Performing Factor (LPF) distribution for  students with a college forecast of 40-60%       5.3 Discussion: Design Principles Applied  With any design, there are trade-offs. End users often prefer  simple designs that seem easy to read, even though they may not  communicate the data as precisely as more complicated designs  [31]. As a result, it is important that co-design processes  incorporate opportunities to look carefully at what users do, rather  than relying exclusively on what they say they prefer, which often  trends towards more aesthetically pleasing, less precise designs.  As discussed above, we worked to identify design principals from  both the risk communication (RC, as summarized in Table 1)  literature and the student feedback (SF, as summarized in Table 2)  literature before presenting potential designs to the guidance  counselors who collaborated with us. This section discusses how  those principles were applied in our reports.    5.3.1 Individual Forecasts  The Individual Forecasts in this study are meant to present  information about each students chances of attending college. As     discussed above, our conversations with guidance counselors  suggested that providing details about students learning and  engagement could be as important as the College Forecasts (CFs),  since this data would help to determine the most appropriate  interventions.    Preliminary designs of the Individual Forecasts followed advice  to use tables rather than graphs (RC1b) to encourage counselors to  look up individuals rather compare between students, and also  bundled predictions for both the CF and the learning/engagement  features (grouping predictions and measures into ranges, rather  than providing raw numbers, for ease of interpretation). In  addition to facilitating a simpler design (e.g., [60]), this also  conformed to research principles cautioning that reports should  not exaggerate the level of precision (RC5, [35]).   We also followed research principles related to the use of cultural  metaphors in this design (RC2a). Specifically, we used a traffic- light coding scheme (e.g., [6]) where red = low performance,  yellow = caution, and green = good performance to highlight  differences in student performance on learning and engagement  features, as discussed above.   Our final reports also duplicate these graphic (color)  representations with alternative means of presentation (RC1f).  This was done in several different ways. First, a  plus/average/minus labeling system was applied to the learning  and engagement measures, duplicating their green/yellow/red  coding scheme. In addition to following an important design  principle, this also had the benefits increasing the accessibility of  the report for individuals with visual impairments and also  making the report easier to interpret when printed, since many  schools budgets limit their printing options to black/white  representations. This same principle was applied to the CF ranges,  so that the dark-green/light-green/yellow/orange/red coding  scheme was duplicated with the following labels: 0-20%, 20-40%,  40-60%, 60-80%, and 80-100% (achieving RC1e). Finally, we  approached the labeling of the Lowest Performing Factor in the  same way. While we had initially only used abbreviations of the  learning and engagement features to identify the LPF for each  student, we found that by using a color coding scheme for this  data that was also reflected in corresponding column labels, while  not as aesthetically pleasing as a plainer design, ultimately  facilitated more accurate interpretations.   As predicted by the literature, guidance counselors both reported  positive interpretations of these design choices and reflected these  positive responses in their ability to accurately interpret  hypothetical data while reading sample diagrams and discussing  the diagrams use. Even the color-coding scheme used to match  the LPF to column titles, which the guidance counselors initially  reported as being a bit distracting, was found to be helpful once  they began to use the reports to form interpretations.   Preliminary designs did not follow several other principles,  including keeping all scales in the same direction (RC1d), which  necessarily means that some variables were not positively framed  (SF2). However, our co-design process confirmed the benefits of  adhering to both principles. For example, not only did guidance  counselors report that they preferred positively-framed variables  (SF2) that could facilitate goal setting (SF3), they also found it  difficult to interpret negatively-labeled factors. As we worked  with them to demonstrate how to interpret interactions between  the learning and engagement variables in early designs (RC3),  there were repeated challenges in interpreting negatively-labeled  features. For example, when boredom was used as a column  label, counselors would alternate between interpreting the +/green   labeling system as meaning low boredom (as intended) and  high boredom (an incorrect interpretation). These interpretation  difficulties vanished when designs changed to match previously  identified principles in the literature.    Finally, we worked to create learning and engagement feature  labels that focused on the evaluation of the performance of the  task rather than the learner (SF1). However, some feature labels  were still ambiguous. (Notably, the use of the word attempted was  excluded from potential labels because of it had strong and  unintended connotations of failure for the guidance counselors, as  in students who attempted a problem but were incapable of  finishing it.) Moreover, negative affect terms like confusion and  boredom do not have clear-cut antonyms. Thus, in addition to  providing an explanation of each variable in the legend for the  Individual Forecasts, we also grounded each label with an  appropriate antonym, given parenthetically in smaller text in each  column. This design choice, which is similar to RC5, enabled us  to clarify the meaning of each learning and engagement feature,  which should also ultimately support guidance counselors in  helping students set concrete goals (SF3).    5.3.2 Group Summary of Lowest Performing Factor  The design of the Group Summary of Lowest Performing Factor  was, in many ways, simpler than that of the Individual Forecasts.  Following work from the risk communication literature that  suggests that pie charts are effective for communicating whole- part relationships to lay people (RC1b), we created the ability to  summarize data for a given group of students (e.g., those in a  single class or those in a particular CF range).  Labels and color-coding schemes for the Lowest Performing  Factors reflect those used in the Individual Forecasts (RC1e),  allowing guidance counselors to quickly move back and forth  between the two reports, and the key duplicates the use of both the  positively framed labels and the corresponding antonyms (RC1f).  In this way, we are able to ensure accurate interpretations of the  learning and engagement indicators reported for each group.   6. DISCUSSION & CONCLUSIONS   As learning analytics tools become more powerful, their use in the  development of practical tools is becoming more common.  Reports for instructors have become routine at all levels, and  reports for academic advisors in higher education are beginning to  become more standard. However, K-12 counterparts of academic  advisors, i.e., school guidance counselors, have yet to have reports  designed for their particular needs.   Research shows that job descriptions for guidance counselors  have become increasingly more data-driven. However, the  distillation of sophisticated modeling and analytics has not  reached this audience, a notable gap in the resources available to  this audience. Thus, as the learning analytics community  continues to grow, this project represents a first step in broadening  the audience of student reports from those who are typically  targeted (students, teachers, and administrators) to include  guidance counselors. Reports specifically designed to assist  guidance counselors should be given further consideration, and in  particular, their efforts to support student development and  teacher pedagogical training will benefit from further support.   Within this article, we discuss two reports designed for guidance  counselors in schools that use the ASSISTments mathematics  learning platform. Specifically, we provide information on  students college trajectories, using predictive analytics models  that can be applied at the end of middle school. Importantly, these  reports include data on student learning and engagement     measures, which will be beneficial to guidance counselors  efforts, even when they are counseling students who ultimately  decide not to pursue college.   We further present our development and design process for these  reports, including the principles from risk communication and  student feedback research that guided our designs. In general,  considerable thought and care should go into the design of reports,  as less effective design can lead to unintended and ineffective, or  even counter-productive, consequences. We anticipate that these  discussions will contribute both to the improvement of the designs  discussed in this study as well as to the development of new report  systems as this community continues to grow.    The next steps of the research presented here are to move from the  pilot work we have already done in partnership with guidance  counselors, to scaling the use of these reports. In this way, we can  better understand whether their use leads to any benefits to  students outcomes, both within the ASSISTments platform and in  their educational pursuits beyond.   7. ACKNOWLEDGMENTS  Our thanks to the guidance counselors who participated in our co- design process and to the NSF (Award #DRL-1031398) for  supporting this project.    8. REFERENCES  [1] Abikoff, H., Courtney, M., Pelham W., Koplewicz, H. 1993.   Teachers' ratings of disruptive behaviors: The influence of  halo effects. Journal of Ab. Child Psych., 21(5), 519-533.   [2] American School Counselor Association (ASCA) 2016a.  https://www.schoolcounselor.org/asca/media/asca/home/Role Statement.pdf Accessed October 2016.   [3] American School Counselor Association (ASCA) 2016b.  https://www.schoolcounselor.org/school-counselors- members/careers-roles Accessed October 2016.   [4] Ancker, J., Kaufman, D. 2007. Rethinking health numeracy:  A multidisciplinary literature review. J. American Medical  Informatics Association, 14(6), 713-721.   [5] Ancker, J., Senathirajah, Y., Kukafka, R., Starren, J. 2006.  Design features of graphs in health risk communication: A  systematic review. J. Am. Med. Infor. Assoc., 13(6), 608-618.   [6] Arnold, K., & Pistilli, M. 2012. Course signals at Purdue:  using learning analytics to increase student success. Proc. 2nd  Intal Conf. Learning Analytics & Knowledge 267-270.   [7] Arroyo, I., Ferguson, K., Johns, J., Dragon, T., Meheranian,  H., Fisher, D., & Woolf, B. P. 2007. Repairing disengage- ment with non-invasive interventions. AIED, 195-202.   [8] Arroyo, I., Woolf, B., Burelson, W., Muldner, K., Rai, D.,  Tai, M. 2014. A multimedia adaptive tutoring system for  mathematics that addresses cognition, metacognition and  affect. Intal J. Artificial Intelligence in Ed., 24(4), 387-426.   [9] Baadte, C., Schnotz, W. 2014. Feedback effects on perfor- mance, motivation & mood: are they moderated by the learn- er's self-concept. Scandinavian J. Ed. Res., 58(5), 570-591.   [10] Baker, R.S.J.d., Corbett, A., Koedinger, K., Evenson, S.,  Roll, I., Wagner, A, Naim, M., Raspat, J., Baker, D., Beck, J.  2006. Adapting to When Students Game an Intelligent  Tutoring System. 8th Intal Conf. Intelligent Tutoring  Systems, 392-401.   [11] Barry, B. 1984. A comparison of the perceptions of secon- dary school counselors, principals & teachers regarding the  ideal & actual functions of secondary school guidance  counselors in Memphis city schools. PhD Memphis State U.   [12] Broderick, Z., OConnor C., Mulcahy C., Heffernan, N., &  Heffernan C. 2012. Increasing parent engagement in student  learning using an intelligent tutoring system. J. Interactive  Learning Research. 22(4), 523-550.   [13] Burnham, J., & Jackson, C. 2000. School counselor roles:  Discrepancies between actual practice and existing models.  Professional School Counseling, 4(1), 41-49.   [14] Butler R. 1987. Task-involving & ego-involving properties of  evaluation: Effects of different feedback conditions on  motivational perceptions, interest, and performance. J. Ed.  Psych., 79, 474-482.     [15] Corbett, A., Anderson, J. 1995. Knowledge tracing:  Modeling the acquisition of procedural knowledge. User  Modeling & User-Adapted Interaction, 4, 253-278.   [16] Corwin, Z., Venegas, K., Oliverez, P., Colyar, J. 2004.  School counsel how appropriate guidance affects educational  equity. Urban Education, 39(4), 442-457.   [17] Cianci, A., Schaubroeck, J., McGill, G. 2010. Achievement  goals, feedback, and task performance. Human Performance,  23(2), 131-154.   [18] Cicourel, A., Kitsuse, J. 1963. The educational decision  makers. Indianapolis, IN: Bobbs-Merrill.   [19] Dieckmann, N. F., Slovic, P., Peters, E. 2009. The use of  narrative evidence and explicit likelihood by decision makers  varying in numeracy. Risk Analysis, 29(10), 1473-1488.   [20] DMello, S., Craig, S, Fike, K, Graesser, A. 2009. Responding  to learners cognitive-affective states with supportive &  shakeup dialogues. Human-computer interaction. Ambient,  ubiquitous & intelligent interaction. Springer: Berlin, 595- 604.   [21] Dunegan, K. 1996. Fines, frames, and images: Examining  formulation effects on punishment decisions. Organizational  Behavior & Human Decision Processes, 68(1), 58-67.   [22] Dweck, C. 2008. Mindsets and math/science achievement.  [23] Feng, M., Heffernan, N. 2006. Informing teachers live about   student learning: Reporting in the Assistment system. Tech.  Instruction Cognition & Learning, 3(1/2), 63.   [24] Foster, G., Ysseldyke, J. 1976. Expectancy and halo effects  as a result of artificially induced teacher bias. Contemporary  Educational Psychology, 1(1), 37-45.   [25] Gerofsky, S. 2011. Ancestral genres of mathematical  graphs. For the Learning of Mathematics, 31(1), 14-19.   [26] Hershkovitz, A., Baker, R.S.J.d., Gobert, J., Wixon, M., Sao  Pedro, M. 2013. Discovery with Models: A Case Study on  Carelessness in Computer-based Science Inquiry. American  Behavioral Scientist, 57(10), 1479-1498.   [27] Holmes, J. 2008. Effect of Message Framing on Reactions to  Feedback Messages, Moderated by Regulatory Focus. MA  Thesis. Virginia Tech.   [28] Hutchinson, R., Barrick, A., Grove, M. 1986. Functions of  secondary school counselors in the public schools: Ideal &  actual. The School Counselor, 34(2), 87-91.     [29] Jarvenpaa, S., Dickson, G. 1988. Graphics & managerial de- cision making: Research based guidelines. Communications  of the ACM. 764-774.   [30] Johnson E., Payne J, Bettman J. 1988. Information displays  & preference reversals. Organ. Beh. Human Decis. Process.  42, 1-21.   [31] Karvonen, K. 2000. The beauty of simplicity. Proc. ACM  Universal Usability. 85-90.   [32] Kluger, A., DeNisi, A. 1996. The effects of feedback inter- ventions on performance: a historical review, a metaanalysis,  and a preliminary feedback intervention theory.  Psychological Bulletin, 119(2), 254.   [33] Knowles, J. 2015. Of needles and haystacks: Building an  accurate statewide dropout early warning system in  Wisconsin. J. Educational Data Mining, 7(3), 18-67.    [34] Krenn, B., Wrth, S., Hergovich, A. 2013. The impact of  feedback on goal setting and task performance. Swiss J.  Psych.   [35] Kosslyn, S.M. 2006. Graph design for the eye and mind.  OUP USA.   [36] Lambie, G., Williamson, L. 2004. The challenge to change  from guidance counseling to professional school counseling:  A historical proposition. Prof. School Counseling, 124-131.   [37] Lent, R., Brown, S., Hackett, G. 1994. Toward a unifying  social cognitive theory of career and academic interest,  choice, & performance. J. Voc. Behavior, 45(1), 79-122.   [38] Levin, I.P., Gaeth, G.J. 1988. How consumers are affected by  the framing of attribute information before and after  consuming the product. J. Consumer Res., 15(3), 374-378.   [39] Lonn, S., Krumm, A., Waddington, R., Teasley, S. 2012.  Bridging the gap from knowledge to action: Putting analytics  in the hands of academic advisors. 2nd Intal Learning  Analytics & Knowledge, 184-187.   [40] Ma, Y. 2009. Family socioeconomic status, parental involve- ment, & college major choicesgender, race/ ethnic, &  nativity patterns. Sociological Perspectives, 52(2), 211-234.   [41] McKillip, M., Godfrey, K., Rawls, A. 2012. Rules of  engagement: Building a college-going culture in an urban  school.  Urban Education.   [42] Misra, R., Mark, J., Khan, S., Kukafka, R. 2010. Using  design principles to foster understanding of complex health  concepts in consumer informatics tools. AMIA Annual  Symposium Proc. 492.    [43] Mitchell, T.M. 1993. Machine Learning. Boston, MA:  McGraw-Hill.   [44] Molloy, E., Boud, D. 2014. Feedback models for learning,  teaching and performance. Handbook of Research on Ed.  Comm. & Technology. Springer, NY. 413-424.   [45] Muldner, K., Wixon, M., Rai, D., Burleson, W., Woolf, B.,  Arroyo, I. 2015. Exploring the impact of a learning dash- board on student affect. LNCSi. 9112, 307-317.    [46] Natter, H., Berry, D. 2005. Effects of presenting the baseline  risk when communicating absolute and relative risk  reductions. Psych, Health & Medicine, 10(4), 326-334.   [47]  Nisbett, R., Wilson, T. 1977. The halo effect: Evidence for  unconscious alteration of judgments. J. Personality & Social  Psychology, 35(4), 250.   [48] Ocumpaugh, J., Baker, R., Gowda, S., Heffernan, N.,  Heffernan, C. 2014. Population validity for Educational Data  Mining models: A case study in affect detection. British  Journal of Educational Technology, 45 (3), 487-501.   [49] Pardos, Z., Baker, R., San Pedro, M.O., Gowda, S., Gowda,  S. 2013. Affective states & state tests: Investigating how af- fect throughout the school year predicts end of year learning  outcomes. Proc. 3rd Intal Conf. Learning Analytics &  Knowledge, 117-124.   [50] Partin, R. 1993. School counselors' time: Where does it  go. The School Counselor, 40(4), 274-281.   [51] Penuel, W., Roschelle, J., Shechtman, N. 2007. Designing  formative assessment software with teachers: An analysis of  the co-design process. Research & Practice in Technology  Enhanced Learning, 2(01), 51-74.   [52] Perna, L. 2006. Studying college access and choice: A  proposed conceptual model. Higher Education, 99-157.   [53] Razzaq, L., Patvarczki, J., Almeida, S., Vartak, M., Feng, M.,  Heffernan, N., Koedinger, K. 2009. The Assistment Builder:  Supporting the life cycle of tutoring system content  creation. IEEE Transactions Learning Tech., 2(2), 157-166.   [54] Rosenbaum, J., Miller, S., Krei, M. 1996. Gatekeeping in an  era of more open gates: High school counselors' views of  their influence on students' college plans. Am. J. Ed., 257-79.   [55] San Pedro, M.O.Z., Baker, R.S.J.d, Bowers, A., Heffernan,  N. 2013. Predicting College Enrollment from Student  Interaction with an Intelligent Tutoring System in Middle  School. 6th Intal  Conf. Educational Data Mining, 177-184.   [56] San Pedro, M.O.Z., Baker, R., Heffernan, N., Ocumpaugh, J.  2015. Exploring College Major Choice and Middle School  Student Behavior, Affect and Learning: What Happens to  Students Who Game the System  5th Intal Learning  Analytics & Knowledge. 36-40.   [57] San Pedro, M.O.Z., Ocumpaugh, J., Baker, R., Heffernan, N.  2014. Predicting STEM & Non-STEM College Major Enroll- ment from Middle School Interaction with Mathematics  Educational Software.  7th Intal Conf. Educational Data  Mining, 276-279.   [58] Sandefur, G., Meier, A., Campbell, M. 2006. Family  resources, social capital, and college attendance. Social Sci.  Research, 35(2), 525-553.   [59] Schwarz, N., Bless, H., Bohner, G., Harlacher, U., Kellen- benz, M. 1991. Response scales as frames of reference: The  impact of frequency range on diagnostic judgments. Applied  Cognitive Psychology, 5(1), 37-49.   [60] Schwabish, J. 2014. An economist's guide to visualizing data.  Journal of Economic Perspectives, 28(1), 209-233.   [61] Shah, P., Hoeffner, J. 2002. Review of graph comprehension  research: Implications for instruction. Educational  Psychology Review, 14(1), 47-69.   [62] Sharp, J. 2011. Semiotics as a theoretical foundation of  information design. Proc. of CONISAR, 1-5.   [63] Shute, V. 2008. Focus on formative feedback. Review of  Educational Research, 78(1), 153-189.   [64] Simkin, D., Hastie, R. 1986. An information processing  analysis of graph perception. J. Am. Stat. Assoc. 82: 45465.     [65] Sparrow, J.1989. Graphical displays in information systems:  some data properties influencing the effectiveness of  alternative forms. Beh. & Information Tech., 8(1), 43-56.   [66] Tversky, B. 2005. Prolegomenon to scientific visualizations.  Visualization in Sci. Ed. Springer: Netherlands, 29-42.   [67] Tversky, A., Kahneman, D. 1981. The framing of decisions  & the psychology of choice. Environal Impact Assessment,  Tech. Assessment, & Risk Analysis. Springer: Berlin.107-29.   [68] Vancouver, J., Tischner, E. 2004. The effect of feedback sign  on task performance depends on self-concept discrepancies.  Journal of Applied Psychology, 89(6), 1092.   [69] Van de Ridder, J., Peters, C., Stokking, K., d Ru, J, tenCate, O.  2015. Framing of feedback impacts students satisfaction,  self-efficacy & performance. Adv. in Health Sci. Ed., 20(3),  803-816.    [70] Vessey, I. 1991. Cognitive Fit: A Theory-Based Analysis of  the Graphs Versus Tables Literature. Dec. Sci., 22:219-40.   [71] Walonoski, J., Heffernan, N. 2006. Prevention of off-task  gaming behavior in intelligent tutoring systems. Intelligent  Tutoring Systems. Springer: Berlin. 722-724.   [72] Wielenga-Meijer, E., Taris, T., Kompier, M., Wigholdus, D.  2010. From task characteristics to learning: A systematic  review. Scandinavian J. Psych. 51(5), 363-375.   [73] Wiliam, D. 2010. The role of formative assessment in  effective learning environments. The nature of learning:  Using research to inspire practice, 135-155.      .       "}
{"index":{"_id":"61"}}
{"datatype":"inproceedings","key":"Brown:2017:DCC:3027385.3027393","author":"Brown, Michael Geoffrey and DeMonbrun, R. Matthew and Teasley, Stephanie D.","title":"Don'T Call It a Comeback: Academic Recovery and the Timing of Educational Technology Adoption","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"489--493","numpages":"5","url":"http://doi.acm.org/10.1145/3027385.3027393","doi":"10.1145/3027385.3027393","acmid":"3027393","publisher":"ACM","address":"New York, NY, USA","keywords":"early warning systems, educational technology, technology adoption, undergraduate education","Abstract":"Recent research using learning analytics data to explore student performance over the course of a term suggests that a substantial percentage of students who are classified as academically struggling manage to recover. In this study, we report the result of a hazard analysis based on students' behavioral engagement with different digital instructional technologies over the course of a semester. We observe substantially different adoption and use behavior between students who did and did not experience academic difficulty in the course. Students who experienced moderate academic difficulty benefited the most from using tools that helped them plan their study behaviors. Students who experienced more severe academic difficulty benefited from tools that helped them prepare for exams. We observed that students adopted most tools and system features before they experienced academic difficulty, and students who adopted early were more likely to recover.","pdf":"Dont Call it a Comeback: Academic recovery and  the timing of educational technology adoption    Michael Geoffrey Brown   mbrowng@umich.edu  University of Michigan  School of Education   610 E. University Ave.  Ann Arbor, MI 48104    R. Matthew DeMonbrun  mdemonbr@umich.edu  University of Michigan  School of Education   610 E. University Ave.  Ann Arbor, MI 48104    Stephanie D. Teasley  steasley@umich.edu  University of Michigan  School of Information   105 S. State St.   Ann Arbor, MI 48109-1284    ABSTRACT  Recent research using learning analytics data to explore student  performance over the course of a term suggests that a substantial  percentage of students who are classified as academically  struggling manage to recover. In this study, we report the result of  a hazard analysis based on students behavioral engagement with  different digital instructional technologies over the course of a  semester. We observe substantially different adoption and use  behavior between students who did and did not experience  academic difficulty in the course. Students who experienced  moderate academic difficulty benefited the most from using tools  that helped them plan their study behaviors. Students who  experienced more severe academic difficulty benefited from tools  that helped them prepare for exams. We observed that students  adopted most tools and system features before they experienced  academic difficulty, and students who adopted early were more  likely to recover.    CCS Concepts   Information systems Data Analytics  Applied  computing Learning Management systems. Applied  computing Computer-assisted instruction.   Keywords  Educational Technology; Technology Adoption; Early Warning  Systems; Undergraduate Education    Permission to make digital or hard copies of all or part of this  work for personal or classroom use is granted without fee  provided that copies are not made or distributed for profit or  commercial advantage and that copies bear this notice and the full  citation on the first page. Copyrights for components of this work  owned by others than the author(s) must be honored. Abstracting  with credit is permitted. To copy otherwise, or republish, to post  on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions  from Permissions@acm.org.   LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada  Copyright is held by the owner/author(s). Publication rights  licensed to ACM.  ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3027393      1. INTRODUCTION  Recent research using learning analytics data to explore student  performance over the course of a term suggests that a substantial  percentage of students who are classified as academically  struggling manage to recover [1]. Yet, very little is known about  the process of academic recovery because traditional educational  research tends to focus on end of semester metrics like final  course grade to make determinations about academic success or  failure [2]. Analytical models that use final grade as an outcome  measure focus on characterizing students who failed at the end of  the term, potentially obscuring the strategies and practices  successful students engaged in that may promote academic  recovery during the term. Exploring the material practice of  technology use in the context of academic recovery could allow us  to identify strategies and practices of great relevance for at-risk  students.   A major affordance of big data is the potential for examining how  individual behavior changes over time on a scale previously  unavailable to educational researchers [3]. We can, with greater  precision, examine what students are doing on a week to week  basis within digital course technologies. Trace data of behavioral  engagement with LMS content and with other digital instructional  tools has the potential to illustrate the material strategies that  students employ during a course.   2. TECHNOLOGY USE & ACADEMIC  RECOVERY  As there is so little existing research on factors that predict  academic recovery within a term, we begin an investigation of  related factors with the material interactions between students and  technologies that 1) form the basis of learning analytics data and  2) are direct evidence of the forms of behavioral engagement that  may be associated with academic improvement.    The material strategies that students develop can have a  significant impact on their academic performance at the end of a  course. For example, Junco and Clem [4] observed in a study of  textbook analytics that when different approaches to using the  textbook were disaggregated, very few behaviors within the  online textbook system that students engaged in were significantly  related to academic performance. Time spent reading was the  strongest predictor of academic performance at the end of the  term, while other tasks like highlighting or note taking were not  significant [4].    Waddington et al., [5] examining course LMS resource use also  observed only a few material practices that were significantly     related to academic performance. Exam preparation resources  were the only tools that significantly predicted improved  performance at the end of the term. Students who increased their  use of the exam prep tool during the course were predicted to have  significantly better end of semester outcomes than students who  stuck with their current resource use strategies [5].    While there is an increasing body of evidence for the relationship  between digital instructional technology use by students and  academic performance outcomes, there is little research that  examines how the timing of student technology use might  influence academic recovery. In fact, most of the research on  undergraduate student adoption of learning technology takes a  snapshot approach, where use is turned into a binary indicator [6].     In this study, we investigate when during an academic term  students begin using a technology on a regular basis (adoption),  the timing of that adoption, and the relationship between adoption  timing, frequency of use, and academic recovery.   Using an existing Early Warning System, called  Student  Explorer  (see below) that classified students into three risk  categories, we investigated the following research questions:   3. RESEARCH QUESTIONS  1. Does learning technology adoption precede, coincide with, or   lag students experience of academic difficulty in the course  2. What factors significantly increase students odds of   improving from an EXPLORE classification (moderate  academic difficulty) or an ENGAGE classification (severe  academic difficulty)   4. METHODOLOGY  4.1 Sample  Students were enrolled in one of four sections of an introductory  statistics course at a large, residential research intensive American  university. This course, called here Stats 101 is a pre-requisite  for a number of academic majors at the institution, such as  economics, organizational studies, and political science. The  course has been designed to allow students to recover  academically if they stumble early in the term by allowing those  students who completed all homework assignments to exclude one  exam grade from the calculation of their final grade.    There were 2,169 students enrolled in Stats 101 in the term  studied here. The course involved lectures twice a week and a  weekly lab section.  All four of the instructors used the same  instructional resources, including assessments and assignments.  The class enrollment was split across the undergraduate student  body: first year (n=292), second year (n=1026), third year  (n=620), and fourth year (n=231) students. The class was evenly  split by gender (51% women).    4.2 Data  We draw data from a variety of sources to construct our analytical  model. First, performance data is drawn from an Early Warning  System called Student Explorer. Classifications are based on  students average academic performance relative to the rest of the  class and their use of the Learning Management System. Students  are classified (in order of performance from highest to lowest) as  ENCOURAGE, EXPLORE, or ENGAGE. Students who have  been classified as ENCOURAGE are performing well in the  course. Students who are classified as EXPLORE are under- performing relative to the mean class performance. Students  classified as ENGAGE are experiencing academic difficulty.   When students have recovered, their academic classification has  improved at least one level (ENGAGE to EXPLORE or  EXPLORE to ENCOURAGE) in the weekly period following a  classification of ENGAGE or EXPLORE [7].  Students do not  receive these classifications; they are only provided to academic  advisors. It is possible for all students to be classified as  ENGAGED.    4.2.1 Variables   We also draw log data from trace records of students use of  different instructional technologies utilized in Stats 101. First,  students are provided access to a digital coaching application  called ECoach [8]. The ECoach system provides students a variety  of resources:   Weekly Help and Advice Messages: ECoach system provides  students with helpful tips and advice about how to prepare for  upcoming assessments. Measured as a binary indicator whether  student viewed that weeks message at least once.   Exam Playbook: A tool providing resources and strategies for  preparing for and reflecting on exams. Exam Preparation tool asks  students to identify what resources they will use to prepare for  exam. Exam Reflection tool provides students the opportunity to  review and reflect upon how they prepared for an exam. Measured  as the percentage of each activity that the student completed  before (playbook) and after an exam (reflection).    Get Things Done: A weekly checklist developed by one of the  instructors that provides suggestions for tasks that will help  students prepare for class. Binary indicator that a student viewed  and checked off at least one task in a given week.     Grade Calculator: An interactive tool that students can use to  estimate their grade based on current and future performance. In  the model, Grade Calculator is a binary indicator for whether or  not the student used the Grade Calculator in a given week.    As part of the course, students are also provided two online  systems for reviewing academic problems.    Name That Scenario: An online review system that allows  students to review a potential problem and identify the type of  inference the problem calls for. Variable indicates that students  completed at least one problem during one of the periods when a  Name That Scenario problem set was open.   Finally, students use of a practice problem mobile application  called Problem Roulette, which is also incorporated into our  model [9]. We examine how often students access the application.  Only sessions where students attempt and complete at least one  practice problem are included.   A student is considered to have adopted a tool if they either  engaged with the tool in at least two distinct sessions for tools  intended to be used multiple times, or for tools that had multiple  features and were intended for use during a set period (like Name  that Scenario or the Exam Playbook) students used at least 10%  of the features during a session. Students who used the Get Things  Done checklist or viewed the Help and Advice messages in two  different weeks would be considered adopters. Students who  completed at least 10% of the Exam Playbook would be  considered adopters of that tool.    We also account for the number of course credits in which a  student is concurrently enrolled; their race and gender (as women  and underrepresented students traditionally experience a grade  penalty in this course [10]); and the students score on a  standardized math placement exam taken before enrollment.      4.3 Hazard Analysis   Our analyses draw from event history techniques to determine the  probability that a student may exit an EXPLORE or ENGAGE  alert status given the independent variables listed above in section  4.2. Student Explorer alert categories are stored in the system on a  weekly basis which requires a discrete-time hazard model, as our  data is composed of discretely-measured time periods.    Discrete-time hazard models employ binary responses (yti), where  the outcome represents whether the event occurred (1=yes; 0=no)  during sequential time periods (t) for each individual (i). In doing  so, we create a weekly observation for whether an individual  student exited a classification (ENGAGE or EXPLORE). The  probability (pti) is estimated for each individual (i) to experience  the event during each time interval (t), given that no event has  occurred prior to the start of t:    pti = Pr(yti = 1|yt1,i = 0)  pti is called the discrete-time hazard function because it represents  the probability of the individual exiting an ENGAGE or  EXPLORE classification during a specific weekly interval.    After determining the probabilities for each individuals time  hazard, the data is fit to a binary response model (i.e., logistic  regression model):   log  (pti / 1  pti) = Dti + xti  In this model, pti represents the probability of the event during the  time interval t, Dti is a vector of functions representing the total  cumulative hazard during the duration by interval t with  coefficients (), and xti is a vector of covariates with coefficients  (). Each individual receives a baseline hazard function  (represented by Dti), while the covariates can either increase or  decrease the hazard function for each individual.    The results of the logistic regression model are provided in terms  of odds ratios for ease of discussion.     Two hazard models were estimated for our analysis: exiting an  EXPLORE status and exiting an ENGAGE status. The odds that a  students academic performance improved over time given their  use of each individual tool are reported. If students had multiple  entry and exits in and out of an alert status, those changes in  classification were included in the model as unique events.  However, less than 2% of the students in the sample entered an  EXPLORE or ENGAGE classification more than once.    5. RESULTS  Our initial analyses focused on students instructional technology  use across the semester. To understand the relationship between  use and academic performance, we divided students into two  groups based on their Student Explorer classifications. Students  who were only classified by the Student Explorer system as  ENCOURAGE throughout the semester- meaning they never  performed below the class averagewere classified into one  group (n=1637). Students who were classified by the Student  Explorer system as EXPLORE or ENGAGE at least once during  the semester were the comparison group. Students who struggled  had generally lower levels of instructional technology adoption.   The more interactive a tool, the greater the likelihood that all  students would adopt that tool (e.g. Get Things, Done, Problem  Roulette, Name that Scenario). In contrast, tools that were  relatively static and had few affordances, like the Weekly Help  and Advice Messages, had broad adoption among ENCOURAGE   students and were generally ignored by EXPLORE/ENGAGE  students. For example, ENCOURAGE students had higher  average adoption of Get Things Done (difference in  proportion=31.2%, p,0.001) and reading Weekly Messages  (28.2%, p<0.001). There was no substantial difference in the  proportion of students who adopted Problem Roulette between the  two groups (2.3%).   5.1 Tool Use  Between groups, substantial variation in adoption was also clear  over the semester. Student use of the Get Things Done checklist  was highly variable week to week, but overall students who were  classified as EXPLORE/ENGAGE used the tool less frequently  and after week eight use the tool very rarely when compared to  ENCOURAGE students (see figure 1).      Figure 1. Week to Week use of Get Things Done   Similarly, a higher percentage of students who were classified as  ENCOURAGE read weekly help messages when compared to  students classified as EXPLORE/ENGAGE. Overall there was a  wide variation in week to week message reading, but students  who struggled were simply less likely to view help messages.     Problem Roulette use also varied substantially, with spikes around  exam times, although use was uniformly higher for all students.  The increase in use over the semester by EXPLORE/ENGAGE  students closely paralleled the use by ENCOURAGE students.     Figure 2. Week to Week use of Problem Roulette   The trend was slightly different when we looked at exam  preparation resources in comparison to weekly resources. Use  increased substantially among ENGAGE students between exams  1 and 2 for the exam preparation tool (by about 25%).     However, use decreased among EXPLORE/ENGAGE students  (by about 2%) between the two exams. Exam reflection use  increased by 46% between exam 1 and 2 for students classified as  ENCOURAGE/ENGAGE. Students who did poorly on the second  exam may have been looking for insight into what they could do  differently before the final exam.      Figure 3. Difference in proportion of Exam Prep and Exam   Reflection Tool Use by Student Explorer Classification   5.2 Adoption and Academic Difficulty  Despite the variation in use between the ENCOURAGE and  EXPLORE/ENGAGE groups, students adoption of most digital  instructional technologies overwhelmingly preceded any  academic difficulty they encountered. For example, students that  adopted the Get Things Done (GTD) checklist began using the  tool early in the term and used it throughout; 437 students out of  the 440 who used it regularly were classified as ENCOURAGE  when they adopted the tool. A very small number of students  (n=3/440) adopted the tool after they had entered an EXPLORE or  ENGAGE classification. This trend also held for the adoption of  the Exam Prep and Reflection tools. Only the Problem Roulette  tool seemed to result in some variation in adoption, with about  20% of students adopting the tool after entering an EXPLORE or  ENGAGE alert status.    In general, adoption preceded academic difficulty. On average,  students who experienced academic challenge used the digital  tools available to them in substantially different ways over the  duration of the semester from students who performed well  throughout the course. We now turn our attention to what kind of  instructional technology use is significantly related to increasing a  students odds of academic improvement from an EXPLORE or  ENGAGE classification over time.    5.3 Recovery Across the Semester   Although we might presume that students make changes to their  resource strategies once they encounter academic difficulty, this  did not seem to be the case in our data. Instead, the students who  were the most likely to recover from an EXPLORE or ENGAGE  classification were students who had already adopted digital  instructional tools early in the course.     Students who experienced moderate academic difficultywho  were classified as EXPLORE at some point during the semester benefitted the most from using the Get Things Done tool (OR=  6.87). Get Things Done provides students with a checklist of  weekly tasks to complete to prepare for lecture. Students who use  Get Things Done throughout the semester were six times more  likely to improve their academic performance than students who   did not. Students who used the Exam Prep tool also had slightly  improved odds of exiting the EXPLORE status (OR=1.001,  p<0.05).    Table 1. Factors Predicting Odds of Recovery from an at Risk  Classification    Improve from  EXPLORE    Improve from  ENGAGE    Current Credits 1.03 (0.04) 0.9961  (0.0025)  Math Placement 0.75 (0.156)* 0.095  (0.001)***  Men (Wome n ref) 1.082 (0.23) 1.321  (0.011)*  Underrepresented 0.999 (0.001) 0.0846  (0.019)  Name that Scenario 1.001 (0.0004) 1.000  (0.001)  Problem Roulette 1.034 (0.026) 1.130  (0.003)***  Exam Prep 1.001 (0.09)* 1.231  (0.011)*  Exam Reflection 1.03  (0.054) 1.019  (0.001)*  Get Things Done 6.87 (1.061)* 1.054  (0.003)  Grade Calculator 1.213 (0.121) 1.174 (0.007)  Weekly Messages 1.340 (0.011) 1.205  (0.014)   Signif. codes: *** 0.001 ** 0.01 * 0.05   This is in contrast to students who were classified as ENGAGE  who appeared to benefit the most from tools that reviewed class  content. Using the exam preparation (OR=1.231) and exam  reflection tools (OR=1.019) increased students odds of improving  their academic performance. Students who reviewed example  problems through the Problem Roulette application also had  significantly higher odds of improving from engage (OR=1.130,  p<0.001)   6. DISCUSSION  Our analyses suggest that students who adopt and incorporate  instructional tool use into their study behaviors have greater odds  of academic recovery than students who do not use these tools.  Our results also suggest that students adopt these tools early in the  semester, well before they experience academic difficulty.  Adoption of instructional technologies may provide students with  resources to draw upon that promote recovery. What resources  prove beneficial for academic recovery appear related to the  amount of academic difficulty that students experience during the  semester.     Students experiencing moderate academic difficulty (EXPLORE  status) may be experiencing problems of self-regulation- of  dedicating the time and energy need to be successful in the course.  The Get Things Done tool supports self-regulation by helping  students organize and reflect on how they approach the course.  Students who entered an EXPLORE alert status and were using  Get Things Done were far more likely to improve throughout the  semester than students who were not using the checklist.    In contrast, students experiencing significant academic difficulty  (ENGAGE status) appear to benefit the most from course  resources that help them prepare for assessments. Resources like  Problem Roulette, the Exam Preparation, and Exam Reflection  tools allow students to review content in preparation for higher  risk course assessments like exams.    Problem Roulette users, in particular, were more likely than any  other group to adopt the tool after they had entered an alert status.  While we observed few events that made students change their     behaviors substantially, it did appear that students who were  experiencing academic difficulty were much more willing to  accommodate a technology like Problem Roulette than features of  the ECoach system.   Only the Exam Preparation tool cut across both models as a  significant factor that predicted improved odds of exiting a  classification. The Exam Prep tool also cuts across the two  strategies inherent in the Get Things Done and Problem Roulette  tools. It provides students the opportunity to think about their plan  for preparing for the exam, and it directs them to resources that  can help them review content.    In general, instructors and tool developers who want to develop  effective interventions should pay close attention to students  resource use behaviors early in the semester, as they appear  relatively fixed, and yet are predictive of a students tendency to  academically recover. While some students will certainly be  academically successful without adopting the array of tools  available to them, students who do experience academic difficulty  might benefit significantly from instructors encouraging the use of  digital instructional technologies early in the semester.    Instructors and instructional designers may want to consider  flagging the failure to adopt early in the course as a way to  identify students for potential intervention. Students who fail to  adopt early on are unlikely to do so once they need the resources.  Behavioral nudges that encourage the adoption of ECoach-like  systems will improve the odds that students recover.    Adoption decisions happen relatively early in the course and  appear sticky. Students may be making these decisions in reaction  to the perceived difficulty of the course, or in response to the  instructors signaling that these resources are important to use.  Instructors may therefore aim to address their importance early  and often.   The findings of our study also suggest improvements to Early  Warning System design. We believe that the models underlying  Early Warning Systems should be designed to factor in timing and  counts of individual tool use, not simply the aggregate hits on the  course LMS. Use (or failure to use) might decrease students  probability of recovery over time, which should influence EWS  classifications.   Our study also illustrates the value of considering material  practices in relationship to academic performance. When scholars  treat technology use and adoption as a uniform behavior, as  opposed to a system of practices afforded by a tools design, they  potentially gloss over what aspects of use support student learning  and achievement [11]. When scholars treat material practices as a  set of concurrent interactions between students and technologies,  powerful insights can be drawn from big data analytics.    7. FUTURE DIRECTIONS   Our results point to some important next steps for studying  academic recovery in residential undergraduate education. First,  affective beliefs about the course and the course technology  should be explored to understand how those factors inform  adoption and use decisions. Second, researchers might consider  the role of students individual motivation and expectations for  success in tool use and adoption.  Finally, these analyses could be  repeated in other contexts to determine how the arrangement of   instructional technology in different disciplinary contexts might  influence academic recovery.    8. ACKNOWLEDGMENTS  Our thanks to the students and instructors involved in Stats 101.   Support for this research was provided by the Learning,  Education, and Design Lab, the Digital Innovation Greenhouse  and the Office of Academic Innovation at the University of  Michigan. We would like to acknowledge the support and  feedback we received from Dr. Patricia Chen, Holly Derry, Dr.  Brenda Gunderson, Ben Hayward, and Dr. Timothy Mckay.   9. REFERENCES   [1] Brown, M. G., DeMonbrun, R. M., Lonn, S., Aguilar, S. J., &   Teasley, S. D. 2016. What and when: the role of course type  and timing in students' academic performance. Proceedings  of the Sixth International Conference on Learning Analytics  & Knowledge (pp. 459-468). ACM.   [2] Zimmerman, B. J. 2008. Investigating self-regulation and  motivation: Historical background, methodological  developments, and future prospects. American Educational  Research Journal, 45(1), 166-183.   [3] Papamitsiou, Z. K., Terzis, V., & Economides, A. A. 2014.  Temporal learning analytics for computer based testing.  Proceedings of the Fourth International Conference on  Learning Analytics And Knowledge. ACM.   [4] Junco, R., & Clem, C. 2015. Predicting course outcomes with  digital textbook usage data. The Internet and Higher  Education, 27, 54-63.   [5] Waddington, R.J., Nam, S.J., Lonn, S., & Teasley, S.D. in  press. Practice (Exams) Make Perfect: Incorporating course  resource use into an early warning system. Journal of  Learning Analytics.    [5] Halverson, L. R., Graham, C. R., Spring, K. J., Drysdale, J. S.,  & Henrie, C. R. 2014. A thematic analysis of the most highly  cited scholarship in the first decade of blended learning  research. The Internet and Higher Education, 20, 20-34.   [7] Krumm, A. E., Waddington, R. J., Teasley, S. D., & Lonn, S.  2014. A Learning Management System-Based Early  Warning System for Academic Advising in Undergraduate  Engineering. Learning Analytics. From Research to Practice,  pp. 103--119.    [8] Huberth, M., Chen, P., Tritz, J., & McKay, T. A. 2015.  Computer-Tailored Student Support in Introductory Physics.  PloS one, 10(9), e0137001.   [9] Evrard, A. E., Mills, M., Winn, D., Jones, K., Tritz, J., &  McKay, T. A. 2015. Problem roulette: Studying introductory  physics in the cloud. American Journal of Physics, 83(1), 76- 84.   [10] Koester, B. P., Grom, G., & McKay, T. A. 2016. Patterns of  Gendered Performance Difference in Introductory STEM  Courses. arXiv preprint arXiv:1608.07565.   [11] Brown, M.G. 2016. Blended instructional practice: A review  of the empirical literature on instructors adoption and use of  online tools in face to face teaching.  The Internet and  Higher Education, 31(1)       "}
{"index":{"_id":"62"}}
{"datatype":"inproceedings","key":"Tsai:2017:LPD:3027385.3029424","author":"Tsai, Yi-Shan and Gasevic, Dragan and Mu~noz-Merino, Pedro J. and Dawson, Shane","title":"LA Policy: Developing an Institutional Policy for Learning Analytics Using the RAPID Outcome Mapping Approach","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"494--495","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029424","doi":"10.1145/3027385.3029424","acmid":"3029424","publisher":"ACM","address":"New York, NY, USA","keywords":"higher education, learning analytics, policy","Abstract":"This workshop aims to promote strategic planning for learning analytics in higher education through developing institutional policies. While adoption of learning analytics is predominantly seen in small-scale and bottom-up patterns, it is believed that a systemic implementation can bring the widest impact to the education system and lasting benefits to learners. However, the success of it highly depends on the adopted strategy that meets the needs of various stakeholders and systematically pushes the institution towards achieving its targets. It is imperative to develop a learning analytics policy that ensures a practice that is valid, effective and ethical. The workshop involves two components. The first component includes a set of presentations about the state of learning analytics in higher education, drawing on results from an Australian and a European project examining institutional learning analytics policy and adoption processes. The second component is an interactive session where participants are encouraged to share their motivations for adopting learning analytics and the diversity of challenges they perceive impede analytics adoption in their institution. Using the RAPID Outcome Mapping Approach (ROMA), participants will create a draft policy that articulates how the various challenges can be addressed. This workshop aims to further develop our understanding of how learning analytics operates in an organizational system and promote a cultural change in how such analytics are adopted in higher education","pdf":"LA Policy: Developing an Institutional Policy for Learning  Analytics using the RAPID Outcome Mapping Approach   Yi-Shan Tsai, Dragan Gasevic   University of Edinburgh  Edinburgh, United Kingdom   +44 131 651 6243   yi-shan.tsai@ed.ac.uk   +44 131 651 3837   dragan.gasevic@ed.ac.uk     Pedro J. Muoz-Merino  Universidad Carlos III de Madrid   Madrid, Spain  +34 91-624-5972   pedmume@it.uc3m.es   Shane Dawson  University of South Australia  Adelaide SA 5001, Australia   +61 8 830 27850   Shane.Dawson@unisa.edu.au           ABSTRACT  This workshop aims to promote strategic planning for learning   analytics in higher education through developing institutional   policies. While adoption of learning analytics is predominantly   seen in small-scale and bottom-up patterns, it is believed that a   systemic implementation can bring the widest impact to the   education system and lasting benefits to learners. However, the   success of it highly depends on the adopted strategy that meets the   needs of various stakeholders and systematically pushes the   institution towards achieving its targets. It is imperative to   develop a learning analytics policy that ensures a practice that is   valid, effective and ethical.    The workshop involves two components. The first component   includes a set of presentations about the state of learning analytics   in higher education, drawing on results from an Australian and a   European project examining institutional learning analytics policy   and adoption processes. The second component is an interactive   session where participants are encouraged to share their   motivations for adopting learning analytics and the diversity of   challenges they perceive impede analytics adoption in their   institution. Using the RAPID Outcome Mapping Approach   (ROMA), participants will create a draft policy that articulates   how the various challenges can be addressed. This workshop aims   to further develop our understanding of how learning analytics   operates in an organizational system and promote a cultural   change in how such analytics are adopted in higher education.   CSS Concepts   Applied computing  Education; Security and privacy    Human and societal aspects of security and privacy   Keywords  Learning analytics; policy; higher education   1. INTRODUCTION  Studies show that there is generally a lack of practical guidance   for the adoption of learning analytics in higher education   institutions [2]. A recent survey conducted by Heads of e-  Learning Forum in the UK shows that only five out of fifty-three   institutional respondents follow a code of practice1 to guide their   learning analytics implementation process [5]. In a review of the   adoption of learning analytics in ten universities across USA,   Australia and UK, Siemens and others found that only a few   universities have started strategic planning for learning analytics   deployment despite that significant data collection activities had   existed in education systems for long [1, 6]. They identified that   learning analytics was often found in small scale or bottom up   developments, which tended to lack systematic development and   planning process. The lack of policies that address both legislative   and non-legislative issues about the implementation of learning   analytics in the higher education sector was also identified in a   systematic literature review conducted by researchers of the   European Commission funded research project  SHEILA2. The   bibliographic research of all publications rendered only eight   codes of practices across Europe and Australia, of which four   were developed within and for specific universities. This number   indicates a gap of holistic planning that can ensure the practice of   learning analytics to be valid, ethical, effective and sustainable.   The capacity to bring about change in higher education   institutions where complex and anarchic adaptive systems exist   has been described as a wicked problem [4]. Macfadyen and   others noted that educational systems tend to be stable and   resistant to change due to a range of political, social, cultural and   technical norms. Nevertheless, they argued that higher education   institutions must implement planning processes for learning   analytics so that stakeholders can easily align the need for change   with institutional goals and priorities. In response to the flexible   and constantly-changing social and institutional contexts,   Macfadyen and colleagues suggest that an adapted version of the   RAPID Outcome Mapping Approach (ROMA) approach can   guide and ultimately lead towards systemic institutional change   that is enabled through learning analytics.   The ROMA model was originally designed to support policy and   strategy process in the field of international development (Figure   1) [7]. The approach consists of an iterative cycle of seven steps   that can be adopted to consider the complexity of institutional   contexts in which factors of people, political structures, data   infrastructures, and institutional capabilities all have mutual   influence on each other and on the success of achieving the   objectives of learning analytics.   ROMA targets at bringing about evidence-based change to                                                                      1 In this workshop, we use policy and code of practice   interchangeably, and both terms refer to a set of guidance that   addresses both legislative and non-legislative issues.   2 http://sheilaproject.eu/   Permission to make digital or hard copies of part or all of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights   for third-party components of this work must be honored. For all other   uses, contact the Owner/Author. Copyright is held by the  owner/author(s). LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029424   http://dx.doi.org/10.1145/3027385.3029424   institutions and a reflective culture in the progress of   implementation in which there is flexibility to adjust the strategy   when new evidence emerges. This approach is believed to have   the potential to maximize success of learning analytics in   institution-wide implementation [2]. As there is no one-size-fits-  all policy for educational change and learning analytics [3], it is   encouraged that every higher education institution should develop   a learning analytics policy that considers its specific context and   addresses challenges wherein. Therefore, we propose a half-day   workshop to initiate conversations among scholars, practitioners,   and institutional senior leaders about policy and the state of   learning analytics in the higher education sector. In addition, there   will be an opportunity to create a draft of policy using ROMA.      Figure 1. RAPID Outcome Mapping Approach (ROMA) [4]   This workshop falls in the topic of meta-issues for LAK17 with   considerations for ethics and law, adoption, and scalability. It will   contribute to a cultural change in the implementation of learning   analytics by raising the awareness of strategic planning and giving   step-by-step guidance to composing a policy.    2. WORKSHOP OBJECTIVES  The workshop has three main objectives, including   - bringing broader understanding of the state of learning  analytics adoption in higher education;    - initiating conversations about challenges in the  implementation of learning analytics; and   - addressing the above challenges within policies drafted by  participants with guidance and support from the workshop   initiators and other participants.   3. TARGET GROUP  The target group is primarily policy makers of learning analytics,   senior management at higher education institutions, learning   analytics practitioners and researchers. The workshop also   welcomes stakeholders that are involved in the working team for   the planning and implementation process, such as project leaders,   data protection and system officers, Information and Technology   officers, academics and student representatives.   4. FORMAT  This half-day workshop will begin with presentations from two   research teams (the European SHEILA and Australian3 projects)   about findings from main activities: literature review (including   examples of concrete institutional policies), interviews with   institutional senior leaders, surveys with a large sample of                                                                      3 http://he-analytics.com/au/   European institutions, and two group concept mapping studies   with learning analytics expert and policy maker groups. The   presentations will cover the following themes: stages of   implementation in higher education institutions, the success   claimed to date, challenges identified in the process, and elements   identified as essential for a higher education institutions learning   analytics policy.   Following the presentations, there will be a discussion time for the   participants to share about the motivations to adopt learning   analytics in their institutions and challenges that they face in the   planning and implementation process.   After the coffee break, the workshop initiators will demonstrate   how to draft a learning analytics policy using the ROMA model,   with evidence drawn upon their research findings. Afterwards, the   participants, in small groups, will draft a policy that considers the   specific contexts of their institutions and the challenges that they   have identified in the first part of the workshop. The workshop   will conclude with plenary discussions about the products that   participants create and practicalities about introducing the policy   to their institutions.   5. ACKNOWLEDGMENTS  We would like to thank European Commission for funding the   SHEILA project (Ref. 562080-EPP-1-2015-1-BE-EPPKA3-PI-  FORW) and the Australian Governments Office for Learning and   Teaching for funding the HE-Analytics project (SP13-3249) and   all participants who joined the project activities and contributed   valuable insights. This document does not represent the opinion of   the European Community and the Australian Government.   6. REFERENCES  [1] Colvin, C., Rogers, T., Wade, A., Dawson, S., Gasevic, D.,   Shum, S.B., Nelson, K., Alexander, S., Lockyer, L.,   Kennedy, G., Corrin, L. and Fisher, J. 2015. Student   retention and learning analytics: a snapshot of Australian   practices and a framework for advancement. The Australian   Government Office for Learning and Teaching.   [2] Ferguson, R., Macfadyen, L.P., Clow, D., Tynan, B.,   Alexander, S. and Dawson, S. 2014. Setting Learning   Analytics in Context: Overcoming the Barriers to Large-  Scale Adoption. Journal of Learning Analytics. 1, 3 (Sep.   2014), 120144.   [3] Gaevi, D., Dawson, S., Rogers, T. and Gasevic, D. 2016.   Learning analytics should not promote one size fits all: the   effects of instructional conditions in predicting academic   success. The Internet and Higher Education. 28, (2016), 68  84.   [4] Macfadyen, L.P., Dawson, S., Pardo, A. and Gaevic, D.   2014. Embracing Big Data in Complex Educational   Systems: The Learning Analytics Imperative and the Policy   Challenge. Research & Practice in Assessment. 9, (2014),   1728.   [5] Newland, B., Martin, L. and Ringan, N. 2015. Learning   analytics in UK HE 2015: a HeLF survey report.   [6] Siemens, G., Dawson, S. and Lynch, G. 2013. Improving the   quality and productivity of the higher education sector:   policy and strategy for systems-level deployment of learning   analytics. Society for Learning Analytics Research for the   Australian Office for Learning and Teaching.   [7] Young, J. and Mendizabal, E. 2009. Helping researchers   become policy entrepreneurs - how to develop engagement   strategies for evidence-based policy-making. Overseas   Development Institute.   between multiple stakeholders in situations of ambiguity, uncertainty and values disagreement   (Rittel & Webber, 1973). A number of theorists have also emphasized that solutions to wicked   problems  actually complex systems of interrelated problems  can seldom be obtained   by independently solving each of the problems of which it is composed . . . Efforts to deal   separately with such aspects of urban life as transportation, health, crime, and education seem   to aggravate the total situation  (Ackoff, 1974, p. 21).     Systems theory offers two key areas of insight that are significant for policy development    for complex educational systems. First, systems theorists recognized that while systems    from a single atom to a universe  may appear to be wildly dissimilar, they are all governed by   common patterns, behaviors and properties: their component parts are multiply interconnected   by information flows, with identifiable and predictable feedbacks, inputs, outputs, controls and   transformation processes; they are dynamic, differentiated and bounded; they are hierarchically   organized and differentiated; and new properties can arise within them as a result of interactions   between elements. Second, systems theory observes that systems tend to be stable, and that their    interconnectedness facilitates resilience (for a review of systems theory , see Capra, 1996).   These observations not only illuminate why piecemeal attempts to effect change in   educational systems are typically ineffective, but also explains why no onesizefitsall prescriptive   approach to policy and strategy development for educational change is available or even possible.   Usable policy frameworks will not be those which offer a to do list of, for example, steps in learning   analytics implementation. Instead, successful frameworks will be those which guide leaders and   participants in exploring and understanding the structures and many interrelationships within   their own complex system, and identifying points where intervention in their own system will be   necessary in order to bring about change.   Drawing on systems and complexity theory, a new generation of authors have begun   to develop accounts of socalled adaptive approaches to policy and planning for complex   systems which can allow institutions to respond flexibly to everchanging social and   institutional contexts and challenges (Berkhout, Leach, & Scoones, 2003; Haynes, 2003;   Milliron, Malcolm, & Kil, 2014; Tiesman, van Buuren, & Gerrits, 2009; Young & Mendizabal,   2009). A full review of adaptive management strategies is beyond the scope of this paper, and   has been comprehensively undertaken by Head and Alford (2013), who highlight the critical   roles of crossinstitutional collaboration, new forms of leadership (moving beyond the   orthodox model of transformational leadership) and the development of enabling structures   and processes (for example, budgeting and finance systems, organizational structure,   human resources management, and approaches to performance measurement and program   evaluation). We offer here two sample policy and planning models that may offer valuable   practical guidance for collaborative teams and leaders in higher education seeking to bring   about systemic institutional change to support learning analytics.  24                     Volume Nine | Winter 2014  From the technological   point of  view, learning   analytics is an emerg-  ing discipline and its   connection with assess-  ment remains largely   unexplored.  Figure 1. The RAPID Outcome Mapping Approach (ROMA)    "}
{"index":{"_id":"63"}}
{"datatype":"inproceedings","key":"Knight:2017:WAL:3027385.3029425","author":"Knight, Simon and Allen, Laura and Gibson, Andrew and McNamara, Danielle and Buckingham Shum, Simon","title":"Writing Analytics Literacy: Bridging from Research to Practice","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"496--497","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029425","doi":"10.1145/3027385.3029425","acmid":"3029425","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics for action, automated writing evaluation, learning analytics, learning analytics literacy, practitioner knowledge, writing analytics","abstract":"There is untapped potential in achieving the full impact of learning analytics through the integration of tools into practical pedagogic contexts. To meet this potential, more work must be conducted to support educators in developing learning analytics literacy. The proposed workshop addresses this need by building capacity in the learning analytics community and developing an approach to resourcing for building 'writing analytics literacy'.","pdf":"Writing Analytics Literacy  Bridging from Research to  Practice     Simon Knight    University of Technology Sydney   Connected Intelligence Centre    Sydney, Australia    Simon.knight@uts.edu.au      Danielle McNamara  Arizona State University   PO Box 872111  Tempe, AZ 85287   dsmcnamara@asu.edu     Laura Allen   Arizona State University   PO Box 872111   Tempe, AZ 85287   LauraKAllen@asu.edu         Andrew Gibson   University of Technology Sydney   Connected Intelligence Centre    Sydney, Australia   Andrew.Gibson@uts.edu.au      Simon Buckingham Shum  University of Technology Sydney   Connected Intelligence Centre    Sydney, Australia    Simon.BuckinghamShum@   uts.edu.au      ABSTRACT  There is untapped potential in achieving the full impact of   learning analytics through the integration of tools into practical   pedagogic contexts. To meet this potential, more work must be   conducted to support educators in developing learning analytics   literacy. The proposed workshop addresses this need by building   capacity in the learning analytics community and developing an   approach to resourcing for building writing analytics literacy.   CCS Concepts   Computing methodologies~Natural language   processing    Applied computing~Education    Applied   computing~Computer-assisted instruction    Applied   computing~Interactive learning environments    Human-centered   computing    Social and professional topics~Computing literacy    Keywords  Learning analytics, writing analytics, analytics for action,   practitioner knowledge, learning analytics literacy,   automated writing evaluation   1. INTRODUCTION  The ability to communicate via writing is a key to literacy, central   to participation in society, and thus central to all educational   contexts [6, 7]. There is a long standing interest in the   development and use of natural language processing (NLP) tools   to analyze this writing [e.g., 5, 8], with tools emerging from the   research and commercial spaces to support formative assessments   of student writing.   Writing Analytics is a developing sub-domain of learning   analytics with a specific focus on supporting writing practices.   Research in this field has the potential to improve formative   feedback in writing exercises and to provide insights to both   educators and students (see previous workshop [1]). Despite this   strong potential, adoption of writing analytics tools has not been   widespread.    1.1 Writing Analytics Literacy  There is untapped potential in supporting educators to make   effective use of such tools. However, writing analytics literacy   in this sense must go beyond simply knowing how to use tools or   access results through simple user interfaces, and beyond tools   that simply output numeric information absent actionable   feedback. Rather, there is a need to engage educators with   resources that support them in designing meaningful tasks,   selecting appropriate tools to support those tasks, and interpreting   the data arising from them. To do this, educators must consider   the desired outcomes of assigned tasks (e.g., demonstrate   knowledge of key topics, use correct citation, use creative   language), and understand the potential  and pitfalls  of NLP to   address those needs. We thus see writing analytics literacy as   positioning analytics and writing-assessment literacies   synergistically. Through building such literacies, we aim to:   1. Develop a synergistic model of writing analytics literacy  and writing assessment literacy   2. Engage practitioners in thinking about (and researching)  how writing assignments in their teaching might provide   meaningful data for learning insights    3. Develop students writing analytics literacy (and, by  extension, their writing) through interaction with   appropriate tools    Developing these literacies will require a multi-faceted approach,   including continued development of research technologies, and   innovation around new approaches to writing instruction. For   these endeavors to have impact, practitioners must integrate them   into pedagogic contexts in which they guide action [2].    Learners have a number of challenges in interpreting analytics for   action [12]: They must connect the analytics to the processes and   overall goals of the learning task (contextual issues); they must   evaluate the quality of the analytic, understanding how it is   Permission to make digital or hard copies of part or all of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights   for third-party components of this work must be honored. For all other   uses, contact the Owner/Author.   Copyright is held by the owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029425   http://dx.doi.org/10.1145/3027385.3029425   developed and how it can inform their learning (trust issues); they   must select to which information to attend   present, and absent   from the analytic  and where to devote time (priority issues); and   they must decide how  as an individual  to respond to analytics   and what they represent (individual issues) [12].   Learners must make decisions based on these interpretations.   Thus, analytics must [12]: present possible options, empowering   learners to decide; provide actionable information for students to   do something with the information; afford autonomy to students,   such that analytics help them identify their own learning patterns,   rather than relying on the analytic for this information [12].    Wise et al. propose the align design framework, in which   educators integrate analytics as an integral element in the   learning process tied to their goals, expectations, and planned   learning process [12], with students given agency to engage   with analytics as a tool to inform their actions, as opposed to   analytics being something with which students must comply   [12]. These principles frame the activity, with context added   through a reference frame that provides an action-oriented   comparator for the interpretation of the analytic, with a principle   of dialogue/audience describing discussion around students   learning goals and processes. For these design-implementations to   be achieved, there is a need to support educators in connecting   analytic design, pedagogy, and theory [4, 13].    1.2 Learning Analytics Carpentry (LAC)  Other increasingly data-driven fields have grappled with   developing both researcher and practitioner knowledge. Data   Carpentry workshops (http://www.datacarpentry.org/), developed   based on Software Carpentry bootcamps [11] (http://software-  carpentry.org/) are short workshops designed to teach basic   concepts, skills, and tools for working with data so researchers   can get more done in less time and with less pain [9]. They are   designed to give novices the starting toolkit to begin working   programmatically with data in their own research. Data carpentry   sessions focus on example data sets targeted at particular domains   of relevance to the learners, with no prior-knowledge assumed.   For example, from an R Hackathon in population genetics, a   community website has been developed of vignettes [3], with   proposals for a collaborative training infrastructure for   bioinformatics including openly-co-developed resources and a   carpentry-based teaching model that blends formal and informal   elements with ongoing peer support [10].    We propose to adopt a learning analytics carpentry model, to (1)   develop capability among LAK researchers in the analysis of   writing data; (2) connect this knowledge to practitioner contexts;   (3) begin to build resources for writing analytics carpentry based   learning    Existing work in this area (e.g., the 2014 EdX Data, Analytics   and Learning, the 2016 LASI topic modeling workshop, etc.)   has focused on building researcher confidence in particular   techniques, with a primary focus on the analytic rather than   integration. The proposed workshop aims to develop resources   that will both build capacity in learning analytic techniques, and   the targeting of those analytics at particular pedagogic contexts.   2. WORKSHOP OBJECTIVES  Workshop attendees will contribute one or more of the following:   1. A tool that has been developed, along with resources  describing particular pedagogic contexts in which it might   be integrated   2. Documentation of a specific learning context in which  writing analytics could be applied   3. Data that could be analyzed with the provided tools, (in  addition, completing an in-workshop pedagogically   meaningful activity to produce live data).   The workshop will be targeted at:   1. Providing a tutorial regarding key tools for writing analytics  research and practice, highlighting existing tools, resources,   and practices   2. Building a resource bank of sample datasets from which  learning vignettes might be developed   3. Creating a wish list of resources to support practitioners in  their learning analytics literacy around writing, including   developing a framework describing the kinds of pedagogic   contexts in which particular tools might be integrated.   The workshop thus proposes to provide both hands-on tutorial   elements, and resource-creation.   3. REFERENCES  [1] Buckingham Shum, S. et al. 2016. Critical Perspectives on   Writing Analytics. (Edinburgh, UK, 2016).   [2] Clow, D. 2012. The learning analytics cycle: closing the   loop effectively. Proceedings of the 2nd International   Conference on Learning Analytics and Knowledge (2012),   134138.   [3] Kamvar, Z.N. et al. 2016. Developing educational   resources for population genetics in r: an open and   collaborative approach. Molecular Ecology Resources.   (Jul. 2016).   [4] Knight, S. et al. 2014. Epistemology, assessment,   pedagogy: where learning meets analytics in the middle   space. Journal of Learning Analytics. 1, 2 (2014).   [5] McNamara, D.S. et al. 2014. Automated evaluation of text   and discourse with Coh-Metrix. Cambridge University   Press.   [6] National Commission On Writing 2003. Report of the   National Commission on Writing in Americas Schools and   Colleges: The Neglected R, The Need for a Writing   Revolution. College Board.   [7] OECD 2013. PISA 2015: Draft reading literacy   framework. OECD Publishing.   [8] Shermis, M.D. and Burstein, J. 2013. Handbook of   Automated Essay Evaluation: Current Applications and   New Directions. Routledge.   [9] Teal, T.K. et al. 2015. Data carpentry: workshops to   increase data literacy for researchers. International Journal   of Digital Curation. 10, 1 (2015), 135143.   [10] Williams, J.J. and Teal, T.K. 2016. A vision for   collaborative training infrastructure for bioinformatics.   Annals of the New York Academy of Sciences. (Sep. 2016),   n/an/a.   [11] Wilson, G. 2006. Software carpentry. Computing in   Science & Engineering. 8, (2006), 66.   [12] Wise, A.F. et al. 2016. Developing Learning Analytics   Design Knowledge in the Middle Space: The Student   Tuning Model and Align Design Framework for Learning   Analytics Use. Online Learning. 20, 2 (Jan. 2016).   [13] Wise, A.F. and Shaffer, D.W. 2015. Why Theory Matters   More than Ever in the Age of Big Data. Journal of   Learning Analytics. 2, 2 (2015), 513.  http://www.datacarpentry.org/ http://software-carpentry.org/) http://software-carpentry.org/)   1. INTRODUCTION  1.1 Writing Analytics Literacy  1.2 Learning Analytics Carpentry (LAC)   2. WORKSHOP OBJECTIVES  3. REFERENCES     "}
{"index":{"_id":"64"}}
{"datatype":"inproceedings","key":"Macfadyen:2017:DIL:3027385.3029426","author":"Macfadyen, Leah P. and Groth, Dennis and Rehrey, George and Shepard, Linda and Greer, Jim and Ward, Douglas and Bennett, Caroline and Kaupp, Jake and Molinaro, Marco and Steinwachs, Matt","title":"Developing Institutional Learning Analytics 'Communities of Transformation' to Support Student Success","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"498--499","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029426","doi":"10.1145/3027385.3029426","acmid":"3029426","publisher":"ACM","address":"New York, NY, USA","keywords":"change management, communities of transformation, faculty engagement, institutional learning analytics, learning analytics fellows program, student success","abstract":"Institutional implementation of learning analytics calls for thoughtful management of cultural change. This interactive halfday workshop responds to the LA literature describing the benefits and challenges of institutional LA implementation by offering participants an opportunity to learn about and begin planning for a program to actively engage faculty as leaders of data exploration around the theme of 'student success'. This session will share experiences from five institutions actively engaged in fostering Learning Analytics Communities (LAC) by identifying key issues, sharing lessons learned, and considering structural frameworks that are transferable to other institutional contexts. Structured discussion and activities will engage participants in developing an action plan for establishing an LAC on their own campus.","pdf":"Developing Institutional Learning Analytics Communities  of Transformation to Support Student Success   Leah P. Macfadyen  The University of British Columbia,   1866 Main Mall,  Vancouver, BC, V6T 1Z1, Canada   +1 604 809 5013,  leah.macfadyen@ubc.ca    Dennis Groth, George Rehrey   & Linda Shepard  Indiana University   Maxwell Hall, 750 E. Kirkwood Ave    Bloomington, IN 47405, USA   +1 812 855 8783,  dgroth@indiana.edu   Jim Greer  University of Saskatchewan   105 Administration Place   Saskatoon, SK, S7N 5A2 Canada   jim.greer@usask.ca    Douglas Ward &   Caroline Bennett  University of Kansas   135 Budig Hall, 1455 Jayhawk  Blvd, Lawrence KS 66045, USA   +1 785 864 7637,  dbward@ku.edu   Jake Kaupp  Queens University    Fac. of Eng. & Applied Science  Kingston, ON, K7L 3N6, Canada   jake.kaupp@queensu.ca     Marco Molinaro &   Matt Steinwachs   University of California, Davis  1335 Grove, Davis, CA 95616,   USA  +1  (916) 860-2108,   mmolinaro@ucdavis.edu    ABSTRACT  Institutional implementation of learning analytics calls for  thoughtful management of cultural change. This interactive half- day workshop responds to the LA literature describing the benefits  and challenges of institutional LA implementation by offering  participants an opportunity to learn about and begin planning for a  program to actively engage faculty as leaders of data exploration  around the theme of student success. This session will share  experiences from five institutions actively engaged in fostering  Learning Analytics Communities (LAC) by identifying key issues,  sharing lessons learned, and considering structural frameworks that  are transferable to other institutional contexts. Structured  discussion and activities will engage participants in developing an  action plan for establishing an LAC on their own campus.   CCS Concepts   Collaborative and social computing Empirical studies in  collaborative and social computing. Applied  computing Education  Keywords  Institutional learning analytics; change management; faculty  engagement; communities of transformation; student success;  learning analytics fellows program.   1. WORKSHOP BACKGROUND  Institutional implementation of learning analytics calls for  thoughtful management of cultural change. In addition to technical  and infrastructure planning, researchers and institutions engaged in   LA planning have identified that effective implementation of  institutional learning analytics requires strategic efforts to engage  the community in conversations about the potential for learning  analytics to improve student success, and to generate excitement  and engagement by key stakeholders (students, faculty, instructors,  senior administrators) [1,2]. As change management specialists  [3,4,5] have pointed out, change in habits, practices and behaviours  is not brought about by simply giving people large volumes of  logical data [6]. Overcoming resistance to innovation and change  calls for planning processes which create conditions that allow  participants to both think and feel positively about change   conditions that appeal to both the heart and the head.    Research suggests that communities of practice have positive  effects on teaching development, and that faculty engagement in  such communities leads to positive changes in teaching behaviors.  [7,8,9]. Moreover, faculty who engage in systematic inquiry on  student learning in their own classrooms and programs  (Scholarship of Teaching and Learning or SoTL) report a wide  range of benefits, including documented improvements in the  quality of their students learning, more of their students achieving  higher standards, and an increased interest in positively influencing  teaching in their department beyond their own practice [10,11]   In order to increase the use of big data for informed decision- making in higher education, we must expand its use to the  departmental and course levels, which calls for faculty ownership  of research questions that examine and define student success.  LACs offer the potential to bring faculty of all ranks into the  learning analytics conversation early, and allow them to lead data  exploration projects and share their findings with colleagues and  peers. Such programs can be conceptualized as fostering  institutional Communities of Transformation [12] that can  address both individual faculty and broader systemic change, and  create innovative spaces that have the potential to shift institutional  and disciplinary norms. This workshop will describe and share our  experience with establishment of institutional LACs that can foster  enthusiasm about the affordances of learning analytics and their  potential to support student success and improved teaching and  learning.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029426     Indiana University Bloomington launched its Student Learning  Analytics Fellows Program1 in 2015 [13], and is now leading a  multi-year project to expand the concept in partnership with Bay  View Alliance (BVA)2 partner universities: The University of  British Columbia, the University of Kansas, Queens University and  the University of Saskatchewan. This workshop will be led by and  will draw on the experiences of LAC leaders in these institutions.   2. WORKSHOP OBJECTIVES  This half-day workshop will help participants create a plan for  building a campus community devoted to using learning analytics  to improve teaching and learning. It will explore the challenges and  successes from similar programs at universities in the BVA, lead  participants through exercises in setting goals, creating meaningful  questions, and encourage involvement from faculty and  departments for data-driven decision making at the course, program  and curricular levels.    The workshop will start with a brief self-assessment on the needs  of individual campuses, and the challenges participants are likely  to encounter in using learning analytics to improve teaching and  learning. Workshop leaders will then draw on their institutional  experiences to help participants address a series of topics and  questions:   Program goals: How can institutions create communities of  transformation around learning analytics How can we expand  faculty inquiry about student success from the course level to the  curricular level How are we facilitating the use of LA by faculty,  departments, and campus leaders   Establishment of community: How might LACs differ from other  types of Faculty Learning Communities (FLCs) How can we best  facilitate interdisciplinary collaborations throughout the inquiry  process   Types of research questions explored: What types of questions  work best in tying learning analytics to teaching and learning  Which have driven research in existing LACs How have these  questions been answered and how have they informed our  understanding of student success    Data collection and management: What types of data are most  valuable in addressing questions about student success How have  these data been collected and managed to increase efficiency of  delivery to faculty investigators What other data might be valuable  (e.g., student surveys)   Supporting faculty LA research: Which methods best support  faculty-led LA research (especially those not familiar with data- rich social science research methods)    Throughout this workshop, participants will be encouraged to use  an action-plan worksheet that assists with analysis of their own  institutional contexts. In the engagement phase, participants will  brainstorm and work with fellow participants to identify local goals  and stakeholders. By the end of the workshop participants will have  created an action plan for development of an LAC on their own  campuses.    Overall, participants will: Evaluate learning analytics readiness on  their campus; Learn how other institutions have used data-driven  decision making to improve student success; Identify key issues  relevant to establishing faculty ownership of learning analytics                                                                        1http://citl.indiana.edu/programs/grants/learninganalytics-CFP.php    projects, personalizing them for their institutional contexts; Identify  potential stakeholders involved in learning analytics efforts at their  institutions; and develop an action plan for creating a learning  analytics community on their campus.   3. ACKNOWLEDGMENTS  We would like to acknowledge the BVA network and especially  Mary Deane Sorcinelli (U. Mass Amherst and BVA Hub member)  for their instrumental role in facilitating and supporting our  collaborative efforts thus far.   4. REFERENCES  [1] Macfadyen, L. P. and Dawson, S. 2012. Numbers are not  enough. Why e-learning analytics failed to inform an institutional  strategic plan. Educational Technology & Society, 15, 3, 149163.  [2] Macfadyen, L. P., Dawson, S., Pardo, A., and Gaevi, D.  2014. Embracing big data in complex educational systems: The  learning analytics imperative and the policy challenge. Research  & Practice in Assessment, 9, 17-28.    [3] Kotler, P. and Zaltman, G. 1971. Social marketing: An  approach to planned social change. Journal of Marketing, 35, 3- 12.    [4] Kavanagh, M. H., and Ashkanasy, N. M. 2006. The impact  of leadership and change management strategy on organizational  culture and individual acceptance of change during a merger.  British Journal of Management, 17, S81-S103.   [5] Kotter, J. P. 1996. Leading Change. Harvard Business  School Press, Boston, MA.    [6] Kotter, J. P., & Cohen, D. S. 2002. The Heart of Change.  Harvard Business School Press, Boston, MA.   [7]  Cox, M.D. 2001. Faculty learning communities: Change  agents for transforming institutions into learning organizations. To  Improve the Academy, 19, 69-93.   [8] Sorcinelli, M. D., Austin, A. E., Eddy, P., and Beach, A.  2006. Creating the future of faculty development: Learning from  the past, understanding the present. Wiley/Jossey-Bass, San  Francisco, CA.   [9]  Stes, A., Min-Leliveld, M., Gijbels, D. and Van Pategewm,  P. 2010.  The impact of instructional development in higher  education: The state-of-the-art of the research.  Educational  Research Review, 5, 25-49.     [10] Chasteen, S.V., Wilcox, B., Caballero, M.D., Perkins, K.K.,  Pollock, S.J., and Wieman, C. J. 2015.  Educational  transformation in upper-division physics: The Science Education  Initiative model, outcomes, and lessons learned. Physical Review  Special Topics Physics Education Research, 11, 2, 020110.    [11]  Huber, M. T., and Hutchings, P. 2005. The Advancement of  Learning: Building the Teaching Commons. Jossey-Bass, Inc.,  San Francisco, CA.    [12] Kezar, A. & Gehrke, S. 2015. Communities of  Transformation and Their Work Scaling STEM Reform. Pullias  Center for Higher Education.  [13] Siering, G. and Shepard, L. 2017. Promoting data-driven  decision-making through a learning analytics fellows program.  ELI Annual Meeting, Houston, TX.  2 http://bayviewalliance.org/      "}
{"index":{"_id":"65"}}
{"datatype":"inproceedings","key":"Bergner:2017:WML:3027385.3029427","author":"Bergner, Yoav and Lang, Charles and Gray, Geraldine","title":"Workshop on Methodology in Learning Analytics (MLA)","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"500--501","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029427","doi":"10.1145/3027385.3029427","acmid":"3029427","publisher":"ACM","address":"New York, NY, USA","keywords":"evaluation, measurement, methodology, models, statistics","Abstract":"Learning analytics is an interdisciplinary and inclusive field, a fact which makes the establishment of methodological norms both challenging and important. This community-building workshop intends to convene methodology-focused researchers to discuss new and established approaches, comment on the state of current practice, author pedagogical manuscripts, and co-develop guidelines to help move the field forward with quality and rigor.","pdf":"Workshop on Methodology in Learning Analytics (MLA)   Yoav Bergner   New York University  New York, NY, USA  yoav.bergner@nyu.edu      Charles Lang  Teachers College, Columbia   University  New York, NY, USA   charles.lang@tc.columbia.edu   Geraldine Gray  Institute of Technology,   Blanchardstown  Dublin, Ireland   geraldine.gray@itb.ie         ABSTRACT  Learning analytics is an interdisciplinary and inclusive field, a fact  which makes the establishment of methodological norms both  challenging and important. This community-building workshop  intends to convene methodology-focused researchers to discuss  new and established approaches, comment on the state of current  practice, author pedagogical manuscripts, and co-develop  guidelines to help move the field forward with quality and rigor.      CCS Concepts   Computing methodologies  Modeling and simulation   Model development and analysis      Keywords  Models; Methodology; Measurement; Statistics; Evaluation.      1. WORKSHOP BACKGROUND  Learning analytics is an interdisciplinary and inclusive field that  brings together educational technologists, psychologists, data  scientists, learning scientists, substantive experts in various  domains, and measurement specialists [7]. For all of the strength  that comes from such diversity, there are also potential pitfalls  when it comes to establishing norms for methodological work. For  example, Clow [3] described learning analytics as, a jackdaw  field of enquiry, picking up shiny techniques, tools and  methodologies This eclectic approach is both a strength and a  weakness: it facilitates rapid development and the ability to build  on established practice and findings, but itto datelacks a  coherent, articulated epistemology of its own. (p. 686).    In the years since this observation, the learning analytics  community has grown rapidly, and the number of shiny  techniques has grown as well. Looking just at the last two   proceedings of the International Conference on Learning  Analytics and Knowledge (LAK) in 2015 and 2016, the variety is  staggering.  Methods range from descriptive statistics to  correlation analyses, classification, clustering, regression,  (M)AN(C)OVA, structural equation modeling, item response  theory, hidden Markov models, time-series analysis, latent  semantic analysis, social network analysis, and the list goes on. It  is understandable and even expected that reviewers and readers of  learning analytics manuscripts are unlikely to be expert evaluators  of the methodological rigor in all of these cases.    There is a naturally occurring process of specialization in any  academic field. However, if growth of adoption outpaces  systematic specialization then there is a risk that methodological  errors will proliferate and that quality of community products will  suffer.      To make matters even more complex, a number of recent papers  have emphasized the sensitivity of quantitative analyses to data  collection and variable operationalization choices, for example  with regard to effects of selection bias [2], results of time-on-task  analyses [4], studies of discussion forum usage [1], and evaluation  of student models [6]. In addition, learning analytics models often  incorporate a selection of proxy variables as indicators of latent  constructs, such as learning and engagement. What proxy  variables actually measure is less clear. For example, measures of  engagement may be influenced by instructional conditions [5],  adding ambiguity, and a lack of consistency,  to our interpretation  of  models of learning.    In short, methodological concerns can arise from a range of  practices including but not limited to selecting inappropriate  methods, misusing methods, inadequate model evaluation or  model comparison, sensitivity to operationalization, and over- reliance on proxy variables. As the learning analytics community  matures, it is particularly important to establish standards for good  practice and to educate new students in accordance with these  standards. Clear methodological guidelines increase the quality of  work and facilitate communication not only within the community  but also with practitioners in other research communities, where  norms may be clearer. This is a challenging problem in large part  because of the aforementioned diversity of approaches. The  present workshop seeks to build a community of researchers with  an interest in methodology and its rigorous application and  development to the field of learning analytics.   There have been several previous LAK workshops and tutorials  that have focused on specific methodologiesa limited set of  examples includes the tutorials for classification and clustering  using Weka (2014, 2016), special topics in discourse analysis  (2013-2014) and writing analysis (2016), and a recurring  workshop on temporal analysis (2012-2016)but not on cross- cutting methodological issues such as developing methodological   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029427     frameworks within learning analytics, framing and prioritizing  methodological issues for the community, and providing resources  to move the field forward.      2. WORKSHOP OBJECTIVES  2.1 Solicit Papers about Learning Analytics  Methodology  The first objective of the present workshop is to solicit new  substantive papers specializing on methodology. We imagine  these papers to fall roughly into the following categories: papers  presenting new methods or adaptation/modification of methods;  position papers which take a critical look at methodological  practice in the community; and pedagogical/instructional papers  oriented at students or researchers who are new to the field or  developing an interest in a particular methodology.    We encourage the exploration of new metrics that are indigenous  to learning analytics, papers that link metrics to the latent  variable(s) they measure, with a view to establishing definitions of  latent variables of relevance to learning analytics, and guidelines  on associating observable metrics with latent variables. Papers on  appropriate metrics for model evaluation and metrics for  comparison of model results where multiple models are reported  on would also exemplify an effort to add rigor to applicable  studies.   2.2 Develop Community Guidelines  Related to the position and instructional papers that may be  presented during the workshop, a second objective of convening  will be to cooperatively develop community guidelines regarding  the uses of various methods including data acquisition, data  analysis and evaluation of results in conference and journal  publications. Up for discussion will be content, process, and  dissemination protocols.   2.3 Provide Expertise for Review Panels  A third objective of the workshop is to take responsibility for  maintaining a database of methodology experts who are active in  the learning analytics community. The expert listing is by no  means intended to be exclusionary or to promote certain  researchers over others but rather to help community members   and editorial committees find methodology experts who are  willing to consult and/or review relevant work.    2.4 Community Building  Last but not least,  an objective of the workshop is to provide a  meeting place for researchers who take a special interest in  methodological issues. We anticipate that a concentrated meeting  will promote continuing collaboration on this important topic.       3. REFERENCES   [1] Bergner, Y., Kerr, D., and Pritchard, D. E. 2015.   Methodological Challenges in the Analysis of MOOC Data  for Exploring the Relationship between Discussion Forum  Views and Learning Outcomes. Proceedings of 8th  International Conference on Educational Data Mining, 234- 241.   [2] Brooks, C., Chavez, O., Tritz, J., and Teasley, S. 2015.  Reducing selection bias in quasi-experimental educational  studies. Proceedings of the Fifth International Conference on  Learning Analytics And Knowledge, - LAK 15, ACM Press,  295299.   [3] Clow, D. 2013. An overview of learning analytics. Teaching  in Higher Education 18 (6), 683695.   [4] Kovanovi, V., Gaevi, D., Dawson, S., Joksimovi, S.,  Baker, R.S.J.D., and Hatala, M. 2015. Penetrating the black  box of time-on-task estimation. Proceedings of the Fifth  International Conference on Learning Analytics And  Knowledge - LAK 15, ACM Press, 184193.   [5] Gaevi, D., Dawson, S., and Siemens, G. 2015. Lets not  forget: Learning analytics are about learning. TechTrends 59  (1), 64-71.   [6] Pelnek, R., Rihk, J., and Papouek, J. 2016. Impact of data  collection on interpretation and evaluation of student models.  Proceedings of the Sixth International Conference on  Learning Analytics & Knowledge, - LAK 16, ACM Press,  4047.   [7] Siemens, G. and Gasevic, D. 2012. Guest Editorial-Learning  and Knowledge Analytics. Educational Technology &  Society 15 (3), 12.        "}
{"index":{"_id":"66"}}
{"datatype":"inproceedings","key":"Mojarad:2017:QDC:3027385.3029428","author":"Mojarad, Shirin and Lewkow, Nicholas and Essa, Alfred and Zhang, Jie and Feild, Jacqueline","title":"Quasi-experimental Design for Causal Inference Using Python and Apache Spark: A Hands-on Tutorial","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"502--503","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029428","doi":"10.1145/3027385.3029428","acmid":"3029428","publisher":"ACM","address":"New York, NY, USA","keywords":"Apache Spark, Python, big data, causal inference, parallel computing, quasiexperiment design","Abstract":"Educational practitioners and policy makers require evidence supporting claims about educational efficacy. Evidence is often found using causal relationships between education inputs and student learning outcomes. Causal inference covers a wide range of topics in education research, including efficacy studies to prove if a new policy, software, curriculum or intervention is effective in improving student learning outcomes. Randomized controlled trials (RCT) are considered a gold standard method to demonstrate causality. However, these studies are expensive, timely and costly, as well as not being ethical to conduct in many educational contexts. Causality can also be deducted purely from observational data. In this tutorial, we will review methodologies for estimating the causal effects of education inputs on student learning outcomes using observational data. This is an inherently complex task due to many hidden variables and their interrelationships in educational research. In this tutorial, we discuss causal inference in the context of educational research with big data. This is the first tutorial of its kind at Learning Analytics and Knowledge Conference (LAK) that provides a hands-on experience with Python and Apache Spark as a practical tool for educational researchers to conduct causal inference. As a prerequisite, attendees are required to have familiarity with Python.","pdf":"Quasi-Experimental Design for Causal Inference Using  Python and Apache Spark: A Hands-on Tutorial  Shirin Mojarad   McGraw-Hill Education  281 Summer Street  Boston, MA, USA   Shirin.mojarad@mheducation.com     Nicholas Lewkow  McGraw-Hill Education    281 Summer Street  Boston, MA, USA   Nicholas.lewkow@mheducation.com  Jie Zhang   McGraw-Hill Education  281 Summer Street  Boston, MA, USA   jie.zhang@mheducation.com      Alfred Essa  McGraw-Hill Education   281 Summer Street  Boston, MA, USA   Alfred.essa@mheducation.com  Jacqueline Feild   McGraw-Hill Education  281 Summer Street  Boston, MA, USA   jacqueline.feild@mheducation.com     ABSTRACT  Educational practitioners and policy makers require evidence  supporting claims about educational efficacy. Evidence is often  found using causal relationships between education inputs and  student learning outcomes. Causal inference covers a wide range  of topics in education research, including efficacy studies to prove  if a new policy, software, curriculum or intervention is effective in  improving student learning outcomes. Randomized controlled  trials (RCT) are considered a gold standard method to  demonstrate causality. However, these studies are expensive,  timely and costly, as well as not being ethical to conduct in many  educational contexts. Causality can also be deducted purely from  observational data. In this tutorial, we will review methodologies  for estimating the causal effects of education inputs on student  learning outcomes using observational data. This is an inherently  complex task due to many hidden variables and their inter- relationships in educational research.  In this tutorial, we discuss  causal inference in the context of educational research with big  data. This is the first tutorial of its kind at Learning Analytics and  Knowledge Conference (LAK) that provides a hands-on  experience with Python and Apache Spark as a practical tool for  educational researchers to conduct causal inference. As a  prerequisite, attendees are required to have familiarity with  Python.   CCS Concepts   Computing methodologies   Knowledge representation and  reasoning; Parallel computing methodologies; Mathematics  of computing   Exploratory data analysis.   Keywords  Causal inference; big data; parallel computing; python; quasi- experiment design; Apache Spark.   1. INTRODUCTION  Educational practitioners and policy makers require evidence,   supporting claims about educational efficacy. Causal claims are  traditionally tested through thorough experiments. Designing  experiments commonly involves identifying groups of students  who receive and do not receive an intervention and comparing the  outcomes across these groups, usually assuming all other  experimental variables are evenly distributed between the two  groups.   Randomized control trials (RCTs) are considered the gold  standard in conducting studies to investigate the effect of a  particular intervention on a specific outcome [11]. In an RCT, the  effect of a particular intervention on an outcome is estimated by  dividing the sample into an intervention group who receives the  intervention and a control group who does not. These two groups  are selected randomly, so that the intervention status is not  confounded with other baseline characteristics of the sample  population [9, 11]. For example, to study the effect of a new  curriculum as opposed to the current curriculum in improving  student outcomes, a large number of students are randomly  assigned to an intervention group or a control group, where they  receive the new or current curriculum respectively. Then, the  course outcomes are measured for the two groups, in order to  determine whether there is a difference in outcome or gains  between groups. This difference represents the effect of new  curriculum compared to the existing one.   Despite efforts to fund RCT research, the application of RCTs is  limited in educational environments as they require large number  of students, which can often be unrealistic [9]. Causal inference  from Observational Studies (OSs) is another form of analysis to  evaluate intervention effects [7]. In causal inference, the causal  effect of an intervention on a particular outcome is studied using  historical data, without the need for randomization in advance [1].  In this tutorial, we will show design of an OS to leverage the large  amounts of data available through online learning platforms and  student information systems.    Propensity score matching (PSM) is a common method in OSs to  study the causal effect of an intervention on a particular outcome  [1, 6]. PSM is used to design and analyze an observational study  such that it resembles the characteristics of a RCT. Using PSM,  one can create a less-confounded comparison from messy real- world data, in order to estimate the effect of an intervention on a  particular outcome, using data not collected with a specific non- confounded comparison in mind.  As such, this method can help  us take greater advantage of the rapidly increasing amount of data  involving varied interventions, stored within digital educational   Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on the  first page. Copyrights for third-party components of this work must  be honored. For all other uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029428.     platforms being used at scale [3, 5].   2. OBJECTIVES  Given the focus on practical skill building, the primary objectives  of the proposed tutorial are:    Understand different matching techniques for quasi- experiment design    Make causal inferences using observational data   Evaluate and compare the quality of matching methods   in quasi-experiment design   Make inferences about the cause and effects using the   outcomes of causal analysis   Utilize Apache Spark to perform simple causal   inference on data in parallel  While there are several conference series (e.g., LAK, EDM)  focusing on the intersection between educational and computing  research, to the best of our knowledge there have not been any  tutorials covering the topic of causal inference in a hands-on  fashion. A part of this tutorial was presented at Boston Data  Festival 2016.   There are packages available in R and Python to conduct causal  inference such as MatchIt [4] in R and CausalInference [2] in  Python. However, using these packages requires detailed  knowledge of matching algorithms and careful considerations to  ensure covariate balance in the resulting treatment and control  groups. TETRAD is another tool for causal discovery and  exploration [8]. Tetrad is unique in its user friendliness and use of  use for building and evaluating causal models to explore causal  relationship between several covariates and an outcome. These  techniques include path analysis, Bayesian networks and  structural equation modeling. In this tutorial, our focus is on using  statistical matching techniques for quasi-experimental designs for  Generalized Causal Inference including different matching  techniques such as propensity score and Mahalanobis distance  matching. In addition, we will evaluate and compare the  effectiveness of different matching methods for causal  inference [10]. Our goal in this tutorial is to make programming,  design of quasi-experiments for causal inferences and statistical  analysis of intervention effect on an outcome accessible for a wide  range of audience. We will cover these in the context of big data  and will provide hands-on tools, which use Apache Spark to make  causal inferences on big educational data.  The tutorial is designed to appeal to researchers from a wide range  of backgrounds including big data, predictive analytics, learning  sciences, educational data mining and researchers interested in  learning about how to have their own tool for causal inference. As  a prerequisite, attendees are required to have familiarity with  Python. The Python code for this workshop will be available on  GitHub in advance so that participants can review and get familiar  with the methods if they are interested. We will have our  colleagues from McGraw-Hill Education to help with the logistics  and to help the participants with the use of material.   The workshop will be conducted over a half day. We will first  introduce causal inference using simple models and methods. This  will get the audience comfortable with several concepts used for  causal inference. Next, each of the methods explained will be  covered by corresponding code in Python. Participants can  download the data and notebooks before or during the session  from our GitHub and stay engaged by running the notebook as we   walk through them. At the end of each objective, we will form  multiple groups and will assign a mini project to each group to  solve. We will discuss the outcome of the exercises before  moving to the next objective. Finally, we will expand upon the  causal inference models by introducing the basics of big data  analysis with Apache Spark. This will include revisiting  previously discussed models with the aim of parallelizing the  algorithms so that they can be used with very large data sets.   3. ACKNOWLEDGMENTS  We would like to thank Professor Ryan Baker for his invaluable  feedback and also MHE Digital Platform Group. Any opinions,  findings, conclusions or recommendations expressed in this paper  are those of the authors and do not necessarily reflect positions or  policies of the company.   4. REFERENCES  [1] Austin, P.C. 2011. An Introduction to Propensity Score   Methods for Reducing the Effects of Confounding in  Observational Studies. Multivariate Behavioral  Research. 46, 3 (May 2011), 399424.   [2] Causalinference:  https://github.com/laurencium/causalinference.  Accessed: 2016-09-29.   [3] Feng, M. et al. 2009. Using learning decomposition and  bootstrapping with randomization to compare the impact  of different educational interventions on learning.  Proceedings of International Conference on Educational  Data Mining. (2009), 5160.   [4] Ho, D.E. et al. 2011. MatchIt: Nonparametric  Preprocessing for Parametric Causal Inference. Journal  of Statistical Software. 42, 8 (2011), 128.   [5] Koedinger, K. et al. 2012. Automated Student Model  Improvement. Educational Data Mining, proceedings of  the 5th International Conference on. (2012), 1724.   [6] Rosenbaum, P. and Rubin, D. 1983. The Central Role of  the Propensity Score in Observational Studies for Causal  Effects. Biometrika. 70, (1983), 4155.   [7] Rubin, D.B. 2007. The design versus the analysis of  observational studies for causal effects: parallels with the  design of randomized trials. Statistics in Medicine. 26, 1  (Jan. 2007), 2036.   [8] Scheines, R. et al. 1998. The TETRAD Project:  Constraint Based Aids to Causal Model Specification.  Multivariate Behavioral Research. 33, 1 (1998), 153.   [9] Silverman, S.L. 2009. From Randomized Controlled  Trials to Observational Studies. The American Journal of  Medicine. 122, 2 (2009), 114120.   [10] Stuart, E.A. 2010. Matching methods for causal  inference: A review and a look forward. 25, 1 (2010), 1 21.   [11] Sullivan, G.M. 2011. Getting Off the Gold Standard:  Randomized Controlled Trials and Education Research.  Journal of Graduate Medical Education. 3, 3 (Sep.  2011), 285289.        "}
{"index":{"_id":"67"}}
{"datatype":"inproceedings","key":"Clow:2017:BFN:3027385.3029429","author":"Clow, Doug and Ferguson, Rebecca and Kitto, Kirsty and Cho, Yong-Sang and Sharkey, Mike and Aguerrebere, Cecilia","title":"Beyond Failure: The 2Nd LAK Failathon","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"504--505","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029429","doi":"10.1145/3027385.3029429","acmid":"3029429","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics, evidence, learning analytics, learning from failure","Abstract":"The 2nd LAK Failathon will build on the successful event in 2016 and extend the workshop beyond discussing individual experiences of failure to exploring how the field can improve, particularly regarding the creation and use of evidence. Failure in research is an increasingly hot topic, with high-profile crises of confidence in the published research literature in medicine and psychology. Among the major factors in this research crisis are the many incentives to report and publish only positive findings. These incentives prevent the field in general from learning from negative findings, and almost entirely preclude the publication of mistakes and errors. Thus providing an alternative forum for practitioners and researchers to learn from each other's failures can be very productive. The first LAK Failathon, held in 2016, provided just such an opportunity for researchers and practitioners to share their failures and negative findings in a lower-stakes environment, to help participants learn from each other's mistakes. It was very successful, and there was strong support for running it as an annual event. This workshop will build on that success, with twin objectives to provide an environment for individuals to learn from each other's failures, and also to co-develop plans for how we as a field can better build and deploy our evidence base","pdf":"Beyond Failure: The 2nd LAK Failathon  Doug Clow1, Rebecca Ferguson1, Kirsty Kitto2, Yong-Sang Cho3,    Mike Sharkey4, Cecilia Aguerrebere5 1The Open University   Walton Hall, Milton Keynes  MK7 6AA, UK   {Firstname.Surname}@open.ac.uk     4Blackboard, Inc  Phoenix, AZ, USA   Mike.Sharkey@blackboard.com     2Queensland University of  Technology, School of Mathematical  Sciences, Level 7, P-Block, 2 George   Street, Brisbane, 4001, Australia  kirsty.kitto@qut.edu.au   3Korea Education & Research  Information Service   64 Dongnau-Ro, Dong-Gu, Daegu,  41061 Korea   zzosang@gmail.com  5Plan Ceibal,    Av. Italia 6201, Edificio Los Ceibos,   Montevideo, Uruguay   caguerrebere@ceibal.edu.uy  ABSTRACT  The 2nd LAK Failathon will build on the successful event in 2016  and extend the workshop beyond discussing individual  experiences of failure to exploring how the field can improve,  particularly regarding the creation and use of evidence.   Failure in research is an increasingly hot topic, with high-profile  crises of confidence in the published research literature in  medicine and psychology. Among the major factors in this  research crisis are the many incentives to report and publish only  positive findings. These incentives prevent the field in general  from learning from negative findings, and almost entirely  preclude the publication of mistakes and errors. Thus providing an  alternative forum for practitioners and researchers to learn from  each others failures can be very productive. The first LAK  Failathon, held in 2016, provided just such an opportunity for  researchers and practitioners to share their failures and negative  findings in a lower-stakes environment, to help participants learn  from each others mistakes. It was very successful, and there was  strong support for running it as an annual event. This workshop  will build on that success, with twin objectives to provide an  environment for individuals to learn from each others failures,  and also to co-develop plans for how we as a field can better build  and deploy our evidence base.    Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   General Terms  Management, Human Factors.   Keywords  Learning analytics, analytics, evidence, learning from failure.   1 WORKSHOP BACKGROUND  1.1 Failure in research  Problems with the published research literature are currently  receiving large amounts of attention, particularly in applied fields.    In health, the optimism that surrounded the evidence-based  medicine movement is beginning to falter, partly as the idea is  diverted from its original goals [1], but more fundamentally, as  issues with the underlying research come to light. Not only is  most published research false [2], but most of the true research  that is published is not useful in clinical practice [3].   In psychology, the replication crisis continues and intensifies. A  prominent effort to replicate a series of 100 classic psychological  results [4] achieved very partial success: A large portion of  replications produced weaker evidence for the original findings,  with only 3647% of replications succeeding, depending on the  measure chosen. It has also proved highly controversial, with  many blog and social media posts, using language that is  sometimes intemperate. One recent high-profile example of a  failed replication is power poses. The original claim was that a  person can, by assuming two simple 1-min poses, embody power  and instantly become more powerful [5]. One of the original  authors has had significant success as a public speaker on the  topic, with a TED talk receiving over 36m views [6], but after  failed replications, one of the authors has very creditably  concluded  that they do not think the effect is real [7].   A wide range of complex and hard-to-overcome factors lies  behind these problems in establishing a strong evidence base for  practice. Many of these concern the use of statistics, including the  use of researcher degrees of freedom to achieve significance [8]   importantly, this is not limited to situations where researchers  conduct multiple unreported comparisons, but also where  researchers can perform a reasonable analysis given their  assumptions and their data, but had the data turned out differently,  they could have done other analyses that were just as reasonable  [9]. Fundamentally, any research carried out with low pre-study  odds is prone to false positives [2]. Incentives on researchers to  publish significant findings play a strong part, and may encourage  publication of low-quality research even if replications were  commonplace and there were significant negative consequences to  publishing studies that were later repudiated [10].   The file drawer effect, whereby uninteresting or negative  findings are not reported, is a major concern. In clinical research,   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029429         the ambitious AllTrials1 project seeks to ensure All trials  registered, all results reported to reduce this problem.   1.2 Evidence in learning analytics  There is no reason to believe that learning analytics is immune to  these problems. One attempt to explore this issue is the Learning  Analytics Community Exchange (LACE) projects Evidence  Hub2, which maps research evidence against four propositions  about learning analytics. The great majority of evidence classified  was positive, with only 14% negative [11], which suggests that  there is a significant publication bias in the field. Further, very  little of the published research could be classified at the higher  levels of the evidence hierarchy (i.e. systematic reviews,  randomised controlled trials) [11]. These are the base levels at  which the problems in health and psychology can be detected, so  their dearth in the evidence base for learning analytics may mean  that the problems in our field are even more profound.   1.3 Why a workshop at LAK  The first LAK Failathon was a success, giving an opportunity for  practitioners and researchers to talk about  and learn from  their  failures in a way that is difficult to provide in any other context.  This second LAK Failathon will build on that success and provide  a similar space in the first half of the workshop.   The critiques in health and psychology propose a wide range of  possible solutions (e.g. [12]), some of which may well be useful in  the field of learning analytics. So the second part of the workshop  will explore, collectively, how we can improve the creation and  use of evidence in our field.   2. WORKSHOP OBJECTIVES AND  INTENDED OUTCOMES  This workshop has two chief objectives: firstly, to provide an  effective space for sharing experiences of failures, and secondly,  to work collaboratively to produce prioritised action plans for the  field of learning analytics to improve.   2.1 Sharing experience of failure  The first part of the workshop aims to allow practitioners and  researchers to learn from each others mistakes. There are strong  pressures on people to publicise success and minimise failures,  which limit the willingness of people to admit their mistakes and  discuss them. Closed forums are routinely used in education, in  part to allow learners to have a safe space to make mistakes, from  which they can learn. So, as with last year, this part of the  workshop will be held under the Chatham House Rule:   When a meeting, or part thereof, is held under the Chatham  House Rule, participants are free to use the information received,  but neither the identity nor the affiliation of the speaker(s), nor  that of any other participant, may be revealed.3  The participants can discuss, write and change their plans based  on what they learn from this part of the workshop, but may not  identify who said it or which organisation or activity it related to.   2.2 Producing action plans for improvement  It is helpful to learn as individuals from each others mistakes, but  it is also helpful to learn and improve collectively. This years                                                                        1 http://www.alltrials.net/  2 http://evidence.laceproject.eu/  3 https://www.chathamhouse.org/about/chatham-house-rule   Failathon is focused particularly on evidence, and this part of the  workshop aims to explore what can be done to improve the  creation and use of evidence in the field of learning analytics.   The chief outcome from the workshop will be a series of action  plans collectively developed by the participants, consisting of  prioritised lists of suggested actions that could be taken by:    SoLAR, the Society for Learning Analytics Research    Future LAK conference organisers and committees    Universities and other research organisations    Companies, developers, and others with interests in  learning analytics   Following this workshop, we will take the plans developed by the  participants to the LAK poster session, to solicit feedback from a  broader audience. This will engage the community more broadly  than the workshop participants, which will raise the profile of  these issues, and give the plans as finally developed greater  legitimacy and, we hope, traction.   3. REFERENCES  [1] Greenhalgh, T., Howick, J., & Maskrey, N. (2014). Evidence   based medicine: a movement in crisis BMJ 2014;348:g3725   [2] Ioannidis, J. P. (2005). Why most published research   findings are false. PLoS Med, 2(8), e124.  [3] Ioannidis, J. P. A. (2016). Why Most Clinical Research Is   Not Useful. PLoS Med 13(6): e1002049.   [4] Open Science Collaboration. (2015). Estimating the   reproducibility of psychological science. Science, 28 Aug  2015: 349(6251).   [5] Carney, D. R., Cuddy, A. J., & Yap, A. J. (2010). Power  posing brief nonverbal displays affect neuroendocrine levels  and risk tolerance. Psychological Science, 21(10), 1363- 1368.   [6] Cuddy, A. (2012). Your body language shapes who you are.  TED talk, https://www.ted.com/talks/  amy_cuddy_your_body_language_shapes_who_you_are    [7] Carney, D. (2016). My position on Power Poses. Blog post:  http://faculty.haas.berkeley.edu/dana_carney/pdf_My%20pos ition%20on%20power%20poses.pdf   [8] Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011).  False-positive psychology undisclosed flexibility in data  collection and analysis allows presenting anything as  significant. Psychological science, 0956797611417632.   [9] Gelman, A. & Loken, E. (2013). The garden of forking  paths: Why multiple comparisons can be a problem, even  when there is no fishing expedition or p-hacking and  the research hypothesis was posited ahead of time.   Blog    post:    http://www.stat.columbia.edu/~gelman/research/unpublished/ p_hacking.pdf   [10] Smaldino, P.E. & McElreath, R. (2016). The natural  selection of bad science. R. Soc. open sci. 3:160384.    [11] Ferguson, R & Clow, D. (2015). Evidence Hub Second  Review D2.8. http://www.laceproject.eu/deliverables/d2-8- evidence-hub-second-review/   [12] Ioannidis, J. P. A. (2014). How to Make More Published  Research True. PLoS Med 11(10): e1001747.  doi:10.1371/journal.pmed.1001747     "}
{"index":{"_id":"68"}}
{"datatype":"inproceedings","key":"Wang:2017:WIL:3027385.3029430","author":"Wang, Yuan and Davis, Dan and Chen, Guanliang and Paquette, Luc","title":"Workshop on Integrated Learning Analytics of MOOC Post-course Development","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"506--507","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029430","doi":"10.1145/3027385.3029430","acmid":"3029430","publisher":"ACM","address":"New York, NY, USA","keywords":"career development, learning analytics, learning outcomes, long-term learning development, massive online open courses","Abstract":"MOOC research is typically limited to evaluations of learner behavior in the context of the learning environment. However, some research has begun to recognize that the impact of MOOCs may extend beyond the confines of the course platform or conclusion of the course time limit. This workshop aims to encourage our community of learning analytics researchers to examine the relationship between performance and engagement within the course and learner behavior and development beyond the course. This workshop intends to build awareness in the community regarding the importance of research measuring multi-platform activity and long-term success after taking a MOOC. We hope to build the community's understanding of what it takes to operationalize MOOC learner success in a novel context by employing data traces across the social web.","pdf":"Workshop on Integrated Learning Analytics of MOOC Post- Course Development  Yuan Wang  Columbia University  New York, NY, USA   elle.wang@columbia.edu     Dan Davis  Delft University of Technology   The Netherlands  d.j.davis@tudelft.nl   Luc Paquette  University of Illinois Urbana-Champaign   Champaign, IL, USA  lpaq@illinois.edu      Guanliang Chen  Delft University of Technology   The Netherlands  Guanliang.chen@tudelft.nl        ABSTRACT  MOOC research is typically limited to evaluations of learner  behavior in the context of the learning environment. However, some  research has begun to recognize that the impact of MOOCs may  extend beyond the confines of the course platform or conclusion of  the course time limit. This workshop aims to encourage our  community of learning analytics researchers to examine the  relationship between performance and engagement within the course  and learner behavior and development beyond the course. This  workshop intends to build awareness in the community regarding  the importance of research measuring multi-platform activity and  long-term success after taking a MOOC.  We hope to build the  communitys understanding of what it takes to operationalize  MOOC learner success in a novel context by employing data traces  across the social web.    CCS Concepts   Applied computing   Computer  assisted instruction;  Interactive learning environments     Keywords  Learning analytics; massive online open courses; long-term learning  development; learning outcomes; career development   1. WORKSHOP BACKGROUND  1.1 Challenges in assessing MOOC Learner Success          MOOC learners access course content in an asynchronous and  unconstrained fashion [3, 12]. This has resulted in fragmented data  scattered outside of the course platform and beyond the course- offering window, causing challenges of collecting learner data. As a  result, much research in MOOCs focuses on learner achievement and  engagement during the course itself, leaving the area of post-course  student longitudinal development relatively untouched. This  lopsidedness in MOOC research reduces our understanding of the  role that MOOCs can play in 21st-century learning.           It is observed that the narrative on MOOCs has shifted from  overwhelmingly optimistic from 2011 to 2014 to substantially more  critical in 2015 [12]. One concern is that it is not clear how well  MOOCs support student learning and career development in  response to changing societal needs [6]. The development of   technology and scale of online education considerably outpace  efforts to evaluate and understand how well it is succeeding at  improving outcomes. The predominant focus of studying MOOC  completion without considering learners longitudinal development  overlooks the possible role that MOOCs may play in the long-term  professional development of many of their users.         Responding to the this challenge, an emerging trend has been  seen on identifying and tracking MOOC learner development outside  of the course platform and beyond the conclusion of the course [1,  13], such as tracking learner activities on relevant social web  platforms, measuring learner post-course development on career  advancement, and measuring leaner participation in communities of  practice related to the MOOCs they engage with.     1.2 Post- MOOC Development   1.2.1 MOOCs and Continued Learning Activities         Currently, the most popular approaches adopted by researchers  to evaluate a MOOC's success is to measure its learners course  engagement and performance. Although this can provide us with  some insights, some researchers argue that students' post-course  learning activities should also be tracked and analyzed to gain a  better understanding of learner behavior.        One key aspect in these post-course learning activities is to what  extent learners apply the knowledge they acquired from a MOOC in  practice (i.e., learning transfer). One example of such research comes  from Chen et al. [1] which targeted the learners from a MOOC  teaching functional programming in edX and analyzed the data they  generated in both edX and GitHub, a social repository for  programmers to store and share code. They found a small percentage  of learners began using the taught language who had never used it  before and continued to do so after the course ended.         In addition to measuring learning transfer, another interesting  aspect to investigate is whether learners continue to study the MOOC  subject or related topics after the end of the course. Just as Chen et  al. [1] used GitHub to measure learning transfer, Chen et al. [2]  analyzed learners' data left in StackOveflow, a social platform for  programmers to ask and answer questions, and observed that some  learners continued asking relevant questions after the course  finished. Moreover, some of them even changed their role from   learner  to  teacher  by answering these questions. As learners'  continued learning activities are greatly diverse, we here identify a  need for new approaches to identify and measure such behavior.    1.2.2 MOOCs and Career Development       Instead of pursuing a degree in a traditional school setting, a  considerable percentage of MOOC learners are seeking career  advancement through taking MOOCs [5]. Some MOOC learners opt  to display their MOOC certificates on their Linkedin profiles [2].   Permission to make digital or hard copies of part or all of this work for personal or  classroom use is granted without fee provided that copies are not made or  distributed for profit or commercial advantage and that copies bear this notice and  the full citation on the first page. Copyrights for third-party components of this  work must be honored. For all other uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029430          In addition to showing interests in accumulating competency,  many MOOC learners have also expressed explicit goals of changing  their careers. For instance, recent studies on MOOC learner  motivation suggest that a substantial proportion of learners register  for a MOOC with an intention of changing their job [9]. Proactively  seeking to change ones current job is correlated with higher  perceived employability by employers [7].         More recently, Dillahunt and colleagues [5] conducted a survey  and found out that enhancing employability was a key reason why  many learners enroll in MOOCs. They categorized desired career  advancement for MOOC learners into four types: transition into a  new field; promotion in their current fields; obtaining new position  in current fields; and improving current job skills. However, while  learners have the goal of increasing their employability and many  employers are interested in prospective employees MOOC  experience [9], there is not yet much evidence of post-course career  improvement as a result of taking a MOOC.       1.2.3 MOOCs and Communities of Practice        In addition to post-course development analyzed on the  individual level, development of a community of practice  complements the development of individual learners. This can be  particularly relevant for a MOOC in an emerging field since the field  needs to attract new members in order for it to grow and develop.  Communities of practice refer to groups of people who share a  common interest and learn how to enrich a shared repertoire of their  practice via joint activities [10]. Learning transcends personal  knowledge acquisition and becomes a dynamic, interconnected bank  of knowledge shared among community members [11]. Becoming a  member of a community of practice is also considered beneficial to  career advancement on the individual level [4], for example, by  moving from being a novice to a full practitioner in the community.  The emergence of virtual media further extended the realm of  communities of practice [8].          MOOCs, as a virtual learning venue, often connect to a  community of practice around the domain of the MOOCs content.  For example, a MOOC teaching methods in the emerging field of  educational data mining (EDM) is likely to attract learners who are  interested in joining the community of practice composed by the  field of EDM. MOOC learners can be perceived as aspiring  practitioners in this community of practice. With their lack of  entrance restrictions, MOOCs can provide a new space to facilitate  legitimate peripheral participation. For an emerging field, fostering  development of the community of practice can be crucial, and there  is initial evidence that some MOOC participants join relevant  scientific societies during or immediately after participating in a  MOOC [13].  This development does not stop at the moment when  the course concludes. Therefore, studying the development of  MOOC participants as members of the community of practice after  the MOOC concludes can be a valuable lens for understanding the  impact of the MOOC.    2. WORKSHOP OBJECTIVES        This workshop aims to generate awareness in the community  that considering learner data beyond the course platform and course- offering window can uncover previously unconsidered learning  behaviors.        As such, in adopting and developing this integrated learning  analytics approach, the overall goal of the present workshop is to  bring together researchers and practitioners from different  disciplines and areas of expertise to herald a new MOOC research  branch on tracking and analyzing longitudinal MOOC learner  development.         Presentations on finished, ongoing, and proposed studies, as  well as facilitated discussion sessions are planned to develop a  preliminary framework to illustrate the current development of the  field and to inspire attendees to come up with exciting new lines of   research of their own. Toward building this framework, the  following guiding questions are included:    Data source: Where and how can learner post- development data be collected    Tools: What analytical tools are useful in analyzing  datasets merged from multiple sources    Methods: What analytical methods have been used What  other methods can be applied    Generalizability: What kind of practices and findings are  domain-general or not    Applicability: How can research findings translate into  actionable insights for various stakeholders (learners,  instructors, administrators, investors, etc.)   3. REFERENCES   [1]   Chen, G., Davis, D., Hauff, C., & Houben, G. J. 2016.   Learning Transfer: Does It Take Place in MOOCs An  Investigation into the Uptake of Functional Programming in  Practice. In Proceedings of the Third (2016) ACM Conference  on Learning@ Scale (pp. 409-418).    [2]   Chen, G., Davis, D., Lin, J., Hauff, C., & Houben, G. J. 2016.  Beyond the MOOC platform: gaining insights about learners  from the social web. In Proceedings of the 8th ACM  Conference on Web Science (pp. 15-24). ACM.   [3]   DeBoer, J., Stump, G. S., Seaton, D., Ho, A., Pritchard, D. E., &  Breslow, L. 2013, July. Bringing student backgrounds online:  MOOC user demographics, site usage, and online learning. In  Educational Data Mining 2013.   [4]  DeFillippi, R. J., & Arthur, M. B. 1994. The boundaryless  career: A competencybased perspective. Journal of  organizational behavior, 15(4), 307-324.   [5]  Dillahunt, T. R., Ng, S., Fiesta, M., & Wang, Z. 2016. Do  Massive Open Online Course Platforms Support Employability  In Proceedings of the 19th ACM Conference on Computer  Supported Cooperative Work & Social Computing. ACM.   [6]  Ferguson, R., Sharples, M., &  Beale, R. 2015. MOOCs 2030: a  future for massive open online learning. In: C. J. Bonk, M. M.  Lee, T.C. Reeves & T. H. Reynolds (Eds.), MOOCs and Open  Education around the World (pp. 315326). Abingdon:  Routledge    [7] Fugate, M., Kinicki, A. J., & Ashforth, B. E. 2004.  Employability: A psycho-social construct, its dimensions, and  applications. Journal of Vocational behavior, 65(1), 14-38.   [8]   Johnson, C. M. 2001. A survey of current research on online  communities of practice. The internet and higher  education, 4(1), 45-60.    [9] Kizilcec, R. F., & Schneider, E. 2015. Motivation as a lens to  understand online learners: Toward data-driven design with the  OLEI scale. ACM Transactions on Computer-Human  Interaction (TOCHI), 22(2), 6.   [10] Lave, J. 1991. Situating learning in communities of   practice. Perspectives on socially shared cognition, 2, 63-82.    [11] Lave, J., & Wenger, E. 2002. Legitimate peripheral  participation in communities of practice. Supporting lifelong  learning, 1, 111-126.    [12] Siemens, G. 2015. The role of MOOCs in the future of  education. MOOCs and Open Education Around the World. C.  J. Bonk, M.M. Lee, T. C. Reeves, & T, H, Reynolds. (Eds.).  New York, NY: Routledge.    [13] Wang, Y., Paquette, L., Baker, R. 2014. A longitudinal study  on learner career advancement in MOOCs. Journal of Learning  Analytics, 1 (3), 203206.     "}
{"index":{"_id":"69"}}
{"datatype":"inproceedings","key":"Macfadyen:2017:DIL:3027385.3029426","author":"Macfadyen, Leah P. and Groth, Dennis and Rehrey, George and Shepard, Linda and Greer, Jim and Ward, Douglas and Bennett, Caroline and Kaupp, Jake and Molinaro, Marco and Steinwachs, Matt","title":"Developing Institutional Learning Analytics 'Communities of Transformation' to Support Student Success","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"498--499","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029426","doi":"10.1145/3027385.3029426","acmid":"3029426","publisher":"ACM","address":"New York, NY, USA","keywords":"change management, communities of transformation, faculty engagement, institutional learning analytics, learning analytics fellows program, student success","Abstract":"Notions of what constitutes quality in design in traditional on-campus or online teaching and learning may not always translate into scaled digital environments. The DesignLAK17 workshop builds on the DesignLAK16 workshop to explore one aspect of this theme, namely the opportunities arising from the use of analytics in scaled assessment design. New paradigms for learning design are exploiting the distinctive characteristics and potentials of analytics, trace data and newer kinds of sensory data usable on digital platforms to transform assessment. But, characteristics of quality assessment design need to be reconsidered, and new metrics for capturing quality are required. This symposium and workshop focuses on what might be appropriate quality metrics and indicators for assessment design in scaled learning. It aims to build a community of interest round the topic, to share perspectives, and to generate design and research ideas.","pdf":"Developing Institutional Learning Analytics Communities  of Transformation to Support Student Success   Leah P. Macfadyen  The University of British Columbia,   1866 Main Mall,  Vancouver, BC, V6T 1Z1, Canada   +1 604 809 5013,  leah.macfadyen@ubc.ca    Dennis Groth, George Rehrey   & Linda Shepard  Indiana University   Maxwell Hall, 750 E. Kirkwood Ave    Bloomington, IN 47405, USA   +1 812 855 8783,  dgroth@indiana.edu   Jim Greer  University of Saskatchewan   105 Administration Place   Saskatoon, SK, S7N 5A2 Canada   jim.greer@usask.ca    Douglas Ward &   Caroline Bennett  University of Kansas   135 Budig Hall, 1455 Jayhawk  Blvd, Lawrence KS 66045, USA   +1 785 864 7637,  dbward@ku.edu   Jake Kaupp  Queens University    Fac. of Eng. & Applied Science  Kingston, ON, K7L 3N6, Canada   jake.kaupp@queensu.ca     Marco Molinaro &   Matt Steinwachs   University of California, Davis  1335 Grove, Davis, CA 95616,   USA  +1  (916) 860-2108,   mmolinaro@ucdavis.edu    ABSTRACT  Institutional implementation of learning analytics calls for  thoughtful management of cultural change. This interactive half- day workshop responds to the LA literature describing the benefits  and challenges of institutional LA implementation by offering  participants an opportunity to learn about and begin planning for a  program to actively engage faculty as leaders of data exploration  around the theme of student success. This session will share  experiences from five institutions actively engaged in fostering  Learning Analytics Communities (LAC) by identifying key issues,  sharing lessons learned, and considering structural frameworks that  are transferable to other institutional contexts. Structured  discussion and activities will engage participants in developing an  action plan for establishing an LAC on their own campus.   CCS Concepts   Collaborative and social computing Empirical studies in  collaborative and social computing. Applied  computing Education  Keywords  Institutional learning analytics; change management; faculty  engagement; communities of transformation; student success;  learning analytics fellows program.   1. WORKSHOP BACKGROUND  Institutional implementation of learning analytics calls for  thoughtful management of cultural change. In addition to technical  and infrastructure planning, researchers and institutions engaged in   LA planning have identified that effective implementation of  institutional learning analytics requires strategic efforts to engage  the community in conversations about the potential for learning  analytics to improve student success, and to generate excitement  and engagement by key stakeholders (students, faculty, instructors,  senior administrators) [1,2]. As change management specialists  [3,4,5] have pointed out, change in habits, practices and behaviours  is not brought about by simply giving people large volumes of  logical data [6]. Overcoming resistance to innovation and change  calls for planning processes which create conditions that allow  participants to both think and feel positively about change   conditions that appeal to both the heart and the head.    Research suggests that communities of practice have positive  effects on teaching development, and that faculty engagement in  such communities leads to positive changes in teaching behaviors.  [7,8,9]. Moreover, faculty who engage in systematic inquiry on  student learning in their own classrooms and programs  (Scholarship of Teaching and Learning or SoTL) report a wide  range of benefits, including documented improvements in the  quality of their students learning, more of their students achieving  higher standards, and an increased interest in positively influencing  teaching in their department beyond their own practice [10,11]   In order to increase the use of big data for informed decision- making in higher education, we must expand its use to the  departmental and course levels, which calls for faculty ownership  of research questions that examine and define student success.  LACs offer the potential to bring faculty of all ranks into the  learning analytics conversation early, and allow them to lead data  exploration projects and share their findings with colleagues and  peers. Such programs can be conceptualized as fostering  institutional Communities of Transformation [12] that can  address both individual faculty and broader systemic change, and  create innovative spaces that have the potential to shift institutional  and disciplinary norms. This workshop will describe and share our  experience with establishment of institutional LACs that can foster  enthusiasm about the affordances of learning analytics and their  potential to support student success and improved teaching and  learning.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029426     Indiana University Bloomington launched its Student Learning  Analytics Fellows Program1 in 2015 [13], and is now leading a  multi-year project to expand the concept in partnership with Bay  View Alliance (BVA)2 partner universities: The University of  British Columbia, the University of Kansas, Queens University and  the University of Saskatchewan. This workshop will be led by and  will draw on the experiences of LAC leaders in these institutions.   2. WORKSHOP OBJECTIVES  This half-day workshop will help participants create a plan for  building a campus community devoted to using learning analytics  to improve teaching and learning. It will explore the challenges and  successes from similar programs at universities in the BVA, lead  participants through exercises in setting goals, creating meaningful  questions, and encourage involvement from faculty and  departments for data-driven decision making at the course, program  and curricular levels.    The workshop will start with a brief self-assessment on the needs  of individual campuses, and the challenges participants are likely  to encounter in using learning analytics to improve teaching and  learning. Workshop leaders will then draw on their institutional  experiences to help participants address a series of topics and  questions:   Program goals: How can institutions create communities of  transformation around learning analytics How can we expand  faculty inquiry about student success from the course level to the  curricular level How are we facilitating the use of LA by faculty,  departments, and campus leaders   Establishment of community: How might LACs differ from other  types of Faculty Learning Communities (FLCs) How can we best  facilitate interdisciplinary collaborations throughout the inquiry  process   Types of research questions explored: What types of questions  work best in tying learning analytics to teaching and learning  Which have driven research in existing LACs How have these  questions been answered and how have they informed our  understanding of student success    Data collection and management: What types of data are most  valuable in addressing questions about student success How have  these data been collected and managed to increase efficiency of  delivery to faculty investigators What other data might be valuable  (e.g., student surveys)   Supporting faculty LA research: Which methods best support  faculty-led LA research (especially those not familiar with data- rich social science research methods)    Throughout this workshop, participants will be encouraged to use  an action-plan worksheet that assists with analysis of their own  institutional contexts. In the engagement phase, participants will  brainstorm and work with fellow participants to identify local goals  and stakeholders. By the end of the workshop participants will have  created an action plan for development of an LAC on their own  campuses.    Overall, participants will: Evaluate learning analytics readiness on  their campus; Learn how other institutions have used data-driven  decision making to improve student success; Identify key issues  relevant to establishing faculty ownership of learning analytics                                                                        1http://citl.indiana.edu/programs/grants/learninganalytics-CFP.php    projects, personalizing them for their institutional contexts; Identify  potential stakeholders involved in learning analytics efforts at their  institutions; and develop an action plan for creating a learning  analytics community on their campus.   3. ACKNOWLEDGMENTS  We would like to acknowledge the BVA network and especially  Mary Deane Sorcinelli (U. Mass Amherst and BVA Hub member)  for their instrumental role in facilitating and supporting our  collaborative efforts thus far.   4. REFERENCES  [1] Macfadyen, L. P. and Dawson, S. 2012. Numbers are not  enough. Why e-learning analytics failed to inform an institutional  strategic plan. Educational Technology & Society, 15, 3, 149163.  [2] Macfadyen, L. P., Dawson, S., Pardo, A., and Gaevi, D.  2014. Embracing big data in complex educational systems: The  learning analytics imperative and the policy challenge. Research  & Practice in Assessment, 9, 17-28.    [3] Kotler, P. and Zaltman, G. 1971. Social marketing: An  approach to planned social change. Journal of Marketing, 35, 3- 12.    [4] Kavanagh, M. H., and Ashkanasy, N. M. 2006. The impact  of leadership and change management strategy on organizational  culture and individual acceptance of change during a merger.  British Journal of Management, 17, S81-S103.   [5] Kotter, J. P. 1996. Leading Change. Harvard Business  School Press, Boston, MA.    [6] Kotter, J. P., & Cohen, D. S. 2002. The Heart of Change.  Harvard Business School Press, Boston, MA.   [7]  Cox, M.D. 2001. Faculty learning communities: Change  agents for transforming institutions into learning organizations. To  Improve the Academy, 19, 69-93.   [8] Sorcinelli, M. D., Austin, A. E., Eddy, P., and Beach, A.  2006. Creating the future of faculty development: Learning from  the past, understanding the present. Wiley/Jossey-Bass, San  Francisco, CA.   [9]  Stes, A., Min-Leliveld, M., Gijbels, D. and Van Pategewm,  P. 2010.  The impact of instructional development in higher  education: The state-of-the-art of the research.  Educational  Research Review, 5, 25-49.     [10] Chasteen, S.V., Wilcox, B., Caballero, M.D., Perkins, K.K.,  Pollock, S.J., and Wieman, C. J. 2015.  Educational  transformation in upper-division physics: The Science Education  Initiative model, outcomes, and lessons learned. Physical Review  Special Topics Physics Education Research, 11, 2, 020110.    [11]  Huber, M. T., and Hutchings, P. 2005. The Advancement of  Learning: Building the Teaching Commons. Jossey-Bass, Inc.,  San Francisco, CA.    [12] Kezar, A. & Gehrke, S. 2015. Communities of  Transformation and Their Work Scaling STEM Reform. Pullias  Center for Higher Education.  [13] Siering, G. and Shepard, L. 2017. Promoting data-driven  decision-making through a learning analytics fellows program.  ELI Annual Meeting, Houston, TX.  2 http://bayviewalliance.org/      "}
{"index":{"_id":"70"}}
{"datatype":"inproceedings","key":"Martinez-Maldonado:2017:NCL:3027385.3029432","author":"Martinez-Maldonado, Roberto and Hernandez-Leo, Davinia and Pardo, Abelardo and Ogata, Hiroaki","title":"2Nd cross-LAK: Learning Analytics Across Physical and Digital Spaces","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"510--511","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029432","doi":"10.1145/3027385.3029432","acmid":"3029432","publisher":"ACM","address":"New York, NY, USA","keywords":"integration, learning analytics, monitoring, seamless learning","Abstract":"Student's learning happens where the learner is, rather than being constrained to a single physical or digital environment. It is of high relevance for the LAK community to provide analytics support in blended learning scenarios where students can interact at diverse learning spaces and with a variety of educational tools. This workshop aims to gather the sub-community of LAK researchers, learning scientists and researchers in other areas, interested in the intersection between ubiquitous, mobile and/or classroom learning analytics. The underlying concern is how to integrate and coordinate learning analytics seeking to understand the particular pedagogical needs and context constraints to provide learning analytics support across digital and physical spaces. The goals of the workshop are to consolidate the Cross-LAK sub-community and provide a forum for idea generation that can build up further collaborations. The workshop will also serve to disseminate current work in the area by both producing proceedings of research papers and working towards a journal special issue.","pdf":"2nd Cross-LAK: Learning Analytics Across                 Physical and Digital Spaces   Roberto Martinez-Maldonado1, Davinia Hernandez-Leo2, Abelardo Pardo3 and Hiroaki Ogata4  1University of Technology Sydney, NSW Australia; 2Universitat Pompeu Fabra, Barcelona, Spain;    3The University of Sydney, NSW Australia; 4Kyushu University, Japan  Roberto.Martinez-Maldonado@uts.edu.au, Davinia.Hernandez@upf.edu, Abelardo.Pardo@sydney.edu.au, Hiroaki.Ogata@gmail.com      ABSTRACT  Students learning happens where the learner is, rather than  being constrained to a single physical or digital environment. It  is of high relevance for the LAK community to provide analytics  support in blended learning scenarios where students can  interact at diverse learning spaces and with a variety of  educational tools. This workshop aims to gather the sub- community of LAK researchers, learning scientists and  researchers in other areas, interested in the intersection between  ubiquitous, mobile and/or classroom learning analytics. The  underlying concern is how to integrate and coordinate learning  analytics seeking to understand the particular pedagogical needs  and context constraints to provide learning analytics support  across digital and physical spaces. The goals of the workshop  are to consolidate the Cross-LAK sub-community and provide a  forum for idea generation that can build up further  collaborations. The workshop will also serve to disseminate  current work in the area by both producing proceedings of  research papers and working towards a journal special issue.   CCS Concepts    Information systems  Information systems applications   Collaborative and social computing systems and tools    Keywords  Learning analytics, seamless learning, integration, monitoring   1. INTRODUCTION AND MOTIVATION  Students learning happens where the learner is [4] rather than  being constrained to a single physical or digital environment [7,  14]. Learning often occurs in spaces and at moments that go  beyond formal education. Increasing access to emerging  communication technologies and the proliferation of mobile and  pervasive devices have made it possible for students to make use  of a wide range of educational (and non-educational) tools [9].  At the same time, educational providers, including schools and  universities, are continuously deploying a variety of educational  technologies and pedagogical resources in both online and face- to-face settings [13]. Educational research has revealed the  pedagogical benefits of letting students experience different  types of content,  real world  challenges, and physical and  social interactions with educators or other learners [2, 10].  Moreover, students commonly work outside the boundaries of  the institutional learning system(s). They may interact face-to- face, use other educational tools or even use tools that were not   specifically designed to serve in learning contexts. Teachers may  want students to use not only the tools offered by the institution,  but also other tools that are more suitable to the context and  subject matter [12].    Pervasive and mobile technologies can be used to allow learners  to get remote access to educational resources from different  physical spaces (e.g. ubiquitous/mobile learning support [15,  17]) or to enrich their learning experiences in the classroom in  ways that were not previously possible (e.g. face-to-face [3, 5,  10]/blended learning support [16]). In parallel, these  technologies are becoming or getting embedded into everyday  objects that can communicate information and generate large  amounts of interaction data. This is creating new possibilities for  learning analytics to provide continued support or a more  holistic view about learning, moving beyond desktop-based  learning resources [1, 6]. Providing continued support in the  classroom, for mobile experiences and using web-based systems  has been explored to different extents and each poses its own  challenges [11, 12]. An overarching concern is how to integrate  analytics across these different spaces and tools in a coordinated  way. In short, there is an increasing interest in providing support  for students learning across physical and digital spaces, and the  means to achieve this are more readily available.    2. WORKSHOP THEMES  We will invite contributions to the Workshop on Learning  Analytics Across Physical and Digital Spaces Research.  Contributions should relate to the design and study of learning  analytics innovations and solutions, including but not limited to  any of the following themes:   1. Support Across Multiple Digital Spaces: Studies of novel  combinations of analytics and instructional approaches and  systems that span across multiple digital learning tools  (including mining, modelling or visualising datasets that  integrate logs from multiple learning tools);   2. Bridging the Physical and Digital Realms: Design and  study of learning situations that include collocated settings  and/or the use of online (remote access) tools  (e.g. including  everyday settings, collocated collaboration situations, multi- device ecologies or blended learning cases);   3. Data Integration of Heterogeneous Learning Data Sources:  Discussion of methodologies and theoretical approaches, and  their technical solutions, to integrate learning activity logs from  multiple sources of learners data (including technical but also  non-technical issues such as ethics, orchestration or data  management) with learning designs and strategies.   3. EVIDENCE OF INTEREST & PC  We expect to conduct a full day workshop with at least 20  participants from the sub-community of LAK researchers  interested in ubiquitous, mobile and/or face-to-face learning  analytics, and learning scientists and researchers from other   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029432     communities who have explored the perspective of learning  across digital and physical spaces. We will encourage  submission of original papers that demonstrate ways to integrate  and coordinate learning analytics to provide continued support  to learning across digital and/or physical spaces. A list of people  who will be invited to serve on the program committee for the  workshop can be found here.    Similar workshops organized by Davinia Hernandez-Leo  in 2011, Roberto Martinez-Maldonado in 2012 and the first  edition of Cross-LAK in LAK 2016 are indicative of sustained  interest of the research community in this sub-field. The first  Cross-LAK workshop was very successful in terms of papers  submitted and participants.    These workshops are the following:    1) International Workshop on Learning Analytics Across  Physical and Digital Spaces (Cross-LAK 16 held in  conjunction with LAK 2016). Website.   2) International Workshop on Digital Ecosystems for  Collaborative Learning (DECL 2012 held in conjunction with  ICLS 2012. Website; and    3) International Workshop on Learning Activities Across  Physical and Virtual Spaces (AcrossSpaces 2011 held in  conjunction with EC-TEL 2011). Website.    4. EXPECTED OUTCOMES  The expected outcomes of the workshop are the following:     Consolidating the Cross-LAK Community. This workshop will  build on the design space and guidelines formulated in the first  edition of Cross-LAK [8] in order to consolidate the synergy  between researchers and propose further steps as a community.    Provide a forum to ignite collaboration. The workshop will  bring together the sub-communities within the learning sciences,  educational technology, and LAK with the goal of contributing  with their expertise in identifying the major issues to be tackled  in the area, generating new ideas for future research and  sparking on each other in ways that can lead to future  collaboration within the LAK community.   Work towards a special issue on Cross-LAK themes.  Proceedings of research papers which will be produced and  selected papers will be invited to be submitted in full to a special  issue (SI) on Cross-LAK in an indexed journal.   5. CONCLUSION  While this workshop can be considered to be grounded on a  consolidated line interest on the topic of learning across spaces,  in this case the focus is on the particular challenges to provide  continued support to students by using learning analytics  techniques.    6. REFERENCES   [1]. Aljohani, N. R. and Davis, H. C. 2012. Learning analytics in   mobile and ubiquitous learning environments. In Proc. of World  Conference on Mobile and Contextual Learning: mLearn 2012  (Helsinki, Finland, Oct 16-18). 70-77.     [2]. Delgado Kloos, C., Hernndez-Leo, D., and Asensio-Prez, J.I.   2012. Technology for Learning across Physical and Virtual  Spaces. Journal of Universal Computer Science, 18, 15, 2093- 2096.   [3]. Di Mitri, D., Scheffel, M., Drachsler, H., Brner, D., & Ternier,  S. 2016. Learning Pulse: Using Wearable Biosensors and   Learning Analytics to Investigate and Predict Learning Success in  Self-regulated Learning. In Proc. of Workshop Cross-LAK  held  at LAK 16, (Edinburgh, UK, Apr 25). 34-39.   [4]. Dourish, P. 2004. What we talk about when we talk about  context. Personal and Ubiquitous Computing, 8, 1, 19-30.   [5]. Domnguez, F., & Chiluiza, K. 2016. Towards a distributed  framework to analyze multimodal data. In Proc. of Workshop  Cross-LAK  held at LAK 16, (Edinburgh, UK, Apr 25). 52-57.   [6]. Kitto, K., Cross, S., Waters, W., and Lupton, M. 2015. Learning  analytics beyond the LMS: the connected learning analytics  toolkit. In Proc. LAK15 (NY, USA, March 16-20). NY: ACM,  11-15   [7]. Looi, C. K., Wong, L. H., & Milrad, M. 2015. Special Issue on  Seamless, Ubiquitous, and Contextual Learning. IEEE  Transactions on Learning Technologies, 1, 2-4.   [8]. Martinez-Maldonado, R., and Hernandez-Leo, D. 2016. Cross- LAK: Learning Analytics Across Physical and Digital Spaces  2016. CEUR Proceedings, 1601.    [9]. Muoz-Cristbal, J. A., Prieto, L. P., Asensio-Prez, J. I.,  Martnez-Mons, A., Jorrn-Abelln, I. M., and Dimitriadis, Y.  2014.  Deploying learning designs across physical and web  spaces: Making pervasive learning affordable for teachers.  Pervasive and Mobile Computing, 14, 31-46.   [10]. Muoz-Cristbal, J. A., Rodrguez-Triana, M. J., Gallego- Lema, V., Arribas-Cubero, H. F., Martnez-Mons, A., &  Asensio-Prez, J. I. 2016. Toward the integration of  monitoring in the orchestration of across-spaces learning  situations. In Proc. of Workshop Cross-LAK  held at LAK  16, (Edinburgh, UK, Apr 25). 15-21.   [11]. Prez-Sanagustn, M.; Ramrez-Gonzlez, G.; Hernndez- Leo, D.; Muoz-Organiero, M.; Santos, P.; Blat, J.; Delgado- Kloos, C. Discovering the campus together: a mobile and  computer-based learning experience, Journal of Network and  Computer Applications, 35, 1, 2012, 176-188.   [12]. Prieto, L. P., Muoz-Cristbal, J. A., Asensio-Prez, J. I., &  Dimitriadis, Y. 2012. Making Learning Designs Happen in  Distributed Learning Environments with GLUE!-PS. In A.  Ravenscroft, S. Lindstaedt, C. Kloos & D. Hernndez-Leo, Eds.,  21st Century Learning for 21st Century Skills, 7563.  Saarbrcken, Germany: Springer Berlin Heidelberg, 489-494.   [13]. Rogers, Y. 2008. Using External Visualizations to Extend and  Integrate Learning in Mobile and Classroom Settings. In J. K.  Gilbert, M. Reiner & M. Nakhleh, Eds., Visualization: Theory  and Practice in Science Education. Springer Netherlands:  Dordrecht, 89-102.   [14]. Sharples, M., and Roschelle, J. 2010. Special section on  mobile and ubiquitous technologies for learning. IEEE  Transactions on Learning Technologies, 1, 4-6.   [15]. Shimada, A., Okubo, F., Yin, C., & Ogata, H. 2016.  Automatic Generation of Personalized Review Materials  Based on Across-Learning-System Analysis. In Proc. of  Workshop Cross-LAK  held at LAK 16, (Edinburgh, UK,  Apr 25). 22-27.   [16]. Wang, M., Shen, R., Novak, D., & Pan., X. 2009. The impact  of mobile learning on students learning behaviours and  performance: Report from a large blended classroom. BJET, 40,  4, 673-695.   [17]. Weyers, B., Nowke, C., Kuhlen, T., Kousuke, M., & Ogata, H.  2016. Web-based Interactive and Visual Data Analysis for  Ubiquitous Learning Analytics. In Proc. of Workshop Cross-LAK   held at LAK 16, (Edinburgh, UK, Apr 25). 65-69.     "}
{"index":{"_id":"71"}}
{"datatype":"inproceedings","key":"Vigentini:2017:FDW:3027385.3029433","author":"Vigentini, Lorenzo and Le'on Urrutia, Manuel and Fields, Ben","title":"FutureLearn Data: What We Currently Have, What We Are Learning and How It is Demonstrating Learning in MOOCs","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"512--513","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029433","doi":"10.1145/3027385.3029433","acmid":"3029433","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, learning analytics, visualization dashboard","Abstract":"Compared to other platforms such as Coursera and EdX, FutureLearn is a relatively new player in the MOOC arena and received limited coverage in the Learning Analytics and Educational Data Mining research. Founded by a partnership between the Open University in the UK, the BBC, The British Library and (originally) 12 universities in the UK, FutureLearn has two distinctive features relevant to the way their data is displayed and analyzed: 1) it was designed with a specific educational philosophy in mind which focuses on the social dimension of learning and 2) every learning activity provide opportunities for formal discussion and commenting. This workshop provides an opportunity to invite contributions and connect individual and groups to share their research activities on an international stage. As the first of its kind, this workshop will bring in a number of scholars and practitioners, as well as data scientists and analyst involved in the reporting, researching and developments emerging from the data offered by the platform.","pdf":"FutureLearn data: what we currently have, what we are  learning and how it is demonstrating learning in MOOCs   Lorenzo Vigentini  UNSW Australia   l.vigentini@unsw.edu.au   Manuel Len Urrutia  University of Southampton UK  m.leon-urrutia@soton.ac.uk   Ben Fields  FutureLearn   ben.fields@futurelearn.com     ABSTRACT  Compared to other platforms such as Coursera and EdX,  FutureLearn is a relatively new player in the MOOC arena and  received limited coverage in the Learning Analytics and  Educational Data Mining research. Founded by a partnership  between the Open University in the UK, the BBC, The British  Library and (originally) 12 universities in the UK, FutureLearn  has two distinctive features relevant to the way their data is  displayed and analyzed: 1) it was designed with a specific  educational philosophy in mind which focuses on the social  dimension of learning and 2) every learning activity provide  opportunities for formal discussion and commenting. This  workshop provides an opportunity to invite contributions and  connect individual and groups to share their research activities on  an international stage. As the first of its kind, this workshop will  bring in a number of scholars and practitioners, as well as data  scientists and analyst involved in the reporting, researching and  developments emerging from the data offered by the platform.    CCS Concepts   Information systems  Information systems applications   Decision support systems  Data analytics  Human-centred  computing  Visualization  Visual analytics.   Keywords  MOOCs; visualization dashboard; learning analytics.   1. INTRODUCTION  Many higher education institutions have invested in the  development of MOOCs. Some have partnered with one or more  leading MOOC providers leveraging on the capabilities of  different platforms (i.e. Coursera, EdX, FutureLearn etc.) [9, 12],  others have been experimenting with a collection of open  resources and encouraged learners to participate in learning  experiences at scale, without the constraints of specific platforms  and promoting a connectivist experience of learning [1, 6, 10, 13,  16, 18]. With the experimentations in learning design, more  started to question the effectiveness of the forms of learning that  can be supported by the introduction, and given that a large  amount of data has become available, it is timely to explore how  to best make use of it.    In fact, with the increased availability of MOOC data, there is an  opportunity to provide insights to educators and developers into  learners behaviours, and empower learners to understand their  patterns of engagement and performance through learning   analytics [19]. The former allows exploring learning design at  scale and has the potential to inform pedagogy. The latter has the  potential to improve the learning experience and develop crucial  metacognitive skills essential for self-directed and lifelong  learners. In more recent times there has been a shift to move from  descriptive analytics to analytics able to inform and direct practice  [5, 21, 22]. This was also advocated by Gasevic and colleagues as  a key area of further research in their review of research in  MOOCs [8]. In fact, while a lot of work has been done, there are  two crucial problems hindering the application of learning  analytics methods to support and shape pedagogy in MOOCs: 1)  the constraints of the platforms (i.e. the tools and course design)  and 2) the availability of data when it is needed.   Looking at the wealth of research in MOOCs, a lot is done post- hoc when the respective platforms release the data for  exploration, and often data is locked within institutions limited by  their agreements with platform providers. Research has looked at  Coursera data [2, 15] and the dashboard offered to partners  institutions [7]. In addition, the relative openness of EdX, allowed  different teams to develop extensions/plugins to access and use  analytics [4, 14, 17, 20].   FutureLearn went down a different pathway, focusing on  standardization and simplicity, offering data files to partner  institutions to enable them to make sense of the interaction  occurring in the various courses. Additionally, a report (based on  R scripts) is offered to stakeholders, but this is limited in a  number of ways: it is static, it is focusing on selected information  and, most importantly, it does not provide real-time access to  data. The lack of a tool to visualise data from the engagement  with FL MOOCs sparked two separate initiatives to develop tools  bringing analytics to different stakeholders [3, 11].   2. SCOPE AND OPPORTUNITY   This workshop provides an opportunity to invite contributions as  short paper submissions, and share the work already done by a  number of partner institutions showcasing existing processes,  methods and tools used to analyse, present and use the data  offered by the FL platform.   The workshop will consist of two streams: a research/practitioner  track and a technical track. The two streams are intended to  present case studies demonstrating how practitioners use the data  to inform pedagogical design, what questions and findings  researchers uncover in the data (and what is still missing), and the  type and nature of technology stack explored to analyse and  present data.    Focusing on a hands-on approach, the workshop will provide  opportunities to both technical and less technical people to  leverage on what is offered and learn from others.    It is expected that participants will share not only their findings,  but also some of the code, so that the work started with this  workshop will continue beyond the conference. The workshop  will also be an opportunity to share issues, problems and to  identify what data is missing and what would be useful to have.    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author. Copyright is held by the  owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029433       3. WHO IS THIS WORKSHOP FOR  Those who wish to understand the possibilities offered by the data  already offered by FutureLearn, discuss and share innovations,  impact on education, and explore future directions in the  application of learning analytics (LA) to Massive Open Online  Courses (MOOC) design and development as well as learning  design with MOOCs. Likely interested participants:    Educators/Teachers and researchers   Technologists and educational developers   Learning scientists and Data scientists/analysts   Academic managers   Entrepreneurs   and anyone else interested in MOOCs (focusing on   FutureLearn in this workshop) and LA   4. OUTCOMES FOR ATTENDING  PARTICIPANTS  All contributions (short 5-pages papers) accepted for this  workshop will be included in the LAK CEUR Workshop  Proceedings (http://ceur-ws.org/).  Participants will be able to:    Get an idea of the state of the art of work with FutureLearn  data across institutions, disciplines and roles;    Discuss cases, issues and problems, sharing outcomes (both  successes and failures in using the data offered);    Reflect on the impact of the work presented on learning  design and the learners experiences;    Enable the development of common tools that educators and  researchers may be able to re-use in their own contexts;    Connect relevant people with one another, in the broad area  of data and LA applied to MOOCs and FutureLearn in  particular.    Explore opportunities of sharing results for cross-course  analysis and benchmarking   The inclusion of the FutureLearn data team demonstrates their  commitment to support partners, collaborate and co-develop  effective solutions to improve research opportunities, learning  design and ultimately the learners experience.   5. REFERENCES  [1] Arnold, P. et al. 2014. Offering cMOOCs collaboratively: The   COER13 experience from the convenors perspective.  eLeanrning Papers. 37, (2014), 6368.   [2] Chandrasekaran, M.K. et al. 2015. Learning Instructor  Intervention from MOOC Forums: Early Results and Issues.  arXiv:1504.07206 [cs]. (Apr. 2015).   [3] Chitsaz, M. et al. 2016. Toward the development of a dynamic  dashboard for FutureLearn MOOCs: insights and directions.  Proceeding of ASCILITE (Adelaide, SA, 2016).   [4] Cobos, R. et al. 2016. Open-DLAs: An Open Dashboard for  Learning Analytics. Proceedings of the Third (2016) ACM  Conference on Learning @ Scale (New York, NY, USA,  2016), 265268.   [5] Corrin, L. et al. 2015. Loop: A learning analytics tool to  provide teachers with useful data visualisations. Proceedings  of ASCILITE (Perth, Australia, 2015), 409413.  http://www.2015conference.ascilite.org/wp- content/uploads/2015/11/ascilite-2015-proceedings.pdf   [6] Dawson, S. et al. 2015. Recognising learner autonomy:  Lessons and reflections from a joint x/c MOOC.   Proceedings of Higher Education Research and  Development Society of Australia 2015.   [7] Do, C.B. et al. 2013. Self-Driven Mastery in Massive Open  Online Courses. MOOCs FORUM. 1, P (Sep. 2013), 1416.   [8] Gasevic, D. et al. 2014. Where is research on massive open  online courses headed A data analysis of the MOOC  Research Initiative. The International Review of Research in  Open and Distributed Learning. 15, 5, 134-176 (Oct. 2014).   [9] Jordan, K. 2014. Initial trends in enrolment and completion of  massive open online courses. The International Review of  Research in Open and Distributed Learning. 15, 1, 134-60.   [10] Kanwar, A. and Mishra, S. 2015. The impact of OER and  MOOCs on ODL: an international perspective. International  Distance Education Development Forum, Peking University,  Beijing, China, 10 October 2015.  http://oasis.col.org/handle/11599/1734   [11] Leon Urrutia, M. et al. 2016. Visualising the MOOC  experience: a dynamic MOOC dashboard built through  institutional collaboration. Proceedings of the European  MOOC Stakeholder Summit 2016. 461-471.   [12] McAuley, A. et al. 2010. The MOOC model for digital  practice. (2010). http://www.davecormier.com/edblog/wp- content/uploads/MOOC_Final.pdf   [13] McIntyre, S. et al. 2015. Learning to Teach OnlineEvolving  approaches to professional development for global reach and  impact. e-Learning Excellence Awards 2015: An Anthology  of Case Histories. 128140.   [14] Pardos, Z.A. and Kao, K. 2015. moocRP: An Open-source  Analytics Platform. Proceedings of the Second (2015) ACM  Conference on Learning @ Scale (New York), 103110.   [15] Parod, B. 2014. Developing an Analytics Dashboard for  Coursera MOOC Discussion Forums.  https://www.cni.org/wp- content/uploads/2014/12/Mon_Parod_DevMOOCDashboard .pdf   [16] Ros, C.P. et al. 2015. Challenges and opportunities of dual- layer MOOCs: Reflections from an edX deployment study.  Proceedings of the 11th International Conference on  Computer Supported Collaborative Learning (CSCL 2015).  848-851.   [17] Ruiz, J.S. et al. 2014. Towards the Development of a  Learning Analytics Extension in Open edX. Proceedings of  the Second International Conference on Technological  Ecosystems for Enhancing Multiculturality (New York, NY,  USA), 299306.   [18] dos Santos, A.I. et al. 2016. Opportunities and challenges for  the future of MOOCs and open education in Europe.   [19] Siemens, G. et al. 2011. Open Learning Analytics: an  integrated & modularized platform. Proposal to design,  implement and evaluate an open platform to integrate  heterogeneous learning analytics techniques..   [20] Veeramachaneni, K. et al. 2014. MOOCdb: Developing  Standards and Systems to Support MOOC Data Science.  arXiv:1406.2015 [cs]. (Jun. 2014).   [21] Wise, A.F. 2014. Designing Pedagogical Interventions to  Support Student Use of Learning Analytics. Proceedings of  the Fourth International Conference on Learning Analytics  And Knowledge (New York, NY, USA, 2014), 203211.   [22] Wise, A.F. et al. 2016. Developing Learning Analytics  Design Knowledge in the Middle Space: The Student  Tuning Model and Align Design Framework for Learning  Analytics Use. Online Learning. 20, 2.  http://olj.onlinelearningconsortium.org/index.php/olj/article/ view/783 .     "}
{"index":{"_id":"72"}}
{"datatype":"inproceedings","key":"Bowe:2017:LAP:3027385.3029436","author":"Bowe, Megan and Chen, Weiqin and Griffiths, Dai and Hoel, Tore and Lee, Jaeho and Ogata, Hiroaki and Richards, Griff and Yuan, Li and Zhang, Jingjing","title":"Learning Analytics and Policy (LAP): International Aspirations, Achievements and Constraints","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"516--517","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029436","doi":"10.1145/3027385.3029436","acmid":"3029436","publisher":"ACM","address":"New York, NY, USA","keywords":"data protection, learning analytics, open data, policy, privacy","Abstract":"The Learning Analytics and Policy (LAP) workshop explores and documents the ways in which policies at national and regional level are shaping the development of learning analytics. It brings together representatives from around the world who report on the circumstances in their own country. The workshop is preceded by an information gathering phase, and followed by the authoring of a report. The aspirations, achievements and constraints in the different countries are contrasted and documented, providing a valuable resource for the future development of learning analytics.","pdf":"Learning Analytics and Policy (LAP)  international  aspirations, achievements and constraints     Megan Bowe  DISC   239 Hampden Av. Narberth, PA 19072   megan@datainteroperability.org     Tore Hoel   Oslo and Akershus University  PO box 4, NO-0130 Oslo, Norway   Tore.Hoel@hioa.no      Griff Richards  Athabasca University   Athabasca, AB T9S 3A3, Canada   griff@sfu.ca   Weiqin Chen   University of Bergen,   PO box 7807, BERGEN, Norway   Weiqin.Chen@uib.no     Jaeho Lee  University of Seoul   163 Seoulsiripdae-ro, Seoul. S. Korea   jaeho@uos.ac.kr     Li Yuan  Cetis LLP,    Lancaster LA2 6ND, UK   Li@cetis.ac.uk    Dai Griffiths  University of Bolton  Bolton BL3 5AB, UK   D.E.Griffiths@bolton.ac.uk     Hiroaki Ogata, Japan  Kyushu University  Fukuoka, Japan   hiroaki.ogata@gmail.com     Jingjing Zhang  Beijing Normal University   19 Xin-Jie-Kou Wai, Beijing, China   jingjing.zhang@bnu.edu.cn   ABSTRACT  The Learning Analytics and Policy (LAP) workshop explores and   documents the ways in which policies at national and regional level   are shaping the development of learning analytics. It brings   together representatives from around the world who report on the   circumstances in their own country. The workshop is preceded by   an information gathering phase, and followed by the authoring of a   report. The aspirations, achievements and constraints in the   different countries are contrasted and documented, providing a   valuable resource for the future development of learning analytics.   CCS Concepts   Social and professional topics~Privacy policies    Social and   professional topics~Government technology policy   Keywords  Learning analytics, policy, privacy, data protection, open data.   1. BACKGROUND TO THE WORKSHOP  Institutional readiness for analytics, has been studied by, for   example the DELTA model from Davenport et al. [1], the work of   Jisc, including Sclater et al. [2], Educause, including Oster et al.   [3], and the OLT in Australia [4]. Such work provides valuable   insight into the scaling up learning analytics. However, the   capability of institutions to implement and leverage learning   analytics systems is determined not only by their internal   conditions and dynamics, but also by the policies and degree of   development of the education system and policy environment of the   state in which they find themselves. Moreover, the state does not   only create constraints on the institution, but also actively   intervenes in establishing the terms of reference and internal   dynamics of the institution. The importance of these factors was   recognized by Pea and Jacks, who proposed as one of their   milestones for measuring progress in building the field of analytics   Changes in policy related to data privacy and data sharing for   education, corporations [5, p.64]. To address these policy issues,   we must delve into the socio-technical sphere, as recommended   by Macfadyen and Dawson [6, p.161].   Several US federal government laws determine the way that   learning analytics can develop [7, p.20], and State policies play a   significant role in the move toward learning analytics [7, p.23].   The LACE project (www.laceproject.eu) has shown that while   there is consensus on the importance of policy, there is no   agreement on what the ideal policies are, particularly with regard   to fair and ethical use of data [8, p.12], [9, p.21-23]. The LAP   workshop builds on LACE in surveying the national and regional   policies which impinge on learning analytics in some key countries   where learning analytics has become established, and undertakes a   comparative analysis of the effects of these varying environments   on the development of applications of learning analytics.    This topic is of relevant to the meta-issues raised by LAK17,   including Ethics and Law, Adoption and Scalability. Developments   in these areas cannot be understood without knowledge of the   various policies that enable and constrain progress. The Workshop   contributes to the research field by documenting relevant policies   across the globe, and theorizing their consequences. This provides   input for research which seeks to understand the successes and   failures of LA, and provide valuable input for countries which are   currently starting to implement LA.   2. OBJECTIVES OF THE WORKSHOP  The objective of the workshop is to explore and document the ways   in which policies at national and regional level are shaping the   development of learning analytics. The scope includes policies   which are explicitly focused on learning analytics, policies which   regulate the use of data, and policies for the use of data in education.   The enquiry is focused on three aspects:   Permission to make digital or hard copies of part or all of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for third-  party components of this work must be honored. For all other uses, contact   the Owner/Author. Copyright is held by the owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029436           a) Aspirations. What do the policy creating bodies in each country   seek to achieve through the use of learning analytics This includes   related policies, such as data protection and privacy, which may not   be labelled with the term learning analytics.   b) Achievements. To what extent can it be claimed that the policy   environment has led to increased or more effective use or   preparation to use of learning analytics   c) Constraints. What are the factors which are preventing the   fulfillment of policies for learning analytics   3. OUTCOMES   3.1 Knowledge Sharing  The policy environment for learning analytics is an immediate   reality for researchers and practitioners in an individual country,   but they are often unaware of the conditions in other countries.   Sharing this knowledge in LAK improves participants ability to   engage in the policy debates in their own country. Moreover, the   examples of achievements and constraints provides a detailed view   of responses to policy for the wider community, and material for   reflection on relationship between policy and practice in   participants own operating environment.    3.2 Synthesis Report  Following the workshop, a report will be prepared synthesizing the   relationship between policy, infrastructure development and   application of learning analytics around the world. It will identify   key variables between countries, and examine how they affect the   aspirations, achievements and constraints which are observed.   Examples include technical and organizational infrastructure, legal   frameworks, educational institutions and culture, acceptability to   practitioners, state investment, the role of commercial players, etc.   The report will develop understanding within the LAK community   of the relationship between policy and learning analytics.   4. PROCESS  To achieve these outcomes, it is not sufficient to gather participants   from around the world for a discussion. Rather, the workshop itself   is the centerpiece of an extended set of activities, and is preceded   by a data collection phase. This involves establishment of common   ground on the scope of the enquiry, the specification of the material   to be gathered by participants, and the pooling and pre-processing   of data in preparation for the workshop. The outcomes of this phase   are statements about the policy environment in each participating   country in a standard format, which are shared before the workshop   and are required reading for participants.    At the workshop itself, the participants consider the implications of   the data that has been gathered, and the emerging open questions.   In addition to setting out the prescribed procedures, timetables and   restrictions, the social mechanisms at work are analyzed. This   discussion is documented, summarizing the data gathered, and the   conclusions of the workshop presented in a synthesis report for   publication. In the light of the workshop report, a journal paper will   be authored by the workshop participants. This will address the   open questions established at the workshop, through an online   discussion and collaborative authoring process. A complex picture   is expected to emerge, with the interaction of different policies,   implementations, and professional and economic interest groups.   To provide a framework for discussion and analysis, this landscape   can be characterized as a policy network, i.e. a set of formal   institutional and informal linkages between governmental and other   actors structured around shared if endlessly negotiated beliefs and   interests in public policymaking and implementation. [10, p.424].   This approach has been widely applied in policy analysis, including   in areas with conflicting social and technological interests, such as   software patents (see [11]).   5. PARTICIPATION  In order to maintain a strong focus and commitment, the workshop   is by invitation and application, and all participants are required to   participate in preparatory activities. The scope includes both   government departments and other agencies and organizations,   such as DISC (with an international remit), Jisc (UK), Educause   (USA), KERIS (Korea), NCET (China), etc. The authors cover   North America, Europe, and Asia, and will invite participants from   their own areas. Four authors are members of LACE, which worked   extensively on standards (see for example [12]) and ethical aspects   of learning analytics. This extensive community of researchers and   practitioners is the principal source of participants.     6. REFERENCES  [1] Davenport, T. H., Harris, J. G., and Morison, R. 2010.   Analytics at Work: Smarter Decisions, Better Results.   Harvard Business School Press Books, Cambridge MA.   [2] Sclater, N., Peasgood, A., and Mullan, J. 2016. Learning   analytics in higher education. Jisc, UK. Retrieved January   9th, 2017, from Jisc: https://www.jisc.ac.uk/reports/learning-  analytics-in-higher-education   [3] Oster, M., Lonn, S., Pistilli, M.D., and Brown, M.G. 2016.   The Learning Analytics Readiness Instrument. Proceedings   of the Sixth International Conference on Learning Analytics   & Knowledge. ACM, NY, 173-182.    [4] Australian Government Office for Learning and Teaching.   2013. Student retention and learning analytics.    [5] Pea, R., Jacks, D. 2014. Building the Field of Learning   Analytics for Personalized Learning at Scale. The Learning   Analytics Workgroup. Stanford University, CA.   [6] Macfadyen, L. P., Dawson, S. 2012. Numbers Are Not   Enough. Why e-Learning Analytics Failed to Inform an   Institutional Strategic Plan. Educational Technology &   Society, 15(3), 149-163.   [7] Wolf, M. A. 2014. Capacity Enablers and Barriers for   Learning Analytics: Implications for Policy and Practice.   Alliance for Excellent Education.   [8] Cardinali, F., Ferguson, R., Griffiths, D., Hoel, T. Karlberg,   P., Paini, M., Reynolds, S., Rienties, B., van der Schaaf, M.,   Scheffel, M., Wastiau, P. 2015. Policy recommendations for   learning analytics from three stakeholder workshops. LACE   Learning Analytics Review, no. 3, July 2015. Retrieved from   LACE, January 9th, 2017, from LACE:   http://www.laceproject.eu/publications/policy-  recommendations-lace-workshops.pdf   [9] Cooper, A., Hoel, T. 2015. Data Sharing Requirements and   Roadmap. LACE project deliverable D7.2.   [10] Rhodes, R. A. W. 2006. Policy Network Analysis. In The   Oxford Handbook of Public Policy, M. Moran, M. Rein and   R. E. Goodin, Eds. Oxford University Press, 423-45.   [11] Leifeld, P., Haunss, S. 2010. A Comparison between   Political Claims Analysis and Discourse Network Analysis:   The Case of Software Patents in the European Union. Report   2010/21. Max Planck Society.   [12] Griffiths, D., Hoel, T. Cooper, A. 2016. Learning Analytics   Interoperability: Requirements, Specifications and Adoption.   Lace Deliverable D7.4.     "}
{"index":{"_id":"73"}}
{"datatype":"inproceedings","key":"Spikol:2017:CFM:3027385.3029437","author":"Spikol, Daniel and Prieto, Luis P. and Rodr'iguez-Triana, M. J. and Worsley, Marcelo and Ochoa, Xavier and Cukurova, Mutlu and Vogel, Bahtijar and Ruffaldi, Emanuele and Ringtved, Ulla Lunde","title":"Current and Future Multimodal Learning Analytics Data Challenges","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"518--519","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029437","doi":"10.1145/3027385.3029437","acmid":"3029437","publisher":"ACM","address":"New York, NY, USA","keywords":"challenges, datasets, multimodal learning analytics","Abstract":"Multimodal Learning Analytics (MMLA) captures, integrates and analyzes learning traces from different sources in order to obtain a more holistic understanding of the learning process, wherever it happens. MMLA leverages the increasingly widespread availability of diverse sensors, high-frequency data collection technologies and sophisticated machine learning and artificial intelligence techniques. The aim of this workshop is twofold: first, to expose participants to, and develop, different multimodal datasets that reflect how MMLA can bring new insights and opportunities to investigate complex learning processes and environments; second, to collaboratively identify a set of grand challenges for further MMLA research, built upon the foundations of previous workshops on the topic.","pdf":"Current and Future Multimodal Learning Analytics Data Challenges  Daniel Spikol Malm University Malm, Sweden  daniel.spikol@mah.se  Luis P. Prieto Tallinn University Tallinn, Estonia  lprisan@tlu.ee  M. J. Rodrguez-Triana REACT Group, EPFL  Lausanne, Switzerland maria.rodrigueztriana@epfl.ch  Marcelo Worsley Northwestern University  Evanston, IL, USA mw@northwestern.edu  Xavier Ochoa ESPOL  Guayaquil, Ecuador xavier@cti.espol.edu.ec  Mutlu Cukurova UCL Knowledge Lab  London, UK m.cukurova@ucl.ac.uk  ABSTRACT Multimodal Learning Analytics (MMLA) captures, integrates and analyzes learning traces from different sources in or- der to obtain a more holistic understanding of the learn- ing process, wherever it happens. MMLA leverages the in- creasingly widespread availability of diverse sensors, high- frequency data collection technologies and sophisticated ma- chine learning and artificial intelligence techniques. The aim of this workshop is twofold: first, to expose participants to, and develop, different multimodal datasets that reflect how MMLA can bring new insights and opportunities to investi- gate complex learning processes and environments; second, to collaboratively identify a set of grand challenges for fur- ther MMLA research, built upon the foundations of previous workshops on the topic.  CCS Concepts Applied computing  Interactive learning environ- ments; Collaborative learning; Information systems  Data analytics;  Keywords Multimodal learning analytics; datasets; challenges.  1. WORKSHOP BACKGROUND The field of learning analytics is shifting from an emerg-  ing area of research, into mainstream educational research and practice. The inherently blended nature of most current learning settings, makes it essential to move beyond learn- ing analytics that rely solely on a single data source (usually log files). Multimodal learning analytics (MMLA) provides insights into such learning processes that happen across mul- tiple contexts between people, devices and resources (both  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 16 Vancouver, BC Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029437  physical and digital), which often are hard to model and orchestrate [4, 5, 7, 8].  MMLA leverages the increasingly widespread availability of sensors and high-frequency data collection technologies to enrich the existing data available. Using such technologies in combination with machine learning and artificial intelligence techniques, LA researchers can now perform text, speech, handwriting, sketch, gesture, affective, neurophysical, or eye gaze analyses [2, 6].  The workshop builds upon previous MMLA workshops at LAK, focusing on hands-on exposure to multimodal datasets and techniques for analysis. In addition, this workshop in- troduced the opportunity for participants to present their own multimodal datasets, as well as future-looking MMLA challenges and concepts, helping define this emerging field through discussion and feedback.  1.1 MMLA Community Foci This full-day workshop was organised following the two  main emphases of this emergent community. The first one is to build up and expand the community by letting partic- ipants explore multimodal techniques and data, using pre- made environments and sample datasets. This helps new- comers to learn and explore, and lets experienced partic- ipants see and discuss different approaches. Furthermore, in this workshop participants also were encouraged to sub- mit contributions proposing new MMLA datasets that the community can use in the future, as well as future-oriented visions and challenges.  The discussion on the initial and participant-submitted multimodal datasets, and the techniques that can be used to gather and analyze them, set the stage for the second emphasis of the community: unearthing the major future challenges in MMLA research, and steering the community towards overcoming them.  2. WORKSHOP OBJECTIVES The workshop has two interrelated objectives that frame  the emerging interest in MMLA. The workshop aims to cre- ate a common ground for better understanding the current state of research and practice. As the MMLA community grows, it is also necessary to launch a set of future grand challenges to guide research, practice, and to bring together the different viewpoints of the field.    Disseminate the state of MMLA research through sharing datasets: Building upon previous workshop edi- tions (in the LAK and ICMI conferences), this workshop ex- plored advanced approaches and techniques for working with multimodal datasets. By providing access to high-quality datasets, participants were able to contrast analyses, trig- gering in-depth discussions and learning about the kinds of insights that can be gained from different techniques.  Discuss the current and future MMLA challenges: By letting participants submit their own multimodal datasets, this workshop continued previous efforts, expanding the dis- cussion about what constitutes good practice for the design of data and analysis of MMLA. Furthermore, as the MMLA community grows, it was necessary to bring together differ- ent viewpoints of the field, to help identify future challenges, and define an agenda that guides MMLA in advancing re- search and practice.  2.1 Target Participants The workshop welcomed researchers that submitted mul-  timodal datasets, techniques or future challenges prior to the workshop. It also welcomed participants without a prior contribution, that desired to learn about MMLA and con- tribute to the discussion. All in all, participants came from different backgrounds:  Existing MMLA researchers. The LAK conference is the annual meeting for MMLA researchers, and continues to build upon the efforts of past workshops, accommodating the growing interest and work in MMLA.  New MMLA researchers. One of the key aims of the workshop is to attract new researchers from the LAK com- munity to MMLA, and especially into its new Special Inter- est Group1 (SIG). The low barrier to participate, with the hands-on sessionand the discussions, provided a dual oppor- tunity to attract new researchers and practitioners.  LA researchers and practitioners. The workshop is also aimed at bringing together other LA researchers and practitioners that can discover there how to incorporate mul- timodal data and techniques into their ongoing research.  2.2 Initial Datasets The workshop used the Math Data Corpus [3], Oral Pre-  sentation Quality [1], and PELARS Project2 datasets. The first two data corpora have been used in previous work- shops, and are well documented. On the other hand, the PELARS dataset was new and provided a contrast for dis- cussion about what makes a good multimodal dataset. The participant-submitted datasets provided seeds and new pos- sibilities for eventual use in future MMLA events.  2.3 Dissemination Strategy The workshop had two main planned outputs to be dis-  seminated. First, the sharing of existing techniques and datasets, created among the research community and in- terested stakeholders. Additionally, the participant submis- sions formed the foundation for a set of grand challenges and a research agenda for MMLA, to support the forthcom- ing MMLA-SIG within the SoLAR community. After the conference, full-length submissions resulting from the dis- cussions will be published on CEUR, and we are exploring the joint edition of future journal special issues on the topic.  1http://sigmla.org/ 2http://www.pelars.eu  The workshop used the LAK17 channels for the call of participation. Additionally, the organizers used their net- work of contacts (and that of the Program Committee), to personally contact a broad range of researchers and practi- tioners that have overlapping interests in related fields. The new-born MMLA SIG will serve as the main channel to con- solidate the interest around the workshop.  2.4 Program Committee Apart from the workshop organisers, the following re-  searchers will be part of the program committee: Alejandra Martnez Mones, UVA, Spain; Barbara Wasson, University of Bergen, Norway; Cynthia DAngelo, SRI, USA; Davinia Hernandez-Leo, UPF, Spain; Denise Whitelock, OU, UK; Manolis Mavrikis, UCL, UK; Nour El Mawas, Institut Mines- Telecom, France; Paulo Blikstein, Stanford, USA; Roberto Martinez-Maldonado, UTS, Australia.  3. ADDITIONAL AUTHORS Additional authors: Bahtijar Vogel (Malmo University,  bahtijar.vogel@mah.se), Emanuele Ruffaldi (Scuola Supe- riore SantAnna, e.ruffaldi@sssup.it), and Ulla Lunde Ringtved (University College Nordjylland, ulr@ucn.dk).  4. REFERENCES [1] L. Chen, G. Feng, J. Joe, C. W. Leong, C. Kitchen, and  C. M. Lee. Towards automated assessment of public speaking skills using multimodal cues. In Proceedings of the 16th International Conference on Multimodal Interaction, ICMI14, pages 200203, New York, NY, USA, nov 2014. ACM Press.  [2] P. J. Donnelly, N. Blanchard, B. Samei, A. M. Olney, X. Sun, B. Ward, S. Kelly, M. Nystran, and S. K. DMello. Automatic teacher modeling from live classroom audio. In Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization, UMAP16, pages 4553, New York, NY, USA, jul 2016. ACM Press.  [3] L.-P. Morency, S. Oviatt, S. Scherer, N. Weibel, and M. Worsley. ICMI 2013 Grand Challenge Workshop on Multimodal Learning Analytics. ICMI13, pages 373378, 2013.  [4] X. Ochoa, M. Worsley, K. Chiluiza, and S. Luz. MLA14: Third Multimodal Learning Analytics Workshop and Grand Challenges. ICMI14, pages 531532, 2014.  [5] X. Ochoa, M. Worsley, N. Weibel, and S. Oviatt. Multimodal Learning Analytics Data Challenges. LAK16, pages 498500, 2016.  [6] L. P. Prieto, K. Sharma, P. Dillenbourg, and M. J. Rodrguez-Triana. Teaching analytics: Towards automatic extraction of orchestration graphs using wearable sensors. In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK16, pages 148157, New York, NY, USA, apr 2016. ACM Press.  [7] S. Scherer, M. Worsley, and L. Morency. 1st International Workshop on Multimodal Learning Analytics. ICMI12, page 609, 2012.  [8] M. Worsley, K. Chiluiza, J. F. Grafsgaard, and X. Ochoa. 2015 Multimodal Learning and Analytics Grand Challenge. ICMI15, pages 525529, 2015.      "}
{"index":{"_id":"74"}}
{"datatype":"inproceedings","key":"Pardo:2017:CDS:3027385.3029441","author":"Pardo, Abelardo and Mart'inez-Maldonado, Roberto and Buckingham Shum, Simon and Schulte, Jurgen and McIntyre, Simon and Gavsevi'c, Dragan and Gao, Jing and Siemens, George","title":"Connecting Data with Student Support Actions in a Course: A Hands-on Tutorial","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"522--523","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029441","doi":"10.1145/3027385.3029441","acmid":"3029441","publisher":"ACM","address":"New York, NY, USA","keywords":"feedback, instructional design, learning analytics, predictive analytics, student support","Abstract":"The amount of data extracted from learning experiences has grown at an astonishing pace both in depth due to the increasing variety of data sources, and in breath with courses now being offered to massive student cohorts. However, in this emerging scenario instructors are now facing the challenge of connecting the knowledge emerging from data analysis with the provision of meaningful support actions to students within the context of an instructional design. The objective of this tutorial is to give attendees a set of hypothetical scenarios in which the knowledge extracted from a learning experience needs to be used to provide frequent personalized feedback to students","pdf":"Connecting Data with Student Support Actions in a  Course: A Hands-on Tutorial   Abelardo Pardo  The University of Sydney   Abelardo.pardo@sydney.edu.au     Jurgen Schulte  University of Technology Sydney   Jurgen.schulte@uts.edu.au     Jing Gao  University of South Australia   Jing.gao@unisa.edu.au   Roberto Martnez-Maldonado  University of Technology Sydney   roberto.martinez- maldonado@uts.edu.au   Simon McIntyre  University of New South Wales   s.mcintyre@unsw.edu.au     George Siemens  University of Texas at Arlington   gsiemens@gmail.com   Simon Buckingham Shum  University of Technology Sydney   Simon.BuckinghamShum@uts.edu.au     Dragan Gaevi   The University of Edinburgh,  University of South Australia  dragan.gasevic@ed.ac.uk  ABSTRACT  The amount of data extracted from learning experiences has  grown at an astonishing pace both in depth due to the increasing  variety of data sources, and in breath with courses now being  offered to massive student cohorts. However, in this emerging  scenario instructors are now facing the challenge of connecting  the knowledge emerging from data analysis with the provision of  meaningful support actions to students within the context of an  instructional design.  The objective of this tutorial is to give attendees a set of  hypothetical scenarios in which the knowledge extracted from a  learning experience needs to be used to provide frequent  personalized feedback to students.   CCS Concepts   Applied computing~Computer-assisted instruction   Keywords  Learning analytics, feedback, predictive analytics, instructional  design, student support.   1. BACKGROUND  In recent years we have witnessed a formidable increase in the  technological mediation present in learning experiences. Learners  in general may now have access to a wider set of resources,  through a larger variety of formats, and interact with peers and  instructors through an increasingly large number of channels.  These new affordances have the potential of providing a more  effective experience, but at the same time the risk of increasing  the complexity of the design and deployment stages due to the  sheer increase in options for the learner.   The technology used in these learning scenarios offers an  unprecedented number of data sources such as clickstreams,  student location, physiological measurements, affect, social   connections etc.   A number of recent research contributions in the area of learning  analytics have highlighted the need to focus more on learner  support actions deployed in a learning experience after a  comprehensive data set has been analyzed with state of the art  data mining or analytics algorithms [1]. Recent advances in data  collection and student modelling [2], text analytics [3], the use of  dashboards [4], etc. have translated into a rich set of potentially  useful tools. However, instructors are still facing significant  challenges when trying to integrate them properly as part of an  instructional design and assess their impact in every-day teaching  and learning activities [5].   A learning experience with a rich variety of format, platforms and  communication channels poses a more complex environment for  students as well. The perception and evaluation of the learning  environment is related both with approaches to study and the  quality of the learning outcomes [6]. In other words, a  technologically richer learning environment may require  additional support to facilitate its perception by the students.  Other research areas such as behavioral economics have identified  the notion of choice architecture, or how small features in  interactive environments can have significant effects in peoples  behavior [7]. The idea was discussed in the context of learning  experiences in relation to address the issues of retention. Students  faced numerous decisions during their career and specifically  targeted nudges or reminders may support them through this  process. Subtle interventions to influence student behavior are  more effective in situations with a large number of choices, when  there are experts that can provide appropriate advise, and when  individual preferences can be estimated [7]. Ideally, instructors  should be able to influence student behavior through interventions  to suggest the best approach to maximize the overall quality of a  learning experience. But these interventions are usually  challenging to deploy, specially at scale.   Massive open online courses (MOOCs) provide learning  experiences for cohorts of thousands of students. In these  scenarios, information about learners is now at unprecedented  levels of depth (detailed accounts of their actions in online  environments) and breath (for large number of students). The data  sets extracted from these courses have prompted the study of  techniques to collect information, summarize and display it in  more amenable formats, and use it to estimate student behavior.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029441     Additionally, these new scenarios have push for the exploration of  new mechanisms to automate as many tasks as possible.   The problem of how to support instructors in the decisions they  make while facilitating a learning experience has not received the  attention it deserves. There is an increasing disconnection between  the role of the instructor, the increasingly complex learning  scenarios, and the rich set of data collected during a learning  experience. Instructors now face the challenge of finding the best  way to embed the potential of data-supported decision making  within their instructional designs as well as day-to-day teaching  and learning activities. From the point of view of the students, the  provision of feedback has been identified as one of the most  problematic aspects when supporting their learning [8]. We are  facing a paradox in which students recognize the importance of  feedback, complain about its lack of quality, and in some cases  they dont engage with it [9]. How is possible that in the age of  ubiquitous data we are still struggling to provide students with  better support   Hattie [10] proposed a model of teaching and learning that was  based on three postulates, and one of which being that student  achievement in a learning experience is enhanced as a function of  feedback. A more refined model proposed that feedback can be  provided at four levels (task, learning process, self-regulation and  self) and is most effective at the learning process and self- regulation levels [11]. Ironically, feedback in the context of higher  education is usually perceived as inherently problematic [12] or an  administrative chore [13] - despite being one of the aspects with  large potential to translate into significant improvements in a  learning experience.   The aim of this tutorial is to propose the combination of these two  elements. On one hand, the manipulation of comprehensive data  sets that provide detailed account of the events occurring in a  learning experience, and on the other, the need to apply a sense- making stage to identify the personalized feedback that can be  provided to students with respect to their learning process and  self-regulation aspects. The hands-on exercises will assume that  the audience has detailed knowledge of a learning environment  and the need to provide frequent, personalized and relevant  comments to students while they participate in a learning  experience.   2. OBJECTIVES  The tutorial will last for half day and will be hands-on with a set  of activities conceived to guide the attendees to design  personalized interventions in a real learning experience. The topic  in the tutorial will appeal to researchers and practitioners in the  areas of learning analytics, educational data mining, instructional  design, and any academic interested in improving the timeliness  and relevance of feedback and support for their students.  Attendees are not required to have experience with any  programming language but need to be familiar with the  conventional data management procedures required in a course  with a large number of students such as for example combining  scores and observations from multiple sources. The objectives of  the proposed tutorial are to:    Understand the use of exploratory data analysis to  summarize data sets derived from a learning experience    Identify student support actions to be deployed while  the experience is being delivered    Define a set of low latency actions (those reaching  students within a day) that are personalized to each  student    Express the connection between data and actions in a  formalism suitable to be deployed at scale.   The tutorial will use the platform OnTask (available at  ontasklearnin.org) and various synthetic data sets to allow  attendees to explore the connections between the data available in  a realistic learning environment, the sense-making procedures  required by instructors, and the deployment of frequent and highly  personalized student support actions.   3. REFERENCES  [1] A. F. Wise,  Designing pedagogical interventions to support   student use of learning analytics,  in International Conference  on Learning Analytics and Knowledge, 2014, pp. 203-211.   [2] R. Pelnek, J. Rihk, and J. Papouek,  Impact of data  collection on interpretation and evaluation of student models,   presented at the International Conference on Learning  Analytics and Knowledge, Edinburgh, UK, 2016.   [3] S. A. Crossley, K. Kyle, and D. S. McNamara,  The tool for  the automatic analysis of text cohesion (TAACO): Automatic  assessment of local, global, and text cohesion,  Behavior  Resesearch Methods, pp. 1-11, Sep 28 2015.   [4] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. L. Santos,   Learning Analytics Dashboard Applications,  American  Behavioral Scientist, vol. 57, pp. 1500-1509, 2013.   [5] A. Bakharia, L. Corrin, P. de Barba, G. Kennedy, D. Gasevic,  R. Mulder, et al.,  A Conceptual Framework linking Learning  Design with Learning Analytics,  in International Conference  on Learning Analytics and Knowledge, S. Dawson, C. Ros,  and H. Drachsler, Eds., ed. Edinburgh, UK: ACM, 2016, pp.  329-338.   [6] K. Trigwell and M. Prosser,  Improving the quality of student  learning: the influence of learning context and student  approaches to learning on learning outcomes,  Higher  education, vol. 22, pp. 251-266, 1991.   [7] R. H. Thaler and C. R. Sunstein,  Nudge. Improving  Decisions About Health, Wealth, and Happiness,  2008.   [8] N. E. Winstone, R. A. Nash, J. Rowntree, and M. Parker,  It'd  be useful, but I wouldn't use it: barriers to university  students feedback seeking and recipience,  Studies in Higher  Education, pp. 1-16, 2016.   [9] C. Withey,  Feedback engagement: forcing feed-forward  amongst law students,  The Law Teacher, vol. 47, pp. 319- 344, 2013.   [10] J. Hattie,  Influences on Student Learning,  in  Inaugural Lecture: Professor of Education, ed. University of  Auckland, 1999.   [11] J. Hattie and H. Timperley,  The Power of Feedback,   Review of Educational Research, vol. 77, pp. 81-112, 2007.   [12] R. Higgins, P. Hartley, and A. Skelton,  Getting the  Message Across: The problem of communicating assessment  feedback,  Teaching in Higher Education, vol. 6, pp. 269-274,  2010.   [13] D. Hounsell,  Toward more sustainable feedback to  students,  in Rethinking Assessment in Higher Education:  Learning for the Longer Term, D. Boud and N. Falchikov,  Eds., ed London and New York: Routledge, 2007.        "}
{"index":{"_id":"75"}}
{"datatype":"inproceedings","key":"Koedinger:2017:CBE:3027385.3029442","author":"Koedinger, Ken and Liu, Ran and Stamper, John and Thille, Candace and Pavlik, Phil","title":"Community Based Educational Data Repositories and Analysis Tools","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"524--525","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029442","doi":"10.1145/3027385.3029442","acmid":"3029442","publisher":"ACM","address":"New York, NY, USA","keywords":"data storage and sharing, data-informed learning theories, learning metrics, modeling, data-informed efforts, scalability","Abstract":"This workshop will explore community based repositories for educational data and analytic tools that are used to connect researchers and reduce the barriers to data sharing. Leading innovators in the field, as well as attendees, will identify and report on bottlenecks that remain toward our goal of a unified repository. We will discuss these as well as possible solutions. We will present LearnSphere, an NSF funded system that supports collaborating on and sharing a wide variety of educational data, learning analytics methods, and visualizations while maintaining confidentiality. We will then have hands-on sessions in which attendees have the opportunity to apply existing learning analytics workflows to their choice of educational datasets in the repository (using a simple drag-and-drop interface), add their own learning analytics workflows (requires very basic coding experience), or both. Leaders and attendees will then jointly discuss the unique benefits as well as the limitations of these solutions. Our goal is to create building blocks to allow researchers to integrate their data and analysis methods with others, in order to advance the future of learning science.","pdf":"Community Based Educational Data Repositories and  Analysis Tools   Ken Koedinger, Ran Liu, John Stamper  Carnegie Mellon University   5000 Forbes Avenue  Pittsburgh, PA 15201   koedinger,ranliu@cmu.edu; jstamper@cs.cmu.edu   Candace Thille  Stanford University  505 Lasuen Mall    Stanford, CA 94305  cthille@stanford.edu   Phil Pavlik  University of Memphis  400 Innovation Drive  Memphis, TN 38152   ppavlik@memphis.edu     ABSTRACT  This workshop will explore community based repositories for  educational data and analytic tools that are used to connect  researchers and reduce the barriers to data sharing. Leading  innovators in the field, as well as attendees, will identify and  report on bottlenecks that remain toward our goal of a unified  repository. We will discuss these as well as possible solutions. We  will present LearnSphere, an NSF funded system that supports  collaborating on and sharing a wide variety of educational data,  learning analytics methods, and visualizations while maintaining  confidentiality. We will then have hands-on sessions in which  attendees have the opportunity to apply existing learning analytics  workflows to their choice of educational datasets in the repository  (using a simple drag-and-drop interface), add their own learning  analytics workflows (requires very basic coding experience), or  both. Leaders and attendees will then jointly discuss the unique  benefits as well as the limitations of these solutions. Our goal is to  create building blocks to allow researchers to integrate their data  and analysis methods with others, in order to advance the future of  learning science.   CCS Concepts   Information systems~Data warehouses   Information systems~Data mining   Keywords  Learning metrics; data storage and sharing; data-informed  learning theories; modeling; data-informed efforts; scalability.   1. WORKSHOP BACKGROUND  A confluence of increasing interest both in educational technology  and in the use of data to improve learning has led to increased  tracking and storing of detailed information on student learning  activities and progress. There is a large variety in the kinds,  density, and volume of such data and to the analytic and adaptive  learning methods that take advantage of it.   Data can range from simple to complex: from information such as  clicks on menu items or structured symbolic expression, to harder  to interpret data, such as free-form essays, discussion board  dialogues, or affective sensors. Another dimension of variation is   the time scale in which observations of student behavior occur.  Click actions are observed within seconds in fluency-oriented  math games or in vocabulary practice. Problem-solving steps are  observed every 20 seconds or so in modeling tool interfaces (e.g.,  spreadsheets, graphers, computer algebra) and in intelligent  tutoring systems for math and science. Answers to  comprehension-monitoring questions are given and learning  resource choices are made every 15 minutes or so in massive open  online courses (MOOCs). Lesson completion is observed across  days in learning management systems, chapter/unit test results are  collected after weeks, end-of-course completion and exam scores  are collected after many months, degree completion occurs across  years, and long-term human goals like landing a job and achieving  a good income occur across lifetimes.   Different paradigms of data-driven education research differ both  in the types of data they tend to use and in the time scale in which  that data is collected.  In fact, relative isolation within disciplinary  silos is arguably fostered and fed by differences in the types and  time scale of data used [4, 5]. There is a broad need for an  overarching data infrastructure to not only support sharing and use  within different levels of student data (e.g., clickstream, MOOC,  discourse, affect), but to also support investigations that bridge  across them and connect across timescales. A community based  infrastructure for sharing this diversity of educational data, as well  as analytic tools and workflows that work within and across  different levels of data, is needed. Such an infrastructure is needed  to support novel, transformative, and multidisciplinary approaches  to analyzing educational data that can help us understand how and  when long-term learning outcomes emerge as a causal  consequence of real-time student interactions within the complex  set of instructional options available [4]. Access to this variety of  educational data can yield actionable knowledge that improves  learning environments and technologies in the medium term, and  revolutionize learning in the longer term.   LearnSphere is an NSF funded system that supports sharing and  collaboration across a wide variety of educational data, including  MOOC data, as well as the integration of data analysis methods  and tools. It is poised to transform scientific discovery and  innovation in education through a scalable data infrastructure  designed to enable educators, learning scientists, and researchers  to easily collaborate on learning analytics over shared data. A  standard set of analysis tools allows researchers to quickly start  gathering information. Researchers can also leverage user- contributed analytic workflows to perform other methods of  analyses.   2. WORKSHOP OBJECTIVES  2.1 Purpose and Goals  The goals of this workshop are threefold. First, leaders and  attendees will engage in high-level discussions about the needs   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author. Copyright is held by the  owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029442     and concerns of the learning analytics community with respect to  sharing data and analysis tools. We will be begin and end the  workshop with these high-level discussions. Second, we will  present information on LearnSphere, an existing system that  supports sharing and collaboration across a wide variety of  educational data as well as the integration of data analysis tools,  as an in-progress solution to some of these community needs.  Finally, we will engage attendees in hands-on experience working  with different learning analytic workflows using LearnSphere.   In the workshop, attendees will learn about, discuss, and gain  hands-on experience the latest technologies used to connect  researchers while reducing barriers to data sharing. The hands-on  session may result in new learning analytics discoveries based on  applying and comparing analysis workflows to attendees choice  of educational datasets. It may also result in attendees successfully  incorporating their own analysis method or tool into LearnSphere,  thus making it available to the entire community of researchers  (including non-programmers). Workshop leaders and attendees  will also engage in lively discussion, providing input on what  barriers remain to sharing data and analyses and brainstorming  potential solutions that could be incorporated into an  infrastructure such as LearnSphere. The workshop will result in  new goals to incorporate additional, community needs-driven  functionalities into LearnSphere. As a broader contribution to the  field of learning analytics, the workshop will bring together  researchers who do analysis on varying levels of educational data,  which will foster collaboration that bridges gaps between these  levels of analysis.   LearnSpheres current infrastructure allows for individuals to  share their data, models, and visualizations as well as link them to  others while maintaining confidentiality. It maintains a central  store of metadata about what datasets exist but has distributed  features allowing contributors control over access to their data.   In addition to supporting sharing of a wide variety of educational  data, LearnSpheres goal is to support any custom analysis  method that can be applied to the datasets and to produce outputs  in a standardized way that facilitates both quantitative and  qualitative model comparisons. This workflow feature allows  researchers to apply their own analysis methods to the vast array  of datasets available in the educational data repository. It affords  researchers the advantages of (1) using the built-in learning curve  visualizations on the outputs of their own analysis workflows, (2)  easily comparing their results both quantitatively and graphically  to the outputs of any other analysis methods that are currently in  LearnSphere (e.g., Bayesian Knowledge Tracing [1], Performance  Factors Analysis [6], MOOC activity analysis [3], and others) or  that have been uploaded to LearnSphere as a custom workflow,  and (3) sharing their own analysis workflows with the community  of researchers. Without any prior programming experience,  researchers can use LearnSpheres drag-and-drop interface to  compare, across alternative analysis methods and across many  different datasets, model fit metrics like AIC, BIC, and cross  validation as well as parameter estimates themselves. In the  hands-on portion of the workshop, we will provide full support to  attendees in exploring one or more of three activities:  1. Applying analysis workflows to datasets and modifying   those workflows using existing LearnSphere components.  (GUI interface; no programming experience required)   2. Editing existing analysis workflow components. (Requires  only end-user programming experience; attendees can make  small modifications to existing code)   3. Writing new workflow components. (Requires programming  experience)      In the discussion sessions, some of the questions we will pose to  attendees include:  1. What are some of the challenges and barriers in doing the   kind of learning analytics you think would be most  interesting and/or important   2. What tools for learning analytics do you currently use and in  what ways are they not quite ideal   3. For those with data that you'd like help in analyzing, what are  the challenges and barriers in getting the kind of help that  you would like  For example, is making your data available  to others problematic because of privacy or proprietary  concerns   In addition to engaging attendees in both discussion and hands-on  experience using a system that supports community data and  analysis sharing, the workshop will involve them in the ongoing  development of LearnSphere. Community driven feedback is  critical to furthering the infrastructure that will support  researchers in transforming our understanding of human learning  in the long term. The ultimate goal is to create building blocks  that allow individual groups of researchers to integrate their data  with other researchers we can advance the learning sciences the  way that harnessing and sharing big data has done for other fields.   3. REFERENCES  [1] Corbett, A.T., & Anderson, J.R. (1995). Knowledge Tracing:   Modeling the Acquisition of Procedural Knowledge. User  Modeling and User-Adapted Interaction, 4, 253-278.   [2] Koedinger, K.R., Booth, J.L., & Klahr, D. (2013).  Instructional complexity and the science to constrain it.  Science, 342(6161), 935-937.   [3] Koedinger, K.R., Kim, J., Jia, J.Z., McLaughlin, E.A., &  Bier, N.L. (2015). Learning is not a spectator sport: Doing is  better than watching for learning from a MOOC. In  Proceedings of the 2nd ACM Conference on Learning@  Scale, pp. 111-120.   [4] Koedinger, K.R., Corbett, A.T., & Perfetti, C. (2012). The  KnowledgeLearningInstruction framework: Bridging the  sciencepractice chasm to enhance robust student learning.  Cognitive science, 36(5), 757-798.   [5] Newell, A. (1990). Unified theories of cognition. Cambridge,  MA: Harvard University Press.   [6] Pavlik, P.I., Cen, H., & Koedinger, K.R. (2009).  Performance factors analysis  A new alternative to  knowledge tracing. In Proceedings of the 14th International  Conference on AIED, 531538.     "}
{"index":{"_id":"76"}}
{"datatype":"inproceedings","key":"Arnold:2017:SEA:3027385.3029434","author":"Arnold, Kimberly E. and Karcher, Brandon and Wright, Casey V. and McKay, James","title":"Student Empowerment, Awareness, and Self-regulation Through a Quantified-self Student Tool","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"526--527","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029434","doi":"10.1145/3027385.3029434","acmid":"3029434","publisher":"ACM","address":"New York, NY, USA","keywords":"higher education, learning analytics, mobile application, quantified-self student, real-time feedback, recommendation engine, reflective learning practices, self-regulated learning","Abstract":"The purpose of this paper is to examine the cross institutional use of a quantified-self application called Pattern, which is designed to promote self-regulation and reflective learning in learners. This paper provides a brief look into how learners report spending their time and react to in-app recommendations. Initial data is encouraging; however, there are limitations of Pattern, and additional research and development must be undertaken.","pdf":"Student Empowerment, Awareness, and Self-Regulation through a  Quantified-Self Student Tool    Kimberly E. Arnold  University of Wisconsin-Madison   1305 Linden Drive  Madison, WI 53706   kimberly.arnold@wisc.edu     Casey V. Wright  Purdue University  155 S. Grant Street   West Lafayette, IN 47907  caseyw@purdue.edu   Brandon Karcher  Purdue University  155 S. Grant Street   West Lafayette, IN 47907  bkarcher@purdue.edu     James McKay   University of Wisconsin-Madison  1305 Linden Drive  Madison, WI 53706   james.mckay@wisc.edu    ABSTRACT  The purpose of this paper is to examine the cross institutional use  of a quantified-self application called Pattern, which is designed to  promote self-regulation and reflective learning in learners. This  paper provides a brief look into how learners report spending their  time and react to in-app recommendations. Initial data is  encouraging; however, there are limitations of Pattern, and  additional research and development must be undertaken.   CCS Concepts   Applied computing~Interactive learning  environments    Applied computing~E-learning    Human- centered computing~Information visualization   Keywords  Learning Analytics; Quantified-Self Student; Self-Regulated  Learning; Reflective Learning Practices; Higher Education; Real- Time Feedback; Recommendation Engine; Mobile Application.     ACM Reference Format:  K.E. Arnold, B. Karcher, C.V. Wright, and J. McKay, 2017. Student  Empowerment, Awareness, and Self-Regulation through a Quantified-Self  Student Tool. In Proceedings of Learning Analytics and Knowledge  Conference, Vancouver, BC, Canada, April 2017, (LAK17), 2 pages.  DOI: 10.1145/3027385.3029434    1. INTRODUCTION  Institutions of higher education are experiencing record enrollment.  However, students, as a whole, are arriving at the academys door  less prepared than ever before. [1] Students arrive without the skills  to thrive at the collegiate level; this is a major issue when learning  how to learn is a critical factor for success in todays global        economy. [2] Merging self-regulated learning theory and the  increasingly popular quantified-self movement into an application  paradigm for education may lead to promising ways to start  counteracting these shifting academic realities.    1.1 Self-regulation and reflective learning   As Tinto is famous for saying, students cannot be absolved from  responsibility for their own learning; [3] so students need to be  taught strategies for effective, independent learning. Understanding  how to self-manage learning activities is paramount in todays  knowledge economy. However, monitoring ones behavior alone  does not equate to complex self-regulation strategies.  In fact,  Butler and Winne argue that self-regulated learning is a deliberate,  judgmental, adaptive process which requires consistent feedback.  [4] Setting goals and devising plans is quintessential to successfully  unlocking meta-cognitive affordances. [5]  Research in the monitoring of self-regulated learning has promising  outcomes. Tabuenca et al. suggest that a positive relationship exists  between logging study time and levels of self-regulated learning.  [6] However, automating and scaling tools that can aid students in  their monitoring (awareness), feedback, and adapting self- regulation practices is a complicated process. The recent  emergence of quantified-self student movements within the  learning analytics community offers additional approaches to  supporting todays diverse learners.  1.2 Quantified-self movement in education   Major components of the quantified-self movement are awareness  and behavioral change, and fitness trackers are a common example  of this technology.  A fitness tracker makes one aware of how much  activity she is undertaking.  A user considers her desired outcomes,  and sets a goal for herself. This goal, combined with her awareness  of her progress toward her goal, motivates her to change her  behavior when necessary. Taking a translation of a fitness tracker  and applying it in educational settings has the potential to transform  how learners approach learning.  Having a technology that enables  a student to be aware of what effort she is putting into learning, so  that she can work toward her own personal goal, by optimizing her  behavior, is one approach to combating potential deficits of self- directed learning that many students face. Serving as a scaffold for  meta-cognitive processes, scholars have placed quantified-self  student technologies in theoretical frameworks for student support  and learning benefit. [7, 8]  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than the author(s) must be honored. Abstracting  with credit is permitted. To copy otherwise, or republish, to post on servers  or to redistribute to lists, requires prior specific permission and/or a fee.  Request permissions from Permissions@acm.org.  LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4870-6/17/03$15.00   DOI: http://dx.doi.org/10.1145/3027385.3029434       LAK17, April 2017, Vancouver, BC, Canada  K. Arnold et al.   The potential for tapping learning data via quantified-self apps  could be transformative for learners already heavily reliant upon  both technology and quantified-self technologies.  This paper focuses on a quantified-self student tool called Pattern.  This tool, developed with principles of improved study skills and  reflective learning as the foundation, provides baseline data about  effectiveness of such tools in diverse learning environments.   Pattern empowers learners by providing them an easy way to track,  analyze, and receive custom feedback about their study activities,  allowing them to seamlessly make data-informed decisions about  their study habits.   2. THE DEVELOPMENT OF PATTERN  Pattern, a web and mobile application created by Purdue  University, allows students to track their study habits; tracking  activity, length, and perceived productivity. This activity log feeds  a recommendation engine which enables Pattern to provide  recommendations for students regarding: i) which time period  during they seem to be most productive, ii) the duration of time in  which they report being most productive, and for specific activities,  iii) which time of day they seem to be most productive. These  recommendations and peer comparisons are automated and  delivered in real-time. Pattern also allows students to view their  activity data on a sophisticated dashboard, as well as export a  record of their study activities. Designed as a student-centric tool,  access to a students identifiable data is controlled by the student.  Instructors may view a dashboard of aggregate and anonymized  data for students in their courses. Academic advisors are only  allowed access to curated student data at an identifiable level if a  student opts to share it.    3. EXPLORATORY USE OF PATTERN   Pattern has been piloted at its home institution, Purdue University,  as well as the University of Wisconsin-Madison. Although  designed to be a student-centric tool, for the pilot, both institutions  took a course-level approach for implementation. This means that  the majority of students are not using Pattern of their own accord,  but rather using it either because their instructor requires it or they  are offered extra credit for participating in a pilot course. Since  spring 2015 when Pattern went into pilot, 2,140 students have  actively used the quantified-self tool at Purdue.  Since spring 2016,  UW-Madison has had roughly 2,200 students actively use Pattern.  4. EARLY RESULTS  Purdue and UW-Madison have been deliberate about gathering  information in order to evaluate the effectiveness of the tool, the  user experience, and to probe for ways to maximize the potential of  self-regulated learning.  Since action is the crux of any learning  analytics application, a focus was placed on the effectiveness of the  recommendations in motivating students to move beyond  awareness to behavioral change. Generally, students reported that  the recommendations given by Pattern were particularly helpful;  71% of Purdue students and 74% of UW-Madison students who  voted rated the recommendations as helpful. The recommendations  are currently fairly general in nature; however, an iterative  approach is being taken in creating a robust recommendation  engine that best serves students in various contexts.  UW-Madison placed a major evaluative lens on the student  experience of Pattern. Unequivocally, students found Pattern easy  to use (n=423,  on a 5.0 scale). Students believed that  Pattern gave them access to information that could not be easily  accessed elsewhere (n=423,  on a 5.0 scale). Amidst  concerns of providing students with data visualizations that could   be misconstrued, students were asked if they understood the  dashboards and visualization provided by Pattern, and most agreed  that they did (n=420,  on a 5.0 scale). However,  participating students had many suggestions for making the tool  easier and more integral to their study habits. When asked if they  would use Pattern again in future semesters, 75% (n=321) said they  would. Examination of text comments showed that students had  mixed feelings about being compared to their peers, and many  expressed concern about the anonymity of their data in  visualizations.    5. LIMITATIONS  The most notable limitation of the initial results is that the data is  self-reported, and may be inaccurate or skewed. Since the data  visualizations and recommendations are based on self-reported  data, it should be taken with that context in mind. The results are  also from a limited set of pilot courses at each institution, and  therefore are not generalizable.    6. CONCLUSIONS  A major tenant of Pattern is student empowerment. Giving students  the opportunity to systematically examine data about their learning  behavior, and that of their peers, allows them to leverage the  affordances of a quantified-self application. By exposing their  behavioral data, students are informed and can act accordingly.  With enhanced recommendations and the expansion of the student  dashboard with additional visualizations, students can adopt a data  driven approach to how they can be efficient with their time.  Learning is complexit cannot be measured in the same way  respiration, heart beats, or steps can be, and while the accuracy of  Pattern data is questionable as it is self- reported, this curation of  data is a start. Over time, quantified-self applications may provide  significant findings to students, teachers, and researchers.  7.  REFERENCES  [1] The College Board (2014). 2014 SAT report on college &  career readiness. Retrieved 6 October 2016 from:   https://www.collegeboard.org/program-results/2014/sat.  [2] Fink, L.D. (2003). Creating significant learning experience:  An integrated approach to designing college courses. John  Wiley Sons.    [3] Tinto, V. (1993) Leaving College: rethinking the causes and  cures of student attrition (2nd ed.). Chicago: University of  Chicago Press.    [4] Butler, D. L., and Winne, P. H. (1995). Feedback and self- regulated learning: A theoretical synthesis. Review of educational  research, 65(3), 245-281, pg 246.  [5] Winne, P. H., and Hadwin, A. F. (1998). Studying as self- regulated learning. Metacognition in educational theory and  practice, 93, 27-30.  [6] Tabuenca, B., Kalz, M., Drachsler, H., and Specht, M. (2015).  Time will tell: The role of mobile learning analytics in self- regulated learning. Computers & Education, 89, 53-74.  [7] Rivera-Pelayo, V., Zacharias, V., Mller, L., and Braun, S.  (2012, April). Applying quantified self approaches to support  reflective learning. In Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (pp. 111-114).  ACM.  [8] Eynon, R. (2015). The quantified self for learning: critical  questions for education. Learning, Media and Technology, 40(4),  407-411.       "}
{"index":{"_id":"77"}}
{"datatype":"inproceedings","key":"Hu:2017:SRS:3027385.3029438","author":"Hu, Xiao and Cheong, Christy W. L. and Ding, Wenwen and Woo, Michelle","title":"A Systematic Review of Studies on Predicting Student Learning Outcomes Using Learning Analytics","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"528--529","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029438","doi":"10.1145/3027385.3029438","acmid":"3029438","publisher":"ACM","address":"New York, NY, USA","keywords":"learning context, learning outcomes, methods, performances, prediction, systematic review","Abstract":"Predicting student learning outcomes is one of the prominent themes in Learning Analytics research. These studies varied to a significant extent in terms of the techniques being used, the contexts in which they were situated, and the consequent effectiveness of the prediction. This paper presented the preliminary results of a systematic review of studies in predictive learning analytics. With the goal to find out what methodologies work for what circumstances, this study will be able to facilitate future research in this area, contributing to relevant system developments that are of pedagogic values.","pdf":"A Systematic Review of Studies on Predicting Student  Learning Outcomes Using Learning Analytics   Xiao Hu  Faculty of Education   University of Hong Kong  Pokfulam, Hong Kong   xiaoxhu@hku.hk   Christy W.L. Cheong  Macao Polytechnic Institute   Rua de Luis Gonzaga  Gomes, Macao   wlcheong@ipm.edu.mo   Wenwen Ding  Faculty of Education   University of Hong Kong  Pokfulam, Hong Kong     wynn@connect.hku.hk   Michelle Woo  Faculty of Education   University of Hong Kong  Pokfulam, Hong Kong   mihlwoo@gmail.com      ABSTRACT  Predicting student learning outcomes is one of the prominent   themes in Learning Analytics research. These studies varied to a   significant extent in terms of the techniques being used, the   contexts in which they were situated, and the consequent   effectiveness of the prediction. This paper presented the   preliminary results of a systematic review of studies in predictive   learning analytics. With the goal to find out what methodologies   work for what circumstances, this study will be able to facilitate   future research in this area, contributing to relevant system   developments that are of pedagogic values.   CCS Concepts   Information systems  Information systems applications    Applied computing  Education.   Keywords  Systematic review; prediction; methods; performances; learning   outcomes; learning context.   1. INTRODUCTION  Existing studies in predictive learning analytics (LA) significantly   varied in the techniques being used, the learning environments in   which they were situated, and the resultant effectiveness of the   prediction models developed. In this study, we collected 39   empirical studies in this field for a systematic review to find out   what methodologies work for what circumstances. This study is   similar to [1], to some extent, which aimed to find out the types of   features important for predicting student learning outcomes and   the prediction algorithms used for the purpose. In addition to these   methodological considerations, we also examine the contexts, the   targets and the performances of the prediction so that effective   matching of prediction methodologies and circumstances can be   enabled. This study, therefore, aims to facilitate researchers in   identifying methodologies to develop prediction models for their   purposes. Findings will be useful for future research on enhancing   prediction model performance. This study will also contribute to   the development of LA systems that use prediction to improve   teaching and learning. This paper presented preliminary findings   from this study for discussion and planning for subsequent further   enhancement.   2. METHODS  As LA is an interdisciplinary field of study, we conducted an   extensive literature search in related disciplines including   Computer Science, Electrical Engineering, Education, Learning   Information System, and Management. We looked for peer-  reviewed journal or conference papers published between 2002   and 2016. These papers should present studies that developed   prediction models for student learning outcomes and tested their   predictability on empirical data. To select papers for this study,   we skimmed through the papers found in the search. Papers of the   following categories were excluded: (1) review articles and   position papers, (2) studies on intelligent tutoring systems, (3)   studies focusing on factors influencing student performance, and   (4) papers published in languages other than English.   We developed a coding framework with regard to (1) the teaching   and learning environment, (2) the type of learning outcome(s)   being predicted, (3) the features extracted for use in the prediction,   (4) the predictive algorithms being used, and (5) the performances   of the prediction on holdout, unseen data. This framework was   developed iteratively using the constant comparative method [2,   3]. We looked at each individual paper retained after the filtering   process and then identified/modified the categories in each aspect   stated above. The papers were coded by two coders. The results   were aggregated and analyzed. Thirty-nine papers1 were coded so   far and the preliminary results are presented below.   3. OVERVIEW OF PAPERS ANALYZED  We identified 45 teaching and learning environments and 14   prediction targets. Over half of the environments identified were   online/blended learning and the datasets presented were mostly   from electronic sources. Course performance was most commonly   noted as prediction targets, followed by student retention/dropout.   Course performance was predicted in binary terms (i.e. successful   / unsuccessful) or in grades (i.e. A/B/C/D/E) for most cases.   329 types of features in total were used for the prediction in the   papers, with 8.44 types being used in average across papers. Note   that one feature type can consist of multiple features (each of   which is equivalent to a column in a data table). Among the   feature types used, 151 unique feature types were identified and   they were of the following categories: (1) demographic features,   (2) student history record and performance, (3) student record and   performance in current course, (4) activity and course features, (5)   learning behavior features, (6) self-reported features, and (7)   others / unclear features. The top three feature categories having   the most feature types included learning behavior features, student   record and performance in current course, and demography   features. The number of unique feature types in each of these                                                                      1Papers analyzed is listed here: https://goo.gl/u6HHJL   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights   for third-party components of this work must be honored. For all other   uses, contact the Owner/Author.  Copyright is held by the owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029438    http://dx.doi.org/10.1145/3027385.3029438   feature categories tended to be half the size of the feature types   used in the papers or even less, except for activity and course   features, where the reduction was relatively less significant. This   observation indicates that there were significant overlaps on   features used among these studies.   To maintain comparability, we selected accuracy, the measure   adopted in most of the papers, as the medium of prediction   performance comparison (8 papers were therefore excluded). 14   different algorithms were accounted. The top five algorithms used   in most papers were Decision Tree (DT), Neural Network (NN),   Clustering-based classification, Rule-based algorithms, and Naive   Bayes (NB). The papers tended to use more than one algorithm   for the prediction. The total number of algorithms accounted was   115, resulting in an average of 3.71 algorithms per paper. The   reported accuracies were within the range of 60-98% for most   cases, indicating the prediction performance was overall better   than that of a random guess approach in binary predictions.   4. PREDICTION PERFORMANCE  COMPARISON  We adopted the following performance benchmarks to categorize   the papers into two tiers. As each paper may have used multiple   algorithms, only the best performance in each paper was   considered for this comparison. Papers satisfying one of the   following criteria are regarded as Tier-1 for their superior   prediction performances (23 papers), others as Tier-2 (17 papers).   One paper was categorized into both tiers because it had two   different prediction targets.   (1) For Accuracy, Detection Sensitivity, Mean Success Rate and  R2, the best performing algorithm used reaches 90% or above.   (2) For Root-Mean-Square Error (RMSE), Mean Squared Error  (MSE), Mean Absolute Error (MAE), and Mean Absolute   Deviation (MAD), the best performing algorithm used reaches   10% or below.   Tier-1 papers involved 26 environments and 27 datasets. Online/   blended learning environments accounted for 77% while all the   datasets were drawn from electronic sources. Tier-2 papers   involved 19 environments and 20 datasets. Online or blended   learning environments were less (58%). While electronic sources   were still predominated in Tier 2, these papers also drew on data   from non-system-based sources including questionnaire data (10%)   and data from external source (5%). This seems to suggest that   online environments and electronic data sources may be   preferable for better prediction performance.   35% of the Tier-1 papers predicted course performance in binary   terms, 22% course final grades, and none course final scores. The   respective percentages in Tier 2 were 25%, 15% and 15%. This   outcome was reasonable as the prediction tasks are easier when   there are fewer available options. Over 60% of the papers   predicting course performance in binary terms and in grades were   in Tier 1 while 57% of the papers predicting student   retention/dropout were in Tier 2. This implied that course   performance in binary terms and in grades seemed more   predictable as compared to student retention/dropout.   The sample size was generally larger in Tier 1 than Tier 2. 50% of   the samples used in Tier 2 were less than 200 while the samples of   such size in Tier 1 only accounted for 30%. In other words, Tier-1   papers generally had more examples to train the prediction models.   This could be one of the possible reasons for the consequent better   performance. The prominent feature types and their ranking in   Tier 1 and Tier 2 papers remained the same as the overall picture   across all papers. The only exception was self-reported features,   which were not used in Tier 1 where learning behavior features   were used more often (Tier 1: 3.39 types/paper; Tier 2: 2.25   types/paper). As learning behavior features are automatically   tracked by systems in online environments, these findings were in   line with the observed prominence of online/blended   environments and electronic data sources among Tier-1 papers.   As each paper may have used more than one algorithm to build   prediction models, we compared the accuracies attained by all   models used in the papers. With 90% accuracy as the benchmark,   30 prediction models were classified into Tier 1 and 85 into Tier 2.   In line with their high frequencies of usage, DT, NN and   Clustering-based classification had been used most often in both   tiers. While the frequencies of NN and Clustering-based   classification were similar in Tier-1 models, NN were used more   often than Clustering-based classification in Tier 2. Rule-based   algorithm, despite its frequent usage, was not used in any of the   Tier-1 models while NB had just two such cases. In follow-up   studies, the models will be further analyzed in conjunction with   the contexts to find out possible interactions of these factors.   5. CONCLUDING REMARKS  This paper presented the preliminary findings of a systematic   review of studies in predictive learning analytics, with an   objective to find out what prediction methodologies work for what   circumstances. At this stage, we found that existing studies tended   to predict course performance (successful/unsuccessful), course   grades and student retention/dropout in online/blended learning   contexts using data drawn from electronic sources. This is   understandable as the data are objective and are usually available   in a large amount under these circumstances. This explanation is   also valid for the prominent feature types accounted in papers   with superior prediction performance. Interestingly, self-reported   features were not in use in any of the papers with superior   prediction performance. Possible reasons might include: 1) these   data are hardly scalable as they need extra efforts to collect; and 2)   they are subjective in nature which may affect their reliability. As   for prediction algorithms and prediction performance, we found   that no experiments with superior prediction performance used   rule-based algorithms, even though they were used in 29% of the   reviewed papers. Decision Tree, Neural Networks and Clustering-  based classification were the most frequently used prediction   techniques. In the next stage, we will delve further into the   prediction experiments to find out what leads to (un-)promising   results and examine if there are any correlations between the   methodologies and the circumstances in which the studies were   situated. We will also continue to expand the pool of papers being   analyzed in order to enhance the generalizability of the findings.    6. REFERENCES  [1] Shahiria, A. M., Husaina, W., & Rashida, N. A. 2015.A   review on predicting students performance using data   mining techniques. Procedia Computer Science, 72, 414-422.   doi:10.1016/j.procs.2015.12.157   [2] Strauss, A. L. 1987. Qualitative Analysis for Social  Scientists. Cambridge University Press, London.   [3] Strauss, A. L., & Corbin, J. 1998. Basics of Qualitative  Research: Techniques and Procedures for Developing   Grounded Theory, 2nd ed. Sage Publications, Thousand   Oaks, CA.        "}
{"index":{"_id":"78"}}
{"datatype":"inproceedings","key":"Grover:2017:FHA:3027385.3029440","author":"Grover, Shuchi and Bienkowski, Marie and Basu, Satabdi and Eagle, Michael and Diana, Nicholas and Stamper, John","title":"A Framework for Hypothesis-driven Approaches to Support Data-driven Learning Analytics in Measuring Computational Thinking in Block-based Programming","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"530--531","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029440","doi":"10.1145/3027385.3029440","acmid":"3029440","publisher":"ACM","address":"New York, NY, USA","Abstract":"K-12 classrooms use block-based programming environments (BBPEs) for teaching computer science and computational thinking (CT). To support assessment of student learning in BBPEs, we propose a learning analytics framework that combines hypothesis- and data-driven approaches to discern students' programming strategies from BBPE log data. We use a principled approach to design assessment tasks to elicit evidence of specific CT skills. Piloting these tasks in high school classrooms enabled us to analyze student programs and video recordings of students as they built their programs. We discuss a priori patterns derived from this analysis to support data-driven analysis of log data in order to better assess understanding and use of CT in BBPEs.","pdf":"A Framework For Hypothesis-Driven Approaches To  Support Data-Driven Learning Analytics In Measuring  Computational Thinking In Block-Based Programming    Shuchi Grover  SRI International, Menlo Park, CA   shuchi.grover@sri.com  Michael Eagle   HCII, Carnegie Mellon University  meagle@cs.cmu.edu  Marie Bienkowski  SRI International, Menlo Park, CA  marie.bienkowski@sri.com   Nicholas Diana  HCII, Carnegie Mellon University   ndiana@cs.cmu.edu   Satabdi Basu  SRI International, Menlo Park, CA   satabdi.basu@sri.com  John Stamper   HCII, Carnegie Mellon University   jstamper@cs.cmu.edu       ABSTRACT  K-12 classrooms use block-based programming environments  (BBPEs) for teaching computer science and computational  thinking (CT). To support assessment of student learning in  BBPEs, we propose a learning analytics framework that combines  hypothesis- and data-driven approaches to discern students  programming strategies from BBPE log data. We use a principled  approach to design assessment tasks to elicit evidence of specific  CT skills. Piloting these tasks in high school classrooms enabled  us to analyze student programs and video recordings of students  as they built their programs. We discuss a priori patterns derived  from this analysis to support data-driven analysis of log data in  order to better assess understanding and use of CT in BBPEs.   1. INTRODUCTION  Most K-12 computer science (CS) courses teach programming to  support learning of computational thinking (CT) practices such as  decomposing problems, debugging, and use of CT concepts to  create computational solutions. However, programming is, and  has been, difficult for novices to learn [5], and assessing K-12  students learning is typically done manually by checking  students final programs, giving an incomplete picture of students  CT skills. Examining process gives a more complete picture [2].   We present a theoretical framework that researchers can use to  design measurement systems for block-based programming  environments (BBPEs) for research or application. Using this  framework, we analyzed log data from a previously designed  assessment of middle school CT in the Alice BBPE; designed new  tasks based on the Evidence Centered Design (ECD) framework  (a principled approach to guide assessment design [3]), and  derived a priori hypothesis-driven patterns. We describe how the  framework can inform future efforts that blend hypothesis- and  data-driven approaches for measuring CT skills.    2. RELATED WORK  Programming is a complex activity that involves understanding a  problem as a computational task, mapping a design for the  program, drawing on problems previously programmed that have   a similar structure, instantiating abstract program patterns, coding  the program, and then testing and debugging. Past research relies  on examining the finished programs for use of programming  constructs, however, learning analytics (LA) approaches [1] offer  better ways to analyze student understanding, misconceptions, and  steps to a solution using data from digital environments such as  number of actions in students programs and number of successful  and unsuccessful program compilations [2]. Clustering techniques  [3] led to various programmer behavior profiles, and unsupervised  methods were used to derive program-state patterns and state  transitions to predict success outcomes [4]    Early efforts have mostly analyzed log data post-hoc, looking for  static constructs or patterns largely from the bottom up [8] using  data-driven LA. New blended LA assess students learning  processes in various digital learning environments by combining  ECD and LA for hypothesis-driven generation of a priori patterns  about learner actions [6]. ECD focuses on three related models:  student (what are targeted cognitive constructs), task (what  activities allow students to demonstrate cognitive constructs),  and evidence (what data provide evidence of cognitive  constructs). ECD helps connect important constructs that we  want to measure with observable behaviors (including patterns of  learner actions). Also, importantly, evidence is obtained by  deliberately putting students in situations or tasks that will elicit  the needed evidence. Once semantically meaningful patterns are  defined a priori, data mining techniques can be used to analyze  the patterns further.    3. METHODOLOGY  This work is part of a broader effort to study learner behavior in  BBPEs, specifically, What patterns of behavior in data logs from  BBPEs provide evidence of learners use of CT concepts and  practices In Phase 1, we analyzed a dataset from an assessment  task designed and used in prior research [7]. 118 females and 202  males aged 10 to 14 years completed the 30-minute task which  involved modifying existing code. Students programs and Alice  log data were collected, and the programs were scored manually  using a rubric for algorithmic thinking and abstraction. We had  complete data for 229 students. We applied ECD to reverse  engineer this task into specific CT concepts and skills and give  evidence of what those might look like in log files. We also  compared action sequences between students who scored high and  low (relative to the median) to determine commonality of  sequences for each group. We found sequences that were  significantly more common among students with high grades and  one sequence that occurred significantly more frequently for  students with low grades. Further analysis showed that a higher   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029440     number of code edit actions as well as more frequent testing were  associated with higher final scores. Also, students who made more  changes to their code tended to test their program more frequently  however, whether students tested their programs frequently, or  after working on a considerable part of the program, did not seem  to have a bearing on their final scores. In this way, we gained  insights into interpreting student actions from logs. In Phase 2, we  applied ECD for forward design of a more complex task to  generate richer process data to observe repeated use of constructs  and CT practices. We designed two new Alice assessment tasks  aligned with focal CT concepts and practices. These tasks were  piloted in two high school introductory CS classrooms with 27  and 28 students. Data included final Alice files and log data for all  students and screen recordings for 6 students. Analysis of logs  revealed similar issues that students struggled with in both tasks:  hard-wired vs. general solutions, improper termination conditions,  parallel vs. sequential execution, effective solution  decomposition, and appropriate random number use.   Analyses of screen captures from the 6 students using a process  over product lens to assess CT practices showed that some  students created their own methods, tested the methods in  isolation, and then used the methods in the main method. These  practices demonstrate abstraction, modularization, and testing in  parts, and could serve as useful patterns to search for in students  log data as evidence for CT skills. In addition, we noticed certain  phases during students programming process when a student was  unable to progress. The student added, deleted, or reorganized  existing actions and repeatedly tested the program after each small  edit. While repeatedly testing and editing is not a practice we need  to discourage, it does become problematic when a student cannot  progress towards the task goal even after repeated editing and  testing. Such circumstances can easily lead to frustration and loss  of engagement, and can thus serve as good candidates for  potential patterns to be detected as students work on their  assessment tasks.    3.1 Hypothesis-Driven Framework  The research described applies ECD to conduct thoughtful and  deliberate hypothesis-driven analyses. These analyses reveal  patterns of behavior that complement data-driven findings of  student programming actions. Our work helps us articulate a  preliminary framework for effectively using a hybrid approach to  interpret student actions in log data from BBPEs in general. The  through line from the ECD domain modeling stage to the patterns  derived in Section 3.2 helps us connect log data entries to CT  skills and practices that we aim to measure (both formatively and  summatively) in order to make claims about student  understanding of CT skills. The framework (Fig. 1) describes an  iterative process that begins with articulating important CT  concepts and practices. Careful design of tasks put students in  situations that evoke behaviors to provide potential observables of  these concepts and practices. Detailed code analysis of varied  solutions reveals students use of constructs (correct or otherwise)  and approaches to solutions. Similarly, analyzing data from  observations reveals aspects of students actions that are never  seen in the final program. These can reveal student  misunderstanding of concepts even if the final solution seemingly  demonstrates correct usage. Combined qualitative analyses of the  solutions and process of a designed task provide a deeper  understanding than is possible from data-driven analytics alone,  including potential code sequences that map to practices that  could be detected in logs. These hypotheses lay the foundation for  detectors for these patterns and provide a richer interpretation of  student process in BBPEs.     Figure. 1: Proposed Framework for Hypothesis-driven Analyses to   Support Data-Driven Analytics   4. CONCLUSION & NEXT STEPS  Our emergent framework for using hypothesis-driven analyses to  support data-driven learning analytics leads to better interpretation  of student actions to assess skills in BBPEs. Currently, in Phase 3,  we are using and refining our framework. We are gathering  program files and log data from ~100 students in three high school  classrooms in Fall/Winter of 2016/17, and classroom observations  and screen recordings with interviews from a few students.    5. ACKNOWLEDGMENTS  We gratefully acknowledge grant support from NSF IIS-1522990, and  Sangeeta Bhatnagar for her support in conducting classroom research.    6. REFERENCES  [1] Baker, R., & Siemens, G. 2014. Educational data mining and   learning analytics. In K. Sawyer (Ed.), Cambridge Handbook of  the Learning Sciences.   [2] Berland, M., Martin, T., Benton, T., Petrick Smith, C., & Davis,  D. 2013. Using Learning Analytics to Understand the Learning  Pathways of Novice Programmers. JLS, 22(4), 564599.   [3] Mislevy, R.J. & Haertel.G. 2006. Implications of Evidence Centered Design for Educational Testing. Educational  Measurement: Issues and Practice, 25(4), 6-20.   [4] Piech, C., et al. 2012. Modeling how students learn to program.  In Proceedings of the 43rd SIGCSE (pp. 153-160). ACM.   [5] Robins, A., Rountree, J., & Rountree, N. 2003. Learning and  teaching programming: A review and discussion. Computer  Science Education, 13(2), 137-172.    [6] Rupp, A. A., et al. 2012. Putting ECD into Practice: The  Interplay of Theory and Data in Evidence Models within a  Digital Learning Environment. JEDM, 4(1), 49110.   [7] Werner, L., Denner, J., Campe, S., & Kawamoto, D.C. 2012.  The Fairy Perf. Assessment: Measuring computational thinking  in middle school. In Proceedings of the 43rd SIGCSE. ACM.   [8] Winne, P. H., & Baker, R. S. 2013. The potentials of educational  data mining for researching metacognition, motivation and self- regulated learning. Journal of Educational Data Mining, 5(1).      "}
{"index":{"_id":"79"}}
{"datatype":"inproceedings","key":"Knight:2017:DLP:3027385.3029443","author":"Knight, Simon and Anderson, Theresa and Tall, Kelly","title":"Dear Learner: Participatory Visualisation of Learning Data for Sensemaking","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"532--533","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029443","doi":"10.1145/3027385.3029443","acmid":"3029443","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, participatory, sensemaking, visualization","Abstract":"We discuss the application of a hand-drawn self-visualization approach to learner-data, to draw attention to the space of representational possibilities, the power of representation interactions, and the performativity of information representation.","pdf":"Dear Learner: Participatory Visualisation of Learning Data  for Sensemaking   Simon Knight        Theresa Anderson  Connected Intelligence Centre    University of Technology Sydney   {firstname.lastname@uts.edu.au}   Kelly Tall        ABSTRACT  We discuss the application of a hand-drawn self-visualization   approach to learner-data, to draw attention to the space of   representational possibilities, the power of representation   interactions, and the performativity of information representation.    CCS Concepts   Human-centered computing~Visualization    Applied   computing~Education    Keywords  Learning Analytics; Visualization; Sensemaking; Participatory   1. INTRODUCTION   1.1 The Quantified Self in Learning Analytics  Quantified self approaches are increasingly present in educational   contexts [5], raising the potential to increase reflective learning   [11]. As Eynon notes, various learning activities e.g., time on   one task, number of words written per hour, emotional state, could   be tracked and connected to specific learning outcomes [5]. In   this way the Quantified Self trend for self-tracking devices to   monitor step-counts, heart-rate, calories and other quantifiable   activity measures, can be applied to learning. Learning Analytics,  then, has developed as a research area in part in response to  the greater availability of data to inform learning and a desire  amongst educators and students that this tracking be  applicable in personal  not only institutional  contexts [6].   1.2 Human Data Interaction  As approaches such as learning analytics become increasingly   available, the need to explore human interactions with this   data/information grows. Thus, fields such as Human Data   Interaction (building on work in human computer interaction    HCI) have emerged to explore how to  support end-users in the   day-to-day management of their personal digital data...  seeing   data as having,  inherently social and relational character  [3].   Conveying learning analytic information across stakeholder   audiences with their respective skills and needs (from individual   students up to institutional leaders) is a challenge, requiring   consideration of collaborative sensemaking [8]. In such   approaches, interactions with analytic devices would be seen as a    distinctively socio-technical problematic, driven as much by a   range of social concerns with the emerging personal data   ecosystem as it is by technological concerns, to develop digital   technologies that support future practices of personal data   interaction within it  [3].    During their studies in the Master of Data Science and Innovation   (MDSI) program at the University of Technology Sydney (UTS),   our students explore this self-tracking phenomenon as part of a   core subject in which they are asked to track their activity over an   extended period. Students explore and analyze their own data, and   with randomized data from others, in a small group of 10 and at a   class/community level. The Assignment is intended to humanize   the exploration of big data by providing a real-life case for   exploring relationships in data, policy debates about data privacy   and insight into ones own life.  Students thus report that the   experience is confrontational in drawing their attention to the   social characteristics of data analysis. When introducing the   assignment we emphasize students can gather data about anything   and need not limit themselves to data measureable by an activity   tracker. Nonetheless, our experience has shown students do   generally stick with these measures because the sensors and tools   available make it so easy for them to do so in automated fashion.   Despite this, we are keen to encourage the students to be creative   in the data they collect and its analysis, and widen their gaze   about the possibilities of data practices, to support people in   understanding and investigating their data.    1.3 Algorithmic Accountability  In learning analytic contexts, the meaning inscribed in personal   data both shapes the ways that learning is understood and enacted   as objects of assessment, and is interpreted, reinterpreted, and   acted with as a dynamic part of that very context. Broad   discussion of this concern has noted that code acts in education   [14], such that:   as algorithms are increasingly being designed to anticipate   users and make predictions about their future behaviours, users   are now reshaping their practices to suit the algorithms they   depend on. This constructs calculated publics, the algorithmic   presentation of a public that shapes its sense of itself. [14]   Thus, the ways in which analytic devices become active agents in   learning  both inscribed with policy and practice commitments,   and enacted or enactive informative artefacts  has led to calls for   greater algorithmic accountability [4], to ensure that the   pedagogic aims of analytic devices are transparent across a range   of stakeholders. Analytic devices, as objects that both shape and   are shaped by learning contexts require complex analyses to make   them legible to learners and educators. To do so, analysis should   be given of the theory and operationalization behind any given   learning-target, as well as the methods for collation and feedback.   Moreover, agents should understand data-feedback as both an   analytic ends, and a shaping component in the analytic device.    1.4 Playing With Data to Build Data Literacy  We have thus begun piloting a personal-data-visualization   approach. Rather than personalizing where representations are   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other   uses, contact the Owner/Author.    Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029443     designed for individuals  but there is an us designing for you    our approach takes a micro-level perspective on making sense of   ones-own data traces and processes of data visualization. As Ben   Williamson notes, analytics a representational tools that provide a   given reading of the learners activity, can be seen to present   visualized facts in a way that separates learners from their own   learning [15]; bringing students into this process marks a shift   away from passive consumption.   We have begun thinking about this concern in terms of how we   engage learners with increasing their analytics literacy through   playful interaction/performance. To orient our students   considerations of big data to the personal, representational, and   qualified in a manner similar to that discussed in [2], we invited   students to engage in a visual data practice that mirrors the   analogue drawing project: dear data (www.dear-data.com), in   which two visual designers send hand-drawn personal-data   postcards to each other [see, 9]. Each week they both draw by   hand a representation of some pre-defined (and shared) data-  theme for that week. Their process is to take a topic each week,   and then, in parallel, collect data about the topic (but not   necessarily the same types of data), only creating the visualization   at the end of the week (and again, these differ). So, inevitably, the   collection and visualization of the data itself has a performative   quality, impacting on the very behavior being observed.    With our students, we are informally prototyping an activity in   which they are asked, over a period of weeks, to collate data on a   theme, by whatever means they wish, and visually represent this   data for sharing. As this work develops, we intend to foreground   learning activities that could be targeted by them for data   collection and visualization. These early experiments with the   activity suggest that by encouraging students to articulate the data   collection and representation through hand-crafted artefacts, we   can draw attention to:   1. The space of possibilities in representation  highlighting the  variety of ways in which the same thematic data might be   collated, segmented, and visualized.   2. Representational interactions  by engaging with each others  representations, not only is the range of potential spaces   highlighted, but the necessity of human sensemaking,   explication or qualification, on a personal level.   3. The performativity of information representation  that  representations are created for a purpose, that they are situated   in that purposeful context, but that they also act on it to frame   discussions and actions (in this case, both through raising   awareness of the data one is collating about oneself, and   through the sharing of these personal-data artefacts).    2. PRODUCT(ION) AS PROCESS: POSTER  AS VISUAL PRACTICE  This poster builds on this data play to invite conference   participants to consider how learners and teachers can tap into the   creative capacity of visual ideation for individual and   collaborative learning about (their) learning data, exploring   tactile, visual modes of self-expression, sense-making and   communication. Casual, rough planning and design activities   intertwining text, image and drawing help explain ideas and make   sense out of complexity, social variance and uncertainty [1, 7].   This (proposed) interactive and emergent poster invites   conference participants to dabble and doodle and think visually.    Exploratory approaches to visualisations of ideas, text, and data   are increasingly recognised for their role in knowledge production   and organisation, particularly with large sets of qualitative and   quantitative data [12]. Harnessing the power of mapping   dialogues in environments that bring together people, data and   technology, is a necessary literacy for 21st century work [10, 13].   As our students grapple with complexity and seemingly   overwhelming sets of data, the enabling of collective sensemaking   becomes a necessary feature of their creative intelligence.   3. REFERENCES  [1] Anderson, T.K. 2013. The 4Ps of innovation culture:   conceptions of creatively engaging with information.   International Conference on Conceptions of Library and   Information Science (2013).   [2] Anderson, T.K. and Martinez-Moldonado, R. 2016. Building   a Qualified Self around Lifecycles of Experience and   Thinking. For Richer, for Poorer, in Sickness or in   Health...The Long-Term Management of Personal   Information, CHI 2016 Workshop on Personal Information   Management (PIM 2016) (San Jose, CA, 2016).   [3] Crabtree, A. and Mortier, R. 2015. Human data interaction:   Historical lessons from social studies and CSCW. ECSCW   2015: Proceedings of the 14th European Conference on   Computer Supported Cooperative Work, 19-23 September   2015, Oslo, Norway (2015), 321.   [4] Diakopoulos, N. 2014. Algorithmic Accountability. Digital   Journalism. 3, 3 (2014), 398415.   [5] Eynon, R. 2015. The quantified self for learning: critical   questions for education. Learning, Media and Technology.   40, 4 (2015), 407411.   [6] Ferguson, R. 2012. The State of Learning Analytics in 2012:   A Review and Future Challenges. Technical Report #kmi-  12-01. The Open University, UK.   [7] Franois, A. 2013. SketchyTruth: somewhere in between the   good news and the bad news lies the truth (a concept for a   cartooning application on mobile devices). University of   Technology Sydney.   [8] Knight, S., Buckingham Shum, S. and Littleton, K. 2013.   Collaborative Sensemaking in Learning Analytics. CSCW   and Education Workshop (San Antonio, Texas, USA, 2013).   [9] Lupi, G. and Posavec, S. Forthcoming. Dear Data: The   Story of a Friendship in Fifty-Two Postcards. Penguin.   [10] Okada, A., Buckingham Shum, S. and Sherborne, T. 2008.   Knowledge cartography. Software Tools and Mapping   Techniques. (2008).   [11] Rivera-Pelayo, V., Zacharias, V., Mller, L. and Braun, S.   2012. Applying quantified self approaches to support   reflective learning. Proceedings of the 2nd International   Conference on Learning Analytics and Knowledge (2012),   111114.   [12] Sadokierski, Z.A. and Sweetapple, K. 2015. Drawing Out:   How designers analyse written texts in visual ways. The   Routledge Companion to Design Research. P. Rodgers and J.   Yee, eds. Routledge.   [13] Selvin, A. and Buckingham Shum, S. 2014. Constructing   Knowledge Art: An Experiential Perspective on Crafting   Participatory Representations. Synthesis Lectures on Human-  Centered Informatics. 7, 3 (2014), 1119.   [14] Williamson, B. 2015. Coding/Learning: Software and Digital   Data in Education. Organizing Algorithms in Digital   Education. B. Williamson, ed. University of Stirling. 2733.   [15] Williamson, B. 2015. Digital education governance: data   visualization, predictive analytics, and real-time policy   instruments. Journal of Education Policy. 0, 0 (Apr. 2015),   119.      http://www.dear-data.com/   1. INTRODUCTION  1.1 The Quantified Self in Learning Analytics  1.2 Human Data Interaction  1.3 Algorithmic Accountability  1.4 Playing With Data to Build Data Literacy   2. PRODUCT(ION) AS PROCESS: POSTER AS VISUAL PRACTICE  3. REFERENCES   "}
{"index":{"_id":"80"}}
{"datatype":"inproceedings","key":"Yaginuma:2017:VAT:3027385.3029444","author":"Yaginuma, Yoshitomo and Furukawa, Masako and Yamada, Tsuneo","title":"Video Annotation Tool for Learning Job Interview","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"534--535","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029444","doi":"10.1145/3027385.3029444","acmid":"3029444","publisher":"ACM","address":"New York, NY, USA","keywords":"job interview, video annotation tool, visualization","Abstract":"In this paper, video annotation tool for learning job interview is proposed. To visualize the difference of obtained descriptions, the proposed tool uses correspondence analysis. The results of correspondence analysis are used to give feedback to learners. By the results, the learner can understand the characteristics of his/her descriptions among the others.","pdf":"Video Annotation Tool for Learning Job Interview     Yoshitomo Yaginuma  The Open University of Japan,    Japan   yaginuma@ouj.ac.jp        Masako Furukawa   National Institute of Informatics,   Japan   furukawa@nii.ac.jp        Tsuneo Yamada   The Open University of Japan,   Japan    tsyamada@ouj.ac.jp     ABSTRACT  In this paper, video annotation tool for learning job interview is  proposed. To visualize the difference of obtained descriptions, the  proposed tool uses correspondence analysis. The results of  correspondence analysis are used to give feedback to learners. By  the results, the learner can understand the characteristics of his/her  descriptions among the others.   CCS Concepts   Applied computingEducationE-learning.   Keywords  Video Annotation Tool, Job Interview, Visualization.   1. INTRODUCTION  In the job interview scene, not only verbal communication but also  non-verbal communication is important [1]. To learn such job  interview, Furukawa et al. developed a multi-angle video  annotation tool with layout editing function [2]. Pardo et al.  utilized video annotation tool to identify learning strategies [3].   On the other hand, analysis of students' learning behaviors plays  an important role to realize adaptive and effective feedback. For  example, Bouchet et al. employed a differential sequence mining  technique to identify activity patterns between student groups [4].  Peckham et al. described how multidimensional k-means  clustering combined with Bloom's Taxonomy can be used to  determine positive and negative cognitive skill sets [5]. Sharma et  al. developed a tool that provides a visual-aid superimposed on the  video [6].   In this paper, we describe video annotation tool for learning job  interview and its feedback mechanism that visualizes difference  between learners.    2. VIDEO ANNOTATION TOOL  For the better understanding of job interview, it is useful to  evaluate job interview scenes of others. For this purpose, we  developed a video annotation tool. This tool was developed as a  standalone application that runs on the Mac OS. Using the  proposed tool, the learners write down what they have noticed in  the video.    Figure 1 shows the interface of the video annotation tool. This tool  can play multi-angle video as shown on the left side of the figure.  In the upper right part of the tool, the learners can set the start time   and the end time of the annotation.  In the center right part of the  tool, the learners can mark the attention area in the video by  drawing a rectangle. In the lower right part of the tool, the learners  can describe what they have noticed. By clicking the bottom  button, the descriptions are saved in CSV format, which is  readable by statistical analysis tools such as R.       Figure 1. Video annotation tool.   3. EXPERIMENT  3.1 Data Acquisition   To evaluate the proposed tool, evaluation experiment was carried  out. An interview scene of Chinese student was used in the  experiment. The interview scene was taken by 3 cameras, which  were placed to take interviewer, interviewee, and both of them.  The questions asked by the interviewer were shown in Table 1.    21 participants annotated the video. 7 of them are Japanese  students, and 7 of them are Chinese students. The other 7 have  experience of evaluating interviewees. These data are used to see  the difference of viewpoints between students and interviewers.   Table 1. Questions in the job interview.   Please introduce yourself.   What is the thing you have been devoted to   What did you major in at university   What is the theme of the graduation thesis   Do you have any job experience   What are you interested in lately   What is the reason for you to apply for us   What do you want to do with us   Please tell me your vision after 5 years.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029444     3.2 Analysis  To visualize the difference of descriptions, the proposed tool uses  correspondence analysis. The correspondence analysis is a method  that is widely used in the natural language processing. At first, by  morphological analysis of descriptions, only the nouns are  extracted. Then, 60 keywords that are effective to distinguish user  groups are extracted. The frequency of these keywords is  calculated for each learner, and reduced to two-dimensional  vectors using correspondence analysis. The locations of the 60  keywords in the same two-dimensional space are also calculated.    The results of correspondence analysis are shown in Figure 2. In  the figure, the learners () are located based on the similarity of  descriptions. J1 - J7 indicate Japanese students. C1 - C7 indicate  Chinese students. I1 - I7 have experience as interviewers. The  related keywords () are also shown in the figure.   In the lower left part of the tool, there are I4, I5, I6 and I7, who  have experience as interviewers. There are keywords such as    Graduation thesis ,  Answer  and  Reason  in this area. This  implies that they are paying attention to the content of the answers.  In the center left part of the tool, there are C1, C3, C6 and C7, who  are Chinese students. There are keywords such as  Voice ,  Eye  direction  and  Greeting  in this area. This implies that they are  paying attention to the appearance of the interviewee. In the upper  left part of the tool, there are J3, J4 and J6, who are Japanese  students. There are keywords such as  Door ,  Room  and   Impoliteness  in this area. This implies that they are paying  attention to the behavior when the interviewee enters the room.  These results are useful to see the characteristics of learners.     Figure 2. Results of correspondence analysis.   4. FEEDBACK TO LEARNERS  Figure 2 is used to give feedback to learners. After a learner  annotates the video, the results of correspondence analysis are  shown including the learner's data. By the results, the learner can  understand the characteristics of his/her descriptions among the  others.    When the user ID in Figure 2 is clicked, related descriptions are  shown as Figure 3. The descriptions of selected user are shown in  the lower right part of the window. Since the overlay of the   descriptions on the video is realized by SMIL (Synchronized  Multimedia Integration Language), the descriptions can be  displayed on a Web browser.      Figure 3. Playback of descriptions.   5. CONCLUSION  In this paper, video annotation tool for learning job interview is  proposed, and preliminary results of evaluation experiments were  shown. Using the proposed tool, the learner can understand the  characteristics of his/her descriptions among the others, and can  understand the viewpoints that the learner has not noticed but the  others have noticed. The detailed evaluation of the proposed tool  will be the future work.   6. REFERENCES  [1] von Raffer-Engel, W. (ed.). 1980. Aspects of Nonverbal   Communication. Swets & Zeitlinger.    [2] Furukawa, M., Yaginuma, Y., Yamada, T. 2004. Developing  a multi-angle video annotation package with layout editing.  In Proceedings of the World Conference on Educational  Multimedia, Hypermedia & Telecommunications, 1773-1778.   [3] Pardo, A., Mirriahi, N., Dawson, S., Zhao, Y., Zhao, A. and  Gasevic, D. 2015. Identifying Learning Strategies Associated  with Active use of Video Annotation Software. In  Proceedings of the 5th International Conference on Learning  Analytics & Knowledge, 255- 259   [4] Bouchet, F., Kinnebrew, J., Biswas, G. and Azevedo, R. 2012.  Identifying Students' Characteristic Learning Behaviors in an  Intelligent Tutoring System Fostering Self-Regulated  Learning. In Proceedings of the 5th International Conference  on Educational Data Mining, 65-72.   [5] Peckham, T. and McCalla, G. 2012. Mining Student Behavior  Patterns in Reading Comprehension Tasks. In Proceedings of  the 5th International Conference on Educational Data Mining,  87-94.   [6] Sharma, K., Alavi, H., Jermann, P., and Dillenbourg, P.  2016.  A Gaze-based Learning Analytics Model: In-Video Visual  Feedback to Improve Learners Attention in MOOCs. In  Proceedings of the 6th International Learning Analytics and  Knowledge Conference, 417-421.     "}
{"index":{"_id":"81"}}
{"datatype":"inproceedings","key":"Oi:2017:RFE:3027385.3029445","author":"Oi, Misato and Yamada, Masanori and Okubo, Fumiya and Shimada, Atsushi and Ogata, Hiroaki","title":"Reproducibility of Findings from Educational Big Data: A Preliminary Study","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"536--537","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029445","doi":"10.1145/3027385.3029445","acmid":"3029445","publisher":"ACM","address":"New York, NY, USA","keywords":"e-book, educational big data, reproducibility","Abstract":"In this paper, we examined whether previous findings on educational big data consisting of e-book logs from a given academic course can be reproduced with different data from other academic courses. The previous findings showed that (1) students who attained consistently good achievement more frequently browsed different e-books and their pages than low achievers and that (2) this difference was found only for logs of preparation for course sessions (preview), not for reviewing material (review). Preliminarily, we analyzed e-book logs from four courses. The results were reproduced in only one course and only partially, that is, (1) high achievers more frequently changed e-books than low achievers (2) for preview. This finding suggests that to allow effective usage of learning and teaching analyses, we need to carefully construct an educational environment to ensure reproducibility.","pdf":"Reproducibility of Findings from Educational Big Data:   A Preliminary Study      Misato Oi  Kyushu University    Nishi-ku, Fukuoka, Japan  oimisato@gmail.com  Masanori Yamada  Kyushu University    Nishi-ku, Fukuoka, Japan  mark@mark-lab.net    Fumiya Okubo  Kyushu University   Nishi-ku, Fukuoka, Japan  fokubo@artsci.kyushu-u.ac.jp   Atsushi Shimada  Kyushu University   Nishi-ku, Fukuoka, Japan  atsushi@limu.ait.kyushu-u.ac.jp   Hiroaki Ogata   Kyushu University   Nishi-ku, Fukuoka, Japan  hiroaki.ogata@gmail.com   ABSTRACT  In this paper, we examined whether previous findings on  educational big data consisting of e-book logs from a given  academic course can be reproduced with different data from other  academic courses. The previous findings showed that (1) students  who attained consistently good achievement more frequently  browsed different e-books and their pages than low achievers and  that (2) this difference was found only for logs of preparation for  course sessions (preview), not for reviewing material (review).  Preliminarily, we analyzed e-book logs from four courses. The  results were reproduced in only one course and only partially, that  is, (1) high achievers more frequently changed e-books than low  achievers (2) for preview. This finding suggests that to allow  effective usage of learning and teaching analyses, we need to  carefully construct an educational environment to ensure  reproducibility.   CCS Concepts   Applied computing~Education  Applied computing~E- learning  Applied computing~Distance learning   Keywords  E-book; Educational big data; reproducibility.   1. INTRODUCTION  In recent years, many countries have implemented and begun  assessment of information and communication technology (ICT) based education and learning materials in schools, especially of  electronic textbooks, called e-(text) books [3]. The present study  focuses on one aspect of e-book use in an educational environment,  that is, the digital footprints of students, which can be aggregated  into educational big data. We can use such data to examine student  performance patterns [1] to help teachers revise courses [2].  To improve teaching and learning, Kyushu University introduced a  single platform learning system (M2B). An e-book system  (BookLooper) of it enables students to browse e-book materials  before/during/after lectures, anywhere and anytime, using their PC  or smartphone; At the end of 2015 it collected approximately  5,320,000 log from approximately 20,000 students. We utilize this   educational big data in our research, including analysis of browsing  patterns against quiz scores [e.g., 6], investigation of effective  learning behavior [e.g., 4. 7], and predictive modeling [e.g., 5].   2.  RESEARCH QUESTION  Ensuring generality and reproducibility of analyses is important for  big data analytics, especially if for practical usage in the education  environment. For example, findings obtained from the logs of an  academic course should be reproducible in other courses. To  address this issue as a preliminary step, we examined our previous  findings regarding logs of a course from educational big data [4]  using data from other courses. We [4] used e-book logs to  investigate characteristics of the learning behavior of high and low  achievers enrolled in an Information Science course in Kyushu  University. We categorized e-book logs as follows: if a log was  recorded before a class session in which the same e-book was used  as a textbook, it was a preview log, and if after, a review log. We  reported that (1) students who attained consistently good  achievement more frequently switched between different e-books  and different pages within e-books than low achievers, but that (2)  this difference was found only for preview logs, not review logs,  and also that (3) there was no significant difference in duration of  browsing between high and low achievers. In the present study, we  investigate whether these results were reproduced in other teachers  Information Science courses using the same e-books as textbooks  as in the original course studied in [4].   3. METHODS  Preliminarily, we analyzed logs from four Information Science  courses (from 2015.04.13 to 2015.07.30). The objective of these  courses was the same (i.e., understanding fundamentals of ICT),  and they used the same series of e-books as textbooks. Table 1  shows details of the courses, all of which were taught by teachers  other than the teacher in [4]. We analyzed all of the logs from  students who attended the courses. The numbers of logs per course  are shown in Table 1. In the same way as [4], we calculated the  following three measurements per hour per student, based on the e- book logs. Change: how many times a student changed e-books.  Page flip: how many pages of the e-book a student flipped.  Duration: how many seconds a student browsed a given e-book for.    Table 1. Details of the courses   Course ID   1 2 3 4  # of sessions 14 14 15 15  # of used e-books  12 12 14 15  # of students 29 48 120 44  # of logs 43,788 38,720 207,106 54,725   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.  Copyright is held by the owner/author(s)  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029445     Table 2. Proportion of grades    Grade Course ID 1 2 3 4  A 45 65 73 43  B 21 23 20 45  C 14 10 3 5  D 17 2 1 5  F 3 0 3 2      We summed each of these for preview and review for each student.  The students were categorized into five groups according to their  grades in the course: A (excellent), B, C, D, and F (failure). Table  2 breaks down grades proportionally by course.   4. RESULTS  All statistical analyses were conducted using the Statistical  Packages for the Social Sciences (SPSS; version 23; IBM). To  examine whether the previous findings were reproducible in each  of the additional courses and to identify any differences between  the courses, we carried out three-way mixed-design ANOVA with  course (4) and grade (5) as between factors and preview/review (2)  as a within factor. This analysis was performed for each  measurement. If the previous findings were to be replicated, a  difference between high and low achievers was expected only for  preview, that is, a significant interaction between grade and  preview/review was expected.   The results show that only change had a significant three-way  interaction, F(11, 222) = 2.09, p = .022, not page flip or duration,  F(11, 222) = 0.79, p = .653; F(11, 222) = 1.75, p = .064. Figure 1  shows average change frequency for all students across the four  courses. For each, two-way ANOVAs with grade (5) as a between  factor and preview/review (2) as a within factor were carried out.  Course 1 shows a significant interaction between grade and  preview/review, F(4, 24) = 3.14, p = .033, but the other courses do  not, Course 2: F(3, 44) = 2,52, p = .070; 3: F(4, 115) = 1.87, p  = .121; 4: F(4, 39) = 0.37, p = .828. For Course 1, multiple  comparisons with Bonferroni correction showed for preview, the  group with A grades showed significantly more frequent changes  than the D group (p < .05). In contrast, for review, there was no  significant difference among the groups for Course 1.   5. DISCUSSION  In the present study, we examined whether the previous findings  for e-book change frequency in one academic course were  reproducible in other academic courses. Results (1) and (2) were  partially replicated: the high achievers (group A) more frequently  changed their e-books than the low achievers (group D) only for  preview, not review. However, this difference was found only in  Course 1, not Course 2, 3, or 4. The reason the findings were only  partially reproduced could be explained by several factors. For  example, as shown in the breakdown by grade (Table 2), grading      criteria were not consistent throughout the courses, and so  characteristics of high achievers and low achievers may be  different among the courses. That might produce large  discrepancies within each group, reflected in the large standard  deviations in the results (Figure 1).   6. CONCLUSION  Our results suggest that for effective application of learning and  teaching analyses of educational big data, we need to carefully set  the educational environment. Employing rubrics may be one way  of doing so.   7. ACKNOWLEDGMENTS  These research results have been achieved under the theme of  Research and Development on Fundamental and Utilization  Technologies for Social Big Data (178A03), as Commissioned  Research for the National Institute of Information and  Communications Technology (NICT), Japan.  8. REFERENCES  [1] Daniel, B. 2015. Big data and analytics in higher education:   Opportunities and challenges. British J Educ Technol, 46, 5  (Sept. 2015), 904-920. DOI=10.1111/bjet.12230   [2] Lockyer, L., Heathcote, E., and Dawson, S. 2013. Informing  pedagogical action: Aligning learning analytics with learning  design. Am Behav Sci, 57, 10 (Sept. 2013), 1439-1459.  DOI=10.1177/0002764213479367   [3] Nakajima, T., Shinohara, S., and Tamura, Y. 2013. Typical  functions of e-Textbook, implementation, and compatibility  verification with use of ePub3 materials. Procedia Comp Sci,  22 (2013), 1344-1353. DOI=10.1016/j.procs.2013.09.223   [4] Oi, M., Okubo, F., Shimada, A., Yin, C., and Ogata, H. 2015.  Analysis of preview and review patterns in undergraduates  e-book logs. Proceedings of the 23rd ICCE (Hangzhou,  China, November 30  December 4, 2015). 166-171.   [5] Okubo, F., Shimada, A., and Yin, C. 2015. Visualization and  prediction of learning activities by using discrete graphs.  Proceedings of the 23rd ICCE (Hangzhou, China, November  30  December 4, 2015). 739-744.   [6] Shimada, A., Okubo, F., and Ogata, H. 2016. Browsing- pattern mining from e-book logs with non-negative matrix  factorization. Proceedings of the 9th EDM (Raleigh, NC,  USA, June 29  July 02, 2016). 636-637.   [7] Yamada, M., Yin, C., Shimada, A., Kojima, K., Okubo, F.,  and Ogata, H. 2015. Preliminary research on self-regulated  learning and learning logs in a ubiquitous learning  environment, ICALT2015 (Hualien, Taiwan, July 6  July 9  2015). 93-95. DOI=10.1109/ICALT.2015.74    ID 1 ID 2 ID 3 Preview ID 4 Preview Preview Preview   Figure 1. Averages of frequency of changes throughout students in four courses.   Review Review Review Review    <<   /ASCII85EncodePages false   /AllowTransparency false   /AutoPositionEPSFiles true   /AutoRotatePages /None   /Binding /Left   /CalGrayProfile (Dot Gain 20%)   /CalRGBProfile (sRGB IEC61966-2.1)   /CalCMYKProfile (U.S. Web Coated 050SWOP051 v2)   /sRGBProfile (sRGB IEC61966-2.1)   /CannotEmbedFontPolicy /Error   /CompatibilityLevel 1.4   /CompressObjects /Tags   /CompressPages true   /ConvertImagesToIndexed true   /PassThroughJPEGImages true   /CreateJobTicket false   /DefaultRenderingIntent /Default   /DetectBlends true   /DetectCurves 0.0000   /ColorConversionStrategy /CMYK   /DoThumbnails false   /EmbedAllFonts true   /EmbedOpenType false   /ParseICCProfilesInComments true   /EmbedJobOptions true   /DSCReportingLevel 0   /EmitDSCWarnings false   /EndPage -1   /ImageMemory 1048576   /LockDistillerParams false   /MaxSubsetPct 100   /Optimize true   /OPM 1   /ParseDSCComments true   /ParseDSCCommentsForDocInfo true   /PreserveCopyPage true   /PreserveDICMYKValues true   /PreserveEPSInfo true   /PreserveFlatness true   /PreserveHalftoneInfo false   /PreserveOPIComments true   /PreserveOverprintSettings true   /StartPage 1   /SubsetFonts true   /TransferFunctionInfo /Apply   /UCRandBGInfo /Preserve   /UsePrologue false   /ColorSettingsFile ()   /AlwaysEmbed [ true   ]   /NeverEmbed [ true   ]   /AntiAliasColorImages false   /CropColorImages true   /ColorImageMinResolution 300   /ColorImageMinResolutionPolicy /OK   /DownsampleColorImages true   /ColorImageDownsampleType /Bicubic   /ColorImageResolution 300   /ColorImageDepth -1   /ColorImageMinDownsampleDepth 1   /ColorImageDownsampleThreshold 1.50000   /EncodeColorImages true   /ColorImageFilter /DCTEncode   /AutoFilterColorImages true   /ColorImageAutoFilterStrategy /JPEG   /ColorACSImageDict <<     /QFactor 0.15     /HSamples [1 1 1 1] /VSamples [1 1 1 1]   >>   /ColorImageDict <<     /QFactor 0.15     /HSamples [1 1 1 1] /VSamples [1 1 1 1]   >>   /JPEG2000ColorACSImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 30   >>   /JPEG2000ColorImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 30   >>   /AntiAliasGrayImages false   /CropGrayImages true   /GrayImageMinResolution 300   /GrayImageMinResolutionPolicy /OK   /DownsampleGrayImages true   /GrayImageDownsampleType /Bicubic   /GrayImageResolution 300   /GrayImageDepth -1   /GrayImageMinDownsampleDepth 2   /GrayImageDownsampleThreshold 1.50000   /EncodeGrayImages true   /GrayImageFilter /DCTEncode   /AutoFilterGrayImages true   /GrayImageAutoFilterStrategy /JPEG   /GrayACSImageDict <<     /QFactor 0.15     /HSamples [1 1 1 1] /VSamples [1 1 1 1]   >>   /GrayImageDict <<     /QFactor 0.15     /HSamples [1 1 1 1] /VSamples [1 1 1 1]   >>   /JPEG2000GrayACSImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 30   >>   /JPEG2000GrayImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 30   >>   /AntiAliasMonoImages false   /CropMonoImages true   /MonoImageMinResolution 1200   /MonoImageMinResolutionPolicy /OK   /DownsampleMonoImages true   /MonoImageDownsampleType /Bicubic   /MonoImageResolution 1200   /MonoImageDepth -1   /MonoImageDownsampleThreshold 1.50000   /EncodeMonoImages true   /MonoImageFilter /CCITTFaxEncode   /MonoImageDict <<     /K -1   >>   /AllowPSXObjects false   /CheckCompliance [     /None   ]   /PDFX1aCheck false   /PDFX3Check false   /PDFXCompliantPDFOnly false   /PDFXNoTrimBoxError true   /PDFXTrimBoxToMediaBoxOffset [     0.00000     0.00000     0.00000     0.00000   ]   /PDFXSetBleedBoxToMediaBox true   /PDFXBleedBoxToTrimBoxOffset [     0.00000     0.00000     0.00000     0.00000   ]   /PDFXOutputIntentProfile ()   /PDFXOutputConditionIdentifier ()   /PDFXOutputCondition ()   /PDFXRegistryName ()   /PDFXTrapped /False    /CreateJDFFile false   /Description <<     /ARA <FEFF06270633062A062E062F0645002006470630064700200627064406250639062F0627062F0627062A002006440625064606340627062100200648062B062706260642002000410064006F00620065002000500044004600200645062A064806270641064206290020064406440637062806270639062900200641064A00200627064406450637062706280639002006300627062A0020062F0631062C0627062A002006270644062C0648062F0629002006270644063906270644064A0629061B0020064A06450643064600200641062A062D00200648062B0627062606420020005000440046002006270644064506460634062306290020062806270633062A062E062F062706450020004100630072006F0062006100740020064800410064006F006200650020005200650061006400650072002006250635062F0627063100200035002E0030002006480627064406250635062F062706310627062A0020062706440623062D062F062B002E0635062F0627063100200035002E0030002006480627064406250635062F062706310627062A0020062706440623062D062F062B002E>     /BGR <FEFF04180437043f043e043b043704320430043904420435002004420435043704380020043d0430044104420440043e0439043a0438002c00200437043000200434043000200441044a0437043404300432043004420435002000410064006f00620065002000500044004600200434043e043a0443043c0435043d04420438002c0020043c0430043a04410438043c0430043b043d043e0020043f044004380433043e04340435043d04380020043704300020043204380441043e043a043e043a0430044704350441044204320435043d0020043f04350447043004420020043704300020043f044004350434043f0435044704300442043d04300020043f043e04340433043e0442043e0432043a0430002e002000200421044a04370434043004340435043d043804420435002000500044004600200434043e043a0443043c0435043d044204380020043c043e0433043004420020043404300020044104350020043e0442043204300440044f0442002004410020004100630072006f00620061007400200438002000410064006f00620065002000520065006100640065007200200035002e00300020043800200441043b0435043404320430044904380020043204350440044104380438002e>     /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e9ad88d2891cf76845370524d53705237300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>     /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc9ad854c18cea76845370524d5370523786557406300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>     /CZE <FEFF005400610074006f0020006e006100730074006100760065006e00ed00200070006f0075017e0069006a007400650020006b0020007600790074007600e101590065006e00ed00200064006f006b0075006d0065006e0074016f002000410064006f006200650020005000440046002c0020006b00740065007200e90020007300650020006e0065006a006c00e90070006500200068006f006400ed002000700072006f0020006b00760061006c00690074006e00ed0020007400690073006b00200061002000700072006500700072006500730073002e002000200056007900740076006f01590065006e00e900200064006f006b0075006d0065006e007400790020005000440046002000620075006400650020006d006f017e006e00e90020006f007400650076015900ed007400200076002000700072006f006700720061006d0065006300680020004100630072006f00620061007400200061002000410064006f00620065002000520065006100640065007200200035002e0030002000610020006e006f0076011b006a016100ed00630068002e>     /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000620065006400730074002000650067006e006500720020007300690067002000740069006c002000700072006500700072006500730073002d007500640073006b007200690076006e0069006e00670020006100660020006800f8006a0020006b00760061006c0069007400650074002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>     /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200076006f006e002000640065006e0065006e002000530069006500200068006f006300680077006500720074006900670065002000500072006500700072006500730073002d0044007200750063006b0065002000650072007a0065007500670065006e0020006d00f60063006800740065006e002e002000450072007300740065006c006c007400650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000410064006f00620065002000520065006100640065007200200035002e00300020006f0064006500720020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>     /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f00730020005000440046002000640065002000410064006f0062006500200061006400650063007500610064006f00730020007000610072006100200069006d0070007200650073006900f3006e0020007000720065002d0065006400690074006f007200690061006c00200064006500200061006c00740061002000630061006c0069006400610064002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>     /ETI <FEFF004b00610073007500740061006700650020006e0065006900640020007300e4007400740065006900640020006b00760061006c006900740065006500740073006500200074007200fc006b006900650065006c007300650020007000720069006e00740069006d0069007300650020006a0061006f006b007300200073006f00620069006c0069006b0065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740069006400650020006c006f006f006d006900730065006b0073002e00200020004c006f006f0064007500640020005000440046002d0064006f006b0075006d0065006e00740065002000730061006100740065002000610076006100640061002000700072006f006700720061006d006d006900640065006700610020004100630072006f0062006100740020006e0069006e0067002000410064006f00620065002000520065006100640065007200200035002e00300020006a00610020007500750065006d006100740065002000760065007200730069006f006f006e00690064006500670061002e000d000a>     /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f00620065002000500044004600200070006f0075007200200075006e00650020007100750061006c0069007400e90020006400270069006d007000720065007300730069006f006e00200070007200e9007000720065007300730065002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>     /GRE <FEFF03a703c103b703c303b903bc03bf03c003bf03b903ae03c303c403b5002003b103c503c403ad03c2002003c403b903c2002003c103c503b803bc03af03c303b503b903c2002003b303b903b1002003bd03b1002003b403b703bc03b903bf03c503c103b303ae03c303b503c403b5002003ad03b303b303c103b103c603b1002000410064006f006200650020005000440046002003c003bf03c5002003b503af03bd03b103b9002003ba03b103c42019002003b503be03bf03c703ae03bd002003ba03b103c403ac03bb03bb03b703bb03b1002003b303b903b1002003c003c103bf002d03b503ba03c403c503c003c903c403b903ba03ad03c2002003b503c103b303b103c303af03b503c2002003c503c803b703bb03ae03c2002003c003bf03b903cc03c403b703c403b103c2002e0020002003a403b10020005000440046002003ad03b303b303c103b103c603b1002003c003bf03c5002003ad03c703b503c403b5002003b403b703bc03b903bf03c503c103b303ae03c303b503b9002003bc03c003bf03c103bf03cd03bd002003bd03b1002003b103bd03bf03b903c703c403bf03cd03bd002003bc03b5002003c403bf0020004100630072006f006200610074002c002003c403bf002000410064006f00620065002000520065006100640065007200200035002e0030002003ba03b103b9002003bc03b503c403b103b303b503bd03ad03c303c403b503c103b503c2002003b503ba03b403cc03c303b503b903c2002e>     /HEB <FEFF05D405E905EA05DE05E905D5002005D105D405D205D305E805D505EA002005D005DC05D4002005DB05D305D9002005DC05D905E605D505E8002005DE05E105DE05DB05D9002000410064006F006200650020005000440046002005D405DE05D505EA05D005DE05D905DD002005DC05D405D305E405E105EA002005E705D305DD002D05D305E405D505E1002005D005D905DB05D505EA05D905EA002E002005DE05E105DE05DB05D90020005000440046002005E905E005D505E605E805D5002005E005D905EA05E005D905DD002005DC05E405EA05D905D705D4002005D105D005DE05E605E205D505EA0020004100630072006F006200610074002005D5002D00410064006F00620065002000520065006100640065007200200035002E0030002005D505D205E805E105D005D505EA002005DE05EA05E705D305DE05D505EA002005D905D505EA05E8002E05D005DE05D905DD002005DC002D005000440046002F0058002D0033002C002005E205D905D905E005D5002005D105DE05D305E805D905DA002005DC05DE05E905EA05DE05E9002005E905DC0020004100630072006F006200610074002E002005DE05E105DE05DB05D90020005000440046002005E905E005D505E605E805D5002005E005D905EA05E005D905DD002005DC05E405EA05D905D705D4002005D105D005DE05E605E205D505EA0020004100630072006F006200610074002005D5002D00410064006F00620065002000520065006100640065007200200035002E0030002005D505D205E805E105D005D505EA002005DE05EA05E705D305DE05D505EA002005D905D505EA05E8002E>     /HRV (Za stvaranje Adobe PDF dokumenata najpogodnijih za visokokvalitetni ispis prije tiskanja koristite ove postavke.  Stvoreni PDF dokumenti mogu se otvoriti Acrobat i Adobe Reader 5.0 i kasnijim verzijama.)     /HUN <FEFF004b0069007600e1006c00f30020006d0069006e0151007300e9006701710020006e0079006f006d00640061006900200065006c0151006b00e90073007a00ed007401510020006e0079006f006d00740061007400e100730068006f007a0020006c006500670069006e006b00e1006200620020006d0065006700660065006c0065006c0151002000410064006f00620065002000500044004600200064006f006b0075006d0065006e00740075006d006f006b0061007400200065007a0065006b006b0065006c0020006100200062006500e1006c006c00ed007400e10073006f006b006b0061006c0020006b00e90073007a00ed0074006800650074002e0020002000410020006c00e90074007200650068006f007a006f00740074002000500044004600200064006f006b0075006d0065006e00740075006d006f006b00200061007a0020004100630072006f006200610074002000e9007300200061007a002000410064006f00620065002000520065006100640065007200200035002e0030002c0020007600610067007900200061007a002000610074007400f3006c0020006b00e9007301510062006200690020007600650072007a006900f3006b006b0061006c0020006e00790069007400680061007400f3006b0020006d00650067002e>     /ITA <FEFF005500740069006c0069007a007a006100720065002000710075006500730074006500200069006d0070006f007300740061007a0069006f006e00690020007000650072002000630072006500610072006500200064006f00630075006d0065006e00740069002000410064006f00620065002000500044004600200070006900f900200061006400610074007400690020006100200075006e00610020007000720065007300740061006d0070006100200064006900200061006c007400610020007100750061006c0069007400e0002e0020004900200064006f00630075006d0065006e007400690020005000440046002000630072006500610074006900200070006f00730073006f006e006f0020006500730073006500720065002000610070006500720074006900200063006f006e0020004100630072006f00620061007400200065002000410064006f00620065002000520065006100640065007200200035002e003000200065002000760065007200730069006f006e006900200073007500630063006500730073006900760065002e>     /JPN <FEFF9ad854c18cea306a30d730ea30d730ec30b951fa529b7528002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a306b306f30d530a930f330c8306e57cb30818fbc307f304c5fc59808306730593002>     /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020ace0d488c9c80020c2dcd5d80020c778c1c4c5d00020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>     /LTH <FEFF004e006100750064006f006b0069007400650020016100690075006f007300200070006100720061006d006500740072007500730020006e006f0072011700640061006d00690020006b0075007200740069002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b00750072006900650020006c0061006200690061007500730069006100690020007000720069007400610069006b007900740069002000610075006b01610074006f00730020006b006f006b007900620117007300200070006100720065006e006700740069006e00690061006d00200073007000610075007300640069006e0069006d00750069002e0020002000530075006b0075007200740069002000500044004600200064006f006b0075006d0065006e007400610069002000670061006c006900200062016b007400690020006100740069006400610072006f006d00690020004100630072006f006200610074002000690072002000410064006f00620065002000520065006100640065007200200035002e0030002000610072002000760117006c00650073006e0117006d00690073002000760065007200730069006a006f006d00690073002e>     /LVI <FEFF0049007a006d0061006e0074006f006a00690065007400200161006f00730020006900650073007400610074012b006a0075006d00750073002c0020006c0061006900200076006500690064006f00740075002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006100730020006900720020012b00700061016100690020007000690065006d01130072006f00740069002000610075006700730074006100730020006b00760061006c0069007401010074006500730020007000690072006d007300690065007300700069006501610061006e006100730020006400720075006b00610069002e00200049007a0076006500690064006f006a006900650074002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006f002000760061007200200061007400760113007200740020006100720020004100630072006f00620061007400200075006e002000410064006f00620065002000520065006100640065007200200035002e0030002c0020006b0101002000610072012b00200074006f0020006a00610075006e0101006b0101006d002000760065007200730069006a0101006d002e>     /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken die zijn geoptimaliseerd voor prepress-afdrukken van hoge kwaliteit. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)     /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d00200065007200200062006500730074002000650067006e0065007400200066006f00720020006600f80072007400720079006b006b0073007500740073006b00720069006600740020006100760020006800f800790020006b00760061006c0069007400650074002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002000730065006e006500720065002e>     /POL <FEFF0055007300740061007700690065006e0069006100200064006f002000740077006f0072007a0065006e0069006100200064006f006b0075006d0065006e007400f300770020005000440046002000700072007a0065007a006e00610063007a006f006e00790063006800200064006f002000770079006400720075006b00f30077002000770020007700790073006f006b00690065006a0020006a0061006b006f015b00630069002e002000200044006f006b0075006d0065006e0074007900200050004400460020006d006f017c006e00610020006f007400770069006500720061010700200077002000700072006f006700720061006d006900650020004100630072006f00620061007400200069002000410064006f00620065002000520065006100640065007200200035002e0030002000690020006e006f00770073007a0079006d002e>     /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f0062006500200050004400460020006d00610069007300200061006400650071007500610064006f00730020007000610072006100200070007200e9002d0069006d0070007200650073007300f50065007300200064006500200061006c007400610020007100750061006c00690064006100640065002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>     /RUM <FEFF005500740069006c0069007a00610163006900200061006300650073007400650020007300650074010300720069002000700065006e007400720075002000610020006300720065006100200064006f00630075006d0065006e00740065002000410064006f006200650020005000440046002000610064006500630076006100740065002000700065006e0074007200750020007400690070010300720069007200650061002000700072006500700072006500730073002000640065002000630061006c006900740061007400650020007300750070006500720069006f006100720103002e002000200044006f00630075006d0065006e00740065006c00650020005000440046002000630072006500610074006500200070006f00740020006600690020006400650073006300680069007300650020006300750020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e00300020015f00690020007600650072007300690075006e0069006c006500200075006c0074006500720069006f006100720065002e>     /RUS <FEFF04180441043f043e043b044c04370443043904420435002004340430043d043d044b04350020043d0430044104420440043e0439043a043800200434043b044f00200441043e043704340430043d0438044f00200434043e043a0443043c0435043d0442043e0432002000410064006f006200650020005000440046002c0020043c0430043a04410438043c0430043b044c043d043e0020043f043e04340445043e0434044f04490438044500200434043b044f00200432044b0441043e043a043e043a0430044704350441044204320435043d043d043e0433043e00200434043e043f0435044704300442043d043e0433043e00200432044b0432043e04340430002e002000200421043e043704340430043d043d044b04350020005000440046002d0434043e043a0443043c0435043d0442044b0020043c043e0436043d043e0020043e0442043a0440044b043204300442044c002004410020043f043e043c043e0449044c044e0020004100630072006f00620061007400200438002000410064006f00620065002000520065006100640065007200200035002e00300020043800200431043e043b043504350020043f043e04370434043d043804450020043204350440044104380439002e>     /SKY <FEFF0054006900650074006f0020006e006100730074006100760065006e0069006100200070006f0075017e0069007400650020006e00610020007600790074007600e100720061006e0069006500200064006f006b0075006d0065006e0074006f0076002000410064006f006200650020005000440046002c0020006b0074006f007200e90020007300610020006e0061006a006c0065007001610069006500200068006f0064006900610020006e00610020006b00760061006c00690074006e00fa00200074006c0061010d00200061002000700072006500700072006500730073002e00200056007900740076006f00720065006e00e900200064006f006b0075006d0065006e007400790020005000440046002000620075006400650020006d006f017e006e00e90020006f00740076006f00720069016500200076002000700072006f006700720061006d006f006300680020004100630072006f00620061007400200061002000410064006f00620065002000520065006100640065007200200035002e0030002000610020006e006f0076016100ed00630068002e>     /SLV <FEFF005400650020006e006100730074006100760069007400760065002000750070006f0072006100620069007400650020007a00610020007500730074007600610072006a0061006e006a006500200064006f006b0075006d0065006e0074006f0076002000410064006f006200650020005000440046002c0020006b006900200073006f0020006e0061006a007000720069006d00650072006e0065006a016100690020007a00610020006b0061006b006f0076006f00730074006e006f0020007400690073006b0061006e006a00650020007300200070007200690070007200610076006f0020006e00610020007400690073006b002e00200020005500730074007600610072006a0065006e006500200064006f006b0075006d0065006e0074006500200050004400460020006a00650020006d006f0067006f010d00650020006f0064007000720065007400690020007a0020004100630072006f00620061007400200069006e002000410064006f00620065002000520065006100640065007200200035002e003000200069006e0020006e006f00760065006a01610069006d002e>     /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f00740020006c00e400680069006e006e00e4002000760061006100740069007600610061006e0020007000610069006e006100740075006b00730065006e002000760061006c006d0069007300740065006c00750074007900f6006800f6006e00200073006f00700069007600690061002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>     /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d002000e400720020006c00e4006d0070006c0069006700610020006600f60072002000700072006500700072006500730073002d007500740073006b00720069006600740020006d006500640020006800f600670020006b00760061006c0069007400650074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>     /TUR <FEFF005900fc006b00730065006b0020006b0061006c006900740065006c0069002000f6006e002000790061007a006401310072006d00610020006200610073006b013100730131006e006100200065006e0020006900790069002000750079006100620069006c006500630065006b002000410064006f006200650020005000440046002000620065006c00670065006c0065007200690020006f006c0075015f007400750072006d0061006b0020006900e70069006e00200062007500200061007900610072006c0061007201310020006b0075006c006c0061006e0131006e002e00200020004f006c0075015f0074007500720075006c0061006e0020005000440046002000620065006c00670065006c0065007200690020004100630072006f006200610074002000760065002000410064006f00620065002000520065006100640065007200200035002e003000200076006500200073006f006e0072006100730131006e00640061006b00690020007300fc007200fc006d006c00650072006c00650020006100e70131006c006100620069006c00690072002e>     /UKR <FEFF04120438043a043e0440043804410442043e043204430439044204350020044604560020043f043004400430043c043504420440043800200434043b044f0020044104420432043e04400435043d043d044f00200434043e043a0443043c0435043d044204560432002000410064006f006200650020005000440046002c0020044f043a04560020043d04300439043a04400430044904350020043f045604340445043e0434044f0442044c00200434043b044f0020043204380441043e043a043e044f043a04560441043d043e0433043e0020043f0435044004350434043404400443043a043e0432043e0433043e0020043404400443043a0443002e00200020042104420432043e04400435043d045600200434043e043a0443043c0435043d0442043800200050004400460020043c043e0436043d04300020043204560434043a0440043804420438002004430020004100630072006f006200610074002004420430002000410064006f00620065002000520065006100640065007200200035002e0030002004300431043e0020043f04560437043d04560448043e04570020043204350440044104560457002e>     /ENU (Use these settings to create Adobe PDF documents best suited for high-quality prepress printing.  Created PDF documents can be opened with Acrobat and Adobe Reader 5.0 and later.)   >>   /Namespace [     (Adobe)     (Common)     (1.0)   ]   /OtherNamespaces [     <<       /AsReaderSpreads false       /CropImagesToFrames true       /ErrorControl /WarnAndContinue       /FlattenerIgnoreSpreadOverrides false       /IncludeGuidesGrids false       /IncludeNonPrinting false       /IncludeSlug false       /Namespace [         (Adobe)         (InDesign)         (4.0)       ]       /OmitPlacedBitmaps false       /OmitPlacedEPS false       /OmitPlacedPDF false       /SimulateOverprint /Legacy     >>     <<       /AddBleedMarks false       /AddColorBars false       /AddCropMarks false       /AddPageInfo false       /AddRegMarks false       /ConvertColors /ConvertToCMYK       /DestinationProfileName ()       /DestinationProfileSelector /DocumentCMYK       /Downsample16BitImages true       /FlattenerPreset <<         /PresetSelector /MediumResolution       >>       /FormElements false       /GenerateStructure false       /IncludeBookmarks false       /IncludeHyperlinks false       /IncludeInteractive false       /IncludeLayers false       /IncludeProfiles false       /MultimediaHandling /UseObjectSettings       /Namespace [         (Adobe)         (CreativeSuite)         (2.0)       ]       /PDFXOutputIntentProfileSelector /DocumentCMYK       /PreserveEditing true       /UntaggedCMYKHandling /LeaveUntagged       /UntaggedRGBHandling /UseDocumentProfile       /UseDocumentBleed false     >>   ] >> setdistillerparams <<   /HWResolution [2400 2400]   /PageSize [612.000 792.000] >> setpagedevice     "}
{"index":{"_id":"82"}}
{"datatype":"inproceedings","key":"Schulte:2017:LSP:3027385.3029446","author":"Schulte, Jurgen and Fernandez de Mendonca, Pedro and Martinez-Maldonado, Roberto and Buckingham Shum, Simon","title":"Large Scale Predictive Process Mining and Analytics of University Degree Course Data","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"538--539","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029446","doi":"10.1145/3027385.3029446","acmid":"3029446","publisher":"ACM","address":"New York, NY, USA","keywords":"educational data mining, educational process visualization, learning analytics, predictive modeling, process mining","Abstract":"For students, in particular freshmen, the degree pathway from semester to semester is not that transparent, although students have a reasonable idea what courses are expected to be taken each semester. An often-pondered question by students is: what can I expect in the next semester? More precisely, given the commitment and engagement I presented in this particular course and the respective performance I achieved, can I expect a similar outcome in the next semester in the particular course I selected? Are the demands and expectations in this course much higher so that I need to adjust my commitment and engagement and overall workload if I expect a similar outcome? Is it better to drop a course to manage expectations rather than to (predictably) fail, and perhaps have to leave the degree altogether? Degree and course advisors and student support units find it challenging to provide evidence based advise to students. This paper presents research into educational process mining and student data analytics in a whole university scale approach with the aim of providing insight into the degree pathway questions raised above. The beta-version of our course level degree pathway tool has been used to shed light for university staff and students alike into our university's 1,300 degrees and associated 6 million course enrolments over the past 20 years.","pdf":"Large Scale Predictive Process Mining and Analytics of  University Degree Course Data  Jurgen Schulte1, Pedro Fernandez de Mendonca1,   Roberto Martinez-Maldonado2, Simon Buckingham Shum2   University of Technology Sydney - Faculty of Science (SciMERIT)1, Connected Intelligence Centre2    P.O. Box 123, Ultimo 2007, Australia  (Jurgen.Schulte, Pedro. FernandezdeMendonca, Roberto.Martinez-Maldonado, Simon.BuckinghamShum)@uts.edu  ABSTRACT For students, in particular freshmen, the degree pathway from  semester to semester is not that transparent, although students  have a reasonable idea what courses are expected to be taken each  semester. An often-pondered question by students is:  what can I  expect in the next semester More precisely, given the  commitment and engagement I presented in this particular course  and the respective performance I achieved, can I expect a similar  outcome in the next semester in the particular course I selected  Are the demands and expectations in this course much higher so  that I need to adjust my commitment and engagement and overall  workload if I expect a similar outcome Is it better to drop a  course to manage expectations rather than to (predictably) fail,  and perhaps have to leave the degree altogether Degree and  course advisors and student support units find it challenging to  provide evidence based advise to students. This paper presents  research into educational process mining and student data  analytics in a whole university scale approach with the aim of  providing insight into the degree pathway questions raised above.  The beta-version of our course level degree pathway tool has been  used to shed light for university staff and students alike into our  universitys 1,300 degrees and associated 6 million course  enrolments over the past 20 years.     CCS Concepts  Applied computing --- Enterprise computing --- Enterprise modelling   Computing methodologies --- Modelling and simulation --- Simulation types and techniques --- Visual analytics   Keywords Process mining; learning analytics; predictive modeling;  educational data mining, educational process visualization.   1. INTRODUCTION Learning is a pathway, comprising the main actors (students,  academics), events (courses) and outcomes (marks), which in a  degree occur in a designed sequence, within a limited timeline. As  cohorts of students pass through courses to complete a degree,  their performance varies, depending on their academic ability and  how well they study. However, the swiftness with which they  progress also depends on how well individual courses are taught,   how well courses are aligned horizontally as well as vertically,  and how well sequences of courses are placed within the degree  course structure. Naturally then, the degree structure has two  views and two related goals; students desire to pass through a  degree in an optimized way (maximum academic performance,  minimum time) and universities commitment to deliver the best  mix of disciplinary knowledge within degree time limits while  maintaining acceptable retention rates.   Degree performance is monitored by both university business  units and degree coordinators. From a business unit perspective  the degree input-output performance (level and rate of intake, and  graduation) and the retention rate are key degree performance  indicators. The degree coordinator has the faculty or school  interest in view, that is, teaching of disciplinary knowledge ought  to have ideal scaffolding so that a student at an initially accepted  entry level has a fair opportunity to pass through the degree,  following a recommended degree pathway, at an appropriate pace.  Ideally, the degree coordinator has intimate knowledge of the  material being taught in each course, and how disciplinary  knowledge and skills learned in one semester are further  developed in the following semester. Often though, this kind of  course level overview is difficult to achieve.   Attempts to mine educational processes of student cohorts so far  have been limited to small cohorts, such a single small degree  course of a few hundred students or sub-major of such degree [1].  There has been mixed success in extracting meaningful insights  through educational process mining [2], one of the major  obstacles being the volume of student data, even when limiting  datasets to sub-cohorts of degree courses. Other challenges  encountered have been profound heterogeneity and complexity  within datasets and concept drifting [3]. All this together make it  difficult interpret the outcome and diminishes the value of  intelligence that can be drawn from it   Our goal was to uncover statistically significant and meaningful  patterns in students course pathway choices, and to provide  student support units, degree and course coordinators with  longitudinal indicators that could be used to inform student  advice. The envisioned indicators could also help to support the  streamlining of course and subject content. The hypothesis is that  the more effectively students can be informed about what effort it  could take them (individually) to master future subjects and stages  in their course, the better their study experience will be, with  better student retention rates.   2. METHODOLOGY Student enrollment performance and progression data were  provided by the UTS Warehouse and Business Intelligence  division following required security and ethics clearance (UTS  HREC REF NO. ETH16-0338).   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author. Copyright is held by the owner/author(s). LAK '17, March 13-17, 2017, Vancouver, BC, Canada ACM 978-1-4503-4870-6/17/03. http://dx.doi.org/10.1145/3027385.3029446      Figure 1: Data and process mining system layout.   The dataset allowed us to mine historical data of 1,300 UTS  degree courses and some 16,000 course units taken by over  300,000 students in over 6 million enrolments. To our  knowledge, this makes it the largest educational progression  pathway study undertaken so far. We employed data and  process mining approaches [3], using tools such as KNIME,  Disco and PRoM (see [2,3] for details), data modelling in R,  and RStudios Shiny server for visualization and end user  access (Figure 1) to demonstrate a proof of concept of a  virtually real-time comprehensible presentation of complex  processes in large scale educational data.   3. DISCUSSION  It became apparent that the data would present challenges to the  envisaged process mining techniques. Firstly, students may repeat  courses, choose electives offered by different degrees, change  degrees or take time off to come back some later time (Figure 2).  The process mining with PRoM and analysis with Disco lead to  rather convoluted processes and an inflated number of processes  resulting in a difficult to interpret spaghetti diagram.     Figure 2: Simplified representation of degree complexity.    Boxes represent different courses in a degree, and colors the  faculties/schools by whom they are offered. Line thickness   represents a relative measure of student flow density.   Secondly, with a 6 million enrolments in the dataset, it was  inevitable for it to contain irregularities, as well as display  historical drift as degree and course names changed (Figure 3).  Education relies on innovation and re-inventing itself as society  evolves and expectations change.      Figure 3: Example of degree and concept drift. Of the six courses  (A to F) in Degree A one changes its name in year 2 (E1 E2),   Degree A then changes its name in year 3 to Degree B while  largely maintaining its overall structure, followed by further   course evolutions.   Process mining is particularly sensitive to such concept drift, but  in education, this is common, and indeed, often desirable for  courses to evolve. We found that these drift and process  complexity issues caused an almost exponential escalation in the  computing time required to extract meaningful sequences. Hence,  it is perhaps not surprising that educational process mining has to  date confined itself to small data samples and proof of principle  studies.   Consequently, we employed a heterogeneity and drift resilient  data mining and analytics approach. We employed KNIME to  interrogate the data for categorizable structure, drift sources and  patterns, and data wrangling for the production of a conveniently  accessible database for the statistical analytics tool (R). This  permitted process visualisation directly from raw data and scaled  well.  Even with the whole university dataset, this approach  permitted virtually real-time, interactive interrogation and  visualization, of both vertical and lateral degree course pathways  by a range of customers, e.g., students, student support units,  degree planners, course coordinators (Figure 4).   4. CONCLUSION  While originally promising, 20 years data on course pathways  proved intractable for conventional process mining, but was  amenable to pre-structured statistical analytics, with  implementation of an interactive querying and visualization tool.  The project is now preparing to extend the historical cohort data  representation to multistep (upward semester) forward projections  and forecasting. This will serve then as a base for the development  of meaningful degree health and course choice indicators.      Figure 4: Snapshot of cohort course pathway   predictor window of the real-time mining tool.   A following pilot with academics and university support units will  assist us to assess the tools potential to provide new insights into  progression choices, levels of success, and meaningfulness of  pathway indicators. Ultimately, our hope is to provide direct  guidance to students.   5. REFERENCES  [1] C. Romero,  S. Ventura, Educational data science in massive   open online courses, Wiley Interdisciplinary Reviews: Data  Mining and Knowledge Discovery,  2016.   [2] Pea-Ayala, A. 2014, 'Educational data mining: A survey and  a data mining-based analysis of recent works', Expert Systems  with Applications, vol. 41, no. 4 PART 1, pp. 14321462    [3] A. H. Cairns, B. Gueni, M. Fhima, A. Cairns, S. David,  N.  Khelifa, Process Mining in the Education Domain,  International Journal on Advances in Intelligent Systems, vol.  8 (1&2), pp. 219-233, 2015.   } } }  Year 1  Year 2  Year 3  Degree Code A Degree Code B  F1  E1  D1  C1  B1  A1  F1  E2  D1  C1  B1  A1  F2  E2  D1  C1  B2  A1  F3  E3  D2  C1  B3  A1  F3  E4  D2  C1  B3  A2  F3  E4  D2  C1  B4  A2  Years      "}
{"index":{"_id":"83"}}
{"datatype":"inproceedings","key":"Clow:2017:BFN:3027385.3029447","author":"Clow, Doug and Ferguson, Rebecca and Kitto, Kirsty and Cho, Yong-Sang and Sharkey, Mike and Aguerrebere, Cecilia","title":"Beyond Failure: The 2Nd LAK Failathon Poster","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"540--541","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029447","doi":"10.1145/3027385.3029447","acmid":"3029447","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics, evidence, learning analytics, learning from failure","Abstract":"This poster will be a chance for a wider LAK audience to engage with the 2nd LAK Failathon workshop. Both of these will build on the successful Failathon event in 2016 and extend beyond discussing individual experiences of failure to exploring how the field can improve, particularly regarding the creation and use of evidence. Failure in research is an increasingly hot topic, with high-profile crises of confidence in the published research literature in medicine and psychology. Among the major factors in this research crisis are the many incentives to report and publish only positive findings. These incentives prevent the field in general from learning from negative findings, and almost entirely preclude the publication of mistakes and errors. Thus providing an alternative forum for practitioners and researchers to learn from each other's failures can be very productive. The first LAK Failathon, held in 2016, provided just such an opportunity for researchers and practitioners to share their failures and negative findings in a lower-stakes environment, to help participants learn from each other's mistakes. It was very successful, and there was strong support for running it as an annual event. The 2nd LAK Failathon workshop will build on that success, with twin objectives to provide an environment for individuals to learn from each other's failures, and also to co-develop plans for how we as a field can better build and deploy our evidence base. This poster is an opportunity for wider feedback on the plans developed in the workshop, with interactive use of sticky notes to add new ideas and coloured dots to illustrate prioritisation. This broadens the participant base in this important work, which should improve the quality of the plans and the commitment of the community to delivering them","pdf":"Beyond Failure: The 2nd LAK Failathon Poster  Doug Clow1, Rebecca Ferguson1, Kirsty Kitto2, Yong-Sang Cho3,    Mike Sharkey4, Cecilia Aguerrebere5 1The Open University   Walton Hall, Milton Keynes  MK7 6AA, UK   {Firstname.Surname}@open.ac.uk     4Blackboard, Inc  Phoenix, AZ, USA   Mike.Sharkey@blackboard.com     2Queensland University of  Technology, School of Mathematical  Sciences, Level 7, P-Block, 2 George   Street, Brisbane, 4001, Australia  kirsty.kitto@qut.edu.au   3Korea Education & Research  Information Service   64 Dongnau-Ro, Dong-Gu, Daegu,  41061 Korea   zzosang@gmail.com  5Plan Ceibal,    Av. Italia 6201, Edificio Los Ceibos,   Montevideo, Uruguay   caguerrebere@ceibal.edu.uy  ABSTRACT  This poster will be a chance for a wider LAK audience to engage  with the 2nd LAK Failathon workshop. Both of these will build on  the successful Failathon event in 2016 and extend beyond  discussing individual experiences of failure to exploring how the  field can improve, particularly regarding the creation and use of  evidence.   Failure in research is an increasingly hot topic, with high-profile  crises of confidence in the published research literature in  medicine and psychology. Among the major factors in this  research crisis are the many incentives to report and publish only  positive findings. These incentives prevent the field in general  from learning from negative findings, and almost entirely  preclude the publication of mistakes and errors. Thus providing an  alternative forum for practitioners and researchers to learn from  each others failures can be very productive. The first LAK  Failathon, held in 2016, provided just such an opportunity for  researchers and practitioners to share their failures and negative  findings in a lower-stakes environment, to help participants learn  from each others mistakes. It was very successful, and there was  strong support for running it as an annual event. The 2nd LAK  Failathon workshop will build on that success, with twin  objectives to provide an environment for individuals to learn from  each others failures, and also to co-develop plans for how we as a  field can better build and deploy our evidence base.    This poster is an opportunity for wider feedback on the plans  developed in the workshop, with interactive use of sticky notes to  add new ideas and coloured dots to illustrate prioritisation. This  broadens the participant base in this important work, which should  improve the quality of the plans and the commitment of the  community to delivering them.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   General Terms  Management, Human Factors.   Keywords  Learning analytics, analytics, evidence, learning from failure.   1 WORKSHOP BACKGROUND   1.1 Failure in research  Problems with the published research literature are currently  receiving large amounts of attention, particularly in applied fields.    In health, the optimism that surrounded the evidence-based  medicine movement is beginning to falter, partly as the idea is  diverted from its original goals [1], but more fundamentally, as  issues with the underlying research come to light. Not only is  most published research false [2], but most of the true research  that is published is not useful in clinical practice [3].   In psychology, the replication crisis continues and intensifies. A  prominent effort to replicate a series of 100 classic psychological  results [4] achieved very partial success: A large portion of  replications produced weaker evidence for the original findings,  with only 3647% of replications succeeding, depending on the  measure chosen. It has also proved highly controversial, with  many blog and social media posts, using language that is  sometimes intemperate. One recent high-profile example of a  failed replication is power poses. The original claim was that a  person can, by assuming two simple 1-min poses, embody power  and instantly become more powerful [5]. One of the original  authors has had significant success as a public speaker on the  topic, with a TED talk receiving over 36m views [6], but after  failed replications, one of the authors has very creditably  concluded  that they do not think the effect is real [7].   A wide range of complex and hard-to-overcome factors lies  behind these problems in establishing a strong evidence base for  practice. Many of these concern the use of statistics, including the  use of researcher degrees of freedom to achieve significance [8]   importantly, this is not limited to situations where researchers  conduct multiple unreported comparisons, but also where  researchers can perform a reasonable analysis given their  assumptions and their data, but had the data turned out differently,  they could have done other analyses that were just as reasonable  [9]. Fundamentally, any research carried out with low pre-study  odds is prone to false positives [2]. Incentives on researchers to  publish significant findings play a strong part, and may encourage  publication of low-quality research even if replications were  commonplace and there were significant negative consequences to  publishing studies that were later repudiated [10].   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.  Copyright is held by the  owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029447     The file drawer effect, whereby uninteresting or negative  findings are not reported, is a major concern. In clinical research,  the ambitious AllTrials1 project seeks to ensure All trials  registered, all results reported to reduce this problem.   1.2 Evidence in learning analytics  There is no reason to believe that learning analytics is immune to  these problems. One attempt to explore this issue is the Learning  Analytics Community Exchange (LACE) projects Evidence  Hub2, which maps research evidence against four propositions  about learning analytics. The great majority of evidence classified  was positive, with only 14% negative [11], which suggests that  there is a significant publication bias in the field. Further, very  little of the published research could be classified at the higher  levels of the evidence hierarchy (i.e. systematic reviews,  randomised controlled trials) [11]. These are the base levels at  which the problems in health and psychology can be detected, so  their dearth in the evidence base for learning analytics may mean  that the problems in our field are even more profound.   1.3 Why a workshop at LAK  The first LAK Failathon was a success, giving an opportunity for  practitioners and researchers to talk about  and learn from  their  failures in a way that is difficult to provide in any other context.  This second LAK Failathon will build on that success and provide  a similar space in the first half of the workshop.  The critiques in health and psychology propose a wide range of  possible solutions (e.g. [12]), some of which may well be useful in  the field of learning analytics. So the second part of the workshop  will explore, collectively, how we can improve the creation and  use of evidence in our field.   2. WORKSHOP OBJECTIVES AND  INTENDED OUTCOMES  The workshop has two chief objectives: firstly, to provide an  effective space for sharing experiences of failures, and secondly,  to work collaboratively to produce prioritised action plans for the  field of learning analytics to improve.   The first part of the workshop is more fully described in the  accompanying workshop publication.   2.1 Producing action plans for improvement  It is helpful to learn as individuals from each others mistakes, but  it is also helpful to learn and improve collectively. This years  Failathon is focused particularly on evidence, and this part of the  workshop aims to explore what can be done to improve the  creation and use of evidence in the field of learning analytics.   The chief outcome from the workshop will be a series of action  plans collectively developed by the participants, consisting of  prioritised lists of suggested actions that could be taken by:    SoLAR, the Society for Learning Analytics Research    Future LAK conference organisers and committees    Universities and other research organisations    Companies, developers, and others with interests in  learning analytics                                                                        1 http://www.alltrials.net/  2 http://evidence.laceproject.eu/   3. POSTER SESSION  Following the workshop, we will take the plans developed by the  participants to the LAK poster session, to solicit feedback from a  broader audience, via coloured dots (green / yellow / red to  indicate support / caveat / oppose proposals) and sticky notes  (green for new ideas, yellow for comments). This will engage the  community more broadly than the workshop participants, which  will raise the profile of these issues, and give the plans as finally  developed greater legitimacy and, we hope, traction.   4. REFERENCES  [1] Greenhalgh, T., Howick, J., & Maskrey, N. (2014) Evidence   based medicine: a movement in crisis BMJ 2014;348:g3725   [2] Ioannidis, J. P. (2005) Why most published research findings   are false. PLoS Med, 2(8), e124.  [3] Ioannidis, J.P.A. (2016). Why Most Clinical Research Is Not   Useful. PLoS Med 13(6): e1002049.   [4] Open Science Collaboration. (2015) Estimating the   reproducibility of psychological science. Science, 28 Aug  2015: 349(6251).   [5] Carney, D. R., Cuddy, A. J., & Yap, A. J. (2010) Power  posing brief nonverbal displays affect neuroendocrine levels  and risk tolerance. Psychological Science, 21(10), 1363- 1368.   [6] Cuddy, A. (2012) Your body language shapes who you are.  TED talk, https://www.ted.com/talks/  amy_cuddy_your_body_language_shapes_who_you_are    [7] Carney, D. (2016) My position on Power Poses. Blog post:  http://faculty.haas.berkeley.edu/dana_carney/pdf_My%20pos ition%20on%20power%20poses.pdf   [8] Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011)  False-positive psychology undisclosed flexibility in data  collection and analysis allows presenting anything as  significant. Psychological science, 0956797611417632.   [9] Gelman, A. & Loken, E. (2013) The garden of forking paths:  Why multiple comparisons can be a problem, even when  there is no fishing expedition or p-hacking and the  research hypothesis was posited ahead of time.   Blog   post:    http://www.stat.columbia.edu/~gelman/research/unpublished/ p_hacking.pdf   [10] Smaldino, P.E. & McElreath, R. (2016) The natural selection  of bad science. R. Soc. open sci. 3:160384.    [11] Ferguson, R. & Clow, D. (2015) Evidence Hub Second  Review D2.8. http://www.laceproject.eu/deliverables/d2-8- evidence-hub-second-review/   [12] Ioannidis J. P. A. (2014) How to Make More Published  Research True. PLoS Med 11(10): e1001747.  doi:10.1371/journal.pmed.1001747     "}
{"index":{"_id":"84"}}
{"datatype":"inproceedings","key":"Chen:2017:EMS:3027385.3029448","author":"Chen, Bodong and Fan, Yizhou and Zhang, Guogang and Wang, Qiong","title":"Examining Motivations and Self-regulated Learning Strategies of Returning MOOCs Learners","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"542--543","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029448","doi":"10.1145/3027385.3029448","acmid":"3029448","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, clustering, teacher professional development","Abstract":"The present study examines behavioral patterns, motivations, and self-regulated learning strategies of returning learners---a special learner subpopulation in massive open online courses (MOOCs). To this end, data were collected from a teacher professional development MOOC that has been offered for seven iterations during 2014--2016. Data analysis identified more than 15% of all registrants as returning learners. Findings from click log analysis identified possible motivations of re-enrollment including improving grades, refreshing theoretical understanding, and solving practical problems. Further analysis uncovered evidence of self-regulated learning strategies among returning learners. Taken together, this study contributes to ongoing inquiry into MOOCs learning pathways, informs future MOOC design, and sheds light on the exploration of MOOCs as a viable option for teacher professional development.","pdf":"Examining Motivations and Self-regulated Learning Strategies of Returning MOOCs Learners  Bodong Chen University of Minnesota  Minneapolis, USA, 55455 chenbd@umn.edu  Yizhou Fan Peking University  Beijing, China, 100871 yizhou0034@126.com  Guogang Zhang Beijing Institute of Technology  Beijing, China, 100871 yhhzgg@163.com  Qiong Wang Peking University  Beijing, China, 100871 wangqiong@pku.edu.cn  ABSTRACT The present study examines behavioral patterns, motiva- tions, and self-regulated learning strategies of returning learn- ersa special learner subpopulation in massive open on- line courses (MOOCs). To this end, data were collected from a teacher professional development MOOC that has been offered for seven iterations during 20142016. Data analysis identified more than 15% of all registrants as re- turning learners. Findings from click log analysis identi- fied possible motivations of re-enrollment including improv- ing grades, refreshing theoretical understanding, and solving practical problems. Further analysis uncovered evidence of self-regulated learning strategies among returning learners. Taken together, this study contributes to ongoing inquiry into MOOCs learning pathways, informs future MOOC de- sign, and sheds light on the exploration of MOOCs as a viable option for teacher professional development.  CCS Concepts Applied computingDistance learning; Information systems  Clustering;  Keywords MOOCs; teacher professional development; clustering  1. INTRODUCTION Massive open online course (MOOC) research has evolved  to recognize various pedagogical models, varied conceptions of learner engagement and success [1, 2], and different learner subpopulations with varied learner intentions [3]. However, so far little work has examined returning learnersa spe- cial learner subpopulation who keep returning to a same MOOC after initial attempts. One study in this area finds  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029448  that while MOOC learners commonly re-take a course for eventual completion, some learners re-take a MOOC despite success in earlier trials [7]. Motivations behind returning learners in MOOCs are thus worth further investigation.  Returning learners are especially worth attention in MOOCs designed for teacher professional development (PD) in devel- oping regions. While traditional teacher PD follows the form of one-shot workshops that emphasize information delivery and fall short on catalyzing classroom changes [6], course re-taking in teacher PD MOOCs demonstrates opportuni- ties for sustained PD integrated in teachers daily practice.  This poster reports early findings from a project inves- tigating MOOCs as a potential option for teacher PD in underdeveloped regions of China. By focusing on returning learners, this project attempts to address the following re- search questions: (1) What were the general demographics of returning learners; (2) What were the potential motiva- tions of their re-enrollment; and (3) To which extent had they demonstrated self-regulated learning strategies.  2. METHODS  2.1 Context The research context was one teacher PD MOOC, titled  Flipped Classroom, offered for seven times during July 2014 and March 2016 by the X-Learning Center of Peking Uni- versity. As learners were free to enroll in each offering, this MOOC provided a unique opportunity for investigating re- turning learners over a long period of time. During all seven iterations, a total of 126,044 learners registered, with K-12 and higher education teachers representing more than 80% of them.  2.2 Data and analyses To study returning learners in this MOOC, we collected  a wide range of data including entry-surveys, course enroll- ment data, course click logs, and learner performance data.  To answer research questions, we first identified returning learners based on course enrollment data. Descriptive anal- ysis of their responses to entry-surveys were used to depict a general picture of their demographics. Click logs and per- formance data were mined to address the other questions.  3. RESULTS    Figure 1: Two time learners course grades.  -0.05  0.05  0.15  0.25  0.35  0.45  0.55  0.65 Videos  PDF Files  Forums  Quizzes &  Homework  Figure 2: Clustering of returning learners.  3.1 Basic demographics of returning learners Results indicated that more than 15% of all registrants  in this MOOC were returning learners, with a median age of 35 and 65.69% females. More than 80% of all return- ing learners were teachers, including K-6 teachers (15.72%), middle-high school teachers (33.70%), and college teachers (31.38%). The geographic locations covered all provinces of Mainland China.  3.2 Possible motivations of re-enrollment As improving performance is a possible motivation for  re-enrollment [7], we first analyzed the changes of course grades between two enrollments of two-time course takers. As shown in Figure 1, the green cluster achieved substan- tially higher score at their second attempt, while the light blue cluster showed less interests in improving scores as they had already passed the course at their prior attempts. We speculate that returning learners in the green cluster were driven by better grades, and those in the light blue cluster were driven by emerging learning needs in their teaching.  3.3 Self-regulated learning strategies Connected with learner motivations were self-regulated  learning (SRL) strategies in MOOC learning such as plan- ning and controlling the learning process [4]. To uncover SRL strategies from the angle of selective course engage- ment, we derived from click logs several variables based on learners use of course resources (e.g., videos, PDF hand- outs, forums) and conducted cluster analysis to distinguish  subgroups in returning learners. Results identified four clus- ters, characterized as video viewers (red), handout collectors (green), strivers (violet), and others (blue; see Figure 2). Connected with potentially diversified motivations [5], re- turning learners demonstrated selective course engagement leading to varied focus on course resources.  Since video viewing was salient among all clusters except for handout collectors, we further conducted association rule mining (using the Apriori algorithm) to identify videos that were most frequently watched by returning learners at their second learning attempts. Results uncovered their inter- ests in course videos covering key learning theories (e.g., Blooms Taxonomy) and practical guidance on flipped class- room (e.g., screen-casting with Camtasia). Their interests in reviewing those videos implied areas returning learners could use additional support.  4. CONCLUSIONS To conclude, this preliminary study attempted to study  returning learnersan underexplored learner subpopulation in MOOCsand uncovered their potentially diverse moti- vations and SRL strategies related to selective engagement. Ongoing efforts aim to deepen these analyses and seek fur- ther explanations through qualitative inquiry.  5. ACKNOWLEDGMENTS This research was funded by the Digital Learning for De-  velopment (DL4D) initiative in Asia. Grant No. 2016-0002.  6. REFERENCES [1] L. B. Breslow and D. E. Pritchard. Studying learning in  the worldwide classroom: Research into edxs first MOOC. Research & Practice Assessment, 8(1):1325, 2013.  [2] J. DeBoer, A. D. Ho, G. S. Stump, and L. Breslow. Changing course: Reconceptualizing educational variables for massive open online courses. Educational researcher, 43(2):7484, Feb. 2014.  [3] R. Ferguson and D. Clow. Examining engagement: Analysing learner subpopulations in massive open online courses (MOOCs). In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge - LAK 15, pages 5158, New York, New York, USA, 2015. ACM Press.  [4] R. F. Kizilcec, M. Perez-Sanagustn, and J. J. Maldonado. Self-regulated learning strategies predict learner behavior and goal attainment in massive open online courses. Computers & education, 104:1833, 2017.  [5] R. F. Kizilcec and E. Schneider. Motivation as a lens to understand online learners: Toward Data-Driven design with the OLEI scale. ACM Trans. Comput. -Hum. Interact., 22(2):6:16:24, Mar. 2015.  [6] G. Kleiman, M. A. Wolf, and D. Frye. The digital learning transition MOOC for educators: exploring a scalable approach to professional development. Technical report, Friday Institute for Educational Innovation, 2013.  [7] A. Woodgate, H. Macleod, A.-M. Scott, and J. Haywood. Differences in online study behaviour between sub-populations of MOOC learners. Educacion XX1, 18(2), 12 June 2015.    "}
{"index":{"_id":"85"}}
{"datatype":"inproceedings","key":"Chen:2017:LLC:3027385.3029449","author":"Chen, Lujie and Dubrawski, Artur","title":"Learning from Learning Curves: Discovering Interpretable Learning Trajectories","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"544--545","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029449","doi":"10.1145/3027385.3029449","acmid":"3029449","publisher":"ACM","address":"New York, NY, USA","keywords":"clustering, learning curves, student models","Abstract":"We propose a data driven method for decomposing population level learning curve models into mutually exclusive distinctive groups each consisting of similar learning trajectories. We validate this method on six knowledge components from the log data from an online tutoring system ASSIST-ment. Preliminary analysis reveals interpretable patterns of skill growth that correlate with students' performance in the subsequently administered state standardized tests.","pdf":"Learning from Learning Curves: Discovering Interpretable Learning Trajectories  Lujie Chen Carnegie Mellon University  Pittsburgh, PA lujiec@andrew.cmu.edu  Artur Dubrawski Carnegie Mellon University  Pittsburgh, PA awd@cs.cmu.edu  ABSTRACT We propose a data driven method for decomposing popula- tion level learning curve models into mutually exclusive dis- tinctive groups each consisting of similar learning trajecto- ries. We validate this method on six knowledge components from the log data from an online tutoring system ASSIST- ment. Preliminary analysis reveals interpretable patterns of skill growth that correlate with students performance in the subsequently administered state standardized tests.  CCS Concepts Information systems  Data analytics; Clustering;  Keywords Learning curves, Student models, Clustering  1. INTRODUCTION Learning curves are intuitive tools of descriptive analy-  sis often used in learning science to characterize students growth of skill or knowledge with experience. A popula- tion level skill-specific learning curve may be constructed by aggregating all students performance at a sequence of time- ordered learning opportunities. This construction implicitly assumes the homogeneity of students learning trajectories, which may not hold in reality due to the disparity either in terms of prior knowledge or individual learning rates.  Much has been explored to cluster students into sub-groups with the goal to improve prediction. For instance [Trivedi et al. 2011] proposed K-means and spectral clustering meth- ods using the dynamic features gathered from students in- teraction with an intelligent tutor, the clusters are then used to train ensemble models for predicting post-test scores. The focus of this work is to explicitly model students learning trajectories for the purpose of discovering and understand- ing the heterogeneous patterns of skill growth of the stu- dents as well as their implications on their performance in the subsequent tests.  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029449  2. DATA AND METHODS We used the Assistments Math 2004-2005 (912 Students)  data accessed via DataShop [Koedinger et al. 2010]. It con- tains logs of 912 middle school students interacting with AS- SISTment, an online tutoring system that provides help or hints at each step of math problem solving. These logs are linked with the post-test scores from MCAS (Massachusetts Comprehensive Assessment System) of the same set of stu- dents. We used the student-step roll up data available from DataShop summarizing students performance at each learn- ing step which was annotated with a version of a Knowledge Component (KC) model comprised of 39 skills.  For each specific KC, we extracted the first-attempt per- formance (correct or incorrect) for each student, with each response indexed by the opportunity count that starts with 1 and increments by 1 for each additional opportunity that student encounters. For steps annotated with multiple KCs, we assigned the step level performance for each of the KC involved. For this analysis, we selected 6 KCs with at least several hundred students each of whom had practiced at least 10 times on those KCs.  Based on the student-level performance data described above, we then fit a Group Based Model (GBM, [Nagin 2005]) to discover distinct groups of learning trajectories. GBM models a set of learning curves as a mixture of poly- nomial functions with timestamps (in our case, the oppor- tunity count indices) as the covariates. For a given learning curve of length T, assuming K groups, the likelihood func- tion of the observed trajectory is provided as follows:  K i=1  i  T j=1  Pi(Yj |Xj) (1)  where i and Pi are respectively the prior probability of data belonging to a certain group and the likelihood function for group indexed by i. Yj is the response variable value at step j, in our case it is students performance in a binary form; and Xj is the covariate, in our case it is the oppor- tunity count (i.e., Xj = j). Given the assumed number of distinct groups of learning trajectories (K) and the chosen order of the polynomials, the model estimates K smooth, polynomial in time trajectories one for each of the groups. The form of the likelihood function Pi is determined by the response variable. In our case, since the response variable is binary value of 0 and 1, we used the Bernoulli distribution. The GBM model also estimates the prior distribution of the group membership across data. The model parameters are inferred using Maximum Likelihood method through a nu-    0.00  0.25  0.50  0.75  1.00  1 2 3 4 5 6 7 8 9 10 Opportunity  A cc  u ra  cy  group 1  2  3  Figure 1: Estimated mean learning curves by group (solid lines) and 95% confidence intervals, overlaid with estimated latent polynomial functions (dotted).  0  10  20  30  1 2 3 Group  m e  a n   M C  A S   s c o  re  group 1  2  3  Figure 2: Mean MCAS scores for each of the learn- ing trajectory groups with 95% confidence intervals.  merical optimization procedure. In this experiment, we fit models with K = 3 groups and polynomial functions up to the third degree for each group.  Posterior group membership for each individual students learning curve can then be computed as follows:  P (Group = i|Y,X) = i  T j=1  Pi(Yj |Xj)  K i=1  i T  j=1  Pi(Yj |Xj) (2)  In order to assess the predictive utility of discovered groups, we correlate the posterior group membership for each stu- dent with their MCAS scores, from there the ANOVA p- values are computed and predictive accuracies are estimated.  3. RESULTS Figure 1 summarizes one main output from GBM for one  of the KC named P.2.8-evaluating-algebraic-expressions, which is learned by 388 students. The plot shows the learn- ing curve distributions aggregated from students belonging to each of the group as computed from maximum posterior group membership probability 2. The plot is overlaid (dot- ted line) with models estimated group trajectories (i.e., the latent polynomial function). As shown, the first group (red) is the most likely to be comprised of students who learned very little within the first 10 steps as suggested by the almost  flat learning curve, while the second group (green) represents a subset of students who seemed to learn quickly t first but then slipped into a forgetful state as evidenced by the drop of their learning curves after 5 steps. The students in the third group (blue) follow a steadily increasing learning tra- jectory. Presumably, this type of students are most likely to show good performance in subsequent tests.  We then correlate the posterior group memberships with MCAS scores. As shown in Figure 2, the mean score for the 1st group is the lowest as expected. Students from the 2nd group achieved significantly higher scores on average but not as high as the 3rd group who exhibited healthy learning curves. ANOVA p-value less than 0.001 suggests an overall correlation between group membership and post test scores. Similarly significant relationships have been found for all of the other five KCs evaluated.  We further estimated the potential predictive utility of group membership by predicting each students MCAS test score using the mean score of the group that the student belongs to, based on which we compute Mean Absolute Error (MAE). For the six KCs we studied, MAD ranges from 8.77 to 9.49, which is on par with the performance of the cluster- ensemble model reported in [Trivedi et al. 2011].  4. DISCUSSION AND FUTURE WORK We presented a method to model heterogeneity of stu-  dents learning trajectories by employing a group-based ap- proach. In our preliminary analysis that models step-level student log data for 6 different KCs, we noted interesting dis- tinct group level patterns of skill growth that can be readily interpreted. In addition, the observed significant correlation between posterior group membership and MCAS scores sug- gests that the apparent heterogeneity of learning trajectories is reflected in the students performance in the future tests. Future work will investigate the likely cause for particular patterns of learning curves (e.g., the forgetting phenomenon in the green plot in Figure 2), to evaluate the opportunity for interventions that might shift students to the more ef- fective trajectories. We will also study the utility of our ap- proach for early detection of group membership when only initial performance data is available, to inform timely inter- ventions.  5. ACKNOWLEDGMENTS The research reported here was supported, in whole or in  part, by the Institute of Education Sciences, U.S.Department of Education, through Grant R305B150008 to Carnegie Mel- lon University. The opinions expressed are those of the au- thors and do not represent the views of the Institute or the U.S. Department of Education.  6. REFERENCES [Koedinger et al. 2010] Koedinger, K. R., R. S. J. Baker,  K. Cunningham, and A. Skogsholm (2010). A Data Repository for the EDM community : The PSLC DataShop. Handbook of Educational Data Mining , 4355.  [Nagin 2005] Nagin, D. (2005). Group-based modeling of development. Cambridge, MA: Harvard University Press.  [Trivedi et al. 2011] Trivedi, S., Z. Pardoz, and N. Heffernan (2011). Spectural Clustering in Educational Data MIning. Proceedings of the 4th International Conference on Educational Data Mining , 129138.    "}
{"index":{"_id":"86"}}
{"datatype":"inproceedings","key":"Marbouti:2017:UVF:3027385.3029450","author":"Marbouti, Farshid and Diefes-Dux, Heidi and Madhavan, Krishna","title":"Utilizing Visualization and Feature Selection Methods to Identify Important Learning Objectives in a Course","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"546--547","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029450","doi":"10.1145/3027385.3029450","acmid":"3029450","publisher":"ACM","address":"New York, NY, USA","keywords":"engineering education, feature selection, first-year engineering, information visualization, learning objectives","Abstract":"There have been numerous efforts to increase students' academic success. One data-driven approach is to highlight the important learning objectives in a course. In this paper, we used visualization and three feature selection methods to highlight the important learning objectives in a course. Identifying important learning objectives as well as the relation among the learning objectives have multiple educational advantages. First, it informs the instructors and students of the important topics in the course; without learning them properly students will not be successful. Second, it highlights any inconsistencies in defining the learning objective, how they are being assessed, and design of the course. Thus, this approach can be used as a course design diagnostic tool.","pdf":"Utilizing Visualization and Feature Selection Methods to  Identify Important Learning Objectives in a Course   Farshid Marbouti  San Jose State University   San Jose, CA  farshid.marbouti@sjsu.edu   Heidi Diefes-Dux, Krishna Madhavan  Purdue University  West Lafayette, IN   {hdiefes, cm}@purdue.edu   ABSTRACT There have been numerous efforts to increase students academic  success. One data-driven approach is to highlight the important  learning objectives in a course. In this paper, we used  visualization and three feature selection methods to highlight the  important learning objectives in a course. Identifying important  learning objectives as well as the relation among the learning  objectives have multiple educational advantages. First, it informs  the instructors and students of the important topics in the course;  without learning them properly students will not be successful.  Second, it highlights any inconsistencies in defining the learning  objective, how they are being assessed, and design of the course.  Thus, this approach can be used as a course design diagnostic  tool.    CCS Concepts  Applied Computing   Education  Computer Assisted Instruction   Keywords Learning Objectives, Feature Selection, Information  Visualization, Engineering Education, First-Year Engineering.   1. INTRODUCTION One approach to help students be successful in their courses and  consequently in their program of study is to utilize data collected  by instructors during courses. Instead of merely relying only on  instructors experience or anecdotal evidence, another way to  learn how to enhance students success is to analyze students  performance data [1, 2] using learning analytics methods. The  results of such analyses provide information for both instructors  and students that can help instructors understand what leads to a  student passing or failing a course and provide better ways to  promote academic success.    2. RESEARCH PURPOSE/QUESTION It is important to understand what leads to students success in a  course. In this paper, we highlighted which learning objectives are  related with success in the course. These relations provide  information that can be used by instructors and advisors to help  at-risk students improve their learning. This paper answers the  following research question: What are the relationships between  student success and achievement of different learning objectives   3. METHODS 3.1 Data Source and Settings  This study used secondary data collected during the Spring 2013  and 2014 semester offering of a first-year engineering (FYE)  course at a large Midwestern U.S. university. In each semester,  approximately 1650 FYE students enrolled in the course. Nearly  20% of the FYE students were female and about 20% were  international students. This course is a required second semester,  2-credit hour course for all FYE students.    In this FYE course, homework assignments were graded based on  learning objectives. Each homework was designed and assessed  based on 6-7 learning objectives (e.g., draw and interpret  flowcharts containing decision branches to characterize an  engineering problem; write evidence-based rationales). Learning  objectives were assessed on a four-point scale: no evidence (0),  under achieved (1), partially achieved (2), or fully achieved (3).    3.2 Success in the Course  Although passing a course at university is usually considered as  receiving a grade of D or better, students who barely pass a course  with a D letter grade can also benefit from early support and  intervention in the course. For this course, at least a C grade is  required to complete the first-year engineering program and  matriculate to an engineering disciplinary program. In addition,  most engineering programs have a minimum requirement of  maintaining a C or better GPA to stay in the program.  Furthermore, if a student decides to transfer to another school or  program, she typically needs to pass most of the courses with  grades of A or B, and sometimes C grades. For these reasons, in  this study, similar to other studies (e.g., [3]), success was defined  as earning at least a C grade, which was equivalent to a final grade  of 68% or higher in the course.    3.3 Feature Selection  Three different feature selection methods were used in this study,  which are described in this section: (1) Correlation: One way to  select a subset of predicting variables that are more related to the  predicted variable is by calculating the Pearson correlations  between them; (2) Gini gain: The second feature selection method  that was used in this study was the Gini gain method. The goal of  this method is to calculate how well a variable can divide the  students into pass or fail categories; (3) Sum of Squared Errors  (SSE) and explained variance: The third feature selection method  used in this study was based on calculating the Sum of Squared  Errors (SSE) and the percentage of variance of the predicted  variable (pass/fail) explained by the predicting variables in a  Generalized Linear Model (GLM). Variables that result in lower  SSE and higher explained variance are better predictors of at-risk  students in the course.     4. RESULTS and DISCUSSION Feature selection methods highlighted learning objectives that are  good predictors of student success in the course. The top five   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author. Copyright is held by the owner/author(s). LAK '17, March 13-17, 2017, Vancouver, BC, Canada ACM 978-1-4503-4870-6/17/03. http://dx.doi.org/10.1145/3027385.3029450    learning objectives are shown in Table 1 for the three feature  selection methods. Correlation and Gini gain resulted in the same  top four learning objectives. Explained variance had only one  common variable with Gini gain and two common variables with  correlation methods.    All top five learning objectives that were selected by the  correlation and Gini gain methods were from homework 5. Two  of the learning objectives from homework 5, HW05-3 (Create a  user-defined function using rules for writing functions) and  HW05-4 (Create test cases to evaluate a user-defined function),  were selected by all three methods. In addition, professional  habits showed up in all three methods (although from two  different homework assignments, 3 and 5). These results indicated  that week 5 topic (user-defined functions) is an important topic.    Table 1. Top five selected learning objectives     Learning Objective # of times  in top 5   C or  re la  tio ns     HW05-6 - Professional Habits. 2  HW05-1 - Construct an appropriate  function definition. 2   HW05-3 - Create a user-defined function  using rules for writing functions. 3   HW05-4 - Create test cases to evaluate a  user-defined function. 3   HW05-2 - Demonstrate how to create a  MATLAB plot for technical presentation.   2   G in  i g ai  n   HW05-1 - Construct an appropriate  function definition. 2   HW05-6 - Professional Habits. 2  HW05-3 - Create a user-defined function  using rules for writing functions. 3   HW05-2 - Demonstrate how to create a  MATLAB plot for technical presentation.   2   HW05-4 - Create test cases to evaluate a  user-defined function. 3   Ex pl  ai ne  d  V  ar ia  nc e HW05-3 - Create a user-defined function   using rules for writing functions. 3   HW03-7 - Professional habits. 1  HW02-3 - Interpret and evaluate logical  statements. 1   HW05-4 - Create test cases to evaluate a  user-defined function. 3   HW01-6 - Coding Standards. 1    Figure 1 shows the correlation between academic success in the  course and the homework learning objectives as well as among  the learning objectives. The correlation between the homework  learning objectives and course success increased from homework  1 to 5. Homework 1 and 2 learning objectives had low correlation  with success in the course. The correlations of homework 3  learning objectives with course success were low or moderate.  Homework 4 learning objectives were split into two categories.  The first four learning objectives had low correlation with success  in the course while the last 3 had moderate correlations.  Homework 5 learning objectives had moderate or good  correlations with the course success.   There were two increases in the correlations among the learning  objectives. First, correlations among learning objectives increased  during week 4 (from HW04-5). After week 4 learning objectives  had higher correlations to other learning objectives and course  success than the first four weeks. Second, the correlations among   learning objectives increased from week 9. Learning objectives  from week 9 to 14 had higher correlations than previous weeks.   All the learning objectives in a homework were correlated to each  other (typically more than 0.3), with the exception of homework  1, 4 and 13. A learning objective from homework 1 had very low  negative correlation with three learning objectives, one from  homework 2, and two from homework 3. Two of homework 4  learning objectives not only were not correlated with other  homework 4 learning objectives but they also had negative  correlation with homework 3 learning objectives. In addition, one  learning objective in week 13, had negative correlations to some  of the other learning objectives.   These negative correlations among some of learning objectives  indicates a problem in either defining these learning objectives or  assessing them. None of the learning objectives in a course should  have negative correlations with each other. The course instructor  can use these results to evaluate the learning objectives and how  they are being assessed in order to improve the course design.   Figure 1. Correlations between success in the course and  learning objectives as well as among the learning objectives.    5. CONCLUSION  In this paper, we presented the preliminary results of identifying  important learning objectives in a first-year engineering course.  The results show how this approach can (1) inform the instructors  and students about important topics in a course, and (2) be used as  diagnostic tool to evaluate the learning objectives, assessments,  and course design.   6. REFERENCES  [1] Huang, S., & Fang, N. 2013. Predicting student academic   performance in an engineering dynamics course: A  comparison of four types of predictive mathematical models.  Computers & Education, 61, 133-145.   [2] White, R. 2012. Predicting likely student performance in a  first year Science, Technology, Society course. International  Journal of Innovation and Learning, 12(1), 72 - 84.     [3] Macfadyen, L. P., & Dawson, S. 2010. Mining LMS data to  develop an early warning system for educators: A proof of  concept. Computers & Education, 54(2), 588-599.       "}
{"index":{"_id":"87"}}
{"datatype":"inproceedings","key":"Manai:2017:WAD:3027385.3029451","author":"Manai, Ouajdi and Yamada, Hiroyuki","title":"How Can We Accelerate Dissemination of Knowledge and Learning?: Developing an Online Knowledge Management Platform for Networked Improvement Communities","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"548--549","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029451","doi":"10.1145/3027385.3029451","acmid":"3029451","publisher":"ACM","address":"New York, NY, USA","keywords":"behavior modeling, curation, dissemination, improvement, improvement science, knowledge management systems, machine learning, networked improvement community, recommender engine, social network analysis, social pressure, statistical analysis","Abstract":"The Networked Improvement Learning and Support (NILS) platform is an online tool designed to accelerate the initiation and development of Networked Improvement Communities in a disciplined manner. Its main goal is to promote social, organizational learning through curation and synthesis and tacit to explicit knowledge conversion to facilitate knowledge construction and ownership by the communities regarding improvement practice in education. In this proposal we will discuss the NILS platform, a few use cases, and a plan of analytics development that advances knowledge dissemination and monitors the health status of networks.","pdf":"How can we accelerate dissemination of knowledge and  learning: Developing an online knowledge management   platform for Networked Improvement Communities  Ouajdi Manai   The Carnegie Foundation for the Advancement of  Teaching   51 Vista Ln, Stanford,   CA 94305 USA   +1(650) 566-5143  manai@carnegiefoundation.org   Hiroyuki Yamada  The Carnegie Foundation for the Advancement of   Teaching  51 Vista Ln, Stanford,    CA 94305 USA  +1(650) 566-5501   yamada@carnegiefoundation.org       ABSTRACT  The Networked Improvement Learning and Support (NILS)  platform is an online tool designed to accelerate the initiation and  development of Networked Improvement Communities in a  disciplined manner. Its main goal is to promote social,  organizational learning through curation and synthesis and tacit to  explicit knowledge conversion to facilitate knowledge  construction and ownership by the communities regarding  improvement practice in education. In this proposal we will  discuss the NILS platform, a few use cases, and a plan of analytics  development that advances knowledge dissemination and  monitors the health status of networks.       GENERAL TERMS   Knowledge Management Systems, Curation, Measurement,  Documentation, Performance, Dissemination, Social Pressure,  Human Factors, Improvement.   KEYWORDS   Networked Improvement Community, Improvement Science,  Behavior Modeling, Statistical Analysis, Social Network  Analysis, Recommender Engine, Machine Learning.   1. NETWORKED IMPROVEMENT  COMMUNITIES  In the past eight years the Carnegie Foundation for the  Advancement of Teaching has pioneered a fundamentally new  vision for the research and development enterprise in education.  In particular, the Carnegie Foundation seeks to join the discipline  of improvement science with the powerful capacities of networks  to foster innovation and social learning for education reform. This  approach is embodied in what we call Networked Improvement  Communities (NICs) [1]. NICs are scientific learning  communities distinguished by four essential characteristics: (a)  focused on a well specified common aim; (b) guided by a deep  understanding of the problem,  the system that produces it, and a  shared theory of practice improvement; (c) disciplined by the   rigor of improvement science; and (d) coordinated to accelerate  the development, testing and refinement of interventions, their  more rapid diffusion out into the field, and their effective  integration into varied educational contexts. Examples of NICs  are Carnegie Math Pathways that aims to improve outcomes of  students who placed into remedial math and Building a Teaching  Effectiveness Network that aims to improve the retention of  effective new teachers [1].   2. NETWORKED IMPROVEMENT  LEARNING AND SUPPORT (NILS)  PLATFORM  To accelerate improvement work by NICs, we have developed an  online platform for knowledge management called Networked  Improvement Learning and Support (NILS). This NILS platform  is designed to promote social, organizational learning and  disseminate tacit and explicit knowledge [2] for improvement in  education by moving much of what we currently do face-to-face  into a virtual learning environment. As we support NIC members  who seek to initiate, grow, and sustain their networks, we  recognize the need to assure that improvement efforts are  grounded analytically, tested empirically, and accelerated through  network-wide social learning, which requires front-line  practitioners to shift from consumers to producers of knowledge  that advances improvement practice. How can we assist them in  building improvement science habits and mind sets Accordingly,  we need a support infrastructure for documenting, capturing, and  organizing improvement knowledge for access and use throughout  widely distributed networks. The design and creation of such a  platform has become critical for the Foundation to advance  widespread use of the NIC as a social arrangement to accelerate  improvement work in the field of education.    The NILS platform is designed to accelerate the initiation and  development of NICs in a disciplined manner. The current landing  page shows driver diagrams NIC members are acting on and  remind them of their formulated theory of practice improvement.  The driver diagram provides a common, controlled language that  helps NIC members be on the same page toward a measurable  common aim with a set of hypotheses and actual change ideas for  actions to achieve the aim [1]. It also enables NIC members to  track Plan-Do-Study-Act (PDSA) cycles with relevant data. The  PDSA cycle is a basic method of inquiry in improvement science  and can be used to turn ideas into action and connect action to  learning [1]. For both tools, the platform presents pictures of those  fellow members they collaborate with so as to promote group  solidarity and affinity and cultivate a sense of belonging to the     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029451     networked community. It allows NIC members to comment on the  driver diagrams and PDSA cycles and create blogs. The NILS  platform provides space for dialogue and discussion and  emphasizes curation (e.g., tagging) and synthesis of information  presented as a means for NIC members constructing and owning a  body of knowledge for improvement practice together. It is  accessible via mobile devices as well as laptop and desktop PCs  and designed to be integrated into their current workflow with  ease of use (see Figure 1).     Figure 1. NILS Platform       3. PLAN OF ANALYTICS  As users utilize the NILS platform, the system automatically  collects user behavioral data as a form of click-stream data. Also,  production data will be available through PDSA cycles and  discussion boards. The third data source will be self-report  surveys. By leveraging machine learning techniques [3], we plan  to analyze these data in order to examine how change ideas evolve  over time as a consequence of interactions among NIC members,  and more particularly, to identify what types of interactions  involving what levels or groups of NIC members contribute most  to this evolution. This analysis of interactive patterns serving to  evolve change ideas can contribute to informing users of the best  practices in knowledge and learning dissemination. We plan to  develop a recommendation engine to notify users of which  group(s) in the NIC is currently working on similar issues and  suggest the users contact them to learn from their practice. This  may be particularly useful for those who struggle for  implementing change ideas and seemingly need support from the  outside. From social network standpoints [4], we will identify  what we call informal leaders who are, for example, active in  executing change ideas and leverage them for further knowledge  dissemination. At the same time we are interested in any social,  cultural norms that may develop over time in NICs, which may  facilitate or inhibit knowledge dissemination. Related to this, we  want to examine any shift in user mindset and habits regarding  improvement science as a NIC member. Our analysis also focuses   on the status of health or sustainability of the NIC and the  development of a relevant analytics dashboard.    4. USE CASE  We expect a few use cases of the NILS platform from two NICs.  The Carnegie Foundation and the Tennessee State Department of  Education engaged in a partnership to initiate the Tennessee Early  Literacy Network, which seeks to improve outcomes in K-3  literacy. The goals of the partnership are to (a) develop a network  charter to improve third-grade literacy, (b) support the initiation of  NICs across the state, (c) develop improvement science and  network expertise at the state, regional, and district levels, and (d)  provide analytics and research in support of the health and vitality  of the network.   Carnegie Math Pathways NIC employs a systemic approach that  includes a comprehensive set of innovations in curriculum,  pedagogy, social and emotional supports, course structure,  analytics, and faculty professional development in order to  improve remedial math education. While the curriculum materials  provide critical just-in-time support for teaching, they are only  one part of the set of changes that generate the incredibly positive  results we see across the country, more than doubling success  rates in half the time to attaining college math credit [1]. The  Pathways programs are not textbooks or curricula that remain  static; rather, the programs are developed by and for a community  of practitioners with researchers, content experts, and Carnegie  Foundation staff as a NIC.   5. CONTRIBUTION TO THE FIELD  We will present our developmental work on the NILS platform.   From our presentation, the field will be able to learn patterns of  knowledge dissemination and evolution, any shifts in users  mindset and habits, analytic techniques to identify those patterns,  as well as data visualizations informative to NIC members from  different levels in the community.     6. REFERENCES  [1] Bryk, A. S., Gomez, L. M., Grunow, A., & LeMahieu, P. G.   (2015). Learning to improve: How America's schools can get  better at getting better. Cambridge, MA: Harvard Education  Press.   [2] Nonaka, I., & Takeuchi, H. (1995). The knowledge-creating  company: How Japanese companies create the dynamics of  innovation. New York, NY: Oxford University Press   [3] Murphy, K. P. (2012). Machine learning: A probabilistic  perspective. Cambridge, MA: The MIT Press.   [4]  Laat, M. De, Lally, V., Lipponen, L., & Simons, R. J.  (2007). Investigating patterns of interaction in networked  learning and computer-supported collaborative learning: A  role for Social Network Analysis. International Journal of  Computer-Supported Collaborative Learning, 2, 87-103                      "}
{"index":{"_id":"88"}}
{"datatype":"inproceedings","key":"Aslan:2017:SES:3027385.3029452","author":"Aslan, Sinem and Okur, Eda and Alyuz, Nese and Mete, Sinem Emine and Oktay, Ece and Genc, Utku and Esme, Asli Arslan","title":"Students' Emotional Self-labels for Personalized Models","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"550--551","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029452","doi":"10.1145/3027385.3029452","acmid":"3029452","publisher":"ACM","address":"New York, NY, USA","keywords":"affective computing, intelligent tutoring systems (ITS), personalized emotional engagement, personalized learning, self-report","Abstract":"There are some implementations towards understanding students' emotional states through automated systems with machine learning models. However, generic AI models of emotions lack enough accuracy to autonomously and meaningfully trigger any interventions. Collecting self-labels from students as they assess their internal states can be a way to collect labeled subject specific data necessary to obtain personalized emotional engagement models. In this paper, we outline preliminary analysis on emotional self-labels collected from students while using a learning platform","pdf":"Students Emotional Self-Labels for Personalized Models    Sinem Aslan  Intel Corporation, USA  sinem.aslan@intel.com     Eda Okur   Intel Corporation, USA  eda.okur@intel.com     Nese Alyuz   Intel Corporation, USA  nese.alyuz.civitci@intel.com     Sinem Emine Mete   Bahcesehir University, Turkey  e.sinemmete@gmail.com     Ece Oktay   Bogazici University, Turkey  oktayece@gmail.com   Utku Genc  Intel Corporation, USA  utku.genc@intel.com   Asli Arslan Esme  Intel Corporation USA   asli.arslan.esme@intel.com   ABSTRACT  There are some implementations towards understanding students  emotional states through automated systems with machine learning  models. However, generic AI models of emotions lack enough  accuracy to autonomously and meaningfully trigger any  interventions. Collecting self-labels from students as they assess  their internal states can be a way to collect labeled subject specific  data necessary to obtain personalized emotional engagement  models. In this paper, we outline preliminary analysis on emotional  self-labels collected from students while using a learning platform.       CCS Concepts   Human-centered computing~Empirical studies in  HCI    Applied computing~Learning management systems      Keywords  Personalized emotional engagement, personalized learning, self- report, affective computing, Intelligent Tutoring Systems (ITS).   1. INTRODUCTION  Digital learning environments with artificial intelligence capacity  (e.g., Intelligent Tutoring Systems - ITSs) have been studied for  enabling personalized learning experiences by leveraging students  emotions [1], [2]. Unfortunately, use of ITSs has generally been  limited to cognitive goals of learning process [4]. Considering the  important role of emotions in learning, ITSs need emotion- awareness capability [3], [5].    Generic AI models of emotions lack enough accuracy to  autonomously and meaningfully trigger any interventions for  improving students emotional states and consequently their  learning outcomes [6]. In [6], we show that models personalized to  each individual using the corresponding labeled subject-specific  data have high performance for emotional engagement detection.  However, for online usage, these models require incoming subject- specific data to be labeled. To address this problem, we investigate  the use of self-labels as self-reported measures of students  emotional states.       Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author. Copyright is held by the owner/author(s).   LAK'17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029452    2. METHODOLOGY OVERVIEW  2.1. Research Questions  There are three major research questions to address: (1) What is the  distribution of emotional states as labeled by the human experts  (2) What is the distribution of emotional self-labels as reported by  the students (3) What are the overlap ratios of emotional states  between emotional self-labels as reported by the students and  human-expert labels    2.2. Data Collection and Labeling   The data collection took place in 13 sessions (40 minutes each) of  a Math Course with 17 students in 9th grade. The students used an  online math platform: They watched instructional videos and  solved related questions. Our data collection application running in  the background, recorded the videos of the individual students  through a camera (i.e., Intel RealSense Camera F200) and  captured students desktop screens. We had around 113 hours of  student data to be labeled with respect to emotional states: Satisfied,  Bored, and Confused. We employed the Human Expert Labeling  Process (HELP) [7] to have the data labeled by five expert labelers  with an undergraduate/postgraduate degree in Educational  Psychology/Psychology. This process resulted in around 845 hours  of total data labeling and about 169 hours of labeling per labeler.    2.3. Emotional Self-Labels   The data collection application collected real-time emotional self- labels from the students as self-reported measures of their  emotional states. To set the groundwork and enable student  cooperation on self-labels, we created a scenario for students (See  Figure 1).    Figure 1. Scenario given to enable cooperative self-labeling.   After introduction of this scenario, we elaborated on the meaning  of the three emotional states [7] with the help of the course teacher.  As suggested by the course teacher, in the self-labeling, we used  Fine as a replacement for the word: Satisfied. There were two  methods we implemented to collect these self-labels:  (1) Voluntary  emotional self-labels: The students were able to provide an     emotional self-label at any time using the window that stayed at the  top right corner of the page (See Figure 2) and (2) mandatory  emotional self-labels: The system asked the students to enter an  emotional self-label at random intervals via a pop-up window.     Figure 2. Visualization of the self-labeling interface.   3. PRELIMINARY ANALYSIS & RESULTS  RQ1: Distribution of Human-Expert Labels  The data were preprocessed to construct instances with a length of  8-seconds and an overlap of 4-seconds. Final instance-wise human- expert labels were then assigned by applying majority voting  together with validity filtering. If there was no majority, Cant  Decide was assigned as the label (See [7]). The overall  distributions for the final human-expert labels are given in Table 1.    Table 1. Distribution of the human-expert labels.      RQ2: Distribution of Self-Labels  The results show that there is a major difference between the  mandatory (See Figure 3(a)) and voluntary (See Figure 3(b)) self- labels in terms of state distributions. When mandatory, Satisfied  state was selected as often as Bored state (39%), and Confused  state was selected less frequently (22%). When voluntary, the  students mostly selected Bored state (68%). The distribution of  the overall human-expert labels (See Figure 3(c)) is similar to the  distribution of the emotional states for the voluntary self-labels.  Figure 3. Emotional state distributions for (a) mandatory and  (b) voluntary self-labels and (c) human-expert labels.   RQ3: Overlap Ratios  We compared the agreement between self-labels and the final  human-expert labels using overlap ratios as the agreement measure:   To calculate the overlap ratios, we compared self-labels assigned  per instance (for previous N seconds) with the final expert labels  assigned per instance (again for previous N seconds). This previous  N seconds is the self-label span to be investigated. For our initial  experiments, we considered 20-second-label span for self-labels  (i.e., a given self-label is valid for the instances of the previous 20  seconds), an overall overlap ratio of 0.58 was obtained: For  voluntary and mandatory self-labels, 0.65 and 0.46 ratios were  achieved, respectively.   4. CONCLUSIONS AND FUTURE WORK  The preliminary results of this study showed that the collection  approach of self-labels impacted the emotional state distribution.  This study also indicated that there was a relatively higher overlap  ratio between voluntary self-labels and human-expert labels. As a  future work, we will conduct further statistical analysis on self- labels (e.g., different label spans, inter-rater agreement), and  experiment on personalizing engagement models using these self- labels.     5. REFERENCES  [1] N. Bosch, S. D'Mello, R. Baker, J. Ocumpaugh, V. Shute, M.   Ventura and W. Zhao,  Automatic detection of learning- centered afective states in the wild,  in Int. Conf. on Intelligent  User Interfaces, 2015.   [2] B. Woolf, W. Burleson, I. Arroyo, T. Dragon, D. Cooper and  R. Picard,  Affect-aware tutors: recognising and responding to  student affect,  Int. Journal of Learning Technology, vol. 4,  no. 3, pp. 129-164, 2009.   [3] R. S. Baker, S. K. D'Mello, M. M. T. Rodrigo, and A. C.  Graesser, A. C. Better to be frustrated than bored: The  incidence, persistence, and impact of learners cognitive affective states during interactions with three different  computer-based learning environments, International  Journal of Human-Computer Studies, vol.68, no.4, pp.223- 241, 2010.   [4] R. W. Picard, S. Papert, W. Bender, B. Blumberg, C. Breazeal,  D. Cavallo, T. Machover, M. Resnick, D. Roy, and C.  Strohecker.  Affective learninga manifesto.  BT  Technology Journal, vol.22, no.4, pp.253-269, 2004.   [5] O. C. Santos,  Emotions and personality in adaptive e-learning  systems: an affective computing perspective.  In Emotions  and Personality in Personalized Services, pp. 263-285.  Springer International Publishing, 2016.   [6] N. Alyuz, E. Okur, E. Oktay, U. Genc, S. Aslan, S. E. Mete,  D. Stanhill, B. Arnrich and A. A. Esme,  Towards an  emotional engagement model: Can affective states of a learner  be automatically detected in a 1:1 learning scenario,  in ACM  Conf. on User Modeling, Adaptation and Personalization  (UMAP) - Workshops, 2016.   [7] S. Aslan, S. E. Mete, E. Okur, E. Oktay, N. Alyuz, U. Genc,  D. Stanhill and A. A. Esme,  Human Expert Labeling Process  (HELP): Towards a reliable higher-order user state labeling by  human experts,  in Int. Conf. on Intelligent Tutoring Systems  (ITS) - Workshops, 2016.    "}
{"index":{"_id":"89"}}
{"datatype":"inproceedings","key":"Xiong:2017:WPM:3027385.3029453","author":"Xiong, Ye and Wu, Yi-Fang Brook","title":"Write-and-learn: Promoting Meaningful Learning Through Concept Map-based Formative Feedback on Writing Assignments","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"552--553","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029453","doi":"10.1145/3027385.3029453","acmid":"3029453","publisher":"ACM","address":"New York, NY, USA","keywords":"automated formative feedback, concept mapping, natural language processing, writing-to-learn","Abstract":"A primary goal of higher education is to promote meaningful learning: the delivery of core academic content to students in innovative ways that allow them to learn and then apply what they have learned. As a pedagogical strategy, Writing-to-Learn (WTL) intends to use writing to improve students' understanding of course content and concepts. To improve students' meaningful learning of conceptual knowledge in WTL activities, the project proposes to develop the Write-and-Learn system to generate automated formative feedback by taking advantage of the concept maps constructed from instructors' lecture notes and individual students' writing assignments. The proposed research aims to provide insights into how to apply concept maps into WTL activities to generate effective formative feedback on the acquisition and development of conceptual knowledge, and explore how and to what extent concept map-based formative feedback can be utilized to scaffold and promote meaningful learning in WTL activities.","pdf":"Write-and-Learn: Promoting Meaningful Learning    through Concept Map-Based Formative Feedback    on Writing Assignments  Ye Xiong   New Jersey Institute of Technology  323 Dr Martin Luther King Jr Blvd   Newark, NJ 07102-1982  973-596-3368   yx98@njit.edu  Yi-Fang Brook Wu  New Jersey Institute of Technology  323 Dr Martin Luther King Jr Blvd   Newark, NJ 07102-1982  973-596-5285   wu@njit.edu     ABSTRACT  A primary goal of higher education is to promote meaningful   learning: the delivery of core academic content to students in   innovative ways that allow them to learn and then apply what they   have learned. As a pedagogical strategy, Writing-to-Learn (WTL)   intends to use writing to improve students understanding of   course content and concepts. To improve students meaningful   learning of conceptual knowledge in WTL activities, the project   proposes to develop the Write-and-Learn system to generate   automated formative feedback by taking advantage of the concept   maps constructed from instructors lecture notes and individual   students writing assignments. The proposed research aims to   provide insights into how to apply concept maps into WTL   activities to generate effective formative feedback on the   acquisition and development of conceptual knowledge, and   explore how and to what extent concept map-based formative   feedback can be utilized to scaffold and promote meaningful   learning in WTL activities.     CCS Concepts   Computing methodologies  Artificial intelligence    Natural language processing  Social and professional topics    Professional topics  Computing education  Student   assessment  Applied computing  Education  E-learning.   Keywords  Writing-to-Learn; Concept Mapping; Automated Formative   Feedback; Natural Language Processing   1. INTRODUCTION  To ensure student success in academic and career development, it   is imperative to promote meaningful learning for students at all   levels: the delivery of core academic content to students in   innovative ways that allow them to learn and then apply what they   have learned [1]. As a pedagogical strategy, Writing-to-Learn   (WTL) has been widely adopted to improve students deep   understanding of conceptual knowledge through various WTL   activities such as summary, reflection essay, and discussion, etc.   These writing assignments are ubiquitous in higher education,   which can reflect what students know about course topics and   develop higher-level cognitive process that facilitates meaningful   learning. However, previous research suggests that without   formative feedback, students are still lack of effective guidance to   improve their conceptual understanding during the WTL   activities. Also, it is too labor intensive for an instructor to   provide timely formative feedback throughout the duration of an   assignment for a whole class of individual students. To address   the need, concept map, as a cognitive visualization and   pedagogical tool to visualize the relationships among different   concepts, can be utilized to generate formative feedback on the   acquisition and development of conceptual knowledge in WTL   activities. Therefore, this project proposes to develop the Write-  and-Learn framework to provide effective formative feedback   with the help of concept maps throughout a WTL activity.    The framework first utilizes the available NLP techniques to build   a domain concept map from the lecture notes for a specific course   subject. During a WTL activity, the NLP techniques can be then   applied to construct a student concept map that models the   concepts and their relationships in the assignment-in-progress. By   comparing the two maps on-demand, the framework will be   focused on generating actionable and individualized formative   feedback automatically, guiding individual students to improve   their writing assignments and learning performance in a WTL   context. To the best of our knowledge, we are the first research   team that proposes to develop the concept map-based formative   feedback to promote meaningful learning in WTL activities. Our   primary research questions include:   RQ1. How effective is the formative feedback generated based on   the comparison between domain concept map and student concept   map   RQ2. Whether and how do students utilize the concept map-based   formative feedback on writing assignments    RQ3. To what extent the concept map-based formative feedback   on writing assignments can affect student learning outcomes   2. SYSTEM ARCHITECTURE  The proposed Write-and-Learn framework is a new kind of   intelligent concept map-based writing and learning support system   that provides effective (i.e., automated, real-time, actionable and   individualized) formative feedback through the comparison   between the visual knowledge representation of lecture notes (i.e.,   domain concept map) and that of writing assignments (i.e., student   concept map) to improve student learning in WTL activities. The   concept map-based writing and learning support system will be   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other   uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   DOI: http://dx.doi.org/10.1145/3027385.3029453     developed via the available NLP techniques, including three   major components: (1) domain concept mapping; (2) student   concept mapping; and (3) formative feedback generation.    2.1 Module 1: Domain Concept Mapping  The module of domain concept mapping is to construct   knowledge representation from lecture notes (i.e., PowerPoint   Slides). With the chosen lecture notes for a specific course subject   as input, the module can automatically extract the domain   concepts and relationships, and output the ranked lists of concepts   and relationships for constructing domain concept map. In order   to generate automated and real-time formative feedback, it is   important to construct a domain concept map from course   materials such as lecture notes, which will be used as a reference   to show the gap between actual and desired learning performance.   A considerable body of literature has demonstrated the feasibility   of automatic or semi-automatic construction of concept maps   from different textual data sources in an educational context,   including lecture notes, textbooks, student essays, and academic   articles, etc. For our research purpose, we will leverage the most   recent and proven effective NLP approach, i.e., syntactic parsing   and Part-of-Speech (POS) tagging, proposed by Atapattu et al. [2-  4] to automatically construct a domain concept map from digital   lecture notes for a specific course subject.    2.2 Module 2: Student Concept Mapping  The module of student concept mapping is to construct knowledge   representation from writing assignments. With the writing   assignments of individual students as input, the module can   automatically extract the concepts and their relationships from   them, and output the ranked lists of concepts and relationships for   constructing the concept maps of individual students. In order to   generate actionable and individualized formative feedback, it is   important to construct the individual student concept map that   models the concepts and their relationships demonstrated in the   assignment-in-progress during the WTL activities. The datasets   used to construct student concept map are the writing assignments   of individual students. In the Write-and-Learn environment, the   writing assignments are typically designed to help students   understand and apply academic content or conceptual knowledge   presented in a course. Following the similar approach as domain   concept mapping, we can construct student concept map.   2.3 Module 3: Formative Feedback  Generation  The module is to generate formative feedback based on the   comparison between relevant domain concept map and student   concept map, which is the focus and most important technical   contribution of the proposed research. With the lists of ranked   concepts and relationships for relevant domain concept map and   student concept map as input, the module outputs a ranked list of   suggestions as formative feedback that shows the missing   concepts or unestablished relationships among concepts that   students might consider for revisions. In order to make sure that   the generated formative feedback is relevant to corresponding   writing assignments, it is important to specify the relevant domain   concept map for each writing assignment.   In the proposed system, the concept map-based formative   feedback will be provided in the form of facilitative feedback,   which contains a ranked list of suggestions that shows the missing   concepts or unestablished relationships among concepts that   students might consider for revisions so as to guide them toward   better responses or deeper learning. The ranked list of suggestions   can show students how to improve their coursework step by step   and provide them with specific guidance in the acquisition and   development of conceptual knowledge. For instance, if any   concept or relationship is missing, the formative feedback will be   generated as follows: Here is a list of suggestions on concepts   and their relationships you might consider based on how   important they are in the course. 1. Concepts you might consider   including: Cloud Computing; 2. Relationships you might consider   establishing: Cloud Computing  Outsourcing; 3. as illustrated   in Figure 1.   3. CURRENT WORK  At the current stage, the proposed concept map-based formative   feedback system as illustrated in Figure 1 is under design and   development and is expected to be ready for a pilot study by   Spring 2017 at NJIT. For our research purpose, the domain   concept map is constructed from the lecture notes for a specific   course offered by the Department of Informatics at NJIT. Besides,   the initial prototype (version one) will be tested in Winter 2016.       4. REFERENCES  [1] From the Alliance for Excellent Education, 2011. A time for   deeper learning preparing students for a changing world.   Education Digest: Essential Readings Condensed for Quick   Review, 77, 4, 43-49.   [2] Atapattu, T., Falkner, K., and Falkner, N., 2012. Automated  extraction of semantic concepts from semi-structured data:   Supporting computer-based education through the analysis of   lecture notes. In Database and Expert Systems Applications   Springer, 161-175.   [3] Atapattu, T., Falkner, K., and Falkner, N., 2014. Acquisition  of triples of knowledge from lecture notes: A natural   langauge processing approach. In Proceedings of the   Proceedings of the 7th International Conference on   Educational Data Mining (London, United Kingdom 2014),   193-196.   [4] Atapattu, T., Falkner, K., and Falkner, N., 2014. Evaluation  of concept importance in concept maps mined from lecture   notes: computer vs human. In Proceedings of the 6th   International Conference on Computer Supported Education   (Barcelona, Spain 2014), 75-84.   Figure 1. Mockup showing how Write-and-Learn   provides the student with concept map-based formative   feedback in the course of writing.           "}
{"index":{"_id":"90"}}
{"datatype":"inproceedings","key":"Lei:2017:DIV:3027385.3029454","author":"Lei, Chi-Un and Gonda, Donn and Hou, Xiangyu and Oh, Elizabeth and Qi, Xinyu and Kwok, Tyrone T. O. and Yeung, Yip-Chun Au and Lau, Ray","title":"Data-assisted Instructional Video Revision via Course-level Exploratory Video Retention Analysis","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"554--555","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029454","doi":"10.1145/3027385.3029454","acmid":"3029454","publisher":"ACM","address":"New York, NY, USA","keywords":"data-informed revision, exploratory analysis, video retention","Abstract":"Since teachers are not physically present in an online class, instructional video is the major carrier of course contents in an online learning environment. This paper aims to investigate how course-level exploratory video retention analysis can be used for identifying moments with abnormal watching behaviors and revising videos for a higher video retention. We have empirically evaluated the effectiveness of video analysis and revisions, based on evaluating retentions of revised videos.","pdf":"Data-Assisted Instructional Video Revision via Course- Level Exploratory Video Retention Analysis   Chi-Un Lei, Donn Gonda, Xiangyu Hou, Elizabeth Oh, Xinyu Qi,  Tyrone T.O. Kwok, Yip-Chun Au Yeung, Ray Lau   Technology-Enriched Learning Initiative, The University of Hong Kong, Pokfulam Road, Hong Kong   Email: {culei, dgonda, hxiangyu, lizhku, andreaq, tyrone.kwok, pauyeung, raybon}@hku.hk       ABSTRACT  Since teachers are not physically present in an online class,   instructional video is the major carrier of course contents in an   online learning environment. This paper aims to investigate how   course-level exploratory video retention analysis can be used for   identifying moments with abnormal watching behaviors and   revising videos for a higher video retention. We have empirically   evaluated the effectiveness of video analysis and revisions, based   on evaluating retentions of revised videos.      CCS Concepts   Applied computing   Education   Distance learning    Applied computing   Education   E-learning   Keywords  video retention; data-informed revision; exploratory analysis;    1. INTRODUCTION  Over the past decade, learning analytics (LA) benefited from the   rapid development of online learning opportunities. Multi-  dimensional data, such as clicking patterns of courseware and   videos, comes into our sight. Thus, how to accurately digest these   data into comprehensive insights becomes a significant task for LA   data visualization. Since teachers are not physically present in an   online class, instructional video is the one of the major carriers of   course contents in an online learning environment [1]. Unorganized   series of instructional videos that are difficult to comprehend may   demotivate students from watching the entire video or even   completing the course. In this situation, exploratory video retention   analysis can help understanding learning progress of students [2].    Since exploratory video retention analysis had been under-studied   in the literature, this paper aims to investigate how course-level   exploratory video retention analysis can be used for   instructional video revision. We hope that through the analysis,   teachers can efficiently identify moments in videos for revisions,   which can eventually improve video watching and learning   experience. Even though video-watching does not necessarily   guarantee effective learning, we believe that if learners are willing   to keep watching videos, they can eventually learn more.   2. METHODOLOGY  In this study, we have used exploratory video retention data to   analyze how learners interacted with the video content in a course.   Through identifying problematic moments in videos (explained in   Section 2.1), teachers can then look in depth to identify root causes   for video revisions. In this study, the video retention indicator   shows the number of views for a particular moment of a video   as a percentage of the total number of views of the video.   2.1 Instructional Video Revision  Instructional videos can be revised if one of the following situations   happens: i) the sequence of instructional videos is not in a coherent   order, ii) the video content is irrelevant to learners, iii) the layout,   visual aids, and mechanics in the video cannot convey the message   clearly, and iv) learners failed to comprehend certain ideas,   examples and/or illustrations. Revisions may include revising video   contents at problematic moments on a video-level or re-ordering   videos in a more coherent sequence on a course-level.   2.2 Data-Assisted Video Revision  Data-assisted revision is favorable due to the following reasons: i)   With the high granularity of learner data, problematic moments can   be specifically indicated. In particular, events are triggered when   the learner interacts (e.g. play, pause, playback) with the video, in   a second-by-second level; ii) Learner data can be obtained from all   learners, including non-active students who has a low tendency to   answer learner surveys; and iii) Data can be directly collected from   the video streaming database. As the measurement is less apparent   to students, the data collected is more authentic for analysis.    2.3 Analysis via Retention Metrics & Graphs  In general, videos should have a high percentage of contents viewed   (i.e. a high video retention metric), as this indicates that the video   keeps its audience. To be specific, course-level retention metric can   be used for identifying the problematic ordering of videos in the   browsing sequence and problematic videos for in-depth analysis.   Meanwhile, video-level retention graph can be used for identifying   moments in the video for revisions. These moments are usually the   ones with atypical browsing behaviors, which indicates learners   difficulties in understanding the content. For example, occurrences   of moments with >100% video retention metric indicate unusual   times of replays. They were usually moments illustrating   unexpectedly complicated textual and visual information within a   short period. In these situations, learners either replay that moment   (causing a retention peak at that moment) or stop watching the   video (causing a drop of retention rate in the rest of the video).    2.4 Limitations of Existing Studies  Studies in [3-5] aim at studying video browsing behaviour on a   video level. However, generated insights in [3,4] have not been   verified through evaluations of revised videos in subsequent   cohorts. In [5], difficulties in digesting video content have been   studied, without proposing pedagogical insights. In [3], course-  level retention metrics have not been explored to show whether the   order of videos are coherent. In [4], only researchers observations   (without learners browsing data) have been used for analysis.    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact   the Owner/Author.    Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029454    http://dx.doi.org/10.1145/3027385.3029454   Table 1. Comparison of video retentions in two cohorts: (a)   Problematic videos in Cohort 1, and (b) Corresponding videos   (with revisions) in Cohort 2. (A): Video length (minutes); (B):   Views; (C): Average video retention metric (%). Videos coded   in #, ^ and * are videos with video decomposing, video   restructuring, and   content revision, respectively.   (a)   Video title (A) (B) (C)   1.3 Camera, action! 5.25 86 33.9   3.1 Teaching at a massive scale 6.55 28 48.3   3.2 Forum management 7 25 45.3   3.3 The HKU experience: HKU03x 5.33 78 41.3   4.1 What is learning analytics 2.52 21 56.4   4.2 Learning analytic cycle 3.97 18 69.1   4.3 Resource usage analysis (I) 5.82 27 50.6   4.4 Resource usage analysis (II) 5.57 21 49.6   Average value of 13 non-revised videos 2.17 48.2 54.2   (b)   Video title (A) (B) (C)   1.3.1 Camera, action! # 2.32 37 73.2   1.3.2 Screen capture # 1.28 33 88.2   1.3.3 Studio filming # 1.77 34 71.5   1.3.4 The HKU experience: HKU03x ^ 5.33 31 75.6   1.5 MOOC forum management ^ 7.02 39 43.6   3.1 What is learning analytics * 2.4 31 77.4   3.2 Learning analytic cycle * 3.88 29 65.4   3.3 Resource usage analysis (I) * 5.63 39 62.9   3.4 Resource usage analysis (II) * 4.05 28 69.5   3.5 Resource usage analysis (III) * 1.77 24 97.3   Average value of 13 non-revised videos 2.17 34.4 75.0   3. DISCUSSIONS: EMPIRICAL RESULTS  Videos in an online course Interactive Online Learning from the   University of Hong Kong has been investigated. The course aims   at teaching teachers designing online teaching. 71 learners accessed   21 videos 931 times in Cohort 1 (April 2016). After revisions, 58   learners accessed 23 videos 772 times in Cohort 2 (August 2016).    3.1 Observations from Cohort 1 for Revisions  i) Although the survey results of Session 3 (MOOC) of the   course are satisfactory, videos in Session 3 have not been   widely watched by all learners, as shown in Table 1(a). For   example, 46% of learners had dropped the video within the   first 24 seconds. Therefore, we believed that engaging   learners, who had responded to surveys, may found this topic   useful. But for most learners, the session about MOOC may   not be relevant to their teaching environments. Therefore, we   have cancelled Session 3, through removing Video 3.1 and   moving Videos 3.2 and 3.3 to other sessions.   ii) Moments with abnormal video browsing behaviors have been  identified. For example, in Video 4.3, there was an abnormal   retention peak at 1:10, as shown in Fig. 1(a). We believe that   there was inappropriate zooming of figures in the video,   showing too much unnecessary information at that moment.   Therefore, we changed the zooming effect of figures and   omitted unnecessary on-screen information.   iii) Video retention rate in Video 1.3 starts to drop at 2:44. If we  look closely on the content, the rest of the video focuses on the   advanced video production skills. This drop in the retention   rate makes sense since we assessed the basic video production   skills only. This insight was further confirmed through the   face-to-face discussion. Therefore, we decided to separate   Video 1.3 into three components in Cohort 2 (See Table 1(b)).   3.2 Evaluations of Revisions in Cohort 2  Effectiveness of data-assisted video revisions can be checked by   the video usage in Cohort 2, as shown in Table 1(b). Retention rate   of videos without revision has been increased by 38.6% only.   Meanwhile, retention rate of videos with data-assisted revisions has   been increased by 56.1%. We believed retention rates have been   improved through restructuring videos. Also, an example of content   revision based on video retention graph is also shown in Fig. 1(b).   After revisions, the abnormal retention peak has disappeared, with   a consequence of higher video retention rate in the latter part of the   video, due to smooth and coherent learning experience. Based on   the learner surveys conducted in both Cohorts 1 and 2, the course   effectiveness has improved from 3.9 to 4.29. Thus, we believed that   revisions of videos helped students learn better in the course.   4. FUTURE WORK  In the future, we aim to include more data samples for an in-depth   statistical analysis, and include more metrics (e.g., amounts, origins   and destinations of replays) for a more comprehensive analysis.   (a)      (b)      Figure 1. Video retention of a lecture video against time: (a)   Problematic video in Cohort 1, with atypical browsing   behaviors, in 1:10, and (b) Revised video in Cohort 2, with the   moment showing the same content indicated by a vertical line.   5. REFERENCES  [1] Zhang, D., et al. 2006. Instructional video in e-learning:   Assessing the impact of interactive video on learning   effectiveness. Information & Management, 43, 1, 15-27.   [2] Ho, A. D., et al. 2015. Harvardx and MITx: Two years of open  online courses (Fall 2012-Summer 2014). Available at SSRN   2586847.   [3] Kim, J., et al. 2014. Understanding in-video dropouts and  interaction peaks in online lecture videos. In Proceedings of   ACM Conf. on Learning@Scale Conf. ACM. 31-40.   [4] Swarts, J. 2012. New modes of help: Best practices for  instructional video. Technical Comm., 59, 3, 195-206.   [5] Li, N., Kidzinski, et al. 2015. How Do In-video Interactions  Reflect Perceived Video Difficulty. In Proc. European   MOOCs Stakeholder Summit 2015. PAU Education, 112-121.     "}
{"index":{"_id":"91"}}
{"datatype":"inproceedings","key":"Edwards:2017:UPA:3027385.3029455","author":"Edwards, Rebecca L. and Davis, Sarah K. and Hadwin, Allyson F. and Milford, Todd M.","title":"Using Predictive Analytics in a Self-regulated Learning University Course to Promote Student Success","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"556--557","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029455","doi":"10.1145/3027385.3029455","acmid":"3029455","publisher":"ACM","address":"New York, NY, USA","keywords":"higher education, learning analytics, predictive modeling, self-regulated learning, student engagement","Abstract":"Prior research offers evidence that differing levels of student engagement are associated with different outcomes in terms of performance. In this study, we investigating the efficacy of a model of behavioural and agentic engagement to predict student performance (low, middle, high) at four timepoints in a semester. The model was significant at all four timepoints. Measures of behavioural and agentic engagement predicted membership across the three groups differently. With a few exceptions, these differences were consistent across timepoints. Looking at variations in student engagement across time can be used to target interventions to support student success at the undergraduate level.","pdf":"Using predictive analytics in a self-regulated learning  university course to promote student success   Rebecca L. Edwards, BSc  University of Victoria   3800 Finnerty Rd.  Victoria, BC V8P 5C2   250-721-6347   rle@uvic.ca   Sarah K. Davis, MA  University of Victoria   skdavis@uvic.ca      Dr. Todd M. Milford  University of Victoria   tmilford@uvic.ca      Dr. Allyson F. Hadwin  University of Victoria   hadwin@uvic.ca   ABSTRACT   Prior research offers evidence that differing levels of student   engagement are associated with different outcomes in terms of   performance. In this study, we investigating the efficacy of a model   of behavioural and agentic engagement to predict student   performance (low, middle, high) at four timepoints in a semester.   The model was significant at all four timepoints. Measures of   behavioural and agentic engagement predicted membership across   the three groups differently. With a few exceptions, these   differences were consistent across timepoints. Looking at   variations in student engagement across time can be used to target   interventions to support student success at the undergraduate level.   CCS Concepts   Applied Computing  Education  LMS   Keywords  Learning analytics; predictive modeling; student engagement; self-  regulated learning; higher education.   1. INTRODUCTION  Prior research suggests that by examining differences in   student engagement we can better understand differences   in student achievement [2]. Four dimensions of student   engagement have been identified: behavioural, emotional,   cognitive, and agentic [2][5]. Behavioural engagement (baseline   participation) and agentic engagement (participation extending   beyond what is expected) are positively related to performance [5].   However, student engagement on these dimensions is likely to   fluctuate across time and individuals. Measures of student   engagement are still developing [6] and there is a lack of research   examining differences in student engagement across time.    Recently, the learning analytics literature has used measures of   student engagement to predict and investigate student success [4].   This type of prediction often has a negative flavour; models have   typically focused on predicting students at risk for course failure or   attrition [3]. While this type of prediction is vital, it largely ignores   a large subset of students: students who fail to be identified as either   successful or failures. These students may not be at immediate   academic risk, but may at increased risk for academic failure or   attrition later in their academic careers [1].    In a prior investigation of student engagement in a learning-to-learn   course, we found that despite high and middle performing students   evidencing similar levels of behavioural engagement, high   performing students evidenced more agentic engagement [Davis,   Edwards, Hadwin, & Milford, personal communication]. The   current study extends findings by investigating the efficacy of a   model of behavioural and agentic engagement at four different   timepoints in the semester to predict student performance.    2. METHOD  2.1 Research Question  How do measures of behavioural and agentic engagement predict   students final course grade at four different timepoints during the   semester   2.2 Context  The study took place within a semester-long elective course,   comprised of 52% first-year students (N=139), where students   learned about self-regulated learning and how to apply it to their   other courses.   Students were enrolled in at least one other   academic course concurrently. Each week there was one 90-minute   lecture and one 90-minute lab in which students applied lecture   material.  Other topics covered in the course included   procrastination, motivation and emotion, time management, test   anxiety, and collaboration.    2.3 Measures  Measures used for this study were measures of academic history, a   prior knowledge (PK) survey and incoming semester GPA (GPA);   measures of behaviour engagement, the number of lecture synthesis   activities completed (LS) and the number of MyPlanners completed   (MP); and a measure of agentic engagement, the number of days   viewed course (Days). Measures are detailed in table 1.   Table 1. Descriptions of variables in the predictive model   Category:   Variable   Description Scale   Academic   history: PK   Measured in the first week, 20   multiple choice questions   about metacognition, learning   and motivation.   20  90   qAcademic   history: GPA   The term GPA received in the   semester prior to course   enrollment.   0  9, where   0 = F and 9 =   A+   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact   the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029455   http://dx.doi.org/10.1145/3027385.3029455   Behavioural   engagement:   LS    Students were assigned a   review question weekly after   lecture. Number completed.   1  9   Behavioural   engagement:   MP   MyPlanner consisted of a brief   weekly planning activity.   Number completed.   1 -10   Agentic   engagement:   Days    Logs from the LMS revealed   the number of unique days a   student accessed the course.    23 - 84   2.4 Procedures  All measures were (a) completed online as a part of the lecture, lab,   or homework, (b) collected from institutional data, or (c) logged by   the course management system. None of the measures directly   contributed to the overall course grade.             2.5 Performance Groups  Data revealed a trimodal distribution of final course grades.   Students were divided into three groups, consistent with three   modes for the final grades: (a) lowstudents at risk of failing (F to   C +, N=59), (b) middlestudents performing adequately (B- to   B+, N=36), and (c) highstudents excelling (A- to A+, N=44).   3. PREDICTIVE MODEL  3.1 Model  Multinomial logistic regressions were performed to predict   performance group membership (low, middle, and high) at four   timepoints in the semester, weeks 3, 6, 9, and 12. Five predictor   variables were included in the model, see table 1, and were selected   for this model because they contributed significantly to the binary   logistic regression models from our previous investigation [Davis,   Edwards, Hadwin, & Milford, personal communication]. All five   variables correlated with group membership between >0.2 and   <0.8. The referent group was the low performance group.   3.2 Results  All models were significant at p<0.05, see table 2. The strength of   the relationship between predictor variables and performance group   was relatively strong with Nagelkerkes R2 between 0.54 and 0.65.    Table 2. Model fit by week.      Table 3. Model sig. by week, group, and variables.         Low  Middle Low  High Middle-High   3 6 9 12 3 6 9 12 3 6 9 12   PK / /   /  / /  / / / / / /  /   GPA s  s / / s s s s s s s S   LS s  s s s s s s s / / /  /   MP / /  /   / /  / s  / / / / /   Days /  /  / /  / s s s s s s S   (s = significant at p<.05; / = not significant)   All predictor variables contributed significantly to the model at   least one time point. The measure of agentic engagement, days   viewed course, (a) predicted group membership at all four   timepoints when comparing middle-high and (b) at the final three   timepoints when comparing low-high, but (c) did not predict group   membership at any timepoints when comparing low-middle. One   measure of behavioural engagement, lecture synthesis, (a)   predicted group membership at all four timepoints when comparing   low-middle and low-high, but (b) did not predict group membership   at any timepoints when comparing middle-high. The other measure   of behavioural engagement, MyPlanner, (a) did not predict group   membership at any timepoint when comparing low-middle or   middle-high, but (b) did predict group membership at week 9 when   comparing low-high.   4. CONCLUSIONS   We expected more differences across the four timepoints between   the groups.  The measure of agentic engagement, days viewed   course, and one measure of behavioural engagement, lecture   synthesis, differed among students in the three performance groups   and these differences generally stayed consistent. The other   measure of behavioural engagement, MyPlanner, did not contribute   to the model. Further investigation of this variable is warranted.    Examining engagement in a self-regulated learning course through   learning analytics is important because it is a context where   students actively modify their study strategies and metacognitive   awareness of academic tasks. Middle group students appeared to be   behaviourally engaged but did not evidence the same levels of   agentic engagement seen in the high group. To promote student   success in already behaviourally engaged middle group students,   they should be encouraged to adapt their agentic engagement.    5. ACKNOWLEDGMENTS  This research is supported by a SSHRC Standard Research Grant   [435-2012-0529] awarded to A.F. Hadwin and a SSHRC Masters   Grant awarded to R.L. Edwards.   6. REFERENCES  [1] Baldasare, A., Vito, M., and Del Casino, V.J. 2016. When a   b isnt good enough. Inside Higher Ed. Retrieved from   https://www.insidehighered.com/views/2016/11/15/developi  ng-metrics-and-models-are-vital-student-learning-and-  retention-essay    [2] Fredricks, J. A., Blumenfeld, P. C., and Paris, A. H. 2004.  School engagement: Potential of the concept, state of the   evidence. Review of educational research,74,1 (Spring   2004), 59-109. DOI = 10.3102/00346543074001059   [3] Liu, D. Y. T., Rogers, T., and Pardo, A. 2015. Learning  analytics-are we at risk of missing the point. In Proceedings   of Ascilite 2015, (Perth, Austraila, Nov. 29  Dec. 2, 2015),   684  687.   [4] Pardo, A. 2014. Designing learning analytic experiences. In  Learning Analytics: From Research to Practice, J.A.   Larusson and B. White, Eds. Springer, New York, NY, 15-  38. DOI = 10.1007/978-1-4614-3305-7_2   [5] Reeve, J. 2013. How students create motivationally  supportive learning environments for themselves: The   concept of agentic engagement. Journal of Educational   Psychology, 105, 3 (Aug. 2013), 579595. DOI =   10.1037/a0032690    [6] Trowler, V. 2010 Student engagement literature review. The  Higher Education Academy, 11, 1-15.      Week 3 Week 6 Week 9 Week 12   Chi Sq 81.57 93.06 104.19 106.42   df 10 10 10 10   Sig 0.00* 0.00* 0.00* 0.00*   Nagelkerke 0.54 0.60 0.64 0.65     "}
{"index":{"_id":"92"}}
{"datatype":"inproceedings","key":"Kumar:2017:VUH:3027385.3029456","author":"Kumar, Vishesh and Tissenbaum, Mike and Berland, Matthew","title":"What Are Visitors Up to?: Helping Museum Facilitators Know What Visitors Are Doing","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"558--559","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029456","doi":"10.1145/3027385.3029456","acmid":"3029456","publisher":"ACM","address":"New York, NY, USA","keywords":"engineering, markov models, museum, real-time analysis, scaffolding, tablet, tinkering","Abstract":"In this paper, we describe a tablet application designed around an interactive game-based science museum exhibit. It is aimed to help provide museum docents useful information about the visitors' actions, in a way that is actionable, and enables docents to provide assistance and prompts to visitors that are more meaningful, compared to what they are typically able to do without this interface augmentation.","pdf":"What Are Visitors Up To Helping Museum Facilitators  Know What Visitors Are Doing   Vishesh Kumar  University of Wisconsin-Madison   Madison, Wisconsin  vishesh.kumar@wisc.edu   Mike Tissenbaum  Massachusetts Institute of Technology   Cambridge, Massachusetts   miketissenbaum@gmail.com   Matthew Berland  University of Wisconsin-Madison   Madison, Wisconsin  mberland@wisc.edu     ABSTRACT  In this paper, we describe a tablet application designed around an  interactive game-based science museum exhibit. It is aimed to  help provide museum docents useful information about the  visitors actions, in a way that is actionable, and enables docents  to provide assistance and prompts to visitors that are more  meaningful, compared to what they are typically able to do  without this interface augmentation.    CCS Concepts   Education  Computer-assisted instruction  Information  systems applications  Decision support systems   Keywords  Tablet; Museum; Engineering; Tinkering; Scaffolding; Markov  Models; Real-time analysis.   1. INTRODUCTION  There is an increasing number of museum exhibits that are  employing digital augmentations as a means of supporting science  learning [1]. At the same time, exhibit designers are also  exploring how less-structured, open ended designs can foster  higher levels of engagement, towards supporting new l learning  opportunities. This open-ended exploration, often termed  tinkering, are characterized by exploratory, experimental, and  iterative processes of learning, are particularly well suited for  STEM-based (Science, Technology, Engineering and Math)  reasoning and collaboration [2, 3].   In these kinds of open-ended designs, feedback and timely support  are especially important, as visitors often lack the necessary  expertise or prior knowledge needed to know how to effectively  tinker and explore. A critical component of learning from  tinkering involves exploration, failure, and re-exploration [5].  This mirrors the notion of perseverance commonly advocated for   in engineering and STEM practices [6]. However, if a participant  faces sustained failure without eventual success, they are likely to  feel disillusioned with their efforts and the STEM practices more  generally, resulting in a reduced chance of persevering in the  future [4]. Thus, it is of high importance that we find ways to  identify participants states, so that appropriate assistance can be  provided when they are stuck in sustained states of unproductive  tinkering.   When museum exhibits supporting these kinds of open-ended  tinkering lack well defined start/end points and permit free  entry/exit of participants, it can become particularly difficult for  museum docents, and even involved underlying technologies, to  keep track of the actions, process, and states of individual  participants. underlining.   Together, these factors make providing timely assessment and  feedback both valuable and hard. In response, we have designed a  tablet application that computationally processes data of  participants actions at the exhibit, and presents it to nearby  museum docents in ways that support real-time decisions based on  accurate models of participants states.   2. CONTEXT  2.1 The Oztoc Exhibit  This tablet app has been designed for use with an existing multi- touch tabletop exhibit in a large urban science museum, which  generally has at least one museum docent (called an explainer,)  to assist, guide, and engage visitors. The exhibit, named Oztoc,  situates visitors as electrical engineers called in to help fictional  scientists who have discovered an uncharted aquatic cave teeming  with never-before documented species of aquatic life [3]. The  participants are asked to lure these fish out for cataloging, by  building glowing fishing lures. Participants manipulate wooden  blocks labelled with symbols of electrical components, which are  tracked on the digital tabletop interface. Making a successful  circuit that has one or more appropriately powered LEDs, causes  elusive fish to appear, and get captured in the lures.   The tabletop is divided into four play spaces, which act as  boundaries for individuals to play within, allowing multiple  people to play simultaneously, as well as interact with each other.  This allows for a host of inter-player interactions wherein players  can see what others are doing, talk to others, and also notice and  learn from others attempts. These interactions can also be used to  inform computational identifications of participants states as  being productive or not.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029456  Please use a 9-point Times Roman font, or other Roman font  with serifs, as close as possible in appearance to Times Roman  in which these guidelines have been set. The goal is to have a 9- point text, as you see here. Please use sans-serif or non- proportional fonts only for special purposes, such as  distinguishing source code text. If Times Roman is not  available, try the font named Computer Modern Roman. On a  Macintosh, use the font named Times.  Right margins should be  justified, not ragged.     2.2 ADAGE  Using Play Data  Participants play actions from the tabletop, are posted to a data  aggregation server using the ADAGE (Assessment Data  Aggregator for Gaming Environments) system [7]. This data is  then available to use for analysis, even in real-time.    The kinds of data events posted by Oztoc to our ADAGE system  include which playspace blocks are being placed on the table,  moved, or connected; when a circuit is assembled: what blocks it  is made of, whether the circuit works or not, and what fish is  captured, if any. We can use this variety of event logs to obtain a  rich picture of each players actions, and extract meaning  regarding the participants states.    The current version of our tablet application uses real-time data to  provide information to explainers about participants actions and  progress the participants have made. We have also used post-hoc  data analysis to develop a trained Hidden Markov Model (HMM)  that successfully identifies participants states as engaging in  sustained unproductivity [8].   3. EXPLAINER EVENT VIEWER (EEV)  The tablet application, herein called the EEV, is an interface  meant to be seen on a handheld device by explainers at the  museum exhibit, and gives at-a-glance highlights of events from  the different play spaces, and when needed, suggested prompts  that the explainer can make use of, if they deem appropriate  (Figure 2).   The choice to provide feedback via suggestions to explainers is  meant to bridge the difficulty either agent  the explainers or the  game table itself  has individually, in making informed decisions  about providing feedback. Allowing the explainer to see salient  details of the participants actions at a glance coupled with their  knowledge about the context of play  who is actually at the table  and who might benefit from assistance  gives them actionable  information so that they can be more effective helpers.   Currently, there are five alerts with suggested prompts for the  explainers to consider using with struggling participants. These  appear in cases such as one playspace, making three circuits to  capture the exact same fish, within a span of two minutes. This is  a coarse indicator that the participant has attained a certain  working state, but is not deviating to try different things at all.  Similarly, there is a prompt when a participant makes three  consecutive circuits all with the same error  repeatedly  overpowering LEDs (caused due to lack of resistors and/or excess  of batteries), or underpowering LEDs.   4. FUTURE WORK  We have recently developed a trained and tested Hidden Markov  Model [9] that can tag each assembled circuit, as productive or  unproductive. This identification uses pattern mining across  circuits made by over 1400 participants, and analyzing meta- information about the circuits, like complexity (number of  components used), functionality (working or not), repetition  relative to ones own history (whether the circuit being made is  unique with respect to the circuits made earlier by a particular  participant), and relative to the history of circuits made in front of  them at the table (i.e. uniqueness of circuit compared to all the  circuits made by others at the table). This tagging system is able  to identify sustained bouts of unproductivity with a high success  rate (i.e. multiple consecutive circuits being tagged as  unproductive has been seen to co-occur with behavior we can  qualitatively call unproductive tinkering in the context of Oztoc).   We are excited to integrate this data analysis stream in the tablet  interface, to have more reliable and actionable advice for the  explainers using the EEV. We aim to have this portion of the  tablet completed and tested by the time of the conference and will  report on early findings on this work.    5. REFERENCES  [1] National Research Council. (2009). Learning science in   informal environments: People, places, and pursuits.  National Academies Press.    [2] Land, S. M. (2000). Cognitive requirements for learning with  open-ended learning environments. Educational Technology  Research and Development, 48(3), 61-78.   [3] Lyons, L., Tissenbaum, M., Berland, M., Eydt, R., Wielgus,  L., & Mechtley, A. (2015, June). Designing visible  engineering: supporting tinkering performances in museums.  In Proceedings of the 14th International Conference on  Interaction Design and Children (pp. 49-58). ACM.   [4] Petrich, M., Wilkinson, K., & Bevan, B. (2013). It looks like  fun, but are they learning. Design, make, play: Growing the  next generation of STEM innovators, 50-70.   [5] Resnick, M., & Rosenbaum, E. (2013). Designing for  tinkerability. Design, make, play: Growing the next  generation of STEM innovators, 163-181.   [6] Ryoo, J. J., Bulalacao, N., Kekelis, L., McLeod, E., &  Henriquez, B. (2015). Tinkering with failure: Equity,  learning, and the iterative design process. In FabLearn 2015  Conference at Stanford University, September 2015.   [7] Stenerson, M. E., Salmon, A., Berland, M., & Squire, K.  (2014, October). Adage: an open API for data collection in  educational games. In Proceedings of the first ACM SIGCHI  annual symposium on Computer-human interaction in  play (pp. 437-438). ACM.   [8] Tissenbaum, M., Kumar, V., & Berland, M. Modeling  Visitor Behavior in a Game-Based Engineering Museum  Exhibit with Hidden Markov Models. In  The Ninth  International Conference on Educational Data Mining.   [9] Tissenbaum, M., Kumar, V., Berland, M. (Accepted, 2017)  What are you doing over there Understanding Transitions  from Unproductive to Productive States in Open-Ended  Inquiry. At the Meeting of the American Educational  Research Association, 2017.  Figure 1. Screenshot of the EEV in action. Each  playspace has a timeline of dots (small grey circles),   with colored dots (blue square, large red circle, small  red circle), indicating the different kinds of fish   captured by the players. Playspaces have associated  prompts such as  Thats a small light! How could you   make you light more noticeable     "}
{"index":{"_id":"93"}}
{"datatype":"inproceedings","key":"Zheng:2017:PEA:3027385.3029457","author":"Zheng, Longwei and Gong, Wei and Gu, Xiaoqing","title":"Predicting e-Textbook Adoption Based on Event Segmentation of Teachers' Usage","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"560--561","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029457","doi":"10.1145/3027385.3029457","acmid":"3029457","publisher":"ACM","address":"New York, NY, USA","keywords":"event segmentation, feature finding, predictive analytics, technology adoption","Abstract":"Customized content of e-textbook require teachers to spend greater efforts using authoring tools and planning activities before class, and teachers with various contexts have different demands on e-textbook. However, some teachers who lack ICT skills and dissatisfy with the features tend to give up using e-textbook. Thus we need to know the status of teachers' usage earlier before we decide to give them some technical supports. This paper describes an analysis method for predicting e-textbook adoption from usage records in early days, and an event segmentation method of teachers' usage is used in effort to provide features of predictive model.","pdf":"Predicting e-Textbook Adoption Based on Event  Segmentation of Teachers Usage   Longwei Zheng  East China Normal University   Shanghai  China, 200062   lwzheng@dec.ecnu.edu.cn   Wei Gong  East China Normal University   Shanghai  China, 200062   gongweiandy@163.com   Xiaoqing Gu  East China Normal University   Shanghai  China, 200062   xqgu@ses.ecnu.edu.cn       ABSTRACT  Customized content of e-textbook require teachers to spend  greater efforts using authoring tools and planning activities before  class, and teachers with various contexts have different demands  on e-textbook. However, some teachers who lack ICT skills and  dissatisfy with the features tend to give up using e-textbook. Thus  we need to know the status of teachers usage earlier before we  decide to give them some technical supports. This paper describes  an analysis method for predicting e-textbook adoption from usage  records in early days, and an event segmentation method of  teachers usage is used in effort to provide features of predictive  model.   CCS Concepts   Social and professional topics~User characteristics   Applied  computing~Education   Keywords  technology adoption; event segmentation; feature finding;  predictive analytics   1.! INTRODUCTION  Customized content of e-textbook require teachers to spend  greater efforts using authoring tools and planning activities before  class, and teachers with various contexts have different demands  on e-textbook [3]. However, some teachers have insufficient ICT  skills or the e-textbooks mismatch teachers ICT demands, then  the rejection of e-textbook happens [2]. Thus we need to know the  status of teachers usage earlier before we decide to give them  some technical supports. We investigate this issue within the  context of data from an e-textbook platform named ZoomClass  which includes a web-based authoring environment and an iPad  application for teachers. Teachers were given access to customize  all e-textbook content for specific teaching objectives, they  typically create courses, upload media resources, design tasks,  assign activities, and insert quizzes on the web-based environment  before class, they also can record and upload photos and videos by  iPad application. The users of ZoomClass are teachers and  students at a primary school of Shanghai. We obtained data on  teacher authoring action records and e-textbook activity records,  for 83 teachers enrolled in this e-textbook platform, observed over  4 terms form 2014 October to 2016 June. The teachers performed   a total of 158,087 actions, created 2,745 courses, and organized  1,601 activities in class. We labeled teachers with low quantity   10 of e-textbook activities for classroom instruction as   inactive users. As a result, there are 40 inactive teachers and 43  active teachers that adopted ZoomClass.   2.! METHODOLOGY  The features creation of this studys predictive model are based on  event segmentation, which means dividing a given number of  teachers action observation into subsets with statistical  characteristics that are similar within each subsets and different  between subsets [1]. Segmentation has been widely employed to  study earth science such as a change point detection [1], and  financial science such as subsequence matching [4]. Zheng and  colleagues [5] developed an analysis method to discover the user  water behavioral habits, in their invention, a novel continuous  event segmentation algorithm was created to automatically  separate the water usage records into multiple individual bath  events, this study employed a same method to create features from  teachers early action record data sets for predictive model.   2.1! Event segmentation  The goal of event segmentation is to automatically organize  teacher actions into separate events, the segmentation method is  only based the date time information of action log records. We  consider action records in chronological order such that   $ = $&,  , $)                                         (1)  where $*  is the +th action record in data set $ with length ,. A  event segment -*,. which is a subset of $ can be given as   -*,. = $*,  , $. ,///1  +  0  ,                           (2)  The time differences between inter-action records in an event are  typically smaller than time differences between inter-action  records from separate events. In the event segmentation algorithm  created by Zheng et al. [5], a threshold of time difference has been  used to determine whether consecutive action records are in a  same event. The algorithm consists of following steps: 1.  Compute inter-action intervals; 2. Compare every interval to the  threshold of time difference. In step 2, If the interval is smaller,  these two inter-actions are considered in a same event, if the  interval is greater, they are divided into two different events. The  algorithm will run through all of inter-action intervals, then we  can obtain individual events from action log sets.   2.2! Threshold Optimization  In step 2 above, an automatic optimization has been used to  achieve individual threshold of inter-event interval for each  teacher, because the various ICT and instruction contexts of  teachers need be considered. The threshold optimization of each   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author. Copyright is held by the  owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029457      teacher consists of following steps: 1. Segment events with  successively varying thresholds of time difference; 2. Compute  event numbers for each threshold; 3. Specify minimum rate of  event numbers change for optimal threshold detection.          Figure 1 shows an example of a teachers segmentations with  varying thresholds. Here, the number of events has a big drop  when threshold is smaller than 11 minutes, it means that the most  inter-action intervals of the teacher are smaller than 11minutes. It  is highly likely to separate an individual event into two or more  sub-events if a small value is determined as threshold. Therefore,  an interval value is more rational to determined as threshold until  the number of individual events touches down and remains almost  unchanged. The slopes of inter-thresholds are used to detect the  signal of change rate. When the average of/1/(In this study, 1 is  set to 8) consecutive slopes of inter-threshold are closet to zero,  the first threshold point in slope window is flagged as optimal  threshold value of an individual teachers inter-event interval. In  Figure 1, the point of 26 minute is possible the optimal threshold.   2.3! Creation of Model Features  The goal of this study is making prediction at a stage where  teachers just started to use ZoomClass, so we focused on the usage  from the first one week to two weeks. Three features were  distilled from teacher usage events segmentation based on early  action records, including average interval of inter-action records  within events, average duration of events, and the number of  continuous using events (The event that contains more than one  isolated action is considered as continuous using event in this  study).   3.! RESULTS  The original, non-standardized features from the first weeks data  are shown in Table 1, including how their values compare  between those who were labeled to have adopted ZoomClass and  those who have not. Conducting an independent samples t-test  indicates that the differences of means of features between two  groups are statistically significant.   We applied two classification algorithms, Logistic Regression and  CART (Classification And Regression Tree), to these three  features, and evaluated the models using Kappa, precision, recall,  and area under the ROC curve (Table 2). We also applied event  segmentation algorithm and predictive model to the datasets by  first 10 days and 14 days. In these analyses, we conducted  stratified fold cross-validation as a check on over-fitting. These  predictive models are acceptable across all metrics. Arguably, the  models based on more observations perform better according to   Kappa and AUC, and CART applied on the dataset by first 10  days achieves the best Recall among the models.    Table 1. Features for teachers who adopted ZoomClass and  did not adopt from the first weeks data    Adopt-ion mean Std. Dev. t-value   Interval of  inter-action   within events  (min)    0 1.544 0.945 -4.006  (p<0.001) 1 2.979 1.883   Duration of  event (min)   0 12.834 8.470 -2.635  (p<0.05) 1 19.756 13.516   Continuous  events (N)   0 12.343 9.842 -3.502  (p<0.001) 1 28.767 26.258     Table 2. Performance of predictive models by 7, 10, 14 days    days Kappa Precision Recall AUC    Logistic  Regression   7 0.361 0.679 0.837 0.676  10 0.440 0.704 0.884 0.713  14 0.476 0.745 0.814 0.736   CART  7 0.362 0.678 0.851 0.675   10 0.434 0.700 0.894 0.710  14 0.494 0.750 0.830 0.744   4.! CONCLUSION  This paper introduced an approach for event segmentation based  on teachers action records within e-textbooks, and the features  distillation from individual event characterize the usage for each  teacher. It is indeed possible to use predictive models to identify  the teachers who would not adopt new technology in early days  with reasonable predictive performance. Future work involves the  discoveries of more characteristics and unknown behaviors within  the e-textbook usage events of teacher, not only the date time  information, but the interaction category and teaching context will  be considered in event segmentation and prediction.   5.! ACKNOWLEDGMENTS  We would like thank Liangjun Zhang and colleagues for the  tutorial of segmentation algorithm in their practice book.   6.! REFERENCES  [1]! Gedikli, A., Aksoy, H., and Unal, N. E. 2008. Segmentation   algorithm for long time series analysis. Stochastic  Environmental Research and Risk Assessment, 22(3), 291- 302.   [2]! Leary, H., Lee, V. R., and Recker, M. 2014. More than just  plain old technology adoption: Understanding variations in  teachers' use of an online planning tool. ICLS 2014  Proceedings, 110.   [3]! Maull, K. E., Saldivar, M. G., and Sumner, T. 2012.  Automated approaches to characterizing educational digital  library usage: linking computational methods with qualitative  analyses. International Journal on Digital Libraries, 13(1),  51-64.   [4]! Wu, H., Salzberg, B., and Zhang, D. 2004. Online event- driven subsequence matching over financial data streams.  In Proceedings of the 2004 ACM SIGMOD international  conference on Management of data (pp. 23-34). ACM.   [5]! Zheng, Q., Liang, G., Quan, Y., Gao, W. and Wang, S. 2015.  Analysis method, apparatus and system for user water bath  behavioral habits. CN105115164A. 2015(in Chinese).   Figure 1. Segmentation with varying threshold.       "}
{"index":{"_id":"94"}}
{"datatype":"inproceedings","key":"Sluijter:2017:BIP:3027385.3029458","author":"Sluijter, J. and Otten, M.","title":"Business Intelligence (BI) for Personalized Student Dashboards","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"562--563","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029458","doi":"10.1145/3027385.3029458","acmid":"3029458","publisher":"ACM","address":"New York, NY, USA","keywords":"business intelligence, excel, grade goals, personalized dashboards, power BI, power query","Abstract":"At Stenden University students from all over the world study together; all these different nationalities and cultures result in different ideas concerning academic success. The basis of this project was to develop a personalized dashboard for students via Microsoft Office 365 Power BI in which students can set their own personal KPI's. The raw data from the Student Information System (SIS) was transformed into clear visualizations that will help students gain better insight into their academic performance. This information can be used either independently or in consultation with their student advisor.","pdf":"Business Intelligence (BI) for Personalized Student  Dashboards   J. Sluijter  Stenden University of Applied Sciences   Rengerslaan 8  Leeuwarden, The Netherlands   +31-(0)6-41518842   Jody.Sluijter@stenden.com   M. Otten  Stenden University of Applied Sciences   Rengerslaan 8  Leeuwarden, The Netherlands   +31-(0)6-19281260  Marloes.Otten@stenden.com        ABSTRACT  At Stenden University students from all over the world study  together; all these different nationalities and cultures result in  different ideas concerning academic success. The basis of this  project was to develop a personalized dashboard for students via  Microsoft Office 365 Power BI in which students can set their  own personal KPIs. The raw data from the Student Information  System (SIS) was transformed into clear visualizations that will  help students gain better insight into their academic performance.  This information can be used either independently or in  consultation with their student advisor.     CCS Concepts   Information systems~Personalization    Human-centered  computing~Information visualization.   Keywords  Personalized Dashboards; Excel; Power Query; Grade Goals;  Business Intelligence; Power BI.   1. INTRODUCTION  Stenden University currently works with the Student Information  System (SIS) ProgressWWW to track students academic  performance. This system provides students an overview of all  their grades presented by date along with their obtained European  Credits (ECs) in a basic list view. The downside of this SIS is that  at the end of their second or third year of education, there is an  extensive list of results published which reduces readability.  Results cannot be grouped by academic year, making it very  difficult to get a quick overview of where the student stands. The  system also does not show progression in terms of grade point  average (GPA) nor an overview of credits earned towards  graduation. Students cannot get, at-a-glance, an update of their  own performance or insight into what credits still need to be  completed prior to going on for example internship in their fourth  academic year. The present system is static, and there is no  interaction or visual modifications possible in its current form.      2. PERSONAL GOALS  Research shows that setting personal goals has a positive impact  on the GPA (Richardson & Abraham, 2012). Personal goals are  better known as grade goals which Locke and Latham (1990)  define as self-assigned minimum goal standards. With the  limitations of our current SIS in mind and the motivational aspect  of grade goals, this project was started to give students better  insight into their academic progression data against their own goal  standards. At Stenden Hotel Management School (SHMS), the  largest department of Stenden University with 2,400 students, we  have students from many different nationalities studying together  and to say that they all strive for the same grade goal standards is  unrealistic. Some students may strive to achieve the highest  possible grades and graduate Cum Laude (graduating with  honors), while other students are satisfied with obtaining simply a  grade sufficient to pass. A study conducted in this matter by  Rienties et al (2011) examined the academic performance of  international students concluding that the international students  with a (mixed) western ethnic background obtained higher  academic results in comparison to Dutch students.   It is also a well-known fact that Dutch students have a different  attitude towards studying. This is confirmed by the Netherlands  Universities Foundation for International Cooperation (Nuffic)  (2013) which states that Dutch students in general have a different  attitude towards studying. According to the Nuffic (2013) Dutch  university students are not focused on high grades, but on  achieving the necessary requirements to obtain their degree.  Taking all these different nationalities and cultures into  consideration, students should be able to set their own goal  standards in the form of key performance indicators (KPIs) and  make sense of their own data with modest effort.   3. INDIVIDUALIZED DASHBOARDS  To be able to equip students with a tool that allows them to track  their progress through visualizations as well as set their own KPIs,  we started with constructing individualized dashboards. This  dashboard would also allow the student to compare their  performance with the peer group of their choice. Each dashboard  connects the student with their academic performance data via  their ID number in the SIS, providing them a real-time update of  their dashboard. The dashboards are created in the Microsoft  Office 365 Program Power BI. This system offers data  transformations via Power Query, which allows for the creation of  visualizations.    Power BI is part of the Microsoft Office 365 Educator license that  both teachers and students have access to free of charge via their   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada.  Copyright 2017 ACM 978-1-4503-4870-6/17/03 $15.00.  DOI: http://dx.doi.org/10.1145/3027385.3029458     institutional login. A nice feature is that the dashboard is  accessible anywhere, anytime and on any device, including  smartphones, independent of operating system.   Using Power BI, different visualizations related to academic  performance can be displayed in what they call tiles. Students  can choose what they would like to display, such as the pass rate  percentage per course or year, still to complete credits (what am I  missing), already achieved grades, GPA, highest grade achieved,  progress to degree time, what lies ahead, etc. It is up to the student  to decide what they would like to see and what they would like to  track.   As mentioned before, at SHMS we have students studying at our  campus from all over the world, each with their own academic  goal standards. Using Power BI, the student will be able to self- assign their own grade goals using the built-in KPI features in  Power BI coupled with an input form developed in SharePoint.  The goal is for them to be able to set their own KPIs per  visualization/ tile. This dashboard takes a Personal Best  approach, which will hopefully inspire and motivate the student to  be successful in their study.   A final feature of this dashboard will be the capability of being  able to compare yourself with your peers. We would like to  emphasize that this is not our main goal, but students can add this  feature in case they find it helpful. They can compare their own  individual result against a peer group of their choice, such as for  example all females, or all first years, or all students from a  certain nationality.    To create this personalized dashboard, we needed to find a way to  easily capture the unstructured data from the SIS and model it so  it could be used in the visualizations.  The raw data from the SIS  was initially not fit for purpose. However, it was adapted and  shaped using the tools in Power BI (namely Power Query) to  transform it into a simple data table. This whole process of  cleaning and standardizing the data took more than a year. The  entire dataset consisted out of 9,413 students with in total 209,919  grade records.  The dataset represented all students registered at  SHMS between September 2009 and September 2016.     Once that process was completed, we used Power BI to shape and  model the data so it could be used in visualizations. Power BI  consists of two parts, the first is the Power BI Desktop to create   reports and the second is the Power BI Service to pin the  visualizations from the reports onto the dashboard and share it  with either colleagues or students. Initially we are using manually  refreshed Excel sheets of student data extract from the SIS,  however in the future we plan to create a more direct link between  Power BI and the SIS that will allow automatic updates.   4. CONCLUSIONS  One final remark concerning the dashboards is that this process  covers two  of the five steps of Academic Analytics as described  by Campbell and Oblinger (2007). With our dashboard we capture  and report, but it is up to the student (either independently or in  consultation with their coach/academic advisor) to predict, act and  refine.  After the introduction of the dashboards, further research  will be conducted on the use of the tool by students and its  perceived value.   5. REFERENCES  [1] Campbell, J.P., and Oblinger, D.G. 2007. Academic   Analytics. Retrieved from  https://net.educause.edu/ir/library/pdf/PUB6101.pdf.    [2] Nuffic (2013), Grading systems in the Netherlands, the  United States and the United Kingdom, retrieved from  https://www.studyinholland.nl/documentation/grading- systems-in-the-netherlands-the-united-states-and-the-united- kingdom.pdf.    [3] Richardson, M., Abraham, C., and Bond, R. 2012.  Psychological Correlates of University Students Academic  Performance: A Systematic Review and Meta-Analysis.  Psychological Bulletin, 138, 2 (March 2012), 353-387. DOI=  http://dx.doi.org/10.1037/a0026838.   [4] Rienties, B., Beausaert, S., Grohnert, T., Niemandsverdriet,  S., and Commers. 2011. Understanding academic  performance of international students: the role of ethnicity,  academic and social integration. Higher Education, 63, 6  (July 2012), 685-700. DOI=  http://dx.doi.org/10.1007/s10734-011-9468-1               "}
{"index":{"_id":"95"}}
{"datatype":"inproceedings","key":"Hansen:2017:LHS:3027385.3029461","author":"Hansen, Cecilie Johanne Slokvik and Wasson, Barbara and Skretting, Hans and Netteland, Grete and Hirnstein, Marina","title":"When Learning is High Stake","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"564--565","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029461","doi":"10.1145/3027385.3029461","acmid":"3029461","publisher":"ACM","address":"New York, NY, USA","keywords":"competence development, learning analytics, open learner model, visualization","Abstract":"Firefighter learning is high stake. They need to maintain certain competence levels related to physical, mental, and firefighting and rescue skills in order to provide the public with a high level of emergency service. Fire and Rescue Services need to maintain an overview of the current competences of their personnel and to react when there is a competence gap. This poster presents our approach to using competence modelling, learner models, learning analytics, and visualisations in order provide insight into competence status and development on the individual, team, and organisation level, and to provide early-alerts and automated messages to instructors responsible for planning training activities, as well as to team leaders responsible for making decisions about teams in high stakes situations.","pdf":"When Learning is High Stake  Cecilie Johanne                Slokvik Hansen       Technology for Practice (T4P), Uni  Research Health, Bergen, Norway   cecilie.hansen@uni.no      Grete Netteland      Dep. Of social science, Sogn and  Fjordane university college, Norway   grete.netteland@hisf.no   Barbara Wasson     T4P, Uni Research Health, Centre for   the Science of Learning and  Technology (SLATE), InfoMedia,   University of Bergen, Norway   barbara.wasson@uib.no            Hans Skretting    InfoMedia, University of Bergen,   Norway    hans.skretting@gmail.com          Marina Hirnstein   Technology for Practice (T4P), Uni  Research Health, Bergen, Norway   marina.hirnstein@uni.no       ABSTRACT  Firefighter learning is high stake. They need to maintain certain  competence levels related to physical, mental, and firefighting  and rescue skills in order to provide the public with a high level  of emergency service. Fire and Rescue Services need to  maintain an overview of the current competences of their  personnel and to react when there is a competence gap. This  poster presents our approach to using competence modelling,  learner models, learning analytics, and visualisations in order  provide insight into competence status and development on the  individual, team, and organisation level, and to provide early- alerts and automated messages to instructors responsible for  planning training activities, as well as to team leaders  responsible for making decisions about teams in high stakes  situations.   CCS Concepts  General and reference Empirical studies Information  systems Decision support systems Human centered  computing User models Visual analytics Applied  computing Education Social and professional topics  Model curricula Student assessment Adult education  Testing, certification and licensing   Keywords  Learning Analytics; Competence development; Open Learner  model; Visualization   1. INTRODUCTION  Can learner models and visualisation of competences, for  individuals, groups, and the organisation as a whole, help to  improve decision making about training needs As part of an  increasing trend towards an evidence-based policy, the EU  policy objectives promote standardisation of competences  through the European Framework for Key Competences for  Lifelong Learning [1]. This is a reference tool for EU countries  and their education and training policies. Norway is no  exception with a focus on competence development within  several sectors, including education, health and safety (NOU,   2012; NDCP, 2013). With a need for a continuous focus on  education and training, many institutions and organisations  experience challenges in collecting and analysing information  about learners, and groups of learners.   Maintaining an overview of competence status in the  organisation is needed in order to make informed decisions  about learning, teaching, and organizational development. The  Fire and Rescue Service (FRS) is an example of where the need  of such overview is crucial, both at an individual employee  level, a team level, and organizational level. In Norway,  firefighters are recruited from vocational schools educating e.g.  carpenter or mason. FRSs are themselves responsible for the  education and training of these potential firefighters, and are  further required to ensure that the firefighters maintain extreme  skills and meet intensive fitness standards. This is not only  related to the two years of training to become a firefighter, but  continuously throughout their entire career.   2. PERSONNEL, EQUIPMENT AND                COMEPTENCE TRACKING  The iComPAss project [2] seeks to develop tools and methods  that can increase the ability to assess and identify competence  gaps in order for instructors to make decisions about instruction  and competence development before they become problematic.  We support this by drawing on learner models [3], competence  modelling, assessment, performance evaluation, and learning  analytics. Our approach builds on the research from two  previous EU-projects (ADAPT-IT and Next-Tell), and on the  partnership between Sotra Fire and Rescue Service (SFRS) and  a software company, Enovate AS, both situated on the west  coast of Norway. SFRS uses a competence tracking and training  activity planning tool called ADAPT-IT  (Figure 1) developed  by Enovate AS.      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author. Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029461     ADAPT-IT addressed the challenges associated with keeping  track of planning of training to meet the competence needs of  the fire and rescue service. The tool was designed to assist the  end users to collect and document key information using a  mixture of web-tools and mobile apps. By using the tool the  management has access to a web interface that is constantly  updated with an overview of the competence status of their fire  department. ADAPT-IT was tailored to improve the planning  of training and documentation for fire departments by  incorporating streamlined competence control, planning, crew  and equipment, deviation in- app reporting using voice and  media, combining assessment tools that inform on relevant  required competence needs.    In iComPAss we addresses how to harvest data from  assessment situations on readiness for- and performance in real  life incidents incident action, and training situations in order to  develop learner models and form the basis of visualisations of  the current situation that support inquiry [4, 5] for decision  making in high stake situations.    ADAPT-IT helps the instructor to keep track of the needs in  the entire organisation according to the requirements of the  different roles. In an interview an instructor at SFRS described  how the tool is used in planning of training activities. ADAPT- IT maintains a competence profile, which is updated with  assessment data from training situations, certifications formal  knowledge quizzes, etc. We are implementing a Competency  Gap Analysis (learning analytics) on the learner models that  supports the instructor in planning training activities and the  renewal of certifications by providing early-alerts and  automated messages when particular situations are discovered.  This enables the instructor to be proactive and invest in  training, courses, and certification of the personnel as needed.  This is done by:    1. collecting assessment data from exercises and real  fire and rescue situations, on identified competences.   2. using defined criteria for collected information in  order to visualise the competence profile through  histograms and spider graphs.   Furthermore, the planned analytics functionality of ADAPT-IT  will also provide visualisations that enable a team leader to  make real-time decisions about team constellations when  responding to an incident  (Figure 2).   3. CURRENT AND FUTURE WORK  It is crucial for firefighters to have the needed competences to  perform the risk intensive tasks required of them in a variety of  fire and rescue situations. As a member of an incident team the    firefighters have to trust all members to have the needed  competences. Therefore, it is crucial for the instructor to have  an overview of the training needs in the department. For  example, interviews with firefighters and leaders identified a  need to be able to report on readiness for smoke diving. In  order to collect readiness data where firefighters reporting for  duty answer questions related to mental and physical readiness  and a question about their availability to smoke dive, a  readiness app is under development. In the app the team leader  will be presented with a dashboard that supports the  assignment of firefighters to the required tasks in a response  situation. The data collected by this readiness app needs to be  supplemented with data from the learner model, data such as  certification status, performance on particular tasks,   competence levels, etc.    Use of technology for learning in the workplace has increased  the amount and variety of electronic data available for use in  learning analytics and visualisation. Therefore it is also  important to study how people use this information. We will  build on our earlier work on a framework for data literacy and  use for teaching [6] and learning [7] as a step in exploring how  the collected and visualised information is interpreted and  transformed into new practices              4. ACKNOWLEDGMENTS  The iComPAss project is supported by the Research Council of  Norway grant number 246765/H20. We thank the Sotra Fire and  Rescue service and Enovate AS.   5. REFERENCES  [1] European Communities (2008). The European Qualifications  Framework for Lifelong Learning. Luxembourg: Office for  Official Publications of the European Communities.   [2] Hansen, C.; Netteland, G; Wasson, B. (2016). Learning  analytics and open learning modelling for professional  competence development of firefighters and future healthcare  leaders. In: CEUR Workshop. Proceedings,1601, 87-90.   [3] Bull, S. & Kay, J. (2007). Student Models that Invite the  Learner In: The SMILI Open Learner Modelling Framework,  International Journal of Artificial Intelligence in Education  17(2), 89-120.   [4] Hansen, C.; Wasson, B. (2016). Teacher inquiry into student  learning: The TISL heart model and method for use in teachers'  professional development. Nordic Journal of Digital Literacy,  10, 24-49.   [5]  Luckin, R.; Hansen, C.; Wasson, B.; Clark, W.; Avramides,  K.; Hunter, J.; Oliver, M. (2016). Teacher Inquiry into Students'  Learning: Researching Pedagogical Innovations. In: Measuring  and Visualizing Learning in the Information-Rich Classroom,  74-91. London: Routledge.   [6] Wasson, B.; Hansen, C. (2016). Data Literacy and Use for  Teaching. In: Measuring and Visualizing Learning in the  Information-Rich Classroom, 56-73. London: Routledge.   [7] Wasson, B.; Hansen, C.; Netteland, G. (2016) Data literacy  and use for learning when using learning analytics for learners.  In: CEUR Workshop Proceedings; Vol. 1596. s. 38-41.      "}
{"index":{"_id":"96"}}
{"datatype":"inproceedings","key":"Zimmerman:2017:MKC:3027385.3029462","author":"Zimmerman, Neil L. and Baker, Ryan S.","title":"Mining Knowledge Components from Many Untagged Questions","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"566--567","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029462","doi":"10.1145/3027385.3029462","acmid":"3029462","publisher":"ACM","address":"New York, NY, USA","keywords":"data mining, knowledge components, knowledge tracing","Abstract":"An ongoing study is being run to ensure that the McGraw-Hill Education LearnSmart platform teaches students as efficiently as possible. The first step in doing so is to identify what Knowledge Components (KCs) exist in the content; while the content is tagged by experts, these tags need to be re-calibrated periodically. LearnSmart courses are organized into chapters corresponding to those found in a textbook; each chapter can have anywhere from about a hundred to a few thousand questions. The KC extraction algorithms proposed by Barnes [1] and Desmarais et al [3] are applied on a chapter-by-chapter basis. To assess the ability of each mined q matrix to describe the observed learning, the PFA model of Pavlik et al [4] is fitted to it and a cross-validated AUC is calculated. The models are assessed based on whether PFA's predictions of student correctness are accurate. Early results show that both algorithms do a reasonable job of describing student progress, but q matrices with very different numbers of KCs fit observed data similarly well. Consequently, further consideration is required before automated extraction is practical in this context","pdf":"Mining Knowledge Components From Many Untagged Questions  Neil L Zimmerman McGraw-Hill Education  281 Summer St Boston, MA 02210  neil.zimmerman@mheducation.com  Ryan S Baker Graduate School of Education  University of Pennsylvania 3700 Walnut Street  Philadelphia, PA 19104 rybaker@upenn.edu  ABSTRACT An ongoing study is being run to ensure that the McGraw- Hill Education LearnSmart platform teaches students as ef- ficiently as possible. The first step in doing so is to identify what Knowledge Components (KCs) exist in the content; while the content is tagged by experts, these tags need to be re-calibrated periodically.  LearnSmart courses are organized into chapters correspond- ing to those found in a textbook; each chapter can have any- where from about a hundred to a few thousand questions. The KC extraction algorithms proposed by Barnes [1] and Desmarais et al [3] are applied on a chapter-by-chapter ba- sis. To assess the ability of each mined q matrix to describe the observed learning, the PFA model of Pavlik et al [4] is fitted to it and a cross-validated AUC is calculated. The models are assessed based on whether PFAs predictions of student correctness are accurate.  Early results show that both algorithms do a reasonable job of describing student progress, but q matrices with very different numbers of KCs fit observed data similarly well. Consequently, further consideration is required before auto- mated extraction is practical in this context.  CCS Concepts Information systemsData mining; Applied com- puting  Computer-assisted instruction; Learning management systems; E-learning;  Keywords Knowledge Components; Knowledge Tracing; Data Mining  1. INTRODUCTION The LearnSmart platform1 is an adaptive learning system  that follows along with the textbook used in a course; stu-  1http://www.mheducation.com/highered/platforms/ learnsmart.html  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029462  dents are given questions corresponding to topics in each chapter in an order prescribed by the system, and must cor- rectly answer a certain number of questions from each topic before the platform has deemed that they have mastered the chapter. LearnSmart is very widely used; in an average month, there are more than 100,000,000 distinct student- question interactions.  The actual learner experience in LearnSmart is dependent on metadata produced by subject matter experts, namely: what topics exist in each chapter, and how much evidence does a correct response to a given question contribute to proving that a student has mastered the topic This paper discusses part of a broad effort to confirm the validity of this metadata, first by mining knowledge components from student interactions and then (as part of a future work), to trace the acquisition of these components and confirm that students are not provided with too little or too much practice on each topic.  This paper focuses on a single chapter from a single course teaching the Spanish language to English speakers. In this chapter there are 133 questions, given to roughly 11,000 students in a sequence deemed appropriate by the adaptive platform. In total, there are approximately 750,000 distinct student-question interactions in this dataset.  2. METHODOLOGY In this work, we attempt to extract KCs from chapters  automatically, as the topic and skill tagging is absent in some textbooks, and the sheer volume of texts makes man- ual tagging impractical. This approach assumes that KCs will not be found across chapters; while this assumption is unlikely to be formally true, the pedagogy of this platform is such that we want to measure KC progress on a chapter- by-chapter basis.  2.1 Imputing missing features While students do not have any control over the sequence  of questions that they seem the adaptive platform generally deems students to be finished with a given chapter after showing them roughly half the questions. While the al- gorithm of Barnes [1] can be easily adapted to work with missing data, regression techniques like non-negative matrix factorization (NMF) cannot.  Therefore, for each question, a logistic model is built using all students who did attempt the question as training data, where the features are the students interactions on all other questions in the chapter, using one-hot encoding to turn    0 20 40 60 80 100 120 140 Num ber of KCs  0.790  0.795  0.800  0.805  0.810  0.815 AU  C/ RO  C AUC/ROC, NMF q m atrix derivat ion, PFA validat ion  Figure 1: AUCROC of PFA, predicting student per- formance using q-matrices derived via NMF. Note the limited range of AUCROC values, shown in the Y axis.  {correct, incorrect, did not attempt} into binary features. A single regularization parameter is selected for all equations based on the cross-validated prediction accuracy among the training set, but the logistic imputation is used only to fill in missing data points; observations are never overwritten.  2.2 Extracting Knowledge Components Knowledge components are customarily mapped to ques-  tions or items in a q-matrix, a binary matrix which con- tains one column for each question and one row for each KC. [6] A custom implementation of the q-matrix algorithm outlined by Barnes [1] was implemented in Apache Spark. Concisely: random q-matrices of high sparsity containing n KCs are generated, and in random order each entry in the matrix is flipped from 0 to 1 or vice versa; if the new matrix is better able to describe actual student interactions the changes are kept. Unfortunately, this stochastic search is very slow, sufficiently so that for now it is set aside.  Using NMF techniques, as outlined by [3], produces re- sults many hundred times faster. Using the NMF factoriza- tion package built into scikit-learn and Apache spark [5], it can find q-matrices spanning the range from two to 133 (i.e., the number of questions in the chapter, or the largest size at which matrix factorization would make sense) in a few hours on a modern desktop computer.  2.3 Scoring Knowledge Components Finally, to score the descriptive ability of each q-matrix,  an implementation of the PFA knowledge tracing model [4] was implemented in Apache Spark. Only the knowledge tracing part of the algorithm was implemented. The score of a q-matrix is judged to be the AUC/ROC of the PFA model predicting a correct answer for each student appear- ance, using a split testing and training set.  So far, this has not yielded conclusive results: while the q-matrices uncovered by NMF produce a fairly high AUC ( 0.8), it is found to be very insensitive to the number of KCs mined. The range of AUCs varies by less than 1% along the entire [2, 131] range of q. We believe that this is due to the dominance of the question-difficulty term in PFA.  3. CONCLUSIONS AND NEXT STEPS There are two immediate and parallel paths that need to  be pursued: ensuring that the flat AUC is not an artifact of either the NMF for factoring or PFA for scoring (e.g., by comparing with the algorithm in [1] or with Bayesian Knowl- edge Tracing [2], or by removing the difficulty component in PFA).  If this flat AUC is a robust feature of the data, then NMF should be revisited using matrix factorization technique that does not require imputing missing data, at least to con- firm that it produces q matrices similar to those produced when imputing data. Additionally, comparing the 2-means method of turning non-binary matrices into binary matrices, used by Desmarais, with explicitly binary matrix factoriza- tion techniques (e.g. [7]), would help ensure that using NMF to produce binary matrices produces comparable results.  4. ACKNOWLEDGMENTS The authors would like to thank the MHE LearnSmart  team for providing access to data used in this analysis.  5. REFERENCES [1] T. Barnes. The q-matrix method: Mining student  response data for knowledge. In Proceedings of the AAAI-2005 Workshop on Educational Data Mining, pages 3946. Assocation for Advancement of Artificial Intelligence, Jul 2005.  [2] A. T. Corbett and J. R. Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction, 4(4):253278, 1994.  [3] M. C. Desmarais, B. Beheshti, and R. Naceur. Item to skills mapping: Deriving a conjunctive q-matrix from data. In International Conference on Intelligent Tutoring Systems, pages 454463. Springer Science & Business Media, 2012.  [4] P. I. Pavlik, H. Cen, and K. R. Koedinger. Performance factors analysis a new alternative to knowledge tracing. In Proceedings of the 2009 Conference on Artificial Intelligence in Education: Building Learning Systems That Care: From Knowledge Representation to Affective Modelling, pages 531538, Amsterdam, The Netherlands, The Netherlands, 2009. IOS Press.  [5] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011.  [6] K. K. Tatsuoka. Rule space: An approach for dealing with misconceptions based on item response theory. Journal of educational measurement, 20(4):345354, 1983.  [7] Z. Zhang, T. Li, C. Ding, and X. Zhang. Binary matrix factorization with applications. In Seventh IEEE International Conference on Data Mining (ICDM 2007), pages 391400. Institute of Electrical and Electronics Engineers, Oct 2007.    "}
{"index":{"_id":"97"}}
{"datatype":"inproceedings","key":"Bannert:2017:RLA:3027385.3029463","author":"Bannert, Maria and Molenaar, Inge and Azevedo, Roger and Jarvela, Sanna and Gavsevi'c, Dragan","title":"Relevance of Learning Analytics to Measure and Support Students' Learning in Adaptive Educational Technologies","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"568--569","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029463","doi":"10.1145/3027385.3029463","acmid":"3029463","publisher":"ACM","address":"New York, NY, USA","keywords":"adaptive educational technologies, educational data mining, learning analytics, multimodal data, self-regulated learning","Abstract":"In this poster, we describe the aim and current activities of the EARLI-Centre for Innovative Research (E-CIR) Measuring and Supporting Student's Self-Regulated Learning in Adaptive Educational Technologies which is funded by the European Association for Research on Learning and Instruction (EARLI) from 2015 to 2019. The aim is to develop our understanding of multimodal data that unobtrusively capture cognitive, meta-cognitive, affective and motivational states of learners over time. This demands for a concerted interdisciplinary dialogue combining findings from psychology and educational sciences with advances in computer sciences and artificial intelligence. The participants in this E-CIR are leading international researchers who have articulated different emerging perspectives and methodologies to measure cognition, metacognition, motivation, and emotions during learning. The participants recognize the need for intensive collaboration to accelerate progress with new interdisciplinary methods including learning analytics to develop more powerful adaptive educational technologies.","pdf":"Relevance of Learning Analytics to Measure and Support  Students Learning in Adaptive Educational Technologies   Maria Bannert  Technical University of Munich   Arcisstrasse 21  Munich, Germany  +49 89 289 24390   maria.bannert@tum.de   Inge Molenar  Radboud University  Montessorilaan 3   Nijmegen, The Netherlands  +31 24 3611942   I.molenaar@pwo.ru.nl   Roger Azevedo  North Carolina State University   2310 Stinson Drive   Raleigh, NC, 27519, USA   +1 919 515 2254  razeved@ncsu.edu     Sanna Jrvel  University of Oulu  P.O.BOX 2000,   FIN-90014 University of Oulu  +35 8405 77 7164   sanna.jarvela@oulu.fi   Dragan Gaevi  The University of Edinburgh    10 Crichton Street  Edinburgh, EH8 9AB, UK   +44 131 651 3837  dragan.gasevic@ed.ac.uk   ABSTRACT  In this poster, we describe the aim and current activities of the  EARLI-Centre for Innovative Research (E-CIR) Measuring and  Supporting Students Self-Regulated Learning in Adaptive Educa- tional Technologies which is funded by the European Association  for Research on Learning and Instruction (EARLI) from 2015 to  2019. The aim is to develop our understanding of multimodal data  that unobtrusively capture cognitive, meta-cognitive, affective and  motivational states of learners over time. This demands for a con- certed interdisciplinary dialogue combining findings from psycho- logy and educational sciences with advances in computer sciences  and artificial intelligence. The participants in this E-CIR are leading  international researchers who have articulated different emerging  perspectives and methodologies to measure cognition, metacognit- ion, motivation, and emotions during learning. The participants re- cognize the need for intensive collaboration to accelerate progress  with new interdisciplinary methods including learning analytics to  develop more powerful adaptive educational technologies.   CCS Concepts  Algorithms, Experimentation, Human Factors, Standardiza- tion, Theory, Verification.    Keywords  Adaptive Educational Technologies; Educational Data Mining;  Learning Analytics; Multimodal Data; Self-Regulated Learning   1. INTRODUCTION  Even though the recent influx of tablets with learning technologies  in education is promising, the challenge lies in improving adaptive  educational technologies to support students self-regulated learn- ing. These technologies offer immediate individualized instruction   including personalized feedback from real-time data of learner ac- tions and performance. Driven by the emerging field of learning  analytics, these technologies seek to tailor learning experiences  based on learners progress through the measurement, collection,  analysis and reporting of multi-modal cognitive, metacognitive, af- fective, and motivational data.   Current adaptive educational technologies focus on students' per- formance (cognition) to adapt learning materials and largely neg- lect important aspects, such as students metacognition, emotion  and motivation. However, multimodality online trace data such as  log-files, eye gaze behaviours, transpiration, facial expressions of  emotions, heart rate and electro-dermal activity can enhance our  understanding of students processes during learning [1]. For ex- ample, eye gaze data reveals the learners focus at different points  of time and is indicative of the level of cognitive load. Measure- ment of transpiration, heart rate and skin galvanic conductivity re- veals emotional reactions. More specifically, combining multi- modal data can reveal both cognitive and affective states of the  learner and can detect arousal levels and the valence of emotional  reactions. In a learning situation, students are confronted with a va- riety of cognitive challenges (e.g. lack of prior knowledge, task dif- ficulty) which can result in emotional reactions (e.g. frustration,  boredom). Therefore, this cooperation aims to develop our under- standing of multimodal data that unobtrusively capture cognitive,  metacognitive, affective and motivational states of learners over  time in order to design adequate instructional support and scaffolds.    2. NEED OF COMPLEMANTARY EXPER- TISE AND RESEARCH QUESTIONS  Valid online-measures of multimodal data during learning and their  analysis demand for a concerted interdisciplinary dialogue com- bining findings from psychology and educational sciences with  advances in computer sciences and artificial intelligence. The par- ticipants in this E-CIR are leading international researchers who  have articulated different emerging perspectives and methodolo- gies to measure cognition, metacognition, motivation and emotions  during learning. The participants recognize the need for intensive  collaboration to accelerate progress with new interdisciplinary  methods to develop more powerful adaptive educational technolo- gies which would not be possible within individual labgroups.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029463      To guide our E-CIR, we outlined two research questions which are  also highly relevant in the field of learning analytics:   1. How can we analyze multimodal, trace data from existing  adaptive educational technologies using different channels  (e.g., verbalization, phsysiology, navigation behavior) to  measure students' cognitive, metacognitive, emotions and  motivation during learning   2. How can these measurements be used to enhance current  adaptive learning technologies supporting learners self-regu- lated learning through visualisation and recommendation  tools   3. E-CIR RESEARCH ACTIVITIES  Our research questions will be addressed in regular meetings where  we present and discuss research of individual members and plan  collaborative research projects in order to investigate the research  topics as a joint effort among lab groups (see Table 1).    Table 1. Overview of E-CIR Activities   Focus Issues  Review of existing methods   for trace and multimodal-data  collection and analysis   Specifying different channels  (e.g. eye-tracking, physio-   logical) and methods (e.g. pro- cess-mining, video-analysis)   Consolidation of methods Analyzing each others datasets  Multiple data-streams Discussing approaches to   multiple data-streams  Application in education Visualization of data for learners   and teachers  Multiple data-streams Sharing of results and planning   new publications  Application in education Recommendation services   Standardization of methods Discussing different settings  Research agenda for next    decade  Outline remaining research    issues    The meetings will be informed by research of each team members  and will stimulate their future research cooperation. For example,  Bannert et al. [2] are using process mining techniques to analyse  verbal protocols collected during self-regulated learning. Espe- cially, they are interested if instructional scaffolds affect not only  the amount of different SRL activities but also the temporal struc- ture and if so, whether the temporal structure of learning events  corresponds with different dimensions of learning performance.   Molenaars group studies time and order in self and socially regu- lated learning using single or multiple data streams. Specifically,  sequential characteristics of S-SRL consider which actions follow  each other, for example planning was shown to play a critical role  in transitions between low and high cognitive activities [3]. Tem- poral characteristics indicate when those actions are taken during  learning and how actions fluctuate other over time. For example,  successful learners are more likely to engage in metacognitive ac- tivities early in the learning process compared to less-successful  learners. Recently, this group has begun to analyze logs of adaptive  technologies to assess students regulation of effort and learning  over extended time periods.    Azevedo [1] and colleagues research has focused on examining the  role of cognitive, metacognitive, affective, and motivational  (CAMM) self-regulatory processes during learning with advanced  learning technologies. More specifically, the overarching research  goal has been to understand the complex interactions between hu- mans and intelligent learning systems by using interdisciplinary   methods (e.g., log-files, physiological sensors, facial expressions of  emotions, etc.) to measure CAMM processes and their impact on  learning, performance, and transfer. To accomplish this goal, his  team conducts laboratory, classroom, and in-situ (e.g., medical sim- ulator, construction site) studies and collect multi-channel data to  develop models of human-computer interaction; examines the na- ture of temporally unfolding self- and other-regulatory processes  (e.g., human-human and human-artificial agents); and, designs in- telligent learning and training systems to detect, track, model, and  foster learners, teachers, and trainers self-regulatory processes.   Jrvel et al. [4] have been exploring what multimodal data can tell  us about SRL processes in authentic collaborative learning tasks.  They have investigated how multichannel data can be used for iden- tifying markers that characterize successful SRL and learning pro- gress and help in understanding and increasing the evidence about  (a) interactions between different facets of regulation (i.e., cogni- tion, motivation, emotion) (b) temporality and cyclical processes of  regulation, and (c) the occurrence and temporality of different types  of regulation (SRL, CoRL, and SSRL).   Finally, Gaevi and his colleagues [5] have been working on ana- lytical methods for the theory-informed study of self-regulated  learning. Their work involves a broad range of methods for analysis  of clickstream, discourse, and more recently psychophysiological  data. Data are generated in learning activities performed in labora- tory experiments and ecologically valid and open-ended learning  environments including flipped classrooms and (massive open)  online courses. The methods are based on unsupervised and super- vised machine learning, sequence and process mining, automated  text analysis, and social and epistemic network analysis. The use of  these methods allows for detection of a) learning strategy; b) cog- nitive, metacognitive, motivational, social, and affective processes;  and c) interaction between different self-regulatory processes. To  allow for triangulation with psychophysiological data collected  typically as continuous data streams, methods for time series anal- ysis and digital signal processing will be explored in the future  work.   4. ACKNOWLEDGMENTS  Our thanks to EARLI for funding our E-CIR.   5. REFERENCES   [1] Azevedo, R., Taub, M., & Mudrick, N.V. (2017). Using   multi-channel trace data to infer and foster self-regulated  learning between humans and advanced learning technolo- gies. In D. Schunk & Greene, J.A (Eds.), Handbook of self- regulation of learning and performance (2nd ed.). New York,  NY: Routledge.    [2] Bannert, M., Reimann, P., & Sonnenberg, C. (2014). Process  mining techniques for analysing patterns and strategies in  students' self-regulated learning. Metacognition and Learn- ing, 9(2), 161-185.    [3] Molenaar, I., & Chiu, M.M. (2014). Dissecting sequences of  regulation and cognition: Statistical discourse analysis of pri- mary school childrens collaborative learning. Metacognition  and Learning, Vol 9(2), 137-160.   [4] Jrvel, S., Malmberg, J., Sobocinski, M., Haataja, E., &  Kirschner, P. (2016). What multimodal data tell about self- regulated learning process Submitted.    [5] Gaevi, D., Dawson, S., & Siemens, G. (2015). Lets not  forget: Learning analytics are about learning. TechTrends,  59, 1, 64-71.    "}
{"index":{"_id":"98"}}
{"datatype":"inproceedings","key":"Stoeffler:2017:EMC:3027385.3029464","author":"Stoeffler, Kristin and Rosen, Yigal and von Davier, Alina","title":"Exploring the Measurement of Collaborative Problem Solving Using a Human-agent Educational Game","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"570--571","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029464","doi":"10.1145/3027385.3029464","acmid":"3029464","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative problem solving, computer agent, performance assessment, teamwork","Abstract":"Collaborative problem solving (CPS) is a process that relies on both cognitive and social skills contributions by those involved in the joint activity. If a student is matched with a problematic group of peers, then there will be no valid measurement of the CPS skills. In the human-agent settings, CPS skills are measured by pairing each individual student with a computer agent or agents that can be programmed to act as team members with varying characteristics relevant to different CPS skills and contexts. This paper describes current research on measuring CPS skills through human-agent interactions in a prototype of a collaborative educational game.","pdf":"Exploring the Measurement of Collaborative Problem  Solving Using a Human-Agent Educational Game   Kristin Stoeffler  ACT, Inc.   500 ACT Dr.  Iowa City, IA 52240   1-319-337-1000  Kristin.Stoeffler@act.org   Yigal Rosen  Harvard University  125 Mt. Auburn St.   Cambridge, MA 01451  1-781-707-6517   Yigal_Rosen@harvard.edu   Alina von Davier  ACT, Inc.   500 ACT Dr.  Iowa City, IA 52240   1-319-337-1000  Alina.vonDavier@act.org     ABSTRACT  Collaborative problem solving (CPS) is a process that relies on both  cognitive and social skills contributions by those involved in the  joint activity. If a student is matched with a problematic group  of peers, then there will be no valid measurement of the CPS  skills. In the human-agent settings, CPS skills are measured by  pairing each individual student with a computer agent or agents that  can be programmed to act as team members with varying  characteristics relevant to different CPS skills and contexts. This  paper describes current research on measuring CPS skills through  human-agent interactions in a prototype of a collaborative  educational game.   ACM Classification Keywords  K.3.1. Computers and Education: Computer Use in Education  H.5.m. Information Inferences and Presentation   Keywords  Collaborative Problem Solving; Performance Assessment;  Computer agent; Teamwork   1. INTRODUCTION   Collaborative problem solving (CPS) is a critical competency for  college and career readiness [9].  Measurement of such varied skills  presents challenges for assessment and education professionals, but  also new opportunities [13]. CPS requires the ability to recognize  the points of view of other persons in a group; to contribute  knowledge, experience, and expertise in a constructive way; to  identify the need for contributions and how to manage them; to  recognize the structures and procedures involved in resolving a  problem; and as a member of the group, to build and develop group  knowledge and understanding [4] [8]. Collaboration on tasks  enables individuals to communicate about problem situations, join  their perspectives and skills, and solve tasks that are difficult or  impossible to achieve individually. Measurement of such varied  skills presents challenges for assessment and education  professionals, but also new opportunities. This project focuses on  CPS performance assessment, which is part of ACTs Holistic  Framework outlined by Camera, et al. [3] as a comprehensive  description of the knowledge and skills individuals need to know   and be able to do to succeed at school and at work. The skills  required to effectively combine problem solving, and behavioral  strategies to successfully solve a problem within a team context are  outlined in the CPS and includes 18 facets related to the construct.  We chose five of these facets for exploration and prototyping.  The innovation of the prototype resides in designing an interactive  educational game according to a comprehensive framework for  assessing and teaching difficult-to-measure skills, the CPS. Our  learning outcome was to explore the feasibility of measuring these  skills in an interactive and engaging way that will eventually allow  for the ability to provide insights and feedback to participants and  teachers via a dashboard model. To this end, multidisciplinary  teams collaborated to design, develop, and deploy a video game for  middle school students and later to adults.        Figure 1. Circuit Runner Game User Interface   2. COLLABORATIVE GAME WITH  AGENT   In the collaborative game, Circuit Runner, a student navigates a  circuit board maze and interacts with a student bot, or computer  agent, to solve a number of challenges presented at locked gates.  Varying levels of collaborative behaviors, communication, and  problem solving skills are required to successfully navigate the  challenges and unlock the gates (see Figure 1). Previous research  findings [9] support the design decision to use a human-agent  versus a human-human mode allowed for the student to interact  with a computer agent with the predetermined ability to express a  larger range of the CPS skills identified as valuable. The human- agent mode has been shown to allow students to demonstrate a  significantly higher level of performance of CPS skills and  outperform students experiencing the human-human mode [9]. In  part this may be due to the valuable role of conflict in eliciting an  extended range of CPS skills as identified by Mitchell [7],  Scardamalia [10], Stahl [11], and Weinberger [12]. These skills      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise,  or republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.   LAK17, March 1317, 2017, Vancouver, BC, Canada.  Copyright  2017 ACM 978-1-4503-4870-6/17/03   http://dx.doi.org/10.1145/3027385.3029464     might be missed in a human-human mode that cannot guarantee  that such conflict will be present.         Figure 2. Game Clusters Based on Dialog Response Data   3. STUDY I: EXPLORING FEASIBILITY  Over 350 students between the ages of 11 and 14 have played the  game. The game was also accompanied by a set of game usability  and research survey questions. The results include multiple  clustering analyses of the evidence provided over a series of game  plays based on common practices for exploratory analytics outlined  by Bauckhage, et al. [2] and Kerr, et al. [4]. In Figure 1, we show a  view of the clusters derived from a K-Means (K=8) analysis over  normalized skill score dimensions. Each dot within the inner  column represents a game instance skill score for three of the skill  categories: Engagement (EN), Finding Information (FI) and  Monitoring Understanding (MU). The outermost column/ dot color  encoding represents a unique clustering of game scores. A black  line denotes the mean value. This analysis provides an overall  labeling of game performance and can also be used to perform  queries/comparisons between games/players, e.g. select K nearest  neighbor (K-NN) [1] associations of games/players. In addition to  this we also performed a Mixture Model clustering of Gaussians, a  process outlined by McLachlan, et al. [6], that makes soft  assignments of game evidence to clusters. This yields a probability  result for each game to a particular category/cluster group (see  Figure 2).    4. STUDY II: FULL-SCALE  DEPLOYMENT   Our current research effort is focused on expanding the range of  CPS skills measured to include additional problem solving and   behavioral components in the context of workplace (adults). We are  also exploring cross-validation with other CPS measures, as well as  applying machine learning algorithms that will allow a discovery  of new connections and player clusters between response data and  game telemetry.  For our poster session we will review game features designed to  elicit evidence of CPS skills in adults, insights gleaned from the  design and development processes of human-agent interactions, as  well as discuss preliminary findings.   5. ADDITIONAL AUTHORS  Additional authors: Amit Agrawal (ACT, Inc., email:  Amit.Agrawal@act.org)   6. REFERENCES  [1] Arya, Sunil, et al. An Optimal Algorithm for Approximate   Nearest Neighbor Searching Fixed Dimensions. Journal of  the ACM (JACM) 45.6 (1998): 891-923.   [2] Bauckhage, Christian, Anders Drachen, and Rafet Sifa.   Clustering Game Behavior Data.  IEEE Transactions on  Computational Intelligence and AI in Games 7.3 (2015):  266-278.   [3] Camara W, O'Connor R, Mattern K, Hanson MA. Beyond  Academics: A Holistic Framework for Enhancing Education  and Workplace Success. ACT Research Report Series. 2015  (4). ACT, Inc.   [4] Kerr, Deirdre, and Gregory KWK Chung.  Identifying Key  Features of Student Performance in Educational Video  Games and Simulations Through Cluster Analysis.  JEDM- Journal of Educational Data Mining 4.1 (2012): 144-182.   [5] McLachlan, G.J. and Basford, K.E., 1988. Mixture models:  Inference and applications to clustering (Vol. 84). Marcel  Dekker.    [6] Mitchell R, Nicholas S. Knowledge creation in groups: The  value of cognitive diversity, transactive memory and open  mindedness norms. Electronic Journal of Knowledge  Management. 2006;4(1):67-74.   [7] OECD (2013). PISA 2015 Collaborative problem solving  framework. OECD Publishing.   [8] Rosen Y. Computer-based assessment of collaborative  problem solving: Exploring the feasibility of human-to-agent  approach. International Journal of Artificial Intelligence in  Education. 2015 Sep 1;25(3):380-406   [9] Scardamalia M. Collective cognitive responsibility for the  advancement of knowledge. Liberal education in a  knowledge society. 2002 Jun 28;97:67-98.   [10] Stahl G. (2006) Group Cognition: Computer Support for  Building Collaborative Knowledge (Acting with  Technology).    [11] Weinberger A, Fischer F. A framework to analyze  argumentative knowledge construction in computer- supported collaborative learning. Computers & education.  2006 Jan 31;46(1):71-95.   [12] Von Davier, A. A., & Halpin, P. F. (2013). Collaborative  problem solving and the assessment of cognitive skills:  Psychometric considerations. ETS Research Report Series,  2013(2), 1-36       "}
{"index":{"_id":"99"}}
{"datatype":"inproceedings","key":"Jaakonmaki:2017:CLA:3027385.3029465","author":"Jaakonmaki, Roope and Drachsler, Hendrik and Kickmeier-Rust, Michael and Dietze, Stefan and Fortenbacher, Albrecht and Marenzi, Ivana","title":"Cooking with Learning Analytics Recipes","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"572--573","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029465","doi":"10.1145/3027385.3029465","acmid":"3029465","publisher":"ACM","address":"New York, NY, USA","keywords":"applications, cookbook, learning analytics, recipes, solutions","Abstract":"Learning Analytics is a melting pot for a multitude of research fields and origin of many developments about learning and its environment. There is a serious hype over the concepts of learning analytics, however, concrete solutions and applications are comparably scarce. Of course, data rich environments, such as MOOCs, come with statistical analytics dashboards, although the educational value is often limited. Practical solutions for scenarios in data-lean environments or for small-scale organizations are rarely adopted. The LA4S project is dedicated to gather practical solutions, provide a tool box for practitioners, and publish a cook book with concrete learning analytics recipes for everyone.","pdf":"Cooking with Learning Analytics Recipes  Roope Jaakonmki   University of Liechtenstein  roope.jaakonmaeki@uni.li     Stefan Dietze   University of Hannover  dietze@l3s.de   Hendrik Drachsler  Open University of the Netherlands   hendrik.drachsler@ou.nl     Albrecht Fortenbacher  HTW Berlin   albrecht.fortenbacher@HTW-Berlin.de   Michael Kickmeier-Rust  TU Graz   michael.kickmeier-rust@tugraz.at     Ivana Marenzi  University of Hannover   marenzi@l3s.de   ABSTRACT  Learning Analytics is a melting pot for a multitude of research  fields and origin of many developments about learning and its  environment. There is a serious hype over the concepts of learning  analytics, however, concrete solutions and applications are  comparably scarce. Of course, data rich environments, such as  MOOCs, come with statistical analytics dashboards, although the  educational value is often limited. Practical solutions for scenarios  in data-lean environments or for small-scale organizations are  rarely adopted. The LA4S project is dedicated to gather practical  solutions, provide a tool box for practitioners, and publish a cook  book with concrete learning analytics recipes for everyone.   CCS Concepts   Applied computingEducation   Keywords  Learning Analytics; Applications; Solutions; Cookbook; Recipes   1. INTRODUCTION  The community of Learning Analytics (LA) research is rapidly  growing and gaining more and more facets of research,  applications, and theoretical contributions [2]. There is a  substantial hype of the concept [3] and LA has become a melting  pot for contributions of various scientific disciplines such as  Educational Science, Data Science, Legal and Ethical Science,  Management Science and various subfields of those.  LA supports the measurement, analysis and reporting of data  about learners and their contexts for purposes of understanding  and optimizing learning and the environments in which it occurs  (statement on Learning Analytics and Knowledge from the 1st  International Conference in 2011). LA focuses on learning and  teaching activities in digital supported learning environments. As  such, it enables making learning and teaching situations more  transparent for both learners and teachers.  Making education more transparent enables to receive feedback  and status reports about your own learning or teaching track, and  identify indicators of how the learning and teaching processes  could be monitored and improved. Hence, the quality of learning  and teaching processes will be easier to improve by collecting  data (e.g. from learning management systems, or MOOCs) and  information about the learners and educators, and by   implementing methods (e.g. social network analysis, process  mining, student performance regressions) that help to attain LA  objectives (e.g. estimating students skills, visualizing learning  paths, monitoring or predicting student performance). In addition,  there might be multiple ways to achieve LA objectives that one  might not be aware of.  In practice, implementing sophisticated LA solutions often  requires high-level knowledge and skills on areas, such as  machine learning, natural language processing, and data mining,  processing, analysis, and interpretation [4]. This challenges  especially individuals in smaller size universities, who would like  to undertake LA initiatives to improve parts of learning or  teaching actions, but are lacking of the core skills, knowledge, or  resources. Nevertheless, even before having a clear vision,  making big investments, or implementing a specific LA strategy,  these educators might also benefit from trying out some LA  solutions, and help them to decide if LA could be helpful for them  in their cause.  Moreover, although LA has been around for almost six years now,  most LA strategies are still on initial phases when considered the  five step sophistication model of LA developed by the Society of  Learning Analytics Research (SoLAR) [4]. The challenge is to  first make educators aware of LA, and then offer them required  information and tools to start experimenting with. The lack of  know-how and resources prevents these stakeholders to start  experimenting with their own LA initiatives and making solution  more mature and applicable.  LA bears great potential and provides solutions to the heavily  challenged field of education (which is particularly true for K12  or K18 education). In general, gathering and analyzing digital  traces from various learning environments enables to describe  what is happening, explain why something is happening, and  predict when something might be happening in certain learning  situations. Hence, by visualizing these teaching and training  situations, they will become much more transparent. In addition,  LA also enables to adapt teaching better to the needs of the  students, and helps in identifying individual key performance  indicators for learning processes. The challenge is to make these  possibilities available for everyone, including educators,  researchers, university administrators, and others interested in the  field who are lacking the required resources.  Probably there are many potential stakeholders, who could be  interested in implementing LA solutions, but they dont know  how to start with LA in the first place. Recent LA research offers  many examples and use cases how to enhance digital learning,  teaching, and training at various levels, but the challenge is to  generate approaches that would bring LA closer to educators,  researchers, university administrators, and others interested in the  field who dont have the required resources. Currently, the  existing LA does not adequately address this problem.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses,  contact the Owner/Author.    Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029465     What we see in practice is that a large number of practitioners are  craving for realistic, practical, simple, and meaningful solutions  that meet the requirements of their daily practice, and that address  major hurdles (ranging from the need to record a solid data basis  to the need to fulfill privacy and data protection requirements).  Hence, the question is how to support the implementation of LA  initiatives in small, partially low-fidelity and perhaps data-lean  environments, so that they could also improve the quality and  measurement of the teaching and learning activities they are  involved with.    2. LA4S APPROACH  For the mission of support practitioners, we initiated LA4S -  Learning Analytics and Learning Process Management for Small  Size Organizations in Higher Education project, which aims at  bringing more discussion to this transdisciplinary effort, and  endeavors to find an approach how to provide small educational  organizations with an access and knowledge about LA tools,  methods, and datasets. The main objectives of this project are to  integrate already carried out experiences in the LA research field,  and transfer innovations from the field into the area of small size  higher education institutions with an aim to lower the threshold to  build up own LA experiences, for example, as a teacher, or a  university administrator. In addition, we aim at advancing the  field of LA by showcasing the latest results, methodologies, tools,  strategies, and models for this targeted audience. This will  encompass theoretical discussions as well as practical examples  and strategies for the implementation of LA in a classroom, a  department, or an institution.   To effectively gather, summarize, realize, and disseminate  practical solutions, the consortium consists of strong LA research  partners such as the Open University of the Netherlands, the  Technical University of Graz, the University of Hannover, and the  University of Applied Science Berlin. The consortium is led by  the University of Liechtenstein. All partners have LA solutions  that shall be brought into practice; first by deriving an  understanding of the state of the art in LA research and practice  from a LAK Dataset corpus, which offers a near-complete corpus  of LA and EDM research throughout the last 7 years [1]. Based on  the existing literature, we compose a relevant information package  to introduce the educators to LA, and the important related topics  concerning ethics, privacy, and essential theoretical approaches.   Thereafter, we compile solutions that enable to try out different  LA approaches, while presenting them in an easy and appealing  way. We call these LA solutions (similar to use cases) LA recipes.  With a wide range of complete and easily understandable recipes  for LA, all interested chefs are able to start cooking without  previous expertise in the field or investing on LA platforms and  software. In more detail, the ultimate task is to provide relevant  ingredients, methods, and kitchenware that a good LA analytics  cook requires. For this cause, it is important to bring together all  LA researchers and leverage their knowledge to boost education  in an area that is highly important for the whole educational field.    3. LEARNING ANALYTICS COOK BOOK  The result of our research will be a cook book, with a  complementary toolbox web site, that will be ready to use in  different educational institutions, and in different educational  environments. The purpose of this cook book is to disseminate  knowledge for a wide group of stakeholders, especially targeting  educators with limited experience in learning analytics. Besides  focusing on the analysis of learning processes, the cook book also  puts emphasis on ethical issues of learning analytics, as well as on   data privacy and data protection, which are critical issues when  implementing learning analytics in any educational institution.  The information will be disseminated in the form of a cook book,  which consists of a set of recipes, accompanied by an introduction  into specific topics of learning analytics, and a toolbox.  A learning analytics recipe is meant to solve a specific task, e.g.  showing the activity of students with respect to a given learning  task. Examples for recipes are the usage of the visual tool  LEMO21 within a Moodle course, or the tool DojoIBL2 for  inquiry-based learning.   Each recipe first describes the objectives of this specific  cooking, including information on the difficulty of the task and  the level of expertise needed. The section ingredients lists  prerequisites and requirements, which could be necessary data,  technologies or methods to be applied. The core of each recipe  consists of a sequence of steps how to accomplish the goal.  Finally, scenarios are presented in which this recipe can be  applied.  The introductory part of the cook book focuses on special topics  of learning analytics which are relevant for a better understanding  of the recipes, or just important information if an educational  institution wants to implement learning analytics. Examples of  topics are linked data, ethics for learning analytics, learning paths  or prediction of learners success. The toolbox is a supplement to  the cook book and contains links to learning analytics tools and  systems, to guides for users and best practice examples. The cook  book offers comprehensive and easy-to-implement solutions for  learning analytics needs, which has the potential to boost  education in small-size educational organizations which do not  have many resources to implement learning analytics.   4. CALL FOR PARTICIPATION  The practical solution and recipes, however, shall not only come  from the LA4S consortium members. We will initiate an open call  for contributions to find the simplest, yet most delicious LA  recipes.    5. REFERENCES   [1] Dietze, S., Taibi, D. and d'Aquin, M. 2017. Facilitating   Scientometrics in Learning Analytics and Educational Data  Mining - the LAK Dataset. Semantic Web Journal 8, 3, 395- 403.   [2] Gasevic, D., Dawson, S., Mirriahi, N. and Long, P. D. 2015.  Learning Analytics  A Growing Field and Community  Engagement. Journal of Learning Analytics 2, 1, 1-6.    [3] Kickmeier-Rust, M. D. and Albert, D. 2016. Theory-driven  Learning Analytics and Open Learner Modelling: The  Teachers Toolbox of Tomorrow In Proceedings of the 6th  International Workshop on Personalization Approaches in  Learning Environments (Halifax, Canada, July 16th, 2016)  1618, 49-52. CEUR Workshop Proceedings.   [4] Siemens, G., Dawson, S. and Lynch, G. 2013. Improving the  quality and productivity of the higher education sector:  Policy and strategy for systems level deployment of learning  analytics. Australian Government: Office for Learning and  Teaching. Retrieved from https://sydney.edu.au/education- portfolio/ei/projects/SoLAR_Report_2014.pdf.                                                                     1 http://lemo2.org  2 http://dojo-ibl.appspot.com/     1. INTRODUCTION  2. LA4S APPROACH  3. LEARNING ANALYTICS COOK BOOK  4. CALL FOR PARTICIPATION  5. REFERENCES   "}
{"index":{"_id":"100"}}
{"datatype":"inproceedings","key":"Schweighart:2017:UIR:3027385.3029466","author":"Schweighart, M.","title":"Using Item Response Theory to Generate an Item Pool for an e-Learning-system","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"574--575","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029466","doi":"10.1145/3027385.3029466","acmid":"3029466","publisher":"ACM","address":"New York, NY, USA","keywords":"adaptive learning, big data analysis, evaluation, item pool, item response theory, learning analytics","Abstract":"","pdf":"Using Item Response Theory to Generate an Item Pool for  an E-Learning-System    M. Schweighart   Department of Sociology  P.O. Box 8010   Austria  markus.schweighart@uni-graz.at      ABSTRACT  This paper1 demonstrates how the application of item response  theory yields useful item characteristics, which further can be  the foundation of item pools and therefore adaptive educational  software to come.     CCS CONCEPTS   Applied computing  Interactive learning environ- ments;  Applied computing  E-learning;  Information  systems  Database performance evaluation     KEYWORDS  Learning Analytics; Big Data Analysis; Item Response Theory;  Item Pool; Adaptive Learning; Evaluation;   1 INTRODUCTION  In foreign language learning the creation of test items on various  levels of difficulty for different domains of language competence  (i.e. reading, listening, grammar, etc.) is a constant challenge.  The difficulty of learning content is crucial for student motiva- tion and efficacy of learning. Students performance in tests is  associated with test-taking effort and boredom, which on their  part strongly depend on the fit of individual ability and test  difficulty. If the test difficulty is too high, students tend to put  less effort in the task and have a higher chance of getting dis- tracted, which further leads to biased test results for these chil- dren [1]. If test items are too easy, learners occasionally get  bored [5]. To build well-balanced item sets is another challenge.  But as students personalities are individual, different and con- stantly evolving, so are their abilities. Hence, no one-test-fits-all- model can meet the numerous needs. Yet, the fit of the individual  ability and the difficulty of the presented content still remains  the key task. The ability of experts to estimate such item- difficulties is contested, particularly when it comes to guessing  the proportion of correct answers [4]. Therefore empirical based  alternatives are sought. As educational software usage and data                                                                     1Permission to make digital or hard copies of part or all of this work for personal or  classroom use is granted without fee provided that copies are not made or distrib- uted for profit or commercial advantage and that copies bear this notice and the full  citation on the first page. Copyrights for third-party components of this work must  be honored. For all other uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029466   storage capacities expand and analyzing methods improve, new  possibilities of determining item difficulties in an effective, none- theless accurate way arise. Using item response theory is one  promising approach in this respect.   The present study follows this path and applies an item response  theory model on data of learning software for foreign English  language learning. The goal is to go beyond simple descriptive  statistics and get more detailed and elaborated information on  items and users via latent trait models, which can serve addi- tionally as the basis of an item pool and therefore adaptive soft- ware to come. The results will further be compared for different  domains of language competence and limitations and prospects  will be discussed.   2 APPROACH  The analyzed data stems from the MORE! Cyberhomework  software that operates on the e-zone online platform, which  itself is provided by the Austrian school book publisher Helbling.  The MORE! Cyberhomework data at hand has been restricted to  5th grade results for clarity reasons, but will be expanded later  on. However, it includes some 40 million results and roughly 350  different Items.   As mentioned above, models based on the item response theory  (IRT) are applied to the current data. In contrast to classical test  theory in item response theory item characteristics and ability of  the users are computed via maximum likelihood equation [2].  One assumption in IRT is that there is a unidimensional trait of a  user that defines the ability of this person to solve items of a  specific difficulty  IRT is therefore also known as latent trait  theory. As there are different domains of competence of lan- guage learning covered by the software, separate IRT-models  have been applied. Difficulties occur due to the fact that some of  the domains are at least partly intertwined. Certain vocabulary  knowledge is for example essential for understanding text or  audio inputs in reading respectively listening exercises. So in a  first step IRT-models have been applied for grammar and vocab- ulary items only.   In the applied two (item-) parameter latent trait model there are  two item features  difficulty and discrimination. Additionally,  each individual gets an ability-score, depending on not just how  many but also which items got solved correctly. Whereas diffi- culty informs about the probability that a specific item gets  correctly solved by a person with average ability, discrimination  tells if higher ability levels consistently result in higher probabil-  mailto:markus.schweighart@uni-graz.at http://dx.doi.org/10.1145/3027385.3029466   ities of solving the item. If the discrimination parameter is 0, all  students have the same chance to get the item right  whatever  their individual level of ability is. The higher the discrimination  parameter, the better the item discriminates. Difficulty corre- sponds with the required ability level for a 50% probability of  solving the item correctly. Negative difficulty means easy, posi- tive difficulty rather hard items.   One Parameter IRT models were already utilized as one pillar of  adaptive e-learning designs in a few cases [3]. There is less evi- dence of more complex IRT models with more than one item  parameter in learning analytics. The most frequently reason put  forward for the decision against such models is the need of a  large number of anchoring cases (users with results for every  single item) in order to get valid results. This was a problem for  the current data base too. To avoid missing out on the promising  discrimination parameter only items with more than 10,000  results had been used for the IRT-models, which turned out to be  true for approximately 80 % of all items.   3 RESULTS  The results suggest that although there is a strong relationship  between mean item scores and the IRT difficulty parameter (r =  .710, p < .01) there are deviations. These occur consistently in  cases with very low discrimination parameters. A qualitative  inspection of these items reveals that most of them indicate  item-based problems, such as very strict spelling requirements or  a lack of clarity in question design. This holds true for both  grammar and vocabulary items.      Figure 1. IRT Parameter for Grammar Items    Figure 1 shows the distribution of difficulty and discrimination  parameters for MORE! 1 grammar items. The average difficulty  level is rather easy as most of the items have negative difficulty  values. Concerning the discrimination parameter there is a con- siderable amount of good discriminating items (> 1.5), ranging  from a difficulty of -4 to -1. Those items are particularly interest- ing for the design of assessment tests, since they can serve as   decent classifiers. Descriptive analysis of items for higher levels  of education (MORE! 2-4) suggest that those items cover a wider  range of difficulty. Further analysis will be employed.   4 DISCUSSION  Two main results can be highlighted. First, a systematic exami- nation of the items based on the combined parameters of the  latent trait model can be helpful in order to improve the item  design. Especially low discrimination parameters indicate prob- lematic items that need revision. Second, the collected parameter  information can be the foundation of an empirically validated  item pool that works as a cornerstone of future adaptive learning  systems. The computation of item characteristics will be the  basis of a semi-automatized evaluation system. New items with  sufficient results will be continuously included in the growing  item pool. Those items with low discrimination values can be  returned to the authors for inspection and revision. Items that  discriminate well can on the other hand be used for efficient  assessment tests, which are especially useful for adaptive train- ing software and self-directed learning packages.   The person parameter will further be computed for more lan- guage domains which will lead to ability levels for each student  in different domains. A student can for example be good at vo- cabulary but not as good with grammar and listening tasks.  Based on that, a cluster analysis can be applied to identify sever- al types of students. Specific sequences of exercises along with  proper recommendations can be provided in order to foster  successful learning processes.   5 ACKNOWLEDGEMENT  This work was realized in the course of the interdisciplinary  project QualiLeso that is funded by the Austrian Research Pro- motion Agency (FFG). Thereby the Department of Sociology of the  University of Graz cooperates with the business partner Wohlhart  Learning Software in order to establish a quality management  system that includes learning analytics elements.   6 REFERENCES  [1]        [2]      [3]        [4]        [5]   Asseburg, R., and Frey, A. 2013. Too hard, too easy, or just  right The relationship between effort or boredom and   ability-difficulty fit. Psychological Test and Assessment  Modeling, 55(1), 92-104.  Baker, F. B. 2003. The basics of item response theory. Univer- sity of Maryland, College Park, MD: ERIC Clearinghouse on  Assessment and Evaluation. 1-172.  Chen, C. M., and Chung, C. J. 2008. Personalized mobile  English vocabulary learning system based on item response  theory and learning memory cycle. Computers & Educa- tion, 51(2), 624-645.  Impara, J. C., and Plake, B. S. 1998. Teachers' ability to esti- mate item difficulty: A test of the assumptions in the Angoff  standard setting method. Journal of Educational Measure- ment, 35(1), 69-81.  Keller, J., and Suzuki, K. 2004. Learner motivation and e- learning design: A multinationally validated process. Journal  of educational Media, 29(3), 229-239.     "}
{"index":{"_id":"101"}}
{"datatype":"inproceedings","key":"Wham:2017:FSO:3027385.3029467","author":"Wham, Drew","title":"Forecasting Student Outcomes at University-wide Scale Using Machine Learning","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"576--577","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029467","doi":"10.1145/3027385.3029467","acmid":"3029467","publisher":"ACM","address":"New York, NY, USA","keywords":"feature finding, machine learning, modeling, scalability, student success","abstract":"Elements of applied statistics and computer science are quickly integrating and being applied to a diverse set of problems in academia and industry. Here I explore the potential value of this multi-disciplinary approach to applications in higher education by applying it to forecasting course level outcomes for individual students at all of Penn State's campuses. Utilizing hundreds of data sources on individual students, ranging from past performance to current course engagement, I demonstrate the potential accuracy of forecasting techniques at identifying high risk students early in the course term. Our preliminary results suggest that %50 of students that earned a D or F in 2015 could have been identified prior to the start of the course.","pdf":"Forecasting Student Outcomes at University-Wide Scale  Using Machine Learning   Drew Wham PhD.  Penn State Data Scientist   3C Shields Building  University Park   1-(843)-327-5278  Fcw5014@psu.edu     ABSTRACT  Elements of applied statistics and computer science are quickly  integrating and being applied to a diverse set of problems in  academia and industry. Here I explore the potential value of this  multi-disciplinary approach to applications in higher education by  applying it to forecasting course level outcomes for individual  students at all of Penn States campuses. Utilizing hundreds of  data sources on individual students, ranging from past  performance to current course engagement, I demonstrate the  potential accuracy of forecasting techniques at identifying high  risk students early in the course term. Our preliminary results  suggest that %50 of students that earned a D or F in 2015 could  have been identified prior to the start of the course.    CCS Concepts  Computing methodologies~Machine learning approaches   Computing methodologies~Supervised learning by  regression    Applied computing~Education   Keywords  Machine Learning; Student Success; Feature Finding; Modeling;  Scalability   1. INTRODUCTION  In 2015 Penn State hosted more than 800,000 unique   student course enrollments across its main, branch and online  campuses. Of those unique course enrollments, %6.4 resulted in a  course withdrawal and %7.1 resulted in a grade of D or F. Both of  these results (GPA < 2.0 or withdrawing) result in regret for  both the student and the institution since students earning a D, F  or W fail to make progress toward graduation. Early and accurate  identification of these students would provide opportunities for  advisors to recommend better course sequencing and/or direct  students to better utilize learning resources when they are  identified as at risk.   A model was therefore developed to predict student  outcomes prior to the start of the semester (day-zero). Others have  previously used logistic regression [2,3] and classification and  regression trees [3] to predict at-risk students prior to enrollment  by modeling end of semester GPA. The approach presented here   is similar in that it is trained on a number of the same features that  range from the students previous academic achievements, the  students demographic characteristics and current campus  involvement. However, the model presented here is more  granular, because it was designed to predict both the probability  of withdrawal and the GPA of every student for every enrolled  class. To this end, it was also trained on historical data from the  individual course and instructor. Here we describe the  development of this model and its resulting relative accuracy.      2. METHODS  2.1 Forecast Model Development    The primary data model was developed from data in Penn  States data warehouse. This data model comprised all transcript  records from Fall 2010 to Summer 2016 totaling over 5.5 million  unique records from over 250,000 unique students. The primary  data model also contained a number of predictive features ranging  from students previous academic achievements (high school  GPA, high school class rank, SAT scores, cumulative GPA etc.),  the students demographic characteristics (gender, age, etc.)  campus involvement (number of enrolled credits, athlete status,  application type etc.) and historical data from the individual  course and instructor (average grade, course level etc.). This  Primary data model was then split into a training and test set, with  all semesters from Fall 2010 to Summer 2015 going into the  training set and Fall 2015, Spring 2016 and Summer 2016 going  into the test set.   Two models were then developed utilizing the statistical  programing language R [4] and Python using the gradient boosted  tree based machine learning algorithm [1] XGBoost [5]. Both  models were trained on 5 years of data (FA2010-SU2015) and  tested on one year of data (FA2015-SU2016). The Day-Zero  withdrawal model was trained using a logistic regression objective  function and a log-loss evaluation metric. After training the  withdrawal model, all students that withdrew from classes and did  not earn a grade were removed from the dataset. We then trained  the Day-Zero grade model using a linear regression objective  function against the earned GPA of the students on a 4.0 scale  using a root mean squared error evaluation metric.    3. RESULTS  3.1 Day-Zero Model   The models were evaluated both on the training data as  well as the test data. The day-zero withdrawal model had a log- loss of 0.174 on the training data and 0.196 on the test data. The   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029467     day-zero grade model had a root mean squared error of 0.725 on  the training data and 0.757 on the test data.    Figure 1 shows the relationship between prediction error  and the frequency of that error in the day-zero grade model on the  test data. Here error was calculated as the predicted grade minus  the observed grade. In this model most errors are relatively small  (near zero) with a slight bias towards under predicting. The mean  absolute error of the day-zero grade model is 0.533. The model,  however, makes more large errors in the positive direction than  in the negative direction. That is, there are more students that do  not do as well as expected than students that do better than  expected. This unbalance distribution at the extremes likely  explains the slight negative bias which is likely due to the  evaluation metric giving additional weight to large errors.          Figure 1. Error of Day Zero Model   To better understand which courses tended to have  relatively high and relatively low prediction accuracy we  calculated the mean absolute error for every unique course  (n=9754) we then filtered out courses for which there were less  than 500 student records and then subset out the 25 courses with  the most accurate and 25 courses with the least accurate average  predictions.    Table 1. Subset of the Courses with the most accurate average  prediction      Table 2. Subset of the courses with the least accurate average  prediction     Table 1 shows a subset of the courses with the most   accurate predictions and table 2 shows a subset of the courses  with the least accurate predictions. The courses with the most  accurate predictions spanned a wide range of disciplines including  Accounting, Biology, Chemistry, Finance, Kinesiology, Nursing  and Spanish. The courses with the least accurate predictions also  spanned a wide range of disciplines including Art, Chemistry,  Economics, Math and Theater. Surprisingly, some disciplines,  such as Chemistry, had courses that appeared on both extremes of  prediction accuracy. This suggests that model accuracy was not  highly linked to any particular discipline. Rather, an emergent  pattern was that courses that tended to have the least accurate  predictions seemed to be more commonly taken early in a students  career whereas courses that tended to have the most accurate  predictions tended to be taken later in the academic career. This is  perhaps an unsurprising result because the amount of data that is  available for making a prediction increases over the students time  at the institution so it follows that better predictions would be  available to 3rd and 4th year students than 1st and 2nd year students.            4. REFERENCES  [1] Friedman, J.H. 2001. Greedy function approximation: A   gradient boosting machine. The Annals of Statistics 29, 1189- 1232.   [2] Gansemer-Topf, A. M., Compton, J., Wohlgemuth, D.,  Forbes, G., & Ralston, E. 2015. Modeling Success: Using  Preenrollment Data to Identify Academically At-Risk  Students. Strategic Enrollment Management Quarterly, 3(2),  109-131.   [3] Kaleita, A. L., Forbes, G. R., Ralston, E., Compton, J. I.,  Wohlgemuth, D. 2016.  Pre-Enrollment Identification of At- Risk Students in a Large Engineering College.  International  Journal of Engineering Education 32(4) 1647.   [4] R Core-Team. 2015. R: A language and environment for  statistical computing. https://www.R-project.org   [5] XGBoost. 2016. https://xgboost.readthedocs.io              "}
{"index":{"_id":"102"}}
{"datatype":"inproceedings","key":"Wham:2017:FSO:3027385.3029467","author":"Wham, Drew","title":"Forecasting Student Outcomes at University-wide Scale Using Machine Learning","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"576--577","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029467","doi":"10.1145/3027385.3029467","acmid":"3029467","publisher":"ACM","address":"New York, NY, USA","keywords":"feature finding, machine learning, modeling, scalability, student success","Abstract":"Elements of applied statistics and computer science are quickly integrating and being applied to a diverse set of problems in academia and industry. Here I explore the potential value of this multi-disciplinary approach to applications in higher education by applying it to forecasting course level outcomes for individual students at all of Penn State's campuses. Utilizing hundreds of data sources on individual students, ranging from past performance to current course engagement, I demonstrate the potential accuracy of forecasting techniques at identifying high risk students early in the course term. Our preliminary results suggest that %50 of students that earned a D or F in 2015 could have been identified prior to the start of the course.","pdf":"Forecasting Student Outcomes at University-Wide Scale  Using Machine Learning   Drew Wham PhD.  Penn State Data Scientist   3C Shields Building  University Park   1-(843)-327-5278  Fcw5014@psu.edu     ABSTRACT  Elements of applied statistics and computer science are quickly  integrating and being applied to a diverse set of problems in  academia and industry. Here I explore the potential value of this  multi-disciplinary approach to applications in higher education by  applying it to forecasting course level outcomes for individual  students at all of Penn States campuses. Utilizing hundreds of  data sources on individual students, ranging from past  performance to current course engagement, I demonstrate the  potential accuracy of forecasting techniques at identifying high  risk students early in the course term. Our preliminary results  suggest that %50 of students that earned a D or F in 2015 could  have been identified prior to the start of the course.    CCS Concepts  Computing methodologies~Machine learning approaches   Computing methodologies~Supervised learning by  regression    Applied computing~Education   Keywords  Machine Learning; Student Success; Feature Finding; Modeling;  Scalability   1. INTRODUCTION  In 2015 Penn State hosted more than 800,000 unique   student course enrollments across its main, branch and online  campuses. Of those unique course enrollments, %6.4 resulted in a  course withdrawal and %7.1 resulted in a grade of D or F. Both of  these results (GPA < 2.0 or withdrawing) result in regret for  both the student and the institution since students earning a D, F  or W fail to make progress toward graduation. Early and accurate  identification of these students would provide opportunities for  advisors to recommend better course sequencing and/or direct  students to better utilize learning resources when they are  identified as at risk.   A model was therefore developed to predict student  outcomes prior to the start of the semester (day-zero). Others have  previously used logistic regression [2,3] and classification and  regression trees [3] to predict at-risk students prior to enrollment  by modeling end of semester GPA. The approach presented here   is similar in that it is trained on a number of the same features that  range from the students previous academic achievements, the  students demographic characteristics and current campus  involvement. However, the model presented here is more  granular, because it was designed to predict both the probability  of withdrawal and the GPA of every student for every enrolled  class. To this end, it was also trained on historical data from the  individual course and instructor. Here we describe the  development of this model and its resulting relative accuracy.      2. METHODS  2.1 Forecast Model Development    The primary data model was developed from data in Penn  States data warehouse. This data model comprised all transcript  records from Fall 2010 to Summer 2016 totaling over 5.5 million  unique records from over 250,000 unique students. The primary  data model also contained a number of predictive features ranging  from students previous academic achievements (high school  GPA, high school class rank, SAT scores, cumulative GPA etc.),  the students demographic characteristics (gender, age, etc.)  campus involvement (number of enrolled credits, athlete status,  application type etc.) and historical data from the individual  course and instructor (average grade, course level etc.). This  Primary data model was then split into a training and test set, with  all semesters from Fall 2010 to Summer 2015 going into the  training set and Fall 2015, Spring 2016 and Summer 2016 going  into the test set.   Two models were then developed utilizing the statistical  programing language R [4] and Python using the gradient boosted  tree based machine learning algorithm [1] XGBoost [5]. Both  models were trained on 5 years of data (FA2010-SU2015) and  tested on one year of data (FA2015-SU2016). The Day-Zero  withdrawal model was trained using a logistic regression objective  function and a log-loss evaluation metric. After training the  withdrawal model, all students that withdrew from classes and did  not earn a grade were removed from the dataset. We then trained  the Day-Zero grade model using a linear regression objective  function against the earned GPA of the students on a 4.0 scale  using a root mean squared error evaluation metric.    3. RESULTS  3.1 Day-Zero Model   The models were evaluated both on the training data as  well as the test data. The day-zero withdrawal model had a log- loss of 0.174 on the training data and 0.196 on the test data. The   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029467     day-zero grade model had a root mean squared error of 0.725 on  the training data and 0.757 on the test data.    Figure 1 shows the relationship between prediction error  and the frequency of that error in the day-zero grade model on the  test data. Here error was calculated as the predicted grade minus  the observed grade. In this model most errors are relatively small  (near zero) with a slight bias towards under predicting. The mean  absolute error of the day-zero grade model is 0.533. The model,  however, makes more large errors in the positive direction than  in the negative direction. That is, there are more students that do  not do as well as expected than students that do better than  expected. This unbalance distribution at the extremes likely  explains the slight negative bias which is likely due to the  evaluation metric giving additional weight to large errors.          Figure 1. Error of Day Zero Model   To better understand which courses tended to have  relatively high and relatively low prediction accuracy we  calculated the mean absolute error for every unique course  (n=9754) we then filtered out courses for which there were less  than 500 student records and then subset out the 25 courses with  the most accurate and 25 courses with the least accurate average  predictions.    Table 1. Subset of the Courses with the most accurate average  prediction      Table 2. Subset of the courses with the least accurate average  prediction     Table 1 shows a subset of the courses with the most   accurate predictions and table 2 shows a subset of the courses  with the least accurate predictions. The courses with the most  accurate predictions spanned a wide range of disciplines including  Accounting, Biology, Chemistry, Finance, Kinesiology, Nursing  and Spanish. The courses with the least accurate predictions also  spanned a wide range of disciplines including Art, Chemistry,  Economics, Math and Theater. Surprisingly, some disciplines,  such as Chemistry, had courses that appeared on both extremes of  prediction accuracy. This suggests that model accuracy was not  highly linked to any particular discipline. Rather, an emergent  pattern was that courses that tended to have the least accurate  predictions seemed to be more commonly taken early in a students  career whereas courses that tended to have the most accurate  predictions tended to be taken later in the academic career. This is  perhaps an unsurprising result because the amount of data that is  available for making a prediction increases over the students time  at the institution so it follows that better predictions would be  available to 3rd and 4th year students than 1st and 2nd year students.            4. REFERENCES  [1] Friedman, J.H. 2001. Greedy function approximation: A   gradient boosting machine. The Annals of Statistics 29, 1189- 1232.   [2] Gansemer-Topf, A. M., Compton, J., Wohlgemuth, D.,  Forbes, G., & Ralston, E. 2015. Modeling Success: Using  Preenrollment Data to Identify Academically At-Risk  Students. Strategic Enrollment Management Quarterly, 3(2),  109-131.   [3] Kaleita, A. L., Forbes, G. R., Ralston, E., Compton, J. I.,  Wohlgemuth, D. 2016.  Pre-Enrollment Identification of At- Risk Students in a Large Engineering College.  International  Journal of Engineering Education 32(4) 1647.   [4] R Core-Team. 2015. R: A language and environment for  statistical computing. https://www.R-project.org   [5] XGBoost. 2016. https://xgboost.readthedocs.io              "}
{"index":{"_id":"103"}}
{"datatype":"inproceedings","key":"Chen:2017:BTE:3027385.3029469","author":"Chen, Guanliang and Davis, Dan and Krause, Markus and Hauff, Claudia and Houben, Geert-Jan","title":"Buying Time: Enabling Learners to Become Earners with a Real-world Paid Task Recommender System","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"578--579","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029469","doi":"10.1145/3027385.3029469","acmid":"3029469","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, learning analytics, learning design","Abstract":"Massive Open Online Courses (MOOCs) aim to educate the world, especially learners from developing countries. While MOOCs are certainly available to the masses, they are not yet fully accessible. Although all course content is just clicks away, deeply engaging with a MOOC requires a substantial time commitment, which frequently becomes a barrier to success. To mitigate the time required to learn from a MOOC, we here introduce a design that enables learners to earn money by applying what they learn in the course to real-world marketplace tasks. We present a Paid Task Recommender System (Rec-$ys), which automatically recommends course-relevant tasks to learners as drawn from online freelance platforms. Rec-$ys has been deployed into a data analysis MOOC and is currently under evaluation.","pdf":"Buying Time: Enabling Learners to become Earners with a Real-World Paid Task Recommender System  Guanliang Chen Delft University of Technology  Delft, the Netherlands guanliang.chen@tudelft.nl  Dan Davis Delft University of Technology  Delft, the Netherlands d.j.davis@tudelft.nl  Markus Krause UC Berkeley ICSI  Berkeley, California USA markus@icsi.berkeley.edu  Claudia Hauff Delft University of Technology  Delft, the Netherlands c.hauff@tudelft.nl  Geert-Jan Houben Delft University of Technology  Delft, the Netherlands g.j.p.m.houben@tudelft.nl  ABSTRACT Massive Open Online Courses (MOOCs) aim to educate the world, especially learners from developing countries. While MOOCs are certainly available to the masses, they are not yet fully accessible. Although all course content is just clicks away, deeply engaging with a MOOC requires a substan- tial time commitment, which frequently becomes a barrier to success. To mitigate the time required to learn from a MOOC, we here introduce a design that enables learners to earn money by applying what they learn in the course to real-world marketplace tasks. We present a Paid Task Recommender System (Rec-$ys), which automatically rec- ommends course-relevant tasks to learners as drawn from online freelance platforms. Rec-$ys has been deployed into a data analysis MOOC and is currently under evaluation.  CCS Concepts Applied computing  Learning management sys- tems;  Keywords Learning Analytics, Learning Design, MOOCs  1. INTRODUCTION To alleviate the challenge of learners devoting time to en-  gage with MOOCs, we have developed a system that try to enable learners to earn money while taking MOOCs, thus buying time. But how can we enable hundreds of thou- sands of learners to earn money all at once To achieve this,  The authors research is supported by the Extension School of the Delft University of Technology. The authors research is supported by the Leiden-Delft- Erasmus Centre for Education and Learning.  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029469  we propose that online freelance platforms like UpWork1 can be included to formulate the solution. If we can automat- ically provide learners with recommendations of freelance paid tasks which are relevant to the course content, learn- ers can compete for solving these tasks as a means to earn money and thus better justify persisting through the course. As a foundation, our previous study [1] demonstrated that learners can solve these real-world paid tasks in high qual- ity. In this poster, we advance the research by implementing a Paid Task Recommender System (Rec-$ys), which auto- matically collects course-related tasks from UpWork and rec- ommends them to learners. The system has been deployed into the EX101x Data Analysis: Take It to theMAX(), which is currently running on the edX platform. The course aims to teach introductory data analysis skills using spreadsheet.  We realize that whether learners will be selected to com- plete a task depends on not only their knowledge but also their experience. In other words, these learners will probably not be selected or earn money. But even so, we are still in- terested what effects these financial incentives of real-world tasks would have on learners and what potential strategies could be adopted to turn learners into earners. We hypothe- size that, by realizing the financial benefits to be gained from the external freelance platform, learners will exhibit higher engagement and completion rates in the course. By deploy- ing Rec-$ys in an experimental setup, we will investigate the following questions:   What are the effects of (continuously) presenting real- world paid tasks relevant to the course   Can learners benefit from real-world paid tasks, e.g., earning money or fostering their interests on the MOOC subject   How does the payoff of the task affect learner engage- ment For example, are easy tasks with low pay- ment more attractive than difficult tasks with high payment  2. SYSTEM ARCHITECTURE To make Rec-$ys easily reusable in a variety of MOOCs  as well as online freelance platforms, we adopt a modular  1https://www.upwork.com/  http://dx.doi.org/10.1145/3027385.3029469   Figure 1: Rec-$ys architecture. The yellow overlay indicates which modules have been constructed or are being constructed.  structure in the development process, as depicted in Figure 1. We briefly describe the structure layer by layer:  MOOC: The MOOC layer serves as the playground for learners to interact with course components as well as Rec-$ys. The course is hosted by edX, which allows us to deploy the system as an iFrame component. By doing this, learners can interact with Rec-$ys in the same way they interact with other course material. Based on the available course material (e.g., lecture video transcripts, course description), the module Content extractor identi- fies two types of keywords: 1) general keywords describ- ing the MOOC subject and 2) content keywords specifying the detailed course content. For the data analysis MOOC, these keywords are identified by first extracting the top- k most frequent terms from the lecture video transcripts and then filtering out irrelevant ones by hand. A more automatic and advanced extraction method should be in- cluded so as to process courses with different subjects.  Data layer: This layer is responsible for: 1) keeping track of learners activities and 2) collecting paid tasks from on- line freelance platforms. To be specific, the MOOC data collector collects all of the data generated during learners interaction with the course material (e.g., watching lecture videos, answering quiz questions) and Rec-$ys (e.g., re- questing more tasks, clicking task links and jumping to freelance platforms, submitting feedback about the rec- ommended task). On the other hand, the module Free- lance task collector takes the general keywords generated by the module Content extractor as input and search for course-relevant tasks from freelance platforms like Up- Work, Witmart, Guru, etc. Currently, we only collect tasks from UpWork. As some retrieved tasks are not rel- evant to the course (e.g., those with high budget and re- quire much more advanced skills/knowledge to solve), the  module Task filtering filters out tasks which meet the fol- lowing criteria: 1) no longer available; 2) without a fixed payment (i.e., hourly job) or the payment exceeds $250. After that, the Data processing module translates the col- lected data into a queryable format and stores them in database. In addition, as there are likely multiple stu- dents (plus the freelancers in freelance platforms) compet- ing for solving the same task, the module Task availability tracker regularly checks the status of tasks and updates them in the database so that the system will only recom- mend learners with tasks that are still available.  Analysis layer: This layer aims to analyzing the relevance of tasks for learners based on their interaction with Rec- $ys. In the long run, we expect that such relevance (so as the recommendations) can be computed in a individually- personalized manner. However, as we currently have lit- tle knowledge about what learner features should be con- sidered when calculating this relevance, the module Task relevance estimation only uses the content keywords gen- erated by the module Content extractor as input and cal- culate the relevance score as these keywords occurrence in the task title, description and required skills stated by the task publisher.  Intervention layer: To avoid a learner keeps receiving the same task or tens of thousands of learners compete for the same task, this layer dedicates to diversifying tasks recommended to different learners. At present, we use a randomization method to achieve. For a learner, we first retrieve the top-k most relevant tasks from the database; then, two out of these tasks are randomly selected as the returned recommendations.  3. ONGOING WORK Rec-$ys has been deployed in a MOOC which runs from  November 22, 2016 to May 23, 2018 in a self-paced mode. This enables us to continuously collect tasks from UpWork and recommend them to learners while observing how learn- ers interact with these tasks over an extended period of time. In the next stage, we plan to: 1) analyze learner activity data in answering our research questions; 2) improve the measurement of task relevance for the course; 3) explore ad- ditional strategies to diversify recommendation results; and 4) explore methods about how our learners should be men- tored so that they are likely to win task bidding in freelance platforms [2].  4. REFERENCES [1] G. Chen, D. Davis, M. Krause, E. Aivaloglou, C. Hauff,  and G. J. Houben. Can learners be earners investigating a design to enable mooc learners to apply their skills and earn money in an online market place. IEEE Transactions on Learning Technologies, pages 112, 2016.  [2] R. Suzuki, N. Salehi, M. S. Lam, J. C. Marroquin, and M. S. Bernstein. Atelier: Repurposing expert crowdsourcing tasks as micro-internships. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI 16, pages 26452656, New York, NY, USA, 2016. ACM.    Introduction  System Architecture  Ongoing Work  References   "}
{"index":{"_id":"104"}}
{"datatype":"inproceedings","key":"Atapattu:2017:DAI:3027385.3029470","author":"Atapattu, Thushari and Falkner, Katrina","title":"Discourse Analysis to Improve the Effective Engagement of MOOC Videos","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"580--581","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029470","doi":"10.1145/3027385.3029470","acmid":"3029470","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, coh-metrix, discourse, linguistic, video analytics","Abstract":"Lecture videos are amongst the most commonly used instructional methods in present Massive Open Online Courses (MOOCs). As the main form of instruction, students' engagement behaviour with MOOC videos directly impacts the students' success or failure. This research focuses on an in-depth analysis of 1.5 million video interactions (e.g. pause, seek video) of a Programming MOOC. Our video-by-video analysis explores the rationale behind the time-wise variation of video interactions. We aim to analyse discourse features (e.g. syntactic simplicity of text, and speaking rate) and their correlation with the video interaction patterns. This paper presents preliminary results and educational video design implications.","pdf":"Discourse Analysis to Improve the Effective Engagement  of MOOC Videos   Thushari Atapattu  School of Computer Science  The University of Adelaide   Adelaide, Australia  (+61)883139077   thushari.atapattu@adelaide.edu.au   Katrina Falkner  School of Computer Science  The University of Adelaide   Adelaide, Australia  (+61)883136178   katrina.falkner@adelaide.edu.au       ABSTRACT  Lecture videos are amongst the most commonly used   instructional methods in present Massive Open Online Courses   (MOOCs).  As the main form of instruction, students   engagement behaviour with MOOC videos directly impacts the   students success or failure. This research focuses on an in-depth   analysis of 1.5 million video interactions (e.g. pause, seek video)   of a Programming MOOC. Our video-by-video analysis   explores the rationale behind the time-wise variation of video   interactions. We aim to analyse discourse features (e.g. syntactic   simplicity of text, and speaking rate) and their correlation with   the video interaction patterns. This paper presents preliminary   results and educational video design implications.   Categories and Subject Descriptors   Applied computing~Distance learning   Keywords  MOOCs; video analytics; discourse; linguistic; Coh-Metrix.   1. INTRODUCTION  Despite the significant usage of assessments, discussion forums   and collaborative activities, lecture videos remain the most   widely used instructional methods throughout the course span of   many Massive Open Online Courses (MOOCs) [1]. Therefore,   students engagement with lecture videos is likely to associate   with students success or failure in the MOOCs. However,   limited research studies have focused on in-depth video   analytics apart from exploring the video watching behaviour [2-  4]. Li et al. [2] found that replays and frequent pauses   significantly correlate with the perceived video difficulty (e.g.   easy, neutral, difficult). Kim et al. [3] explored the aspects of   video interaction patterns and in-video drop-outs and found that   61% of video drop-outs are associated with the visual transition   (e.g. changing from whiteboard explanation to a talking head).   Our work is building on the existing research by Kim et al. [3] to   explore the association between video interactions and non-  visual (i.e. verbal) transition. Another preliminary study by Kim   et al. [4] found that video interaction peaks occur during topic   transition. We extend this approach to conduct an in-depth   video-by-video analysis to measure the effect of discourse   features for the variation of video interactions events (e.g. pause,   seek video).  Our analysis explores whether the underlying   pedagogical and discourse processing theories are considered   when creating MOOC videos.    2. METHODOLOGY  2.1 Data  We analyse the AdelaideX Programming MOOC (code101x)   offered during 2015. During the initial offering, 26,129   participants were registered (active  13,930, received verified   certificate  831). The course covers introductory programming   concepts and creating artwork, animations with ProcessingJS.   The course lasts for 6 weeks with an average of 8 videos per   week. The average length of a video is 3.63 minutes. The course   was taught by 3 lecturers, each sharing approximately one-third   of the syllabus. The 3 lecturers share similar presentation style   (e.g. talking head, programming screens). We extracted de-  identifiable 1.5 million records of video interaction events (e.g.   play/pause, seek video)1. Our text corpus for discourse analysis   contains 51 video transcript files (SubRip Text) with a total of   1631 sentences and associated time.    2.2 Method  Our study aims to answer the following research question;   Do the discourse features correlate with interaction patterns of   MOOC videos If so, what specific features are they   2.2.1 Video data processing  The cleaning of the dataset mainly focuses on removing records   without a video id. We eliminate first and last few seconds (5-10   seconds) of each video due to auto-load/stop. We extracted the   amount of video interactions per second in each video. From   this, we eliminate the records when students do not return to the   video after a reasonable timeframe, assuming that the particular   pause is not related to curiosity or confusion. Likewise, we   analyse seek event (e.g. skip interval), speed change, show   closed captions/transcripts which could potentially associate   with the issues of discourse processing. We exclude load and   stop events from our analysis.   2.2.2 Discourse analysis  Discourse analysis involves extracting sentences from   transcripts to measure discourse and linguistic features. We   utilise Coh-Metrix 3.0 [5] to identify features (see Table 1).   However, we re-implement many of the features since the tool   does not provide an API for sentence-by-sentence analysis and   the manual use of web tool2 is a time-consuming task.                                                                       1 http://edx.readthedocs.io/projects/devdata/en/latest/internal_data_formats/tracking_logs.html   2 http://tool.cohmetrix.com/   Permission to make digital or hard copies of part or all of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for third-  party components of this work must be honored. For all other uses, contact  the Owner/Author. Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029470     3. RESULTS  Figure 1 shows all video interactions (n = 73,257) of a sample   video. A manual inspection suggests that non-visual transitions   are motives for some interaction peaks. For instance, introducing   the new concept function causes for the highlighted peak   (Figure 1). Note: Analysis of individual events (e.g. pause) is   not presented in this paper due to space limitations.      Fig. 1: Video interaction-time of a sample video   We measure the correlation between discourse features and   video interactions using Pearson Correlation Coefficient (r).    Table 1. Correlation between discourse features and video   interactions   Feature Sub-feature (correlation)   Descriptive Word count (0.558*), Syllable count (0.556*)   Text Easability   PC   Narrativity (-0.347), syntactic simplicity (-0.247), word   concreteness (-0.156), referential cohesion (-0.103),   deep cohesion (-0.005), connectivity (-0.514*)   Lexical  diversity   Type-token ratio (-0.47*), MLTD (0.487*)   Connectives Causal (-0.56*), logical (0.022), contrastive (0.03),    Word  information   Pronoun (-0.115) , content word frequency (-0.317*),  familiarity (0.193) , concreteness (0.186),    Speaking rate Time per sentence (0.562*)   *p<0.001   According to the Table 1, descriptive features (e.g. word count)   demonstrate a strong positive correlation with video interactions   (Figure 2). This denotes that the longer the sentence, it is likely   that the students highly interact with the video (e.g. pause)   which is an indicator for confusion or curiosity [3]. Text   Easability features (e.g. narrativity) are mostly supportive for   discourse processing [5], demonstrating a negative correlation   with the video interactions. However, these correlations are not   significant except connectivity. We obtained a contradictory   result for lexical diversity. Lexical diversity measures the ratio   between unique words and the total number of tokens in a   sentence. Lexically diverse texts are difficult to process, and   hence expected to correlate positively with video interactions.   However, type-token ratio demonstrates a strong negative   correlation. A manual inspection of a random sample of the   corpus suggests that lexical diversity negatively correlates with   the sentence length. To overcome this, an estimation algorithm   is proposed to measure lexical diversity (i.e. MLTD) [5], which   provides our expected outcome. An interesting finding is   observed on causal connectives. Sentences with causal   connectives (e.g. because, so) demonstrate a strong negative   correlation (r= -0.56) with video interactions. Causal   relationships present a consequence, allowing learners to   understand the reason-why relation. Our results claim that   inclusion of causal connectives in the video discourse will   improve the students discourse processing. In contrast, other   connectives such as logical (e.g. and, or), contrastive (e.g.   however) do not correlate with video interactions. However,   these claims are not conclusive as the study presented here is   preliminary. We also analyse the impact of pronouns (e.g. I,   you, them) in text for the video engagement and obtained no   correlation. However, frequent content words supported for   discourse processing (r= -0.316). Moreover, spoken discourse   features like speaking rate positively correlated with video   interactions, demonstrating that the lecturers time spent on each   sentence will impact on students engagement with the video.      Fig. 2: Correlation between video interactions and word   count of a sample video; video interactions = frequency*50   4. IMPLICATIONS  This preliminary work establishes a step towards the   consideration of discourse to construct educational videos for   MOOCs. Our results highlight the importance of avoiding   lengthy sentences, reducing lexical diversity and speaking rate,   increasing the use of causal connectives, and frequent use of   content words. MOOC educators/video designers could consider   the data-driven implications of this research to improve video   engagement. In future, we intend to expand this research using   larger text corpora and few more MOOCs to measure the effect   of features like cohesion, semantic analysis, and situation model.    5. ACKNOWLEDGEMENT  Authors would like to acknowledge the AdelaideX team for   providing data for conducting this research.   6. REFERENCES  [1] Breslow, L. B. et al. 2013. Studying Learning in the   Worldwide Classroom: Research into edXs First MOOC.   Research & Practice in Assessment. 8, 13-25.   [2] Li, N., . Kidziski, P. Jermann and P. Dillenbourg. 2015.  MOOC Video Interaction Patterns: What Do They Tell Us   Design for Teaching and Learning in a Networked World.   Lecture Notes in Computer Science, 9307    [3] Kim, J., P. J. Guo, D. T. Seaton, P. Mitros, K. Z. Gajos and  R. C. Miller. 2014a. Understanding in-video dropouts and   interaction peaks in online lecture videos. In Proceedings   of the First ACM Conference on Learning @ Scale.    [4] Kim, J., K. Z. Gajos, S. Li, R. C. Miller and C. J. Cai.  2014b. Leveraging video interaction data and content   analysis to improve video learning. In CHI 2014 Workshop   on Learning Innovation at Scale.   [5] McNamara, D. S. A. C. Graesser, P. M. McCarthy and Z.  Cai. 2014. Automated Evaluation of Text and Discourse   with Coh-Metrix. Cambridge University Press, Cambridge,   M.A.        "}
{"index":{"_id":"105"}}
{"datatype":"inproceedings","key":"Kovanovic:2017:URT:3027385.3029471","author":"Kovanovi'c, Vitomir and Joksimovi'c, Sre'cko and Poquet, Oleksandra and Hennis, Thieme and Dawson, Shane and Gavsevi'c, Dragan and de Vries, Pieter and Hatala, Marek and Siemens, George","title":"Understanding the Relationship Between Technology Use and Cognitive Presence in MOOCs","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"582--583","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029471","doi":"10.1145/3027385.3029471","acmid":"3029471","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, community of inquiry model, student clustering","Abstract":"In this poster, we present the results of the study which examined the relationship between student differences in their use of the available technology and their perceived levels of cognitive presence within the MOOC context. The cognitive presence is a construct used to measure the level of practical inquiry in the Communities of Inquiry model. Our results revealed the existence of three clusters based on student technology use. The clusters significantly differed in terms of their levels of cognitive presence, most notably they differed on the levels of problem resolution.","pdf":"Understanding the relationship between technology use and cognitive presence in MOOCs  Vitomir Kovanovic Srecko Joksimovic Oleksandra Poquet The University of Edinburgh The University of Edinburgh University of South Australia Edinburgh, United Kingdom Edinburgh, United Kingdom Adelaide, Australia v.kovanovic@ed.ac.uk s.joksimovic@ed.ac.uk sspoquet@gmail.com  Thieme Hennis Shane Dawson Dragan Gaevic Delft University of Technology University of South Australia The University of Edinburgh  Delft, Netherlands Adelaide, Australia Edinburgh, United Kingdom thieme@hennis.nl Shane.Dawson@unisa.edu.au dgasevic@acm.org  ABSTRACT In this poster, we present the results of the study which exam- ined the relationship between student differences in their use of the available technology and their perceived levels of cognitive pres- ence within the MOOC context. The cognitive presence is a con- struct used to measure the level of practical inquiry in the Commu- nities of Inquiry model. Our results revealed the existence of three clusters based on student technology use. The clusters significantly differed in terms of their levels of cognitive presence, most notably they differed on the levels of problem resolution.  CCS Concepts Applied computing  Distance learning; Information sys- tems Clustering;  Keywords Community of Inquiry model, MOOCs, Student clustering.  1. INTRODUCTION The goal of this study is to replicate previously published work  by Kovanovic et al. [3], examining the association between stu- dent technology use and the development of cognitive presence. Specifically, the study aimed at investigating to what extent pat- terns of the association between student technology use and de- velopment of cognitive presence identified in the traditional online settings hold in the context of learning in MOOCs. The theoreti- cal foundation is given by the Community of Inquiry (CoI) frame- work [2], a popular model of online learning which defines three key dimensions of online learning experience: cognitive presence, social presence, and teaching presence. The CoI model defines cog- nitive presence as focusing on the development of critical and deep thinking skills through sustained communication [2] and consisting of four phases: 1) triggering event phase (problem identified), 2) exploration (brainstorming of ideas and solutions), 3) integration (synthesis of relevant knowledge), and 4) resolution (application and testing of the new knowledge). The original study Kovanovic et al. [3] found six clusters of students, significantly different in terms of the technology use and the levels of cognitive presence, in particular, exploration and integration phrases. The previous study also unveiled several successful interaction approaches, and the importance of the quality of student interactions, rather than their quantity, on their development of cognitive presence.  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029471  2. METHODS The data for this study comes from the Fall 2014 offering of the  Introduction to Functional Programming, an eight-weeks MOOC offered by the Delft University of Technology on the edX platform. The MOOC focused on introducing functional programming us- ing the Haskell programming language. Overall, 38,029 students enrolled, out of which 1,968 obtained the certificate. At the end of the course, we administered the CoI questionnaire, a survey in- strument with 34 five-point Likert scale items, designed to measure the perceived levels of three presences within online learning envi- ronments. In addition to the survey responses (818 responses), the data used in this study also included edX trace logs and discussion forum data about 23,648 students.  To identify different groups of students based on their technology use, we extracted 29 measures of student engagement, grouped into several groups of related variables: 1) Course component access: Number of times a particular course  page(s) were accessed (i.e., course homepage, wiki, progress, syllabus, or module/module item1)  2) Graded assignments: Number of times an action related to quizzes (viewing a quiz or checking answers) was performed.  3) Video lectures: Number of times each video related action (load, play, pause, change speed, show subtitles) was performed.  4) Course navigation: Number of times a navigational action (pre- vious/next module, jump to a module), was executed.  5) Discussion access: Number of times an action related to view- ing a discussion was executed (opening a discussion, opening discussion on a learning page, or searching discussions).  6) Discussion contribution: Number of discussions contributions made, including the number of threads created, the number of QA threads created2, the number of comments written, the av- erage number of letters written when creating a thread, and the average number of letters written when posting a comment.  7) Discussion engagement: Additional measures assessing the level of discussion engagement, such as the number of replies re- ceived, the number of message upvotes given/received, and the number of endorsements received/given.  We also extracted four measures related to the perceived levels of cognitive presence (i.e., perceived level of triggering event, ex- ploration, integration, and resolution) in the course by averaging the responses (from 1: strongly disagree to 5: strongly agree)3.  The analysis procedure followed the approach used by Kovanovic et al. [3]. We first conducted a hierarchical cluster analysis using 29 extracted measures of students technology use, with the Euclidean distance and Wards agglomeration method. The optimal number of three clusters is identified through the analysis of clustering den-  1 In the edX terminology, term courseware is used to denote learning modules. 2 Questions are particular type of threads where one answer is rated as the correct answer. 3 In the CoI instrument, each cognitive presence phase has three questions designated to measure its perceived levels.  http://dx.doi.org/10.1145/3027385.3029471   C1 C2 C3  Cluster 1  Cluster 2  Cluster 3  Figure 1: Clustering dendogram.  drogram (Fig. 1) based on the height of the merging steps. After the student clusters had been identified, we performed the  multivariate analysis of variance (MANOVA) procedure to examine the significance of the differences among the identified clusters in terms of their technology use, and also levels of cognitive presence. Each significant MANOVA was followed by a robust MANOVA analysis, and univariate analysis of variance (ANOVA) and Lev- enes test for the homogeneity of variance. In cases where there was no homogeneity of variance, we used non-parametric Kruskal- Wallis tests. Each significant ANOVA analysis was also followed by Tukey pairwise comparisons, and each significant Kruskal-Wallis test with a pairwise comparison. In all cases where multiple com- parisons were performed, we used the Bonferroni correction.  3. RESULTS AND DISCUSSION The hierarchical clustering indicated the existence of three sep-  arate student clusters (Fig. 1). Based on the differences between cluster centroids (Fig. 2) we labeled three clusters as:  Cluster 1: Passive users (15,868 students), that are character-  ized by the overall low engagement, with the main focus on watching videos without working on graded assignments.   Cluster 2: Task-focused users (3,532 students), which focus on obtaining certificate and have balanced use of the whole system.   Cluster 3: Highly active users (4,248 students) that have a high engagement in the course, with an active use of the discussions as well as submitting graded assignments. To analyze the differences in technology use between clusters,  we conducted MANOVA analysis using 29 clustering measures. A statistically significant MANOVA was observed, Pillais Trace = 0.86, F(58,4,724) = 614.4, p < 0.0001, with multivariate effect size 2 = 0.43 which is considered a large effect size4. Results indicate that 43% of the variance in the canonically derived depen- dent variable can be accounted by the cluster assignment. Given the significant departure from the homogeneity of variance for all clus- tering measures, we used Kruskal-Wallis test, with all comparisons being statistically significant, p < 0.001. The pairwise compar- isons indicated significant differences among three clusters for all course content-related measures and three variables related to ac- cess to discussions (i.e., ThreadAccess, Threadaccessinline, and DiscussionSearch). For the remaining discussion-related variables, the differences between clusters one and three, and two and three were significant, but not between clusters one and two.  As the next step of the analysis, we conducted a multivariate analysis of variance (MANOVA) to examine the differences among clusters in terms of the perceived levels of cognitive presence. Sta- tistically, significant MANOVA results were obtained, Pillais Trace = 0.02, F(8,1626) = 2.05, p = 0.038. The results were confirmed by a robust rank-based MANOVA, which also yielded significant results (Wills rank = 0.98,2(8) = 15.85, p = 0.044). The multi- variate effect size of 2 = 0.0099 was obtained which is considered 4 Commonly adopted 2 effect size ranges [1]: 0.01 small, 0.06 medium, and 0.14 large effect size.  C o m m e n t s E n d . R e c .  C o m m e n t s E n d . G i v e n  U p v o t e s R e c e i v e d  U p v o t e s G i v e n  C o m m e n t s R e c e i v e d  C o m m e n t s C h a r s A v g  T h r e a d s C h a r s A v g  C o m m e n t s W r i t t e n  Q u e s t i o n s S t a r t e d  D i s c u s s i o n s S t a r t e d  D i s c u s s i o n S e a r c h  T h r e a d A c c e s s I n l i n e  T h r e a d A c c e s s  M o d u l e P r e v  M o d u l e N e x t  M o d u l e J u m p  V i d e o S h o w S u b s  V i d e o L o a d  V i d e o C h a n g e S p e e d  V i d e o P a u s e  V i d e o P l a y  P r o b l e m S h o w  P r o b l e m G r a d e  O p e n C o u r s e w a r e I t e m  O p e n S y l l a b u s  O p e n C o u r s e P r o g r e s s  O p e n W i k i  O p e n C o u r s e w a r e  O p e n H o m e   0  .5 0  .0 0  .5 1  .0 1  .5  z   s c o  re s  C lu  s te  r  1  C lu  s te  r  2  C lu  s te  r  3  Figure 2: Centroids of identified cluster centers. Shaded area corresponds to discussion-related clustering features.  a small effect size. The follow-up ANOVA analysis indicated sig- nificant differences in terms of resolution, F(2,815) = 5.86, p = 0.0029, with effect size 2 = 0.014 which is also considered a small effect size. Follow-up Tukey pairwise analyses revealed sig- nificant differences between clusters one and two (0.21 SD), and one and three (0.20 SD), with students from cluster one having lower perceived levels of resolution in both cases.  Our results yield several compelling findings, showing the sim- ilarities and differences between MOOCs and traditional online courses in terms of technology use and cognitive presence devel- opment. The results of this study are aligned with our previous work [3], where we also observed a considerable number of stu- dents being disengaged. However, in MOOC context this is even more emphasized. Still, the observed differences in the levels of cognitive presence were much smaller than it was found in a for- credit online course by Kovanovic et al. [3]. The smaller observed differences are likely the result of the use of self-reported survey instrument which has a severe self-selection bias, and much lower discriminatory power than content analysis method used in Ko- vanovic et al. [3] study. Finally, the observed difference regarding the resolution is aligned with the [4] study, which also observed unique dynamics of reaching resolution phase within MOOC con- texts. The observed differences, and the challenges with the self- reported assessment of cognitive presence also further emphasize the strong need for automated ways of assessing student levels of cognitive presence through analysis discussion transcripts, in par- ticular within the MOOC context [5].  4. ADDITIONAL AUTHORS Pieter de Vries, Delft University of Technology, Netherlands  (Pieter.deVries@tudelft.nl). Marek Hatala, Simon Fraser University, Canada (mhatala@sfu.ca). George Siemens, Univer- sity of Texas at Arlington, USA (gsiemens@uta.edu).  5. REFERENCES [1] J. Cohen. The Analysis of Variance. In Statistical power analysis for  the behavioral sciences, pages 273406. L. Erlbaum Associates, Hills- dale, N.J., 1988.  [2] D. R. Garrison, T. Anderson, and W. Archer. Critical Inquiry in a Text- Based Environment: Computer Conferencing in Higher Education. The Internet and Higher Education, 2(23):87105, 1999.  [3] V. Kovanovic, D. Gaevic, S. Joksimovic, M. Hatala, and O. Adesope. Analytics of communities of inquiry: Effects of learning technology use on cognitive presence in asynchronous online discussions. The In- ternet and Higher Education, 27:7489, Oct. 2015.  [4] V. Kovanovic, S. Joksimovic, O. Poquet, T. Hennis, I. Cukic, P. d. Vries, M. Hatala, S. Dawson, G. Siemens, and D. Gaevic. Exploring Communities of Inquiry in Massive Open Online Courses. Manuscript submitted for publication, 2016.  [5] V. Kovanovic, S. Joksimovic, Z. Waters, D. Gaevic, K. Kitto, M. Hatala, and G. Siemens. Towards automated content analysis of dis- cussion transcripts: A cognitive presence case. In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, LAK 16, pages 1524, New York, NY, USA, 2016. ACM.    Introduction  Methods  Results and discussion  Additional Authors  REFERENCES   "}
{"index":{"_id":"106"}}
{"datatype":"inproceedings","key":"Olivares:2017:SLA:3027385.3029472","author":"Olivares, Daniel M. and Hundhausen, Christopher D.","title":"Supporting Learning Analytics in Computing Education","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"584--585","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029472","doi":"10.1145/3027385.3029472","acmid":"3029472","publisher":"ACM","address":"New York, NY, USA","keywords":"OSBLE, computing education research, data collection, learning analytics, learning management system, social learning, visualizations","Abstract":"As is the case for many undergraduate STEM degree programs, computing degree programs are plagued by high attrition rates. This is especially true in early computing courses, in which failure and drop-out rates in the 35 to 50 percent range are common. By collecting learning process data as students engage in computer programming assignments, computing educators can place themselves in a position not only to better understand students' struggles, but also to better tailor instructional interventions to students' needs. We have developed OSBLE+, a learning management and analytics environment that interfaces with a computer programming environment to support the automatic collection of learners' programming process and social data as they work on programming assignments, while also providing an interactive environment for the analysis and visualization of those data. In ongoing work, we are using OSBLE+ to explore two possibilities: (a) leveraging learning and social data to strategically deliver automated learning interventions, and (b) presenting learners with visual representations of their learning data in order to prompt them to reflect on and discuss their learning processes.","pdf":"Supporting Learning Analytics in Computing Education  Daniel M. Olivares   Human-Centered Environments for Learning and  Programming (HELP) Lab   School of Electrical Engineering and Computer Science  Washington State University   Pullman, WA 99164 USA   daniel.olivares@wsu.edu  Christopher D. Hundhausen  Human-Centered Environments for Learning and   Programming (HELP) Lab  School of Electrical Engineering and Computer Science   Washington State University  Pullman, WA 99164 USA   hundhaus@wsu.edu    ABSTRACT  As is the case for many undergraduate STEM degree programs,   computing degree programs are plagued by high attrition rates. This   is especially true in early computing courses, in which failure and   drop-out rates in the 35 to 50 percent range are common. By   collecting learning process data as students engage in computer   programming assignments, computing educators can place   themselves in a position not only to better understand students   struggles, but also to better tailor instructional interventions to   students needs. We have developed OSBLE+, a learning   management and analytics environment that interfaces with a   computer programming environment to support the automatic   collection of learners programming process and social data as they   work on programming assignments, while also providing an   interactive environment for the analysis and visualization of those   data. In ongoing work, we are using OSBLE+ to explore two   possibilities: (a) leveraging learning and social data to strategically   deliver automated learning interventions, and (b) presenting   learners with visual representations of their learning data in order   to prompt them to reflect on and discuss their learning processes.   CCS Concepts   Social and professional topics~Computing education     Applied computing~Learning management systems   Keywords  Computing education research; Learning analytics; Learning   Management System; Social Learning; Data collection;   Visualizations; OSBLE   1. INTRODUCTION  STEM undergraduate degree programs are notorious for having   high drop-out and failure rates. Computing degree programs are   among the worst culprits; attrition rates of 35 to 50 percent are   common, especially in early courses. Learning analytics constitutes   a promising way to address this problem: If we can capture learning   process data as students work on computer programming   assignments, we can use those data to understand their struggles,   and ultimately to better tailor instruction to their needs.   To that end, we have been developing OSBLE+, a learning   management and analytics environment that facilitates the   collection and analysis of learning process data in computing   courses (see Figure 1). These data include not only programming   process data such as compilations, edits, and run-time exceptions,   but also social data: students asynchronous discussions as they   engage in computer programming. In this poster, we will present   the design of OSBLE+, including its software architecture and user   interface, and we will highlight our ongoing efforts to further   develop OSBLE+ and explore its ability to help facilitate improved   learning processes and outcomes in computing education.   2. RELATED WORK  Within computing education, initial interest in learning analytics   was spurred by an interest in building predictive models of learning   success. Many such models (e.g., [2, 4, 8]) were based on   programming process data collected within an integrated   development environment (IDE), where students typically spend   large amounts of out-of-class time working on individual   programming assignments. An outgrowth of one of these [4] is the   Black Box project [1], which makes publicly-available a large   corpus of data automatically collected through the BlueJ  novice   programming environment [6].    While Black Box makes programming process data available, it   does not provide a means of analyzing the data. By supporting the   interactive visual exploration of learning process data, learning   analytics dashboards (e.g., see [7]) provide one such means. They   can be used to augment face-to-face teaching, online learning, or   even blended learning settings [7]. They can be part of LMSs such   as Moodle, custom LMSs as seen in [5], or a student centered   dashboard that uses of both learning analytics and formative   assessment [5]. OSBLE+ contributes to this line of research   through its development of a learning analytics dashboard for   computing instructors.   3. OSBLE+ DESIGN  Figure 2 presents an overview of the OSBLE+ software   architecture, which includes three key components:    The Visual Studio integrated development environment (IDE) is  where computing students spend much of their out-of-class time   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact   the Owner/Author.    Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029472 Figure 1. The OSBLE+ learning analytics dashboard     working on individual computer programming assignments,   which are typically the centerpiece of computing courses.     The OSBLE+ plug-in to the Visual Studio IDE serves two key  purposes: (a) it augments Visual Studio with an activity stream,   which provides an asynchronous forum for students to discuss   issues that arise during the programming process (see Figure 3);   and (b) it automatically collects students programming process   data (e.g., edits, compilation attempts, run-time exceptions) and   social data (activity stream posts and replies).    The OSBLE+ learning management system (LMS) provides  many traditional LMS functions, such as a file system for   disseminating course materials, and an assignment system for   posting and submitting course assignments. However, because it   receives learning process data from the OSBLE+ plug-in, it can   also present those data to learners and instructors. In the present   implementation, OSBLE+ provides the instructor with a learning   analytics environment for exploring student learning process   data Within this environment, instructors can select a set of data   to visualize, along with a range of dates, and then see line graphs   of those data within a calendar view (see Figure 1).   4. STATUS AND FUTURE WORK  To date, we have used various versions of OSBLE+ in large CS1   and CS2 courses at Washington State University. .In an empirical   study of the CS2 course, student social participation was strongly   correlated with assignment success [3]. Motivated by this result and   by social learning theory, we are presently exploring ways to use   students programming and social data as a basis for automated   interventionsdelivered directly within the IDEthat nudge   students toward greater social participation at strategic points in the   learning process. We are also interested in injecting visual   representations of learner data into the activity stream, in order to   prompt students to engage in reflective conversations about their   learning processes.   5. ACKNOWLEDGMENTS  This project is funded by the National Science Foundation under   grant no IIS-1321045   6. REFERENCES   [1] Brown, N.C.C. et al. 2014. Blackbox: A Large Scale   Repository of Novice Programmers Activity. Proceedings of   the 45th ACM Technical Symposium on Computer Science   Education. ACM. 223228.   [2] Carter, A.S. et al. 2015. The Normalized Programming State   Model: Predicting Student Performance in Computing   Courses Based on Programming Behavior. Proceedings of the   Eleventh Annual International Conference on International   Computing Education Research. ACM. 141150.   [3] Carter, A.S. and Hundhausen, C.D. 2016. With a Little Help   From My Friends: An Empirical Study of the Interplay of   Students Social Activities, Programming Activities, and   Course Success. Proceedings of the 2016 ACM Conference on   International Computing Education Research. ACM. 201  209.   [4] Jadud, M.C. 2006. Methods and Tools for Exploring Novice   Compilation Behaviour. Proceedings of the Second   International Workshop on Computing Education Research.   ACM. 7384.   [5] Rling, G. et al. 2008. Enhancing Learning Management   Systems to Better Support Computer Science Education.   SIGCSE Bull. 40, 4 (Nov. 2008), 142166.   [6] Utting, I. et al. 2012. Web-scale Data Gathering with BlueJ.   Proceedings of the Ninth Annual International Conference on   International Computing Education Research. ACM. 14.   [7] Verbert, K. et al. 2014. Learning Dashboards: An Overview   and Future Research Opportunities. Personal Ubiquitous   Comput. 18, 6 (Aug. 2014), 14991514.   [8] Watson, C. et al. 2014. No Tests Required: Comparing   Traditional and Dynamic Predictors of Programming Success.   Proceedings of the 45th ACM Technical Symposium on   Computer Science Education. ACM. 469474.  Figure 2. An overview of the OSBLE+ software architecture   Figure 3. The Visual Studio IDE with the OSBLE+ plug-in     "}
{"index":{"_id":"107"}}
{"datatype":"inproceedings","key":"Gardner:2017:ISD:3027385.3029473","author":"Gardner, Josh and Onuoha, Ogechi and Brooks, Christopher","title":"Integrating Syllabus Data into Student Success Models","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"586--587","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029473","doi":"10.1145/3027385.3029473","acmid":"3029473","publisher":"ACM","address":"New York, NY, USA","Abstract":"In this work, we present (1) a methodology for collecting, evaluating, and utilizing human-annotated data about course syllabi in predictive models of student success, and (2) an empirical analysis of the predictiveness of such features as they relate to others in modeling end-of-course grades in traditional higher education courses. We present a two-stage approach to (1) that addresses several challenges unique to the annotation task, and address (2) using variable importance metrics from a series of exploratory models. We demonstrate that the process of supplementing traditional course data with human-annotated data can potentially improve predictive models with information not contained in university records, and highlight specific features that demonstrate these potential information gains.","pdf":"Integrating Syllabus Data into Student Success Models  Josh Gardner   School of Information  University of Michigan  jpgard@umich.edu    Ogechi Onuoha  School of Informatics   University of Edinburgh  oge.blessing@gmail.com   Christopher Brooks  School of Information   University of Michigan   brooksch@umich.edu     ABSTRACT  In this work, we present (1) a methodology for collecting,  evaluating, and utilizing human-annotated data about course  syllabi in predictive models of student success, and (2) an  empirical analysis of the predictiveness of such features as they  relate to others in modeling end-of-course grades in traditional  higher education courses. We present a two-stage approach to (1)  that addresses several challenges unique to the annotation task,  and address (2) using variable importance metrics from a series of  exploratory models. We demonstrate that the process of  supplementing traditional course data with human-annotated data  can potentially improve predictive models with information not  contained in university records, and highlight specific features  that demonstrate these potential information gains.   CCS Concepts   Applied computing ~ Education  Computing methodologies ~  Classification and regression trees   1. INTRODUCTION  The task of grade prediction is of interest to a variety of  stakeholders in higher education, including students, instructors,  advisors, and course designers. However, data that may be useful  in predicting these grades is often not collected by default and can  be resource-intensive to collect. This includes information about  pedagogy, course requirements, assignments, materials, and other  course features. If we can identify features that might be useful in  predictive modeling and a reliable, scaleable process for gathering  these data, we might improve the educational process for all of  these stakeholders. However, both tasks have received little  attention in prior learning analytics research to date [5].   2. DATA  Our analysis utilized 1,149 syllabi taken from courses in the  largest college within the University of Michigan. Using a two- stage process, we collected a set of 27 categorical features from  each syllabus. These human-annotated syllabus features were  combined with an existing student-course-term level dataset with  features representing student demographics (n = 11), student  academics (n = 1), and course features (n = 17; referred to as SIS  features as they come from the institutional student information  system and are different from syllabus features, but also refer to  courses). The combined dataset includes 27,935 observations  covering 596 unique courses, 49 departments, and approximately  20% of all semester grades.   3. ANALYSIS  Our analysis included a data collection and preprocessing phase in  which human-annotated syllabus features were collected and their  quality assessed, and an exploratory predictive modeling phase in  which we evaluated the relative importance of syllabus features.   3.1 Annotated Syllabus Features  As human annotation is resource-intensive, we devised a scaleable  two-stage annotation process that allowed us to minimize the  number of annotations while also evaluating their accuracy. In the  first phase, a sample set of 52 syllabi were annotated by up to five  raters each1. We calculated observed inter-rater agreement, using  Fleiss Kappa, for each feature in this initial sample to serve as a  measure of each features quality on the full set of syllabi, which  were annotated by only a single rater. For the 24 features having  {yes, no, unclear} as choices, we eliminated any ratings of  unclear as a proxy for rater noncompliance. There was  substantial variation in the level of inter-rater agreement across  features, ranging from k = 0.67 (substantial agreement) to below  zero (worse agreement than expected by chance), with mean 0.24  and standard deviation 0.18. Low- ( < 0.2) features  were excluded from most models to reduce potential noise. In the  second phase, the full set of 1,149 syllabi were sent for annotation  by individual raters. We received complete annotations for 934,  which were used in the exploratory modeling below.      3.2 Exploratory Modeling  We fit a series of classification trees on the full dataset comprised  of demographic, academic, SIS, and syllabus features to explore                                                                        1 Raters were obtained using Amazon Mechanical Turk.       Table 1: Selected syllabus features. Features marked with  asterisk (*) were rated on {0,1,2,3,4,5,6,7,8,9,10,10+} scale;  all others rated on {yes, no, unclear} scale.     Feature Description  Assignments*  How many assignments are there in the class    Attendance   Is attendance or participation graded    Curve  Is this class graded on a curve   Fees Does this course have any extra fees/costs   Instructors*  How many instructors does the class have    Late Policy  Is there a policy for late assignments    Picture   Is there a cartoon, comic, or other illustration  in this syllabus   Presentation  Are in-person student presentations required    Software Does the course require any software      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and  that copies bear this notice and the full citation on the first page.  Copyrights for third-party components of this work must be honored.  For all other uses, contact the Owner/Author. Copyright is held by the  owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029473       both the overall performance of models utilizing these features,  and the relative importance of individual features to determine  whether similar data collection efforts may be useful to  institutions interested in modeling or predicting student grades.  Classification trees were particularly well-suited to this task for  their ability to capture complex interactions between variables,  their handling of missing data and both categorical and continuous  features, and the useful variable importance metrics generated in  the process of recursive binary tree-fitting.   We utilized the caret package in R for model-building, which  generates trees by recursively partitioning the feature space based  on the highest-purity binary splits [4]. Models were fit using an  80-20 training-test split within courses (i.e., 80% of the data from  each course was used for training), with five repeats of 10-fold  cross-validation. A categorical variable with the official course  grade (A+, A, A-, etc.) was used as the outcome variable. In order  to isolate the predictiveness of each individual feature, to provide  a consistent baseline of performance, and to skim off variation  which could be explained by one of the best-known grade  predictors (students cumulative GPA), we first fit bivariate  models that used each individual feature in conjunction with  cumulative GPA to predict students final grades (i.e., we fit  models using p + GPA, for each of the p features). We then  utilized the variable importance metrics from each model to  provide a measure of the predictive efficacy of each feature.    Variable importance for a given feature is calculated as the sum of  the goodness of split measures for each split for which it was the  primary variable, plus goodness  adjusted agreement for all splits  in which it was a surrogate [2]. Essentially, this metric can be  thought of as a measure of the purity of splits obtained using this  variable, conditional on the other variables present in the model.    This bivariate approach (a) provided a common reference point,  cumulative GPA, for the importance (and therefore the  predictiveness) of individual features; (b) avoided penalizing sets  of correlated features, which are pruned out of classification trees,  by examining them separately; and (c) allowed us to examine how  the feature provided additional predictive power to explain only  residual variation from a well-known predictor  GPA  and avoid  examining a given feature in isolation, which might lead us to  overestimate its actual performance by allowing it to account for  variation easily explained by other well-known, readily available  predictors such as GPA. Similar approaches have been applied to  random forest models, but we adopted a CART model in favor of  its ability to more easily handle missing data [1]. Being mindful of  the high variance of CART trees, our robustness checks with  alternative specifications (described below) helped to limit model  variance as a confounding factor in this analysis [3].    We find several results relevant to both modeling and data  collection efforts. Our focus, being in the early stages of exploring  syllabus data, was on understanding features individually so as to  guide future data collection efforts. As expected, cumulative GPA  is the strongest predictor of academic performance, but several  syllabus variables also showed high importance including:  number of instructors, number of assignments, and indicator  variables for whether the course required in-person presentation(s)  or included a graded component for attendance or participation.  The results of the bivariate models for the 30 highest-importance  features are shown. As an additional robustness check, we fit a  series of models containing all of the variables within each group  of features (demographic, SIS, syllabus) plus cumulative GPA.  The results of these grouped models (omitted due to space   constraints) were consistent with those reported above, with rank  ordering of syllabus features exactly matching their relative  rankings in the bivariate data.         We do not find conclusive evidence that the human-annotated  syllabus features provide a substantial boost to model  performance relative to models containing all features except  syllabus features, but this may have been due to models  underfitting to these features. Future modeling efforts should  explore models with both more and less flexible functional forms,   hyperparameter optimization, and larger datasets to achieve  deeper understanding of syllabus features and in what contexts  they most augment the predictive performance of traditional  academic datasets.    4. CONCLUSIONS AND DIRECTIONS  Syllabus features have the potential to complement more  traditional feature sets in models predicting student grades. In this  work we report on (1) a method for both collecting these features,  as well as (2) evaluating their predictiveness. Future work can  advance the goals of this study by replicating this analysis with  similar and additional course features, exploring the predictive  potential of syllabus features with different modeling approaches  and datasets, and possibly conducting causal investigations into  the impact of such instructional features on student performance.   5. REFERENCES  [1] Archer, K. and Kimes, R. 2008. Empirical characterization of   random forest variable importance measures. Computational  Statistics & Data Analysis 52.4 : 2249-2260.   [2] Atkinson, E. and Therneau, T. 2015. An introduction to  recursive partitioning using the RPART routines. Rochester:  Mayo Foundation.   [3]  Breiman, L. 1996. Heuristics of instability and stabilization  in model selection. The annals of statistics 24.6: 2350-2383.   [4] Kuhn, M. et al. 2016. caret: Classification and Regression  Training. R package version 6.0-71.   [5] The Open Syllabus Project, http://opensyllabusproject.org                                     SBJCT CD == PSYCH N INSTRUCTORS == 1  PRMRY CRER DES == Rackham SBJCT CD == COMM SBJCT CD == MCDB SBJCT CD == CHEM  SBJCT CD == ANTHRCUL SBJCT CD == ORGSTUDY  CLASS LVL == 5 CLASS SCTN CD == 200 SBJCT CD == ENGLISH  CLASS LVL == 7 N ASSIGNMENTS == 10  SBJCT CD == ASIANLAN CLASS SCTN CD == 300  SBJCT CD == NURS SBJCT CD == MATH  PRESENTATION == Yes CLASS SCTN CD == 100  N INSTRUCTORS == 4 ATTEND/PARTCP GRADE == Yes  CLASS LVL == 6 N ASSIGNMENTS == 1 CLASS SCTN CD == 2 N INSTRUCTORS == 2  SBJCT CD == GERMAN STDNT CTZN STAT SHORT DES == U.S. Citzn  N ASSIGNMENTS == 6 CURVE == Unclear  FEES == Yes  10 20 30 Variable Importance Score (scaled to 100)  Fe at  ur e  Feature Group        SIS Features  Student Demographics  Syllabus Features  Variable Importance for All Features Relative to Cumulative GPA Top 30 Shown  Figure 1: Variable importance scores from bivariate  models (relative to cumulative GPA; top 30 shown).     "}
{"index":{"_id":"108"}}
{"datatype":"inproceedings","key":"Healion:2017:TPM:3027385.3029474","author":"Healion, Donal and Russell, Sam and Cukurova, Mutlu and Spikol, Daniel","title":"Tracing Physical Movement During Practice-based Learning Through Multimodal Learning Analytics","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"588--589","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029474","doi":"10.1145/3027385.3029474","acmid":"3029474","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative learning environment, collaborative problem solving, learning analytics, movement, practice-based learning","Abstract":"In this paper, we pose the question, can the tracking and analysis of the physical movements of students and teachers within a Practice-Based Learning (PBL) environment reveal information about the learning process that is relevant and informative to Learning Analytics (LA) implementations? Using the example of trials conducted in the design of a LA system, we aim to show how the analysis of physical movement from a macro level can help to enrich our understanding of what is happening in the classroom. The results suggest that Multimodal Learning Analytics (MMLA) could be used to generate valuable information about the human factors of the collaborative learning process and we propose how this information could assist in the provision of relevant supports for small group work. More research is needed to confirm the initial findings with larger sample sizes and refine the data capture and analysis methodology to allow automation.","pdf":"Tracing physical movement during practice-based  learning through Multimodal Learning Analytics   Donal Healion  National College of Art &   Design, Dublin  Dublin, Ireland   healiond@staff.ncad.ie   Sam Russell  National College of Art &   Design, Dublin  Dublin, Ireland   russells@staff.ncad.ie   Mutlu Cukurova  UCL Knowledge Lab   University College London,  United Kingdom   m.cukurova@ucl.ac.uk   Daniel Spikol  Malm University   Malm,   Sweden   daniel.spikol@mah.se     ABSTRACT  In this paper, we pose the question, can the tracking and analysis of  the physical movements of students and teachers within a Practice- Based Learning (PBL) environment reveal information about the  learning process that is relevant and informative to Learning  Analytics (LA) implementations Using the example of trials  conducted in the design of a LA system, we aim to show how the  analysis of physical movement from a macro level can help to  enrich our understanding of what is happening in the classroom.  The results suggest that Multimodal Learning Analytics (MMLA)  could be used to generate valuable information about the human  factors of the collaborative learning process and we propose how  this information could assist in the provision of relevant supports  for small group work. More research is needed to confirm the initial  findings with larger sample sizes and refine the data capture and  analysis methodology to allow automation.   CCS Concepts   Applied computing~Interactive learning environments   Applied computing~Collaborative learning   Information systems~Data analytics   Keywords  Practice-based learning; collaborative problem solving;  collaborative learning environment; learning analytics; movement.   1. INTRODUCTION  This paper discusses the exploration of physical movement by both  students and teachers while engaged in Collaborative Problem  Solving (CPS) during PBL activities. Using data gathered from  multimodal sources, we examine the movements of students and  teachers that occur within the learning environment, to gain  information about the physical nature of collaborative group work.  Our research question is how can students' and teachers' movement  around the furniture be used to gain a better understanding of  students collaborative learning processes For the purposes of this  paper, the movements analysed are at the macro level (i.e. within  the classroom space). These movements are tracked via a range of  multimodal sensing devices with analysis carried out by a mixture  between human and machine coding. The intent of this research is  to use an enhanced understanding of the collaborative learning  process to inform the development of LA implementations.      2. BACKGROUND  The work described in this paper has been carried out as part of the  Practice-based Experiential Learning Analytics Research and  Support (PELARS) project, a three year, EU funded FP7 research  and design project that seeks to create a Learning Analytics System  (LAS) suitable for implementation in the teaching of PBL activities  in three learning contexts, secondary Science, Technology,  Engineering and Mathematics (STEM) subjects, third level  interaction design and third level engineering education. The  project seeks to understand how students learn while engaged in  open-ended CPS in PBL activities in these scenarios [1]. The LAS  is designed to achieve this through the aggregation and analysis of  various multimodal data streams generated by sensing technologies  embedded in the learning environment (video, audio, still image  capture and data log files) and user generated data (via sentiment  feedback buttons and a mobile application).   3. METHODOLOGY  Outlined below are two trials carried out during cycles of iterative  prototyping, trials and evaluation established within the PELARS  project to test the LAS and associated learning environment in the  wild and incorporate user feedback into subsequent design  iterations.     The hypothesis of the first trial is that students engaged in a PBL  task at standing height tables would physically move more than  those seated at standard height tables and that these movements  would give rise to more interactions with their peers. Six table tops  of various shapes were produced with one table top of each shape  mounted at 770mm (sitting height) and one at 1,020mm (standing  height). Table 1 in the results section shows the trial findings.     The purpose of the second trial (along with testing the LAS) was to  record the movement and interactions of students engaged in a PBL  task at specially designed standing tables with circular table tops to  allow comparison with those of their peers seated at standard height  rectangular tables. It further sought to track the movements and  interactions of the teachers/facilitators during the activity. Two  sample spaghetti diagrams visualising the movements of a student  and a facilitator during this trial are shown in the results section.  Both of these trials were conducted in a classroom type  environment with randomised groups of three and four, mixed  gender, secondary school-level students.   4. RESULTS  Table 1 below shows the results from Trial 1. It details the number  of movements away from the appointed table, interactions with  other students, number of interventions by four facilitators at each  table and number of times the students reform at the table in the  same or different configurations. The figures shown in the rows for  each table are the aggregated totals for each of the three students at  that table.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author. Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029474       Table 1. Movements and interactions during Trial 1   Ta bl  e  N  o.    M ov  es    fro m   T ab  le    In iti  at ed    In  te ra  ct io  n   Su bj  ec t o  f  in  te ra  ct io  n   Fa ci  lit at  or    vi sit  s t o   ta bl  e  G  ro up    re  fo rm  s:  Sa  m e   G ro  up    re fo  rm s:   D iff  er en  t   HTR 37 8 8 19 5 23   HTH 39 9 1 18 4 17   HTS 12 2 2 19 7 5   LTR 4 2 6 19 0 1   LTH 1 0 5 24 0 0   LTS 9 9 1 23 0 0   HTR = High Table Round, HTH = High Table Hexagonal,  HTS = High Table Square, LTR = Low Table Round,   LTH = Low Table Hexagonal, LTS = Low Table Square    The diagrams below were generated from data collected in Trial 2  to show the greatest amount of student movement (Fig. 1 Student  5) and movements of the main facilitator (Fig. 2 Facilitator 1). Each  line represents a return movement for the subject involved unless  an onward movement is indicated.     Figure 1. Spaghetti diagram showing the movements of   Student 5 during Trial 2     Figure 2. Spaghetti diagram showing the movements of   Facilitator 1 during Trial 2   5. DISCUSSION  Collaboration is a coordinated, synchronous activity that is the  result of a continued attempt to construct and maintain a shared  conception of a problem [3] (p.70). It is a complex learning  process that requires certain conditions to be satisfied for its  success. These include shared goals, discussions and negotiations  for accommodating others perspectives and organised actions to  reach a more desirable state from a problem state [2]. However, the  fundamental condition for collaboration is the social interaction of  students. Not all social interactions lead to collaboration, but all  collaboration is a product of social interaction. Hence, in this paper,  we investigated different furniture designs using a MMLA system  and established that different furniture designs lead to varied levels  of social interaction.  Our results show that standing height  hexagonal and round tables lead to more movements and social  interactions between students compared to sitting height and square  tables. Given the small sample size, varying contexts and  participant student profiles of the trials, more research is required  to confirm results and expand on this current work. However, the  outcome of this exploration points towards the importance of the  consideration of human factors in the design of learning  environments and the LA technology that is implemented within it.   The results confirm our hypothesis that the physical form of a  workstation design has a bearing on group formation and dynamics.  In this paper, we show that the design of furniture elements within  the learning environment has an effect on the number of  movements and interactions between the student groups. Of  significance to LA design is that in order to acquire as complete a  data set as possible, adequate resources for the PBL activity need  to be supplied within the range of the LAS sensors or the sensing  technology should have the capability to follow the student when  away from the appointed workstation. These findings point to  meaningful information to help researchers, teachers and learners  understand, through the use of MMLA, what is happening in the  classroom while engaged in CPS in PBL activities. Further work is  required to validate these initial findings with larger sample sizes  and to refine the data analysis process through automation.   6. ACKNOWLEDGMENTS  This work is co-funded by the European Union under the PELARS  project (Grant Agreement # 619738) under the Seventh Framework  Programme of the European Commission.   7. REFERENCES  [1] Cukurova, M., Avramides, K., Spikol, D., Luckin, R. &   Mavrikis, M. 2016. An analysis framework for collaborative  problem solving in practice-based learning activities: a  mixed-method approach. In Proc. of the Sixth Int. Conf. on  Learning Analytics & Knowledge (LAK '16). 84-88. ACM,  New York, NY, USA.  http://dx.doi.org/10.1145/2883851.2883900    [2] Roschelle, J., & Teasley, S. D. 1995. The construction of  shared knowledge in collaborative problem-solving. In C. E.  O'Malley (Ed.), Computer-supported collaborative learning.  6997. Berlin: Springer-Verlag.   [3] OECD. 2015. Draft Collaborative Problem Solving  Framework. Retrieved from:  http://www.oecd.org/pisa/pisaproducts/Draft PISA 2015  Collaborative Problem Solving Framework.pdf    "}
{"index":{"_id":"109"}}
{"datatype":"inproceedings","key":"Burns:2017:ASS:3027385.3029475","author":"Burns, Sean and Corwin, Kimberley","title":"Automating Student Survey Reports in Online Education for Faculty and Instructional Designers","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"590--591","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029475","doi":"10.1145/3027385.3029475","acmid":"3029475","publisher":"ACM","address":"New York, NY, USA","keywords":"R, RODBCext, automated reporting, course design, course quality, likert, rmarkdown, student feedback, survey, xtable","Abstract":"In this paper, we discuss Colorado State University Online's progress toward designing automated survey reports for student feedback data collected through our newly designed LTI survey tool. Using multiple R packages, including 'rmarkdown' and 'likert', the reporting tool imports student survey response data and generates reports for faculty and instructional designers. These reports focus on student perceptions of communication, course design, academic challenge, general satisfaction, and more. These reports display visual representations of the Likert-type response frequencies, basic descriptive statistics, and free-response comments. Surveys are administered just before half-way through the semester to provide formative feedback and just before the end of the semester to provide summative feedback. In this way, faculty and instructional designers can obtain a quick and easily digestible report to make changes and improvements to their classes with minimal effort in the back end production.","pdf":"Automating Student Survey Reports in Online Education  for Faculty and Instructional Designers  Sean Burns   Colorado State University  Fort Collins, CO 80523   Sean.Burns@colostate.edu   Kimberley Corwin  Colorado State University   Fort Collins, CO 80523  Kimberley.Corwin@colostate.edu   ABSTRACT  In this paper, we discuss Colorado State University Online's   progress toward designing automated survey reports for student   feedback data collected through our newly designed LTI survey   tool. Using multiple R packages, including 'rmarkdown' and 'likert',   the reporting tool imports student survey response data and   generates reports for faculty and instructional designers. These   reports focus on student perceptions of communication, course   design, academic challenge, general satisfaction, and more. These   reports display visual representations of the Likert-type response   frequencies, basic descriptive statistics, and free-response   comments. Surveys are administered just before half-way through   the semester to provide formative feedback and just before the end   of the semester to provide summative feedback. In this way, faculty   and instructional designers can obtain a quick and easily digestible   report to make changes and improvements to their classes with   minimal effort in the back end production.   CCS Concepts   Applied ComputingDocument management and text   processingDocument preparationMarkup languages    Software and its engineeringSoftware notations and   toolsGeneral programming languagesLanguage   featuresModules/packages.    Keywords  Automated reporting, R, rmarkdown, RODBCext, likert, xtable,   Survey, Student Feedback, Course Design, Course Quality   1. INTRODUCTION  Incorporating student feedback into efforts to improve course   design has been suggested and encouraged many times [1,2]. At   Colorado State University (CSU) Online, our faculty and   instructional designers have wanted additional feedback from   students for many years. The assessment team has gone through   many iterations of survey designs and reports in order to provide   that desired feedback.   We faced a number of issues with our old system; students   originally needed to click through three windows, an additional   required login, and self-select their course. Additional challenges   included data clean up issues and a long manual reporting   processes. Our web designers created a Survey LTI that links   directly into each Canvas course section, thus eliminating the   possibility of self-selecting errors and significantly reducing the   number of clicks to gain access to the survey. Now we can easily   use SQL queries and many fantastic R packages to access,   download, clean, and process the data into easily digestible reports   for both faculty and instructional designers.   Permission to make digital or hard copies of part or all of this work for personal  or classroom use is granted without fee provided that copies are not made or   distributed for profit or commercial advantage and that copies bear this notice   and the full citation on the first page. Copyrights for third-party components of   this work must be honored. For all other uses, contact the Owner/Author.    Copyright is held by the owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029475    The reports generated at CSU Online provide breakdowns of   student perceptions of course design, clarity of learning objectives,   communication, academic challenge, general satisfaction, and free   response comments about the general quality of the course.   Students are surveyed twice over the course of the semester: at the   forty percent mark, and the eighty percent mark. This allows for   faculty to receive formative feedback in time to change or improve   aspects of their instruction and materials. Furthermore, they receive   summative feedback near the end of the semester to determine if   any implemented changes had an effect on student perceptions.   Meanwhile, instructional designers who work with faculty can take   the feedback and provide any necessary assistance to the faculty   members. Instructional designers receive multiple reports, and so   gain perspective on overall student feedback patterns to help them   plan and improve future course designs.   2. Methods  During the Fall 2016 semester, we built a reporting tool using R   version 3.2.3 (R Core Team, 2015) that accesses data in the CSU   SQL server, applies designated statistical analyses, generates well-  formatted graphical displays, and compiles the output of these   analyses into a PDF or HTML document. These reports are   generated using the 'rmarkdown' package (Allaire et al. 2016;   hereafter R Markdown), which allows a user to combine text, code,   and code output into a variety of preformatted documents,   including PDFs and HTML pages.    This reporting tool was designed to summarize data from a student   perception survey in courses developed by CSU Online and CSU's   Institute for Teaching and Learning. The survey is hosted on CSU's   learning management system, Canvas, via a recently designed LTI.   The survey tool stores student responses on the university's SQL   server. Using the 'RODBCext' package (Mateusz et al. 2016),   which is an ODBC (Open Database Connectivity) interface, we   connect to the SQL database and import data into R using a   parameterized SQL query. The query parameterization allows the   user to easily update which survey's data is accessed.     The Fall 2016 pilot survey in the survey tool asks two free response   questions as well as 16 Likert-type questions that are grouped into   five categories. While the reporting tool neatly prints the written   free response comments at the end, the bulk of the report focuses   on presenting the results from the Likert questions. The code blocks   in the R Markdown file that analyze and build graphics and tables   for these Likert questions primarily rely on the 'likert' (Bryer &   Speerschneider, 2015), 'psych' (Revelle, 2016), and 'xtable' (Dahl,   2016) packages.      The 'likert' package determines response frequencies for multiple   questions grouped by the user and generates a well-formatted,   centered, bar graph from this data (Figure 1). Creating a plot using   'likert' requires the user to group all related questions into a single   data frame with each question's responses recorded in a single   column. The user must then manually update the formatting of the   data frame and perform the 'likert' analyses. Since the survey tool   is designed to build new surveys with different questions in the   future, continuing to manually update the code in this manner   would prove labor and time intensive once new surveys and   http://dx.doi.org/10.1145/3027385.3029475   questions are added. As such, we designed a mechanism for   automatically grouping questions and subsequently tabulating   response frequencies, generating plots, and calculating descriptive   statistics.                 Within the file that imports the data from SQL, a new column is   created with a group identity code. Each group is labeled with the   word 'GROUP' followed by a three digit number. The questions   within each group are specified alphabetically. For example, the   third question measuring student perceptions of communication   within the course might be coded as 'GROUP103c.' The R code   then loops through all of the columns in the main dataset and groups   together those with group codes that have the same first eight   characters. These data frames are then compiled into lists.   Functions that plot the Likert data (Figure 1), generate response   frequencies and descriptive statistics (Table 1), and format data   tables are then performed on each data frame in that list in an   automatically looping form. The output from these loops are then   referenced within the R Markdown file in the appropriate location   and inserted when the file is rendered.         Table 1. Example of Basic Descriptive Statistics Table         A separate file is used to specify the survey ID and render the R   Markdown file without ever having to access either of the original   source files. Two options are available to the user for rendering the   R Markdown file: render a report for a single course by selecting   the course of interest within a simple user interface or generate a   report for all courses that currently have student responses   recorded. Each report produced through the single class selection   method can be created in approximately 10 seconds. This includes   the time it takes to select the course of interest in the user interface.   To generate reports for all 21 courses in which the survey was   administered takes 1.85 minutes, meaning the tool generates a   single report in 5.29 seconds.   3. Lessons Learned  3.1 Response Rate Issues  As with all online surveys, we have experienced issues with low   response rates. During our first run early in the Fall 2016 term, we   had multiple classes with over twenty students return only a handful   of responses, or fail to return any responses at all. To improve upon   this, we began to engage in the following behaviors: 1) Suggest, at   the start of the semester, that all faculty encourage their students to   respond to the survey and remind them how the valuable nature of   their feedback; 2) Examine response rates three or four days before   the survey closes and email reminders to complete the survey; 3)   Place the survey link directly in each Canvas courses module.   In addition, to further improve response rates we are planning to   incorporate the following into our system: 1) Host a giveaway for   a tablet each semester for students who complete both the mid- and   post- surveys; 2) Send out an automated reminder within Canvas   directly to students to encourage them to complete the survey (and   remind them of the potential prize); 3) Display the average time it   has taken students to complete the survey and emphasize the   anonymity of their responses at the top of the survey page (currently   about three minutes).   3.2 Learning How to Read Reports  Faculty and instructional designers have required minimal   instruction in how to read the delivered reports, but the creation of   a How-to document would prove beneficial. The document   should give the basic description of how the question groups are   separated, where to look in the figures for positive responses, where   to look for negative responses, and a description of the basic   descriptive statistics provided.   4. Conclusion  CSU Online has developed surveys and an automated survey   reporting system to provide formative and summative student   feedback to faculty and instructional designers with the goal of   improving course quality. The reporting system uses R and multiple   R packages to produce a descriptive statistical report that is easily   accessible visually and can be generated for a large number of   classes in a small amount of time and with minimal effort. The   initial stages of this system have been successful, and we have   many plans to improve the viability of the reporting system. Future   work will focus on displaying these reports using R Shiny to allow   for increased interactivity with reports by faculty and instructional   designers.    5. ACKNOWLEDGMENTS  Our thanks to the CSU Online Web Development Team (Matt   Creighton and Matt Titchner) for putting in a fantastic effort to   design a great LTI Tool that made all of this possible.   6. REFERENCES  [1] Allaire, J.J., Cheng, J., Xie, Y., McPherson, J., Chang, W., Allen, J.,   Wickham, H., Atkins, A. and Hyndman, R., 2016. rmarkdown:   Dynamic documents for R. R package version   1.1. DOI= https://CRAN.R-project.org/package=rmarkdown    [2] Bryer, J. and Speerschneider, K., 2015. likert: Functions to analyze  and visualize likert type items. R package version   1.3.3. DOI=https://CRAN.R-project.org/package=likert   [3] Dahl, D.B., 2016. xtable: Export tables to LaTeX or HTML. R  package version 1.8-2. DOI= https://CRAN.R-  project.org/package=xtable   [4] Hazari, S. and Schnorr, D., 1999. Leveraging student feedback to  improve teaching in web-based courses. The Journal, 26(11), 30-38.    [5] Nathenson, M.B. and Henderson, E.S., 1980. Using student feedback  to improve learning materials. Taylor & Francis.   [6] R Core Team (2015). R: A language and environment for statistical  computing. R Foundation for Statistical Computing, Vienna, Austria.   DOI= https://www.R-project.org/.    [7] Revelle, W., 2016 psych: Procedures for personality and  psychological research, Northwestern University, Evanston, Illinois,   USA, http://CRAN.R-project.org/package=psych Version = 1.6.6.    [8] Zoltak, M., Ripley, B. and Lapsley, M., 2016. RODBCext:  Parameterized queries extension for RODBC. R package version   0.2.6. DOI=https://CRAN.R-project.org/package=RODBCext     Figure 1. Example of Likert Visualization      .         https://cran.r-project.org/package=rmarkdown https://cran.r-project.org/package=likert https://cran.r-project.org/package=xtable https://cran.r-project.org/package=xtable https://www.r-project.org/ http://cran.r-project.org/package=psych https://cran.r-project.org/package=RODBCext   "}
{"index":{"_id":"110"}}
{"datatype":"inproceedings","key":"Fortenbacher:2017:LLA:3027385.3029476","author":"Fortenbacher, Albrecht and Pinkwart, Niels and Yun, Haeseon","title":"[LISA] Learning Analytics for Sensor-based Adaptive Learning","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"592--593","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029476","doi":"10.1145/3027385.3029476","acmid":"3029476","publisher":"ACM","address":"New York, NY, USA","keywords":"adaptive learning, learner awareness, learner-centric learning analytics, self-regulated learning, sensor based learning","Abstract":"This paper reports on research conducted in a project named LISA which aims at supporting learners through learner-centered learning analytics using physiological sensor data as well as environmental sensors. We present the concept and a prototypical realization of a mobile sensor device used in LISA.","pdf":"[LISA] Learning Analytics for Sensor-Based Adaptive Learning  Albrecht Fortenbacher HTW Berlin  Wilhelminenhofstr. 75A 12459 Berlin, Germany forte@htw-berlin.de  Niels Pinkwart Humboldt-Universitt zu Berlin  Rudower Chaussee 25 12489 Berlin, Germany  pinkwart@hu-berlin.de  Haeseon Yun HTW Berlin  Wilhelminenhofstr. 75A 12459 Berlin, Germany yun@htw-berlin.de  ABSTRACT This paper reports on research conducted in a project named LISA which aims at supporting learners through learner- centered learning analytics using physiological sensor data as well as environmental sensors. We present the concept and a prototypical realization of a mobile sensor device used in LISA.  CCS Concepts Applied Computing  Education;  Keywords Adaptive learning; learner awareness; self-regulated learn- ing; sensor based learning; learner-centric learning analytics  1. INTRODUCTION LISA is a research project funded by the German govern-  ment aimed at improving learner support through the use of sensor data. Solutions for sensor-based adaptive learn- ing developed within the LISA project will be integrated into learning environments and products of three enterprise project partners.  The core idea of LISA is to bring together learner-centric learning analytics methods with sensor data indicating the emotional state of a learner. While (we believe) this is a very promising approach, it poses a wide range of challeng- ing research questions: what sensors are adequate for this kind of learning support at all, how reliable are the signals, what learning indicators can be derived from them, what ed- ucational models and learning analytics methods are needed if personal sensor data are available, what forms of human- computer interfaces are appropriate, and how can we deal with ethical, legal and social issues when using (or abusing) sensor data In the following sections, we briefly describe some of the related concepts, achievements and open ques- tions in LISA, regarding user-centric learning analytics and a mobile sensor device as a learning companion.  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029476  2. PRINCIPLES OF SENSOR-BASED ADAP- TIVE LEARNING  Affective factors including learners motivation, stress or flow have been investigated in various educational psychol- ogy studies, linking these factors to measurable variables like clicks, postings, messages, views, writes and likes in on- line learning environments. There is also evidence on the correlation between certain types of sensor data (e.g. heart rate), and higher-level states of persons that are relevant for learning (e.g., anxiety).  These findings build the foundation for the educational approach in the LISA research project. Our approach re- lies on gathering sensor data about learners as they engage in their activities, storing and processing this data, com- bined with data coming from other relevant sources such as log data from the learning technology in use. During this process, data sovereignty and privacy are key issues to consider, given the potentially sensitive nature of the data (e.g., biological signals might not only be relevant for learn- ing support but may also reveal health information). The data - in an analyzed form that is manageable and under- standable for the users - will be fed back to the learners (and not primarily to the teachers, as in many other learning an- alytics applications) in order to help them self-regulate their learning processes [3]. Feedback can be given through visu- alization of a learning-related state (based on sensor data), through direct feedback messages (take a break - you seem to be stressed), or through an adaptive modification of the learning environment, e.g. increase or decrease of a tasks difficulty.  3. PROTOTYPE OF A SENSOR DEVICE FOR LEARNING SUPPORT  Sensors have immense potential in a learning context. They can provide information related to a learning space (e.g. air quality) or to a learner (e.g. emotional state). Wearable sensors such as electro- dermal activity sensors (EDA) and electro-cardiogram sensors (ECG) can provide physiological data which can be presented to the learner to support self reflection in learning.  In the first LISA prototype for a wearable sensor device, both EDA and ECG sensors (manufactured by Bitalino) were integrated with an Arduino microcontroller as shown in Figure 1, which later was replaced by a Nucleo micro- controller unit. EDA sensors have been utilized in previous studies to investigate emotions such as anger, anxiety, fear, sadness, amusement, happiness, pleasure and relief [8]. In    combination with EDA sensors, ECG sensors can be used to describe an emotional state [9].  Figure 1: Wearable Prototype with EDA and ECG Sensors.  In this prototype, the EDA signal was detected via two electrodes on two fingers, and ECG was detected at the top left part of a chest right be- low a collarbone, via two elec- trodes. The ECG and EDA signals provide a good op- portunity to investigate the physiological changes of learn- ers during a learning session. As explored in [7], the EDA signal differs when a learner views an action movie com- pared to being in a relaxed stage. Also the heart rate and its variability detected by ECG are associated with emo- tions and cognition [4].  With this prototype, fur- ther studies with emotional tasks and cognitive tasks are scheduled, relating physiological data to learning states. To support self-regulated learning, we plan to experiment with different forms of mirroring back the sensor data to learn- ers. This includes raw forms as well as aggregated forms (constructed using learning analytics methods), employing visualizations such as bar charts, pie charts, histograms, radar plots, scatter plots, tables, timelines, networks, tag or word clouds, skill meters, concept maps and hierarchical tree structures [2].  4. SYSTEM ARCHITECTURE One of the core components of LISA is a mobile learning  companion. It consists of a (wearable) sensor device to- gether with a mobile device (SmartMonitor). Both devices communicate via a BLE (Bluetooth Low Energy) channel.  Separating the learning companion into two components, the LISA concept can support different types of sensor data - e.g. body temperature or EEG data, but also facial ex- pressions or eye movements derived from a camera - which might be helpful in different learning environments.  The SmartMonitor should be a non-invasive device in that it interacts with a learner without distracting from learn- ing. Open questions such as generality and portability to different learning contexts, or natural interaction between learning companion and learner [6] will be explored as part of the LISA project.  The learning companion has an important function with regard to ethical implications of the use (and abuse) of per- sonal sensor data: it must give full control over personal sensor data to the learner. Aggregation and transmission of sensor data must be transparent (and understandable) for the users, who should also be able to opt out from the usage of sensor data for learning support. Based on the analysis of sensor data, the learning companion might display (alert) messages on the screen, as well as explicit feedback messages from a learning analytics component.  The LISA backend contains a repository of educational data, stemming from learning tools as well as from the learn- ing companion. It communicates with both the SmartMoni- tor and any learning application via xAPI [1]. Extensions to  the xAPI standard allow to implement a data privacy policy. Together with data privacy settings in the Smart Monitor, this gives the control over personal and educational data back to the user, which allows for a powerful and transpar- ent ELSI concept for LISA [5].  5. OUTLOOK The enrichment of educational technology with personal  sensor data, combining learning activities with indicators for emotional or cognitive states, is very promising, but also challenging with respect to many issues such as educational designs, privacy, and appropriate learner-centric learning an- alytics methods. In the course of the LISA project, our approaches for sensor-based adaptive learning will be inte- grated and evaluated in the context of different real-world learning scenarios.  6. REFERENCES [1] A. Bakharia, K. Kitto, A. Pardo, D. Gasevic, and  S. Dawson. Recipe for Success - Lessons Learnt from Using xAPI within the Connected Learning Analytics Toolkit. In Sixth International Conference on Learning Analytics & Knowledge, pages 378382, 2016.  [2] S. Bull, B. Ginon, B. Clelia, and M. Johnson. Introduction of Learning Visualisations and Metacognitive Support in a Persuadable Open Learner Model. In Sixth International Conference on Learning Analytics & Knowledge, pages 3039. ACM, 2016.  [3] D. L. Butler and P. H. Winne. Feedback and self-regulated learning: A theoretical synthesis. Review of Educational Research Fall, 65(3):245281, 1995.  [4] G. Chanel and C. Muhl. Connecting brains and bodies: Applying physiological computing to support social interaction. Interacting with Computers, 27(5):534550, 2015.  [5] H. Drachsler and W. Greller. Privacy and Learning Analytics - a DELICATE issue. A Checklist for Trusted Learning Analytics. Accepted for 6th International Conference on Learning Analytics and Knowledge, Edinburgh, UK, April 25-29 2016., 2016.  [6] B. Goodman, F. Linton, and R. Gaimari. Encouraging Student Reflection and Articulation Using a Learning Companion: A Commentary. International Artificial Intelligence in Education Society 2015, 26:476488, 2015.  [7] C. Kappeler-Setz, F. Gravenhorst, J. Schumm, B. Arnrich, and G. Troster. Towards long term monitoring of electrodermal activity in daily life. Pers Ubiquit Comput, 17:261271, 2013.  [8] S. D. Kreibig. Autonomic nervous system activity in emotion: a review. Biological psychology, 84(3):394421, jul 2010.  [9] J. Schneider, D. Borner, P. Van Rosmalen, and M. Specht. Augmenting the senses: A review on sensor-based learning support. Sensors (Switzerland), 15(2):40974133, 2015.    "}
{"index":{"_id":"111"}}
{"datatype":"inproceedings","key":"Choi:2017:SWT:3027385.3029477","author":"Choi, Heeryung and Brooks, Christopher and Collins-Thompson, Kevyn","title":"What Does Student Writing Tell Us About Their Thinking on Social Justice?","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"594--595","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029477","doi":"10.1145/3027385.3029477","acmid":"3029477","publisher":"ACM","address":"New York, NY, USA","keywords":"natural language processing, social justice, social work, writing analytics","Abstract":"In this work we investigate the use of deep learning for text analysis to measure elements of student thinking related to issues of privilege, oppression, diversity and social justice. We leverage historical expert annotations as well as a large lexical model to create a more generalizable vocabulary for identifying these characteristics in short student writing. We demonstrate the feasibility of this approach, and identify further areas for research.","pdf":"What does student writing tell us about their thinking on social justice  Heeryung Choi School of Information University of Michigan heeryung@umich.edu  Christopher Brooks School of Information University of Michigan brooksch@umich.edu  Kevyn Collins-ompson School of Information University of Michigan kevynct@umich.edu  ABSTRACT In this work we investigate the use of deep learning for text anal- ysis to measure elements of student thinking related to issues of privilege, oppression, diversity and social justice. We leverage his- torical expert annotations as well as a large lexical model to create a more generalizable vocabulary for identifying these characteristics in short student writing. We demonstrate the feasibility of this approach, and identify further areas for research.  CCS CONCEPTS Social and professional topics Student assessment; Model curricula; Adult education; Computing methodologies Nat- ural language processing;  KEYWORDS Writing analytics, social work, social justice, natural language pro- cessing ACM Reference format: Heeryung Choi, Christopher Brooks, and Kevyn Collins-ompson. 2016. What does student writing tell us about their thinking on social justice. In Proceedings of Learning Analytics & Knowledge Conference, Vancouver, BC, Canada, March 13-17, 2017 (LAK 17), 2 pages. DOI: hp://dx.doi.org/10.1145/3027385.3029477  1 INTRODUCTION AND RELATED WORKS While individual courses are oen mapped to explicit learning out- comes, and course outcomes are aggregated to demonstrate higher levels of learning in a curriculum, it is also possible to consider thematic elements which run throughout a curriculum. In this work, we explore one such thematic element in the context of the discipline of social work at the University of Michigan. In particular, we are interested in nding evidence of student thinking about priv- ilege, oppression, diversity, and social justice (PODS). While these PODS skills are ill-dened, multifaceted, and highly contextual, the intent is that all courses within the discipline should strengthen student skills with respect top PODS thinking, regardless of the specic course objectives [4]. Due to brevity, we refer the interested reader to [4] for details about the PODS framework used.  Tomeasure elements of PODS, the University of Michigan School of Social Work engaged in an open coding activity, summarizing Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 17, Vancouver, BC, Canada  2017 Copyright held by the owner/author(s). 978-1-4503-4870-6/17/03. . .$15.00 DOI: hp://dx.doi.org/10.1145/3027385.3029477  student responses to short, topical scenarios related to social work practice for several years [1][3]. is was conducted primarily by manual coding of student writing: having an expert in the PODS framework go through writings of students accumulated over the years, identifying words demonstrating evidence of PODS thinking, and hence forming a short close vocabulary. In this poster we augment this approach with a machine-learned customized vocabulary which is supported by the work of the empath project [2]. Empath uses deep learning to form word associations from large textual datasets, and creates new lexical categories based on a few seed words.  Our aim in using empath is to boost the set of expert words provided into a broader set, increasing topic coverage and ideally increasing the generalizability of the historical lexicon. us the research question we explore is: Can we better detect elements of PODS in student writing through unsupervised machine learning  Our key ndings are as follows:  (1) ere is a low agreement between non-expert human coders on student writing, which shows the diculty of identify- ing elements of PODS thinking in student writing.  (2) ere is even a lower agreement between a historical expert- derived PODS lexical category and non-expert human coders on new student writing, which demonstrates the lack of generalizability of historical expert annotations.  (3) ere is a higher agreement between an empath-boosted lexical category based on historical expert annotations and non-expert human raters in the same task, which shows the potential generalization benets of using deep learning.  us the contribution of this work is evidence to support the use of unsupervised machine learning when aempting to identify elements of PODS thinking in student writing.  2 APPROACH To compare the ecacy and validity of this approach, three dier- ent comparison methods were constructed to assess students short answers. Each of these methods was related to PODS thinking, and hence was divided into four PODS classications: Privilege, Op- pression, Diversity, and Social Justice. In this work, we considered only the Social Justice category, since half of the questions directly asked students to write a paragraph describing Social Justice issues related to the vignee.  e rst method, which we call the expert set, was a vocabu- lary composed of expert-derived annotations based on manually choosing PODS-related words from student writing in historical iterations of the course.    LAK 17, March 13-17, 2017, Vancouver, BC, Canada Heeryung Choi, Christopher Brooks, and Kevyn Collins-Thompson  e second method, which we refer to as the boosted set, was generated through the empath application. e lexical category for this method was built from expert words using one of the empath default corpora (ction) and a word limit of n=200. All of the non unigram results returned by empath were converted into unigrams.  e third method was comprised of two vocabularies created by non-expert human coders (non-experts). ese coders read students responses and tagged terms related to the concept of Social Justice. e human coders were untrained graduate students who were not majoring in the eld of social work. roughout all approaches and analyses words were stemmed with a Lancaster stemmer.  3 DATASET e dataset used includes 472 responses to a writing prompt an- swered by 118 students taking introductory graduate-level social work courses. An average word count of the responses was about 90 words with a standard deviation of 57.87. e course was composed of ve sections taught by ve dierent instructors. All students were shown one of two dierent writing scenarios. e data used in this work was based on student responses to the following ques- tion: Write a paragraph describing the social justice actions that you believe would helpful to social workers in this [the writing scenario] case.  4 RESULTS We converted student responses into binary feature sets (dummy variables) and evaluated the three dierent approaches using Fleiss Kappa for inter-coder-agreement (). We found:  (1) ere is low agreement between the two human non- ex- perts for this task (=0.37, n=2).  (2) Despite the low agreement, this was higher than the agree- ment among the non-expert vocabularies and the vocabu- lary created by the expert on historical data (=0.21, n=3).  (3) Furthermore, the vocabulary created by the expert showed equally poor agreement with both non-experts (=0.06 and =0.08 respectively.)  (4) Amachine-learned vocabulary trained on an expert-derived historical data has low agreement with the expert-derived vocabulary (=0.21, n=2).  (5) At the same time, this machine-learned vocabulary has higher agreementwhen comparedwith human non-experts both as a group (=0.24, n=3) and pairwise than the vocab- ulary created by the expert on historical data (=0.17 and =0.17 in both cases).  5 DISCUSSION ese ndings have several ramications for future research. First, the general lack of agreement between non-expert coders shows  that the task is dicult for untrained human raters. Furthermore, the result that the low  value decreases when including an expert human coder based on historical data suggests that the issue is dicult to generalize. ese two ndings suggest a deeper under- standing of the domain is needed by raters, along with vocabulary which is broader than historical answers have provided.  e low  value between the boosted set and the expert set was somewhat expected: the method employed by empath is aimed at generalizing the vocabulary, and not in merely copying the seed words. What is surprising and encouraging is the performance of the boosted set when compared to the non-experts: an increasing .  greater than that found between the expert and the non-experts suggests that the machine-learned model has generalized the vo- cabulary for new data. us the topic boosting has leveraged new relationships from the empath corpus on the expert model.  6 FUTUREWORK An immediate concern of ours is to increase  values between hu- man coders by recruiting experts or engaging in more sophisticated analyses (e.g. discussion between coders on issues of disagreement). Simultaneously we will begin to explore the size of the boosted dataset. Empath uses cosine similarity to rank related terms, and we will consider the impact changing this parameter has on inter- coder agreement between the boosted dataset and the non-expert datasets. Finally, we will explore the impact dierent training cor- pora have on the accuracy of the deep learned model. e empath environment currently supports three dierent corpora, and we are investigating augmenting this with new, more discipline-specic documents.  ACKNOWLEDGMENTS We thank Dale Fitch, Beth Glover Reed, Tina Louise, and all of the instructors who allowed us to explore this issue with their students. is work was supported in part by the Michigan Institute of Data Science (MIDAS) learning analytics challenge initiative.  REFERENCES [1] Paula Allen-Meares. 2007. Cultural Competence: An Ethical Requirement. J.  Ethn. Cult. Divers Soc. Work 16, 3-4 (2007), 8392. [2] Ethan Fast, Binbin Chen, and Michael S Bernstein. 2016. Empath: Understanding  Topic Signals in Large-Scale Text. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (Feb 2016), 46474657.  [3] Dale Fitch and Beth Glover Reed. 2010. Automated Curricular Assessment Using alitative Data.  [4] University of Michigan School of Social Work. 2016. Section 4.01.03: Privilege, Oppression, Diversity and Social Justice (PODS) & Arma- tive Action. hp://ssw.umich.edu/msw-student-guide/section/4.01.03/325/ privilege-oppression-diversity-and-social-justice-pods-armative-action  http://ssw.umich.edu/msw-student-guide/section/4.01.03/325/privilege-oppression-diversity-and-social-justice-pods-affirmative-action http://ssw.umich.edu/msw-student-guide/section/4.01.03/325/privilege-oppression-diversity-and-social-justice-pods-affirmative-action   Abstract  1 Introduction and related works  2 Approach  3 Dataset  4 Results  5 Discussion  6 Future work  Acknowledgments  References   "}
{"index":{"_id":"112"}}
{"datatype":"inproceedings","key":"Jeremic:2017:MSI:3027385.3029478","author":"Jeremic, Zoran and Kumar, Vive and Graf, Sabine","title":"MORPH: Supporting the Integration of Learning Analytics at Institutional Level","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"596--597","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029478","doi":"10.1145/3027385.3029478","acmid":"3029478","publisher":"ACM","address":"New York, NY, USA","keywords":"batch processing, dashboards, data streaming, institutional learning environments, learning analytics, real-time processing","Abstract":"While there is high potential in using learning analytics to provide educational institutions as well as teachers and learners with actionable information and improve learning experiences, currently only very few learning analytics tools are actually used in educational institutions. In this paper, we introduce MORPH, a platform that facilitates the integration of learning analytics modules and tools into institutional learning systems. MORPH provides a robust distributed architecture which combines batch, stream and real-time data processing using a parallel processing model to enable and support efficient processing of large amounts of data. Furthermore, it provides common management and administration features that enable the seamless integration of learning analytics research modules and tools into existing institutional learning systems.","pdf":"MORPH: Supporting the Integration of Learning Analytics  at Institutional Level   Zoran Jeremic, Vive Kumar and Sabine Graf  School of Computing and Information Systems, Athabasca University, Canada   zjeremic@athabascau.ca; vive@athabascau.ca; sabineg@athabascau.ca        ABSTRACT  While there is high potential in using learning analytics to provide  educational institutions as well as teachers and learners with  actionable information and improve learning experiences,  currently only very few learning analytics tools are actually used  in educational institutions. In this paper, we introduce MORPH, a  platform that facilitates the integration of learning analytics  modules and tools into institutional learning systems. MORPH  provides a robust distributed architecture which combines batch,  stream and real-time data processing using a parallel processing  model to enable and support efficient processing of large amounts  of data. Furthermore, it provides common management and  administration features that enable the seamless integration of  learning analytics research modules and tools into existing  institutional learning systems.   CCS Concepts   Applied computingInteractive learning environments   Software and its engineeringSoftware prototyping   Keywords  Learning Analytics; Institutional Learning Environments; Data  Streaming; Batch Processing; Real-time Processing; Dashboards    1. INTRODUCTION  In recent years, learning analytics is evolving rapidly and  garnering broad interest of both educational institutions and  researchers [1]. Over the last years a lot of research has been  conducted on different topics related to learning analytics and  several learning analytics tools and prototype systems have been  developed. However, only a very small number of such  tools/systems has been adopted by educational institutions.   When designing, developing and evaluating learning analytics  research tools and systems, the primary focus of researchers is  often on the learning analytics research itself, including issues  such as the proper collection of data from a learning system, the  processing of data through different algorithms, and data  visualization (e.g., through dashboards, alerts, notifications, etc.).  However, general software engineering issues as well as  management and administrative aspects for integrating such  research modules into learning systems of whole educational  institutions are often neglected. But such issues are crucial to   build robust research modules that can seamlessly integrate in  existing institutional systems of educational institutions.  Furthermore, designing and implementing learning analytics for a  whole educational institution typically involves complex  computing and aggregating of large amounts of data which cannot  be performed by traditional data management technologies [2].  However, many research modules are built and/or tested to run on  limited or small-scale datasets (e.g., for one or few courses) to  evaluate these research modules and their underlying algorithms.  These modules often would not perform well once integrated with  real-world educational systems which host hundreds of courses.  Accordingly, a successful implementation of research modules in  learning analytics requires a new application development  paradigm to create a technical solution that effectively operates  with large amounts of data and allows the integration of different  research modules.   In order to address the above-mentioned problems, we designed  and developed MORPH, a dynamic and evolving platform, which  facilitates the integration of innovative research tools/modules in  the areas of learning analytics, visualization, learner profiling,  personalization and others, into existing institutional learning  environments. MORPH provides a robust distributed architecture  which combines batch, stream and real-time data processing,  using a parallel processing model. Research modules integrated  with MORPH are structured around the provided architecture  which ensures that data are processed in a reliable and efficient  way. MORPH provides functionalities for seamless integration,  management and administration of research modules in  institutional learning management systems (LMSs). With this  approach, a significant burden can be taken from researchers  when aiming at developing research modules that can be used in  real-world scenarios, so that they can focus on research-related  issues rather than on software engineering tasks.   2. LEARNING ANALYTICS IN MORPH  MORPHs architecture is built to connect research modules to  LMSs while providing support for data processing and analytics  of huge amounts of data as well as for management and  administrative issues to integrate different research modules into  LMSs used by whole educational institutions. To address the  problem of aggregation and processing of large amounts of data, a  distributed architecture is used, in which MORPH comprises two  basic high level components: (1) Data Processing & Analytics  subsystem and (2) Data Collection & Presentation subsystem,  which are loosely coupled using RESTful services and Distributed  Messaging System. The Data Processing & Analytics subsystem  includes all information management components, i.e. data store  which is separated from the LMS database, as well as components  to search, transform, integrate and process data. The Data  Collection & Presentation subsystem is integrated with the LMS,  and provides different services related to the management and  administration of research modules as well as visual components  used by research modules such as dashboards, notifications, etc.      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029478     Each research module is plugged into both subsystems and could  be easily disconnected (and reconnected). In addition, MORPH  provides management and administrative support for  administrators and teachers. Administrators can easily turn on/off  research modules and make them available to teachers, while  teachers can activate/deactivate research modules in their courses,  and tune different settings. Moreover, MORPH provides different  APIs and reusable components such as dashboards, alerts,  notifications, which can be reused by different research modules.    2.1 Data Streaming and Processing   To meet the complex requirements of learning analytics modules  which typically process large amounts of data in different ways,  data streaming, batch processing and real-time processing needs  to be supported [3]. To ensure highly efficient data processing in  MORPH, a separate data store is used for storing and searching  data, so no data has to be retrieved from the LMS database.  MORPH extends the LMS with a Data Collection & Presentation  subsystem. The Data Collection subcomponent collects events,  appends other relevant data and submits event data as a data  stream to the Data Processing & Analytics subsystem through the  Distributed Messaging System. The Data Processing & Analytics  subsystem receives the event data, classifies it and forwards the  data to its subcomponents called Stream Processors which are  subscribed for specific event types. Stream Processors define  operations that are applied to each individual data item as it passes  through (e.g., calculating the average time spent on a page,  session durations, etc.). A common scenario of use of Stream  Processors is to provide event counters (e.g., page views per  session, unique page views, etc.), which may then be used by  research modules to produce more complex information.   MOPRH offers two ways of processing (and accessing) data:  batch processing and real-time processing. Batch processing is  well-suited for complex calculations where access to large  amounts of data is required and where results are not needed in  real-time. A research module can use MORPHs APIs to specify  (1) when to activate batch processing (e.g., based on a specific  input such as a user event or at specific time intervals), (2) which  data should be processed in batch (e.g., only last user session  events or data related to a specific user), and (3) which operations  should be performed on the data. MORPH provides different  types of batch job APIs for research modules and handles aspects  such as triggering, parallelization and failover. Once a MORPH  batch job is activated, data is collected from the data store based  on the provided parameters, processed using the provided  operations, and then the results are stored back to the data store.  Real-time processing can be used in MORPH, for example, for  providing recommendations, personalization and interventions.  The goal is to respond in real-time or near real-time to certain user  actions or situations identified through received events or direct  requests. MORPH supports two approaches of real-time  processing. The first approach is to retrieve data stored as a result  of data streams or batch job processing, aggregate and process this  data, run algorithms and/or analytical techniques on them, and  visualize the respective output for the end user. Another approach  is to extend a Learning Session Analyzer (LSA) provided by  MORPH, through a research module. Such LSA allows analyzing  the current and previous events of a respective learning session, as  defined in a research module. A LSA is controlled and triggered  by MORPH each time a user performs an action and uses a short- term data store to store the events of a learning session for  processing and analysis. As the result of such processing and  analysis, certain interventions can be provided to the end users.    2.2 Learning Interventions in MORPH  Besides data streaming and processing, MORPH also facilitates  the process of showing actionable information and  recommendations in a LMS. Several options are supported by  MORPH and in the following paragraphs, two important options  for learning analytics modules are presented in more detail.   MORPH provides a set of different notification types that can be  presented to students (or teachers) for direct interventions within  the LMS. Interruptive notifications cover a LMS page to let  students know that something very important requires their  immediate attention. Non-interruptive notifications appear in the  right-bottom corner of the screen and display a personalized  message. These notifications can have different visual appearance  to indicate success or risks, and provide information and  warnings. The third type of notifications are messages that are  displayed above learning content and are typically used to provide  information, advice or guidance. Each type of intervention can be  either displayed on any page of the course or only on a specific  page. For example, a message that encourages a student to take a  more active part in forums can be displayed on any page in the  course, while a notification that proposes additional resources to a  learning object can be displayed on only that learning object.  MORPH also provides advanced dashboard functionalities that  were designed to provide rich, meaningful and timely feedback to  students and teachers. Dashboards are tailored on the fly for each  course and each user based on the research modules activated in  the course and on the users role in the LMS. A dashboard is  divided into logical sections which are presented on the same page  or on different tabs, depending on the total number of widgets that  are displayed. Widgets are containers which hold actual visual  representations of data generated by research modules. A research  module can define the type of diagram to be displayed and  provide data in JSON format to populate a widget; and MORPH  takes care of the visualization based on the provided parameters  and data. However, if there is a need for more complex  visualizations that are not supported by MORPH, it is possible to  provide custom implementation for a widget.     3. CONCLUSION  In this paper, we presented MORPH, a platform that facilitates the  integration of learning analytics research modules into  institutional learning systems. MORPH provides different support  for such integration including management and administrative  support as well as support for streaming and processing of large  amounts of data. As such, it enables researchers to focus more on  learning analytics research rather than on general software  engineering issues, while ensuring that their research modules are  capable of being integrated in learning systems of whole  educational institutions. Future work deals with evaluating  MORPH with different research modules.   4. REFERENCES  [1] Chatti, M. A., Lukarov, V., Ths, H., Muslim, A., Yousef, A.   M. F., Wahid, U., Greven, C., Chakrabarti, A., Schroeder, U.  2014. Learning analytics: Challenges and future research  directions. E-Learning and Education (eleed), 10.    [2] Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R.,  Roxburgh, C., Byers, A.H. 2011. Big Data: The Next  Frontier for Innovation, Competition, and Productivity.  Report. McKinsey Global Institute.   [3] Shahrivari, S. 2014. Beyond batch processing: Towards real- time and streaming big data. Computers 2014, 3(4), 117-129.        1. INTRODUCTION  2. LEARNING ANALYTICS IN MORPH  2.1 Data Streaming and Processing  2.2 Learning Interventions in MORPH   3. CONCLUSION  4. REFERENCES   "}
{"index":{"_id":"113"}}
{"datatype":"inproceedings","key":"Okubo:2017:NNA:3027385.3029479","author":"Okubo, F. and Yamashita, T. and Shimada, A. and Ogata, H.","title":"A Neural Network Approach for Students' Performance Prediction","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"598--599","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029479","doi":"10.1145/3027385.3029479","acmid":"3029479","publisher":"ACM","address":"New York, NY, USA","keywords":"learning log, predication of student's performance, recurrent neural network","Abstract":"In this paper, we propose a method for predicting final grades of students by a Recurrent Neural Network (RNN) from the log data stored in the educational systems. We applied this method to the log data from 108 students and examined the accuracy of prediction. From the experimental results, comparing with multiple regression analysis, it is confirmed that an RNN is effective to early prediction of final grades.","pdf":"A Neural Network Approach for Students Performance Prediction   F. Okubo   Faculty of Arts and Science,    Kyushu University   Japan   fokubo@artsci.kyushu-u.ac.jp   T. Yamashita   Department of Computer Science,    Chubu University   Japan   yamashita@cs.chubu.ac.jp   A. Shimada   Faculty of Arts and Science,    Kyushu University   Japan   atsushi@limu.ait.kyushu-u.ac.jp   H. Ogata   Faculty of Arts and Science,    Kyushu University   Japan   hiroaki.ogata@gmail.com   ABSTRACT   In this paper, we propose a method for predicting final grades of   students by a Recurrent Neural Network (RNN) from the log data   stored in the educational systems. We applied this method to the   log data from 108 students and examined the accuracy of   prediction. From the experimental results, comparing with   multiple regression analysis, it is confirmed that an RNN is   effective to early prediction of final grades.   CCS CONCEPTS    Applied computingEducationComputer-assisted   instruction   KEYWORDS   Learning log, predication of students performance, recurrent   neural network   1    INTRODUCTION   In recent years, the use of ICT based educational systems has been   widely spread. These systems enable us to collect many types of   log data that corresponds to learning activities of students. By   analyzing these logs using data mining techniques, we can   determine learning patterns of students, which helps teachers in   detecting at-risk students ([1]). At Kyushu University, a   learning support system called the M2B system was introduced in   October 2014. The M2B system consists of three subsystems, the   e-learning system Moodle, the e-portfolio system Mahara, and the   e-book system BookLooper provided by Kyocera Maruzen, Inc.   Using the logs of these systems, a number of investigations have   been conducted ([3], [4], [5]).     Permission to make digital or hard copies of part or all of this work for personal or   classroom use is granted without fee provided that copies are not made or distributed   for profit or commercial advantage and that copies bear this notice and the full   citation on the first page. Copyrights for third-party components of this work must be   honored. For all other uses, contact the owner/author(s).   Copyright is held by the owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.    http://dx.doi.org/10.1145/3027385.3029479       An early prediction of students final grades is an important   task in the field of learning analytics, e.g., investigated in [6]   using regression analysis. In this paper, we propose a method for   predicting students final grades by a neural network approach,   using the log data of the M2B system. Particularly, in order to   treat time series data of each week in a course, we use a variant of   a neural network, called a Recurrent Neural Network (RNN) ([2]).   By comparing our results with the result obtained using regression   analysis, we show the performance of prediction of students final   grades using RNN.   2 DATA COLLECTION   We collected the learning logs of 108 students attending   Information Science course, which started in April 2016. In this   course, the teacher and students used the LMS, the e-portfolio   system and the e-book system. The students were required each   week to submit a report, to answer a quiz, to write a logbook of a   lecture, and to read slides for preview and review using the three   systems. The logs of these learning activities were automatically   graded by the system based on the criteria shown in Table 1:         Activities 5 4 3 2 1 0   Attendance  Atten  dance     Being   late      absen  ce   Quiz (rate of   collect answer)   Above   80%   Above   60%   Above   40%   Above   20%   Above   10%  o.w.   Report  Submi  ssion     Late   submi  ssion       No    submi  ssion   Course views  Upper  10%   Upper  20%   Upper  30%   Upper  40%   Upper  50%   o.w.   Slide views in   Booklooper   Upper   10%   Upper   20%   Upper   30%   Upper   40%   Upper   50%  o.w.   Markers in    Booklooper   Upper   10%   Upper   20%   Upper   30%   Upper   40%   Upper   50%  o.w.   Memos in   Booklooper   Upper  10%   Upper  20%   Upper  30%   Upper  40%   Upper  50%   o.w.   Actions in    Booklooper   Upper   10%   Upper   20%   Upper   30%   Upper   40%   Upper   50%  o.w.   Word count in   Mahara   Upper   10%   Upper   20%   Upper   30%   Upper   40%   Upper   50%  o.w.    Table 1. Criteria for grading learning activities           3 RECURRENT NEURAL NETWORK   A Recurrent Neural Network (RNN) handles time series data.   Unlike a general Neural Network, an RNN has a recursive loop as   shown in Figure 1. The RNN propagates the internal information   previous time at the current time, and obtains the output value   based on the current information and the past information. Thus, it   is possible to output in consideration of the past state.   The parameters of the RNN are trained by Backpropagation   through time (BPTT). The BPTT propagates the error between the   ground truth and the output at time t tracing back to time t-1.   Similarly, an error at time t-1 is propagated at time t-2, and   training is performed retroactively. Although the RNN   theoretically can output with consideration of all the past   information, in fact, the error is not able to propagate to far past.   Therefore, it is an output considering only the information of the   past several times.   To address this problem, Long Short Term Memory (LSTM) is   employed as a unit in middle layer that stores long term   information. The LSTM has a memory for storing the internal   state. The memory information stored in LSTM is kept efficient   information or deleted discard information by input or internal   state at previous time. In this paper, LSTM is used as a middle   layer unit of RNN.   4 CONCLUSIONS   For each 1i15, the RNN was trained by the log data until the i-  th week, that consisted of a vector of nine kinds of grades of each   week shown in Table 1 (treated as input), and the final grade A, B,   C, D, or F of students (treated as output).  Using the obtained   RNN, the prediction of the final grades of students was performed.   We also examined the prediction of final grade using multiple   regression analysis, where the final grades A, B, C, D, and F were   replaced by 95, 85, 75, 65, and 55, respectively. For each 1i15,    the multiple regression analysis using the data until the i-th week   was performed. Moreover, by using the multiple regression   equation, the final grades were predicted. The accuracy of    prediction by the RNN and the accuracy of prediction by the      multiple regression analysis together with the value of adjusted R2   are summarized in Table 2.  We can observe that the accuracy by    the RNN is above 90% using the data until 6th week, while the   accuracy by the multiple regression analysis is less than 90%   using the data until 9th week. Hence, it can be said that RNN is   effective for early prediction of final grades. We remark that the   learning activities that contribute to obtain a certain grade can be   inferred from the weight of the obtained RNN.   5 CONCLUSION   In this paper, we proposed a method for predicting final grades of   students by a Recurrent Neural Network (RNN) from the log data   stored in the educational systems. The log data represented the   learning activities of students who used the learning management   system, the e-portfolio system, and the e-book system. We applied   this method to the log data from 108 students. The accuracy of   prediction by the RNN is above 90% using the log data until 6th   week. This fact shows that comparing with multiple regression   analysis, RNN is effective to early prediction of the final grades.   ACKNOWLEDGMENTS   The research results have been achieved by Research and   Development on Fundamental and Utilization Technologies for   Social Big Data (178A03), the Commissioned Research of   National Institute of Information and Communications   Technology (NICT), Japan; Grant-in-Aid for scientific   Research(S) No.16H06304.   REFERENCES   [1] Baradwaj, B. & Pal, S. 2011. Mining Educational Data to   Analyze Students Performance, International Journal of   Advanced Computer Science and Applications, 6, 2, 63-69.    [2] Bodn, M. 2002. A Guide to Recurrent Neural Networks and   Backpropagation, The Dallas Project, SICS Technical Report,   1-10.   [3] Ogata, H., Yin, C., Oi, M., Okubo, F., Shimada, A., Kojima,   K. & Yamada, M. 2015. E-Book-based Learning Analytics in   University Education, Proc. ICCE2015, 401-406.   [4] Okubo, F., Hirokawa, S., Oi, M., Shimada, A., Kojima, K. &   Yamada, M. & Ogata, H. 2016. Learning Activity Features of   High Performance Students, Proceedings of the 1st   International Workshop on Learning Analytics Across   Physical and Digital Spaces (Cross-LAK 2016), 28-33.   [5] Okubo, F., Shimada, A., Yin, C. & Ogata, H. 2015.   Visualization and Prediction of Learning Activities by Using   Discrete Graphs, Proc. ICCE2015, 739-744.   [6] You, J. W. 2016. Identifying significant indicators using LMS   data to predict course achievement in online learning, Internet   and Higher Education, 29, 23-30.     Week 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15   RNN 50% 64% 73% 81% 87% 93% 93% 94% 98% 100% 100% 100% 100% 100% 99%   Regression   analysis   41% 46% 46% 52% 61% 63% 67% 75% 89% 92% 94% 94% 96% 100% 100%   Adjusted R2 .158 .212 .281 .325 .353 .379 .502 .620 .744 .772 .757 .758 .790 .951 .988    Table 2. The accuracy of prediction for the final grade by the RNN, the multiple regression analysis, and adjusted R2.   Figure 1. Recurrent Neural Network     "}
{"index":{"_id":"114"}}
{"datatype":"inproceedings","key":"Quigley:2017:ULA:3027385.3029482","author":"Quigley, David and McNamara, Conor and Sumner, Tamara","title":"Using Learning Analytics in Iterative Design of a Digital Modeling Tool","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"602--603","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029482","doi":"10.1145/3027385.3029482","acmid":"3029482","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative modeling, iterative design, scientific modeling","Abstract":"Iterative design is a powerful method for developing digital classroom tools and curricula. We explore how infusing learning analytics into this process has influenced our development of EcoSurvey, a digital modeling tool for mapping the organisms and interactions in the local ecosystem. We have found that analytic techniques can help us discover areas in which students struggle to engage with scientific modeling, and we can iteratively use learning analytics to demonstrate the impact of design changes.","pdf":"Using Learning Analytics in Iterative Design of a Digital Modeling Tool  David Quigley University of Colorado Boulder Institute for Cognitive Science  Department of Computer Science  1777 Exposition Drive Boulder, Colorado  david.quigley@colorado.edu  Conor McNamara University of Colorado Boulder  Department of Computer Science  1777 Exposition Drive Boulder, Colorado  Conor.R.Mcnamara @colorado.edu  Tamara Sumner University of Colorado Boulder Institute for Cognitive Science  Department of Computer Science  1777 Exposition Drive Boulder, Colorado  tamara.sumner@colorado.edu  ABSTRACT Iterative design is a powerful method for developing digi- tal classroom tools and curricula. We explore how infusing learning analytics into this process has influenced our devel- opment of EcoSurvey, a digital modeling tool for mapping the organisms and interactions in the local ecosystem. We have found that analytic techniques can help us discover areas in which students struggle to engage with scientific modeling, and we can iteratively use learning analytics to demonstrate the impact of design changes.  CCS Concepts Human-centered computing  Interaction design theory, concepts and paradigms; Applied comput- ing  Collaborative learning;  Keywords Scientific Modeling, Collaborative Modeling, Iterative De- sign  1. INTRODUCTION Scientific modeling is a core component of modern K-12  science education, allowing students to represent their cur- rent understanding of a system (or parts of a system) under study, to aid in the development of questions and explana- tions, to generate data that can be used to make predictions, and to communicate ideas to others. [4] Various learning sciences researchers (e.g. [6, 3]) have explored areas related to scientific modeling, and have highlighted model creation, review, revision, usage, and iteration as core components of scientific practice. To facilitate students learning scientific modeling, tools and curricula should scaffold the experience to help students build their own models for real-world usage.  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  LAK 17 March 13-17, 2017, Vancouver, BC, Canada c 2017 Copyright held by the owner/author(s).  ACM ISBN 978-1-4503-4870-6/17/03.  DOI: http://dx.doi.org/10.1145/3027385.3029482  Iteration has been demonstrated as a critical component of educational design [1]. This process requires recurring cycles of design and testing to help create a final product that best supports real-world needs. This poster explores how the iterative design process has influenced our digital modeling tool (EcoSurvey) and how we are using learning analytics techniques to measure improvements across iterations.  2. RESEARCH CONTEXT Our research centers around student usage of EcoSurvey,  a collaborative digital tool students use to model the organ- isms and interactions in their local ecosystem. This tool is part of the Inquiry Hub project, a design-based research ini- tiative focused on developing student-centered approaches to teaching and learning through partnership with a large ur- ban school district in the midwestern United States [7]. Our approach leverages iterative cycles of co-design and class- room testing to improve EcoSurvey and curriculum materi- als as well as drive our understanding of the collaborative modeling space.  The ecosystems unit of our biology curriculum has stu- dents choose a tree to plant on their school grounds to im- prove the biodiversity and resilience of the area. The stu- dents use EcoSurvey to develop a class-level model of the existing ecosystem, using this as evidence for a tree choice that will suit local needs. The workflow of EcoSurvey follows existing modeling practices theory [6] to have students itera- tively create, review, revise, and use their models. To begin, students go outside and use their personal devices, such as smartphones, to photograph and catalogue organisms they encounter. They then use these photos and notes to create organism cards in EcoSurvey. In addition, students add relations such as preys upon, supports, or competes with between organisms to capture interactions. Once the class has built an initial model, the students participate in a review session, pairing off and providing feedback on each others cards and relations. These review sessions can high- light incomplete and incorrect information within cards as well as show areas in which new cards would help connect portions of the model together. Students then take this feed- back and revise their models. The model then becomes a piece of evidence students use in their argument for which tree to choose. The construction of this argument can once again demonstrate errors and gaps in the model, driving the students to conduct more revision to better support their    claim. An example portion of an EcoSurvey model and an organism card can be seen in figure 1.  Figure 1: EcoSurvey model and organism card detail view.  We measure the variation across classroom models in terms of the number and nature of the organisms and interactions in students final logs. These features include raw numbers of each feature and the balance between types of organisms and interactions. For example, we determine if students in a class documented similar numbers of all types of interac- tions, or if they mostly incorporated a certain kind of rela- tion such as preys upon. We also measure the variance in distribution of interactions among organisms; an ecosystem model functions very differently if most of the relations con- nect to few keystone species or if there is an even distribution of relations among organisms.  We also analyze usage logs. These logs include a user ID, the action taken, and the context of the action. This context information includes the users class, the card the student was working with, the date and time of the action, and the device the student used. We organize these logs into ac- tion sequences for each student in order to answer questions about individual, classroom, and teacher differences related to engagement with modeling practices. We leverage ex- isting ideas for mining sequential data [8] and extracting relevant sequences [2] to determine which patterns are pre- dictive of a student taking part in creating stronger models.  3. ITERATIVE ANALYSIS Our analysis of the first version of EcoSurvey provided  valuable insights into the nature of the models that students created and the ways in which they engaged with modeling practices [5]. Overall, we found significant variance across final models and significant differences in the how students under different teachers engaged with all five modeling prac- tices. Our sequential analysis of student actions discovered that patterns related to revision, evaluation, and iteration were the most predictive of a students teacher. These re- sults motivated changes for the second version of EcoSurvey in order to improve student access to these key modeling practices.  Our design cycles give us the opportunity to examine the impact of design changes between iterations. By mapping student actions to existing modeling practice frameworks, we can perform direct comparisons across versions and eval- uate the impact of changes in newer versions. For example, we incorporated a dynamic graph view of the model into students workflow (as seen in figure 1). Rather than rely on intuition, however, we are currently using the same tech- niques to evaluate the results of our second deployment. We  can use the exact same analyses of organism and interaction richness and variance to determine how models compare be- tween deployments. We can also leverage our sequence fea- ture extraction techniques to see if the same features are still predictive of teacher differences.  These direct comparisons allow us to evaluate differences in both models student engagement with the modeling prac- tices. We expect students that use the dynamic graph fea- ture will demonstrate more thorough use of the modeling practices and that their subsequent group models will in- clude more thorough interaction data.  4. CONCLUSION Overall, scientific modeling in the classroom is a complex  problem space. That said, digital tools can provide a great amount of support for students, and the data streams of student usage can provide interesting insight into the stu- dent modeling process. In order to understand the impact of design choices on student behavior, we leverage learning analytic techniques over multiple cycles of our iterative de- sign process. The differences (and similarities) in analytic results provide a unique set of evidence for the impact of de- sign changes on student engagement with modeling practices and the strength of students final models.  5. REFERENCES [1] T. D.-B. R. Collective. Design-based research: An  emerging paradigm for educational inquiry. Educational Researcher, 32(1):58, 2003.  [2] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The WEKA data mining software. ACM SIGKDD Explorations, 11(1):1018, 2009.  [3] J. B. Homer. Why We Iterate: Scientific Modeling in Theory and Practice. System Dynamics Review, 12(1):119, 1996.  [4] National Research Council. A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas. The National Academies Press, Washington, DC, 2012.  [5] D. Quigley, J. Ostwald, and T. Sumner. Scientific Modeling: Using learning analytics to examine student practices and classroom variation. in Proceedings of the Seventh International Conference on Learning Analytics & Knowledge, pages 110, in press.  [6] C. V. Schwarz, B. J. Reiser, E. A. Davis, L. Kenyon, A. Acher, D. Fortus, Y. Shwartz, B. Hug, and J. Krajcik. Developing a learning progression for scientific modeling: Making scientific modeling accessible and meaningful for learners. Journal of Research in Science Teaching, 46(6):632654, 2009.  [7] S. Severance, W. R. Penuel, T. Sumner, and H. Leary. Organizing for Teacher Agency in Curricular Co-Design. Journal of the Learning Sciences, 25(4):531564, 2016.  [8] Z. Xing, J. Pei, and E. Keogh. A brief survey on sequence classification. ACM SIGKDD Explorations Newsletter, 12(1):40, 2010.    "}
{"index":{"_id":"115"}}
{"datatype":"inproceedings","key":"Elouazizi:2017:AAA:3027385.3029484","author":"Elouazizi, Noureddine and Birol, Gulnur and Jandciu, Eric and Oberg, Gunilla and Welsh, Ashley and Han, Andrea and Campbell, Alice","title":"Automated Analysis of Aspects of Written Argumentation","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"606--607","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029484","doi":"10.1145/3027385.3029484","acmid":"3029484","publisher":"ACM","address":"New York, NY, USA","keywords":"automated assessment, latent semantics analysis","Abstract":"In this paper, we report on a model that uses a mathematically and cognitively augmented Latent Semantic Analysis method to automatically assess aspects of written argumentation, produced by students in a science communication course","pdf":"Automated Analysis of Aspects of Written Argumentation    Noureddine Elouazizi1, Glnur Birol1, Eric Jandciu1, Gunilla berg2,   Ashley Welsh1, Andrea Han3, Alice Campbell1       Skylight/SCLT 1 , Faculty of Science  1, 2  and CTLT  3 , University of British Columbia, Vancouver BC   noureddine.elouazizi@science.ubc.ca      ABSTRACT  In this paper, we report on a model that uses a mathematically and   cognitively augmented Latent Semantic Analysis method to   automatically assess aspects of written argumentation, produced   by students in a science communication course.     CCS Concepts    Computing methodologies  Natural language processing   Keywords  Latent Semantics Analysis, Automated Assessment   1. INTRODUCTION  Learning argumentation through writing requires students to use   complex cognitive processes, such as advancing claims, hedging,   asserting, paraphrasing and taking stances [1]. While instructors   make every effort to assess the major elements of scientific   argumentation in student essays using rubrics, the enormous   volume of material makes it impossible to code for every possible   element. Furthermore, there is a risk that this amount of feedback   would not be consumable by students. However, a detailed   argumentation analysis could yield useful information about   where students generally perform well and poorly. Instructors   could then use this information to design suitable activities   targeting the problem areas in argumentation. Cognizant of this   potential impact on student learning, we created an automated   argumentation analysis model and software. In this paper, we   report on the application of a latent semantics analysis (LSA)   model that automatically assesses argumentation in students   writing.   2. THEORY  2.1 Argumentation    Written language is the direct cognitive by-product that   externalizes how students build arguments supported by evidence.   In our context, we define argumentation as a complex cognitive   act produced by a writer, and evaluated by a reader using the   meaning conveyed by natural language. In short, it is a   combination of a logical product and a rhetorical process [2].   Assuming that language is core to learning and that thought and   language are inseparable [4], examining students argumentation   offers opportunities for gaining insights into how students engage   in scientific reasoning.    2.2 Automatic assessment of argumentation   Regardless of the architectures, frameworks, interfaces and   functionalities, all computer-based systems that support some   form of argumentation draw on natural language processing   (NLP) methods [9]. The NLP field investigates methods for the   mathematical representation of language, allowing straightforward   analyses of compositionality, to enable machines to interpret   aspects of human language. The core assumptions are that the   elementary components of natural language within a text can be   identified and their meanings, relationships and dependencies   analyzed to uncover aspects of argumentation.    3. RELATED WORK  Previously developed models and systems focused on the use of   argumentation to teach students hypothetical reasoning and to   engage students in domain-specific problem solving [[6], [8], [5]].   More recently, Shum et al [7] offered a model and system   integrating insights from linguistic parsing and learning analytics   to assess dimensions of reflective writing to deepen learners   understanding. Our model and system builds on the insights in the   studies referred to above and focuses on the analysis of written   essays to assess the quality of the argumentation. The design of   our model integrates insights from the fields of cognition and   computation of language, learning analytics, and education theory.   4. THE STUDY  4.1 Context  Students essays used for developing this model originate from a   first-year writing intensive science course, offered at The Faculty   of Science, University of British Columbia. The overarching   course goals are to define and discuss the elements of a scientific   approach, to teach students ways of communicating science   through writing and peer reviewing.   4.2 Data  Our text corpus is unannotated and it includes 1346 term papers,   2020 essays, totaling 15.147.000 word corpus. The essays and   term papers were collected over four academic terms (from 2014   to 2016), and from 673 Science students, who consented in   written form. Each text is an essay assignment that is produced by   the students based on writing prompts for the assignments essays   and based on no writing prompts for the text of the term papers.      4.3 Method (model)   Our model uses LSA to automatically assess aspects of   argumentation in written essays. LSA is an automatic statistical   (probabilistic) method for representing the meaning of words,   phrases and passages in text [3]. The power of the LSA model lies   in its mathematical ability to use vectors to map out semantic   spaces found in an essay. This contributes to the automatic   interpretation of an essay through the dependencies revealed by   the semantic maps. Since the NLP task in this model and system   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights  for third-part components of the work must be honored. For all other   uses, contact the owner/Author. Copyright is held by the owner/author-s.    LAK17, March 1317, 2017, Vancouver, BC, Canada.  ACM 978-1-4503-4870-6/17/03.   DOI: http://dx.doi.org/10.1145/3027385.3029484   mailto:noureddine.elouazizi@science.ubc.ca   is a natural language understanding task at its core, we adapted   and added both mathematical and cognitive extensions to render   LSA more context-sensitive and more cognitively plausible. From   a mathematical perspective, we added the ability to use Euclidean   distances to measure the length of the vectors and Cosine to   measure the similarity of vectors. From a cognitive processing   perspective, we augmented LSA with the use of ngrams that   encode syntax-informed inferential paths, and form a targeted   semantic network of the concepts that convey aspects of   argumentation. As such, our model of LSA narrows the   probabilistic space and maps more dependencies across concepts.   This model removes dimensions that contain noise in assessing   written argumentation, and retains dimensions that discriminate   clearly between different aspects of written argumentation.    We combined the holistic method and the componential method   of LSA. The holistic assessment method contributes to a more   accurate measure of the overall quality of argumentation in an   essay. The componential assessment method assesses a specific   aspect of the argumentation in the essays (see: TABLE 1).    5. PRELEMINARY RESULTS  We handpicked the essays that were graded high by the instructors   and we labelled them as gold standard essays, and we tested   whether our LSA model would be able to identify the high graded   essays from the rest. The heat map (Figure1) indicates that the   augmented LSA model is able to discriminate, on its own,   between the essays that are gold standards and the essays that are   not. Dark blue colors at the bottom of the heat map are gold   standard essays.    We re-run the augmented LSA model and computed through the   combination of LSA-Cosine and LSA-Pearson to establish the   degree of similarities across the essays assessed in terms of the   quality of argumentation. We observed that the gold standard   essay 1 shares more similarities, in terms of its argumentation   characteristics, with other gold standard essays than with the non-   gold standard essays. The componential comparison of the   argumentation dimensions of, for example, <paraphrasing>-  <hedging>, and <arguing>-<stancing>, and other argumentation   concepts all consistently discriminate the gold standard essays   from the rest of the essays.   6. CONCLUSIONS AND FUTURE WORK  This augmented LSA model has several advantages. It integrates   the human judgment as part of its assessment through using the   pre-graded essays to assess aspects of the argumentation in other   essays. It offers an approach that can be adapted (calibrated) to   analyze different aspects of argumentation in written text. We are   working on further augmentation and validation of this LSA   model. We hope to report on those results by the time of the   conference.       Table 1. Some of the argumentation concepts    LEXICAL INDICATORS (SEMANTIC MATRICES)   HEDGING assume, believe, suppose, presume,     STANCING I assert, I stand, I hypothesize,     ARGUING  observe, predict, ascribe, question,    INDEXING its, it, their, these, this, those,     L-CONNECTORS therefore, if, the, because, either, or,    PARAPHRASING  indicated, proposed, suggested, ...   UNDERSTANDING classify, associate, categorize, express,     APPLYING determine, examine, demonstrate,    ELABORATING extend, add, clarify, always,      Figure 1. Heat map comparing essays in a course section   7. ACKNOWLEDGMENTS  We gratefully acknowledge the financial support for this project   provided by UBCs TLEF grant (project grant: 22G36907) and by   the Science Center for Learning and Teaching (Skylight) at the   UBCs Faculty of Science.   8. REFERENCES  [1] Besnard, B. and A. Hunter. 2008. Elements of   Argumentation. The MIT Press.   [2] Bermejo-Luque, Lilian. 2011. Giving Reasons: A Linguistic- Pragmatic Approach to Argumentation Theory.   Argumentation Library, 20. Dordrecht: Springer.   [3] Landauer, T.K. 1998. Learning and representing verbal  meaning: the latent semantic analysis theory. Current   Directions in Psychological Science, 7, 161164.   [4] Mayer, R. E. 1996. Learners as information processors:  Legacies and limitations of educational psychologys second   metaphor. Educational Psychologist, 31, 151161.   [5] Scheuer, O., Niebuhr, S.,  Dragon, T.,  McLaren B. M.,  &   Pinkwart, N. 2012. Adaptive support for graphical   argumentation - the LASAD approach.  IEEE Learning   Technology Newsletter, 14(1), 8-11.    [6] Shum, S.B., MacLean, J., Bellotti, A., V. M. E., &  Hammond, N. V. 1997. Graphical argumentation and design   cognition. Human-Computer Interaction, 12(3), 267300.    [7] Shum, S.B., Sndor, ., Goldsmith, R., Wang, X., Bass, R.  & Mcwilliams, M. 2016. Reflecting on reflective writing   analytics: Assessment challenges and iterative evaluation of   a prototype tool. In ACM International Conference   Proceeding Series, 213-222.   [8] Sionti, M., Ai, H., Ros, C. P., Resnick, L. 2011. A  Framework for Analyzing Development of Argumentation   through Classroom Discussions. In Educational   Technologies for Teaching Argumentation Skills, Niels   Pinkwart & Bruce McClaren, Eds., Bentham Science.   [9] Stegmann, K., Weinberger, A., & Fischer, F. 2007.  Facilitating argumentative knowledge construction with   computer-supported collaboration scripts. International   Journal of Computer-Supported Collaborative Learning   (ijCSCL), 2(4), 421447.     "}
{"index":{"_id":"116"}}
{"datatype":"inproceedings","key":"Vytasek:2017:TMS:3027385.3029486","author":"Vytasek, Jovita M. and Wise, Alyssa F. and Woloshen, Sonya","title":"Topic Models to Support Instructors in MOOC Forums","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"610--611","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029486","doi":"10.1145/3027385.3029486","acmid":"3029486","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOC, discussion forums, topic modeling","Abstract":"This paper explores the potential of using nave topic modeling to support instructors in navigating MOOC discussion forums. Categorizing discussion threads into topics can provide an overview of the discussion, improve navigation of the forum, and support replying to a representative sample of content related posts. We investigate four different approaches to using topic models to organize and present discussion posts, highlighting the strength and weaknesses of each approach to support instructors.","pdf":"Topic Models to Support Instructors in MOOC Forums  Jovita M. Vytasek   Simon Fraser University  8888 University Drive   Burnaby, B.C., Canada  +1 778-782-3111   jvytasek@sfu.ca  Alyssa F. Wise  New York University   New York, NY  10003, USA   +1 212-998-1212  alyssa.wise@nyu.edu   Sonya Woloshen  Simon Fraser University   8888 University Drive  Burnaby, B.C., Canada   +1 778-782-3111  swoloshe@sfu.ca   ABSTRACT  This paper explores the potential of using nave topic modeling to  support instructors in navigating MOOC discussion forums.  Categorizing discussion threads into topics can provide an  overview of the discussion, improve navigation of the forum, and  support replying to a representative sample of content related  posts. We investigate four different approaches to using topic  models to organize and present discussion posts, highlighting the  strength and weaknesses of each approach to support instructors.   CCS Concepts   Applied computing  Education  E-learning   Keywords  MOOC, discussion forums, topic modeling.   1. INTRODUCTION  Searching and navigating Massive Open Online Course (MOOC)  discussion forums, composed of high volumes of participants and  posts, is a challenging task for instructors and TAs [8]. To address  this, most forums provide searching, discussion thread labeling  [4], and some use students upvote of popular posts to denote  their importance [3]. These strategies can be helpful if an  instructor is searching for a particular type of question/comment,  but in many cases instructors read forum posts to gain a general  sense of what their students are discussing. This is an important  goal for instructors as they try to address the particular needs and  interests of their students. Instructors might want to know if  students are struggling with the content, how they are discussing  ideas, or provide support to encourage students to work  collaboratively. This serves the main intent of forums as a place to  discuss ideas, course content, ask questions, answer problems and  build networks [4][8][11].    Currently if an instructor wants to know about forum discussions  they need to read large quantities of posts, but this is overly  burdensome. Topic modeling offers a novel way to reduce this  burden by grouping similar posts together by topic [1] and  providing a topic description of their content. Grouping posts by  topics for instructors can potentially provide holistic insights into  the types of discussions occurring. Prior work using topic models  on MOOC forum data has mostly taken a researcher perspective  to understand discussions, for example to study: evolution of  discussion structures [4], organization of MOOC resources [6],   topics linked to student attrition [10], or identify MOOC topic  discussions in social media [5]. More recently, however, there has  been growing interest in developing and presenting topic models  to instructors to support their teaching.  Chen and colleagues [2]  used topic models to help instructors review large quantities of  student reflection papers. Other work [Paepcke, personal  communication] focused on the use of word clouds to visualize  MOOC discussion topic models for instructor use. The current  study expands this work by exploring how different approaches to  build nave topic models highlight different features for MOOC  instructors.    2. STUDY FRAMING  This study explores and compares the utility of four different  approaches to creating topic models to support instructors in  navigating and reviewing MOOC discussion posts. It is the first  phase of a project to design visualizations of topic models and  assess their use with teachers. This initial work addresses the  research question: What potential information is offered to  instructors by different variations of topic models The first model  is a baseline using all posts. The second approach generates topic  models from pre-sorted posts grouped into content/non-content  categories. The third approach uses a second level of topic  modeling to further group posts into subtopics. The fourth  approach creates models based on weekly course segmentation.   3. METHODS AND RESULTS  Data was all 813 starting posts from the discussion forum of a  Medical Statistics MOOC offered in 2014. Starting posts were  used for modeling as they are a parsimonious way to examine the  range of topics discussed and common way to search through a  discussion forum [9][12]. The MALLET toolkit [7] with standard  stop word list was used to develop the topic models. Number of  topics (k) was varied to optimize comprehension. In each analysis,  posts were assigned to their most related topic. Models two  through four used posts pre-categorized as content or non-content  based on a previously developed and validated classifier [12].  The first model uses a traditional topic modeling approach,  including all posts as one corpus. A 20-topic model best  differentiated posts but was difficult to navigate given the large  number of topics to examine and quantity of posts within. To  explore ways of improving ease of navigation and interpretability,  the second approach assessed the potential of using topic models  on the discussion corpus after dividing it into content/non-content.  Ten topics were generated from content and non-content posts in  order to compare a total of 20 topics to the prior model. This  substantially improved the interpretability of the topics. For  instance, the terms quiz and explain appeared in all models.  The content/non-content model provided greater context to  understand these terms as quiz was contextualized within themes  surrounding conceptual questions vs. how to access the quiz. This  also improved navigability  instructors could select either content  or non-content, then compare 10 topics, as opposed to comparing  all 20. Topic contribution to post improved from M=0.33   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  users, contact the Owner/Author.   Copyright is held by the owner/author(s).   LAK17, March 1317, 2017, Vancouver, BC, Canada.  ACM 978-1-4503-4870-6/17/03  http://dx.doi.org/10.1145/3027385.3029486     SD=0.15 to M=0.36 SD=0.12 and M=0.35 SD=0.13, content/non- content respectively. The limitation of this approach is the large  number of posts within each topic (M=45). This may be time  consuming for an instructor to review and could overlook smaller  topics within these broad topics. To address this, the third and  fourth models explored different approaches to further classifying  posts within a topic. The traditional solution would be to increase  k (# of topics) to reduce the number of posts within each topic;  however, this puts a cognitive burden on the instructor to compare  a greater number of unorganized topics. As an alternative, the  third design makes use of nested subtopics within the larger topic  to increase k but group related topics. Syphoning topics into  subtopics allows instructors to choose from a reduced list of topics  that open a list of related subtopics which link to posts. Figure 1  provides an example of this navigation using 5 subtopics.         The advantage is that instead of viewing 50 discrete topics,  nesting provides instructors with a choice among 10 topics with 5  related subtopics in each. Posts within the subtopics are ordered  by topic contribution providing instructors with easy access to the  most representative post from each subtopic. Grouping related  subtopics and listing their descriptor words can provide an  overview of the discussion without the need to read individual  posts. Instructors could use subtopic representative posts to gain  insight into the variety of questions asked. If reviewing past  courses for future improvements, this overview approach could be  beneficial to look at trends in problem areas or compare courses.  For example, an instructor could view all topics related to  algebraic computation difficulties across course chapters and  design an additional lesson to address it. A limitation to this  approach is that posts in a topic span a variety of weeks in the  course (see figure 1); however, instructors may want to see  discussion topics as they relate to their presentation of the  material. The fourth approach addresses this issue of temporality  by grouping posts according to the course schedule, creating  subtopics for each weeks posts (see figure 2). Instructors could  use a weekly summary of the discussion with representative posts   from different topics, or to address unresolved questions before  moving on to a new lesson. Additionally, listing posts by topic  groups similar questions highlighting repetition. Instructors can  quickly link questions to a prior answer post or create an   announcement that addresses a series of similar questions.  Limitations of this model are that the total number of posts change  weekly impacting the optimal number of k topics, and it is  difficult to view course wide trends.    4. DISCUSSION  This study contributes to the important question of how to make  the benefits of topic models useful to a practitioner audience of  instructors and their unique needs. Assessing the four different  approaches to grouping and classifying discussion posts using  topic models highlighted their unique benefits and drawback for  different instructional purposes. Results indicate organizing topic  models into content/non-content improves topic interpretability  with small gains in the relatedness of topic to posts. The creation  of subtopics within larger topics improved manageability of  interpretation and ease of use by reducing the number of topic  comparisons and grouping related subtopics. Model 3 highlights  the advantages of seeing topics across the course while model 4  highlights the advantages of maintaining the time sequence of the  course, representing a tradeoff between the ability to explore  either temporal features or broader concept features depending on  instructional needs. In the next phase of the project, these findings  will be used to design visualizations of the topic model designs  and assess their usability with instructors.   5. REFERENCES  [1] Blei, D. 2012. Probabilistic Topic Models. Communications   of the ACM, 55 (4): 77-84.  [2] Chen, Y., Yu, B., Zhang, X., & Yu, Y. 2016. Topic modeling   for evaluating students' reflective writing: a case study of  pre-service teachers' journals. In Proceedings of LAK16 .  ACM.1-5.   [3] Coetzee, D., Fox, A., Hearst, M. A., & Hartmann, B. 2014.  Should your MOOC forum use a reputation system. In  Proceedings of CSCW14. ACM. 1176-1187.   [4] Ezen-Can, A., Boyer, K. E., Kellogg, S., & Booth, S. (2015).  Unsupervised modeling for understanding MOOC discussion  forums: a learning analytics approach. In Proceedings of  LAK15. ACM. 146-150.   [5] Joksimovic, S, Kovanovic, V, Jovanovic, J, Zouaq, A,  Gasevic, D & Hatala, M. 2015. What do cMOOC  participants talk about in Social Media. In Proceedings of  LAK15. ACM. 156-165.   [6] Khamidoullina, I., Reggers, T., & Zeiliger, R. 2001.  NESTOR-integrated tools for active navigation and  constructive learning. In Proceedings of EDMEDIA (959).  AACE    [7] McCallum, A. 2002. MALLET: A Machine Learning for  Language Toolkit. Retrieve from: http://mallet.cs.umass.edu.    [8] Ramesh, A., Goldwasser, D., Huang, B., Iii, H. D., & Getoor,  L. 2014. Understanding MOOC discussion forums using  seeded LDA. In Proceedings of ACL. 29-33.   [9] Shi, X., Zhu, J., Cai, R., & Zhang, L. (2009, June). User  grouping behavior in online forums. In Proceedings of  SIGKDD. ACM. 777-786.   [10] Yang, S. H., Kolcz, A., Schlaikjer, A., & Gupta, P. 2014,  August). Large-scale high-precision topic modeling on  twitter. In Proceedings SIGKDD. 1907-1916. ACM.   [11] Yuan, L., Powell, S. 2013. MOOCs and open education:  Implications for higher education. CETIS White Paper.   [12] Wise, A. F., Cui, Y., Jin, W., & Vytasek, J. 2017. Mining for  gold: Identifying content-related MOOC discussion threads  across domains through linguistic modeling. The Internet and  Higher Education, 32, 11-28.      Figure 1. Topics and subtopics.   Figure 2. Weekly topic model view.       "}
{"index":{"_id":"117"}}
{"datatype":"inproceedings","key":"Alabi:2017:BIL:3027385.3029487","author":"Alabi, Halimat and Hatala, Marek","title":"Best Intentions: Learner Feedback on Learning Analytics Visualization Design","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"612--613","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029487","doi":"10.1145/3027385.3029487","acmid":"3029487","publisher":"ACM","address":"New York, NY, USA","keywords":"evaluation, information visualization, judgments of learning, learning analytics","Abstract":"A mixed methods approach was undertaken in this exploratory study to better understand how learners perceive and utilize learning analytics visualizations during online discussions activities. Internal conditions such as goal orientation and numeracy were measured alongside the external conditions created by the discussion structure and learning analytics. Our results emphasize key factors that should be considered when designing learning analytics tools.","pdf":"      Best Intentions: Learner Feedback on   Learning Analytics Visualization Design   Halimat Alabi, Marek Hatala  Simon Fraser University   250-13450 102nd Avenue  Surrey, BC V3T 0A3   {halabi, mhatala}@sfu.ca    ABSTRACT  A mixed methods approach was undertaken in this exploratory  study to better understand how learners perceive and utilize  learning analytics visualizations during online discussions  activities. Internal conditions such as goal orientation and  numeracy were measured alongside the external conditions  created by the discussion structure and learning analytics. Our  results emphasize key factors that should be considered when  designing learning analytics tools.   CCS Concepts   Information systems  Data analytics  Human-centered  computing  Empirical studies in visualization  Applied  computing  Computer-assisted instruction   Keywords  Learning analytics, information visualization, judgments of  learning, evaluation   1. INTRODUCTION  A primary functions of learning analytics (LA) is to aid in the  development, monitoring, and regulation of learning strategies. As  a self-regulated learning (SLR) feedback provisioning tool, LA  contribute to the conditions that influence learners interpretations  of their academic standings in relationship to their own goals  and/or the performance of their peers. To employ appropriate  regulatory strategies learners must draw the correct conclusions  from the information visualized, yet how this happens is poorly  understood. It is of the utmost importance then for educational  technology designers to ensure that the pedagogically founded  intentions exemplified by the LA visualizations produce the  desired effects.    2. BACKGROUND  The types of assumptions that people make with graphs are  influenced by several factors including graphicacy or graph  literacy, numeracy, subject matter expertise, and individual  difference. It is known that visualizations carry framing effects;  low numerate individuals are more apt to be misled by framing  and format effects [6]. Individuals with low graphicacy tend to  ignore the relationships depicted within [5]. Conversely, high  numeracy and cognitive reflexivity  the tendency for in depth   rational thought  are both correlated with successful performance  on probabilistic prediction tasks [9]. Particularly within the  context of learning, internal motivational and metacognitive states  are pivotal. These internal conditions move learners to delve  deeper, to reflect, act upon, and trust the feedback provided by  LA. Together LA visualizations act in concert with these internal  conditions, forming the visual foundation of learners judgments  of learning (JOL). Though there exists no established set of best  practices for graphically displaying risky information [7], may be  tailored to be contextually relevant for non-expert users skill and  experience level with visualizations. Educational technology  designers must recognize these contextual factors in order to  mitigate the application of incorrect learning strategies that may  ultimately risk learners academic success.    3. METHODOLOGY   Designed to visually emphasize either post quality or performance  in comparison to high performing peers, two types of LA  visualizations were deployed in online small group discussion  activities. We hypothesized that (1) learners with mastery  achievement goals would view both types of visualizations more  frequently, that (2) learners with a performance orientation would  utilize the top contributor visualization more often, and that (3)  highly numerate learners would demonstrate a preference for  numerical, less abstract visualizations. Quantitative performance  information  including posts, replies, discussion thread and  visualization views  was collected directly from the learning  management system. Questionnaires on numeracy and goal  orientation were distributed to determine if individual differences  in these areas would impact learners behaviours. In one-on-one  interviews learners provided feedback on (1) their actual use of  the visualizations in the discussion activities, (2) the perceived  utility of new visualizations in simulated discussions, and (3)  perceived utility after interaction with new LA visualization  prototypes.    3.1 Participants   Study participants were volunteers from online and blended  learning undergraduate courses offered by Simon Fraser  University. Across 8 discussion activities 178 learners had the  opportunity to utilize one of the two randomly assigned  visualizations, with remaining learners in each course serving as  the control group. Twelve learners volunteered for additional  follow-up interviews.   3.2 Procedure   In courses 1 and 2 one third of participants saw the visualizations.  A crossover design was used in courses 3 and 4; one quarter of  participating learners in each of two discussions was assigned  either the top contributor or quality visualizations. Learners were  not required to complete all of the questionnaires to participate in  the study, resulting in disparate numbers between each section.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada    ACM 978-1-4503-4870-6/17/03.        http://dx.doi.org/10.1145/3027385.3029487   http://dx.doi.org/10.1145/3027385.3029487         3.2.1 Instruments  The 22 achievement goal questionnaire (AGQ) instrument  divides responses into four categories corresponding to  achievement goals  mastery or performance, and approach or  avoidance  together representing relatively stable learning  orientations [2]. A short self-assessment of numerical preference  and aptitude, the Subjective Numeracy Scale (SNS) is strongly  correlated with objective tests of numeracy [3, 6] such as the  Berlin Numeracy Test (BNT) [1], also used in this study. The  Cognitive Reflection Test (CRT) examines cognitive impulsivity  [4, 8]. Taken together they represent the internal conditions, the  academic orientation that learners bring to the activity.    The 2 course visualizations and the proposed prototypes all  supported both self and socially shared learning regulation. The  four proposed visualizations  a polar graph, avatar, bar-graph- based cityscape, and ecosystem  ranged in aesthetic complexity.  Two versions of each visualization were constructed to allow  individual and group performance comparisons.      4. FINDINGS  We sought to examine how learners use the formative feedback  from LA visualizations to monitor, make learning judgments, and  metacognitively reflect upon their self-regulatory learning  strategies. Of the factors hypothesized to attribute to differences in  learners performance with LA  including goal orientation, self- reported or objective numeracy  none were substantiated in the  quantitative data from all of the classes combined. Results on the  SNS and CRT were low; 75% of learners were at or below the  median SNS score, 67% at or below the median CRT score.  Similarly, 50% of the interviewees scored in the lowest quartile of  the BNT, and 75% in the lowest tertile of the CRT. Indications of  superficial cognition were evident in the number of learners who  did not read the instructions for how to access or interpret the  visualizations. This behavior was so common that LA designers  must plan for it; options include the employment of pop up  messages or locking the visualizations until the explanatory text is  reviewed. The high number of interviewees who made incorrect  assumptions about the information depicted might also indicate  low graphicacy. Though it has been theorized that 80% of the  population are graphically literate, these results are in line with an  international review of adult graphicacy that places Canadian  graphicacy at 50%.1   Interview questions focused on JOL made with the LA,  particularly if, when, and how these judgements resulted in  behavioural changes. The LA were utilized most often by  interviewees motivated by competition. Though both  visualizations support social regulation of learning, the top  contributor visualization resulted in more messages being posted  than the quality visualization. Results were mixed when  interviewees were asked if they would use the visualizations to  decide whom to interact with. Use of the LA as a memory aid was  even less common, though one interviewee requested LA  visualizations indicating change over time to track her progress.   Our initial coding scheme had to be adjusted to reflect affect,  which was mentioned by all participants as being influential to  their behaviors. Specifically stated was the need for elucidations  of trust within both the visualizations and their underlying                                                                        1 Programme for the International Assessment of Adult   Competencies (PIAAC) found at http://www23.statcan.gc.ca- /imdb/p2SV.plFunction=getSurvey&SDDS=4406     algorithms. This presents a challenge since 7 of the 12  interviewees had difficulty understanding the visualizations, and 8  of the 12 misunderstood some aspect of the collected data. This  was due at least in part to not reading the instructions or tooltips.  Some participants viewed the LA, but gave them little credence  when they differed from what was expected. Instead, these  learners preferred to persist in their own biased opinions of their  performance, potentially exhibiting the effects of anchoring bias.    The sizeable impact of affect on JOL and subsequent behaviour  was echoed later in the interviews, when participants rated the  visualization prototypes. The majority of participants, 11 of 12,  chose the visualizations that were either visually stimulating or  made them feel good. Tellingly, the person who rated the  visualization with the least affective impact the highest said, I  dont really like anything that has an emotional connection,  because I dont want to be judging how I feel about myself based  on what an algorithm is saying my [discussion activity] posts  are.   5. FUTURE DIRECTIONS   Even when based in pedagogy, sometimes the best designerly  intentions are misconstrued. To support learners accurate JOL,  we must better understand the internal and external conditions that  contribute to the learning environment. To this end this study is  currently being conducted with online learners, recognizing that  online learners may be more reliant on LA than blended learners  to navigate course discussions.   6. ACKNOWLEDGMENTS   Support was provided by a SSHRC Doctoral Research Grant.   7. REFERENCES  [1] Cokely, E. T., Galesic, M., Schulz, E., & Ghazal, S. (2012).   Measuring risk literacy: The Berlin numeracy test. Judgment  and Decision Making, 7, 2547.   [2] Elliot, A. J., & McGregor, H. A. (2001). A 2  2  achievement goal framework. J. of Personality and Social  Psychology, 80(3), 501519.    [3] Fagerlin, A., Zikmund-Fisher, B. J., Ubel, P. A., Jankovic,  A., Derry, H. A., & Smith, D. M. (2007). Measuring  Numeracy without a Math Test: Development of the  Subjective Numeracy Scale. Medical Decision Making,  27(5), 672680.    [4] Frederick, S. (2005). Cognitive Reflection and Decision  Making. J. of Economic Perspectives, 19(4), 2542.    [5] Okan, Y., Garcia-Retamero, R., Galesic, M., & Cokely, E. T.  (2012). When Higher Bars Are Not Larger Quantities: On  Individual Differences in the Use of Spatial Information in  Graph Comprehension. Spatial Cognition & Computation,  12(2-3), 195218.    [6] Peters, E. (2012). Beyond comprehension the role of  numeracy in judgments and decisions. Current Directions in  Psychological Science, 21(1), 3135.    [7] Stone, E. R., Gabard, A. R., Groves, A. E., & Lipkus, I. M.  (2015). Effects of Numerical Versus Foreground-Only Icon  Displays on Understanding of Risk Magnitudes. J. of Health  Communication, 20(10), 12301241.    [8] Toplak, M. E., West, R. F., & Stanovich, K. E. (2011). The  Cognitive Reflection Test as a predictor of performance on  heuristics-and-biases tasks. Memory & Cognition, 39(7),  12751289.      "}
{"index":{"_id":"118"}}
{"datatype":"inproceedings","key":"Molenaar:2017:ELA:3027385.3029488","author":"Molenaar, Inge and Knoop-van Campen, Carolien A. N. and Hasselman, Fred","title":"The Effects of a Learning Analytics Empowered Technology on Students' Arithmetic Skill Development","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"614--615","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029488","doi":"10.1145/3027385.3029488","acmid":"3029488","publisher":"ACM","address":"New York, NY, USA","keywords":"ability levels, arithmetic's, educational technologies, primary education","Abstract":"Learning analytics empowered educational technologies (LA-ET) in primary classrooms allow for blended learning scenarios with teacher-lead instructions, class-paced and individually-paced practice. This quasi-experimental study investigates the effects of a LA-ET on the development of students' arithmetic skills over one schoolyear. Children learning in a traditional paper  pencil condition were compared to learners using a LA-ET on tablet computers in grade 4. The educational technology combined teacher dashboards (extracted analytics) and class and individually paced assignments (embedded analytics). The results indicated that children in the LA-ET condition made significantly more progress on arithmetic skills in one schoolyear compared to children in the paper  pencil condition.","pdf":"The Effects of a Learning Analytics Empowered  Technology on Students Arithmetic skill Development  Inge Molenaar  Radboud University  Montessorilaan 3   Nijmegen  +31 24-3611942   i.molenaar@pwo.ru.nl     Carolien A. N. Knoop-van Campen  Radboud University  Montessorilaan 3   Nijmegen  +31 24-3615511   c.knoop-vancampen@pwo.ru.nl   Fred Hasselman  Radboud University  Montessorilaan 3   Nijmegen  +31 24-3611942   f.hasselman@pwo.ru.nl   ABSTRACT  Learning analytics empowered educational technologies (LA-ET)  in primary classrooms allow for blended learning scenarios with  teacher-lead instructions, class-paced and individually-paced  practice. This quasi-experimental study investigates the effects of a  LA-ET on the development of students arithmetic skills over one  schoolyear. Children learning in a traditional paper & pencil  condition were compared to learners using a LA-ET on tablet  computers in grade 4. The educational technology combined  teacher dashboards (extracted analytics) and class and individually  paced assignments (embedded analytics). The results indicated that  children in the LA-ET condition made significantly more progress  on arithmetic skills in one schoolyear compared to children in the  paper & pencil condition.    Categories and subject descriptors   Applied computing~E-learning   General Terms  Algorithms, Experimentation, Human Factors, Standardization,  Theory, Verification.   Keywords  Educational technologies, primary education, arithmetics, ability  levels   1. INTRODUCTION  Even though the recent influx of tablets in primary education is  based on the vision that educational technology empowered with  learning analytics (LA-ET) will revolutionize education, empirical  results supporting this claim are scarce [1]. Specifically, these  advances are expected to enhance the effectiveness of education [2,  3]. The present study investigates the effects of an LA_ET on the  development of childrens arithmetic skills compared to a  traditional paper & pencil learning environment in grade 4 of  primary education. Most research on effectiveness of educational  technologies entails so-called individually paced technologies: i.e.  these technologies adapt pace and difficulty of assignments to  students skills. Computer Assisted Instruction (CAI) and  Intelligent Tutoring Systems (ITS) are both examples of   individually paced technologies [3]. A meta-analysis evidenced  that comprehensive blended learning scenarios, in which CAI or  ITS are combined with classroom activities, elicit larger learning  gains, compared to using educational technologies in isolation [3].  Yet, both CAI and ITS are not often used in blended scenarios  combining teacher-lead instructions, class-paced and individually- paced practice. This can be explained by the fact that CAI and ITS  are both individually paced, which entails that students learn and  advance at different speed. For teachers, it is difficult to connect  individually paced practice with classroom-wide activities that are  meaningful for all students. Extracted analytics in the form of  dashboards can support teachers to view class wide as well as  students individual  progress. This can function as a bridge to  combine class-paced activities (instruction and practice) and  individually paced practice.   2. THE PRESENT STUDY  Hence teaching with LA-ET can support blended educational  scenarios in primary education. These scenarios combine teacher- lead instructions, class-paced and individually-paced practice. In  this study, the learning analytics empowered educational  technology provided teachers with dashboards that gave an  overview of class and student progress. Moreover, individual  practice was adjusted to the students current  performance level.  This quasi-experimental study investigates the effect of  using this  LA-ET on the development of students arithmetic skills in grade 4  over one schoolyear. Children learning in a traditional paper &  pencil condition were compared to children using a LA-ET on  tablet computers. We expect that on average, skill development will  be larger for the students who work with LA-ET compared to the  paper & pencil condition. This hypothesis is based on positive  effects of earlier CAI and ITS studies [3] and research indicating  the benefits of applying CAI in blended learning scenarios [3].  Additionally, dashboards (extracted analytics) are expected to  support teachers to connect teacher-lead instructions, class-paced  and individually-paced practice, which can also enhance students  arithmetics skill development.    3. METHOD  3.1 Participants  In this study, 41 schools participated, with 1579 children in grade  4. Informed consent was obtained from the schools before students  were allowed to participate. Students data was stored anonymously.    3.2 Learning analytics empowered technology  The LA-ET used in this study is called Snappet. This technology  is used for both arithmetic and language education across schools  in the Netherlands. The practice activities are comparable between  conditions as the arithmetic assignments in the LA-ET are   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029488     comparable to those used in the paper & pencil workbooks. The  technology operates on tablet computers and features both  embedded and extracted learning analytics. Class and individually  paced assignments are available and children receive immediate  feedback after finishing each assignment. Individually paced  assignments are adjusted automatically to students current  performance level. Teachers have access to 3 different types of  dashboards with which they can monitor class and student progress.   3.3 MEASURMENTS  3.3.1 Arithmetic skill development and ability levels   The Dutch national standardized arithmetic assessment, CITO  Rekenen-Wiskunde 2.0 [Arithmetic-Mathematics] was used to  measure childrens arithmetic. Childrens raw scores on these  assessments were thus converted into national standardized normed  ability scores that showed the degree to which the student masters  arithmetic skills.   3.4 Procedure  The data collection took place over one full schoolyear from June  2015 to June 2016. The standardized test scores (CITO) are from  June 2015 (Time 1) and June 2016 (Time 2).   3.5 Data-analysis  To examine the effect of LA-ET on students arithmetic skill  development over one schoolyear, a random effects model was with  Time (Time 1 / Time 2) as within-subject-factor, Condition (LA- ET / Control) as between-subject-factor, and Subject and School as  random effects.   4. RESULTS  Analysis of arithmetic skills showed a significant effect of  Condition and an interaction effect of Time X Condition.  Disregarding Condition there was a significant increase in scores  over time, t(1427.7) = 34.33, p < .001. The students in both  conditions made significant progress during the year. Disregarding  Time, no significant difference was found between the LA-ET and  the Control condition, t(39.3) = -1.83, p = 0.13. Students in both  conditions scored equally high on arithmetic skills. The interaction  effect between Time and Condition was significant, t(1427.6) = - 8.15, p < 0.001. Students in the LA-ET condition made more  progress on arithmetic skills than students in the control condition  (see Table 1 and Figure 1).     Table 1. Arithmetic Skills per Condition    Time 1 Time 2 Progress    N M SD N M SD    LA-ET 742 80.39 14.68 778 94.64 12.54 14.25   Control 730 81.66 13.41 727 92.20 11.76 10.54                              5. DISCUSSION  In this study, we examined the effect of an LA-ED with both  embedded and extracted analytics on the development of arithmetic  skills over one schoolyear. The results indicated that children in the  LA-ET condition made significantly more progress on arithmetic  skills in one schoolyear compared to children in the paper & pencil  condition. This supports the notion that educational technologies  that translate data into actionable information for teachers indeed  support student learning. Moreover, in this study we learned that  educational technologies combining extracted and embedded  learning analytics did indeed support blended education scenarios  including teacher-lead instructions, class-paced and individually- paced practice. However, many variations in blended scenarios  were noticed, which stresses the need for advanced efforts to  balance embedded analytics and extracted analytics in such a way  that system strengths and human strengths together optimally  support learning in learning analytics empowered technologies in  primary education.   6. ACKNOWLEDGMENTS  We give thanks to Kennisnet for financial support for this study.   7. REFERENCES  [1] A. Papamitsou, Z. Economides, A. Anastasios. 2014.   Learning Analytics and Educational Data Mining in  Practice: A Systematic Literature Review of Empirical  Evidence. Journal of Educational Technology & Society,  17(4), 49-64.   [2] D. Tempelaar, B. Rienties, B. Giesbers. 2015. In search  for the most informative data for feedback generation:  Learning Analytics in a data-rich context. Computers in  Human Behavior, 47, 157-167.   [3] K. van Lehn. 2006. The behavior of tutoring systems.  International Journal of Artificial Intelligence in  Education. 16 (3), 227-265.   [4] G. Siemens. 2013. Learning analytics: The emergence of  a discipline. American Behavioral Scientist, 57, 1380- 1400.     Figure 1 Arithmetic Skills per Condition over Time     "}
{"index":{"_id":"119"}}
{"datatype":"inproceedings","key":"Hu:2017:ODM:3027385.3029483","author":"Hu, Xiao and Hou, Xiangyu and Lei, Chi-Un and Yang, Chengrui and Ng, Jeremy","title":"An Outcome-based Dashboard for Moodle and Open edX","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"604--605","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029483","doi":"10.1145/3027385.3029483","acmid":"3029483","publisher":"ACM","address":"New York, NY, USA","keywords":"Open edX, dashboard, moodle, outcome-based learning","Abstract":"This poster presents a cross-platform learning analytics dashboard on Moodle and Open edX for monitoring outcome-based learning progress. The dashboard visualizes students' interactions with the platforms in near real-time, aiming to help teachers and students monitor students' learning progress. The dashboard has been used in four large-size general education courses in a comprehensive university in Hong Kong, undergoing evaluation and improvement.","pdf":"An Outcome-based Dashboard for Moodle and Open EdX   Xiao Hu1, Xiangyu Hou2, Chi-Un Lei2, Chengrui Yang3 Jeremy Ng1  1 Faculty of Education, 2 Technology-Enriched Learning Initiative, 3 Deparment of Computer Science   The University of Hong, Pokfulam, Hong Kong   {xiaoxhu, hxiangyu, culei, aaron102, jeremyng}@hku.hk   ABSTRACT  This poster presents a cross-platform learning analytics dashboard  on Moodle and Open edX for monitoring outcome-based learning   progress. The dashboard visualizes students interactions with the  platforms in near real-time, aiming to help teachers and students  monitor students learning progress. The dashboard has been used  in four large-size general education courses in a comprehensive  university in Hong Kong, undergoing evaluation and improvement.       CCS Concepts   Human-centered computing  Visual analytics   Applied   computing  Learning management systems    Keywords  Moodle; Open edX, outcome-based learning; dashboard   1. INTRODUCTION  Learning Management Systems (LMS) such as Moodle record  learners interactions with various course components, providing  opportunities to monitor students progress in near real-time. In  response to the need of outcome-based learning [1], a Learning   Analytics (LA) dashboard was developed to visualize students  learning progress towards course learning outcomes. Based on  online behavior logs recorded in the LMS, the dashboard may help  teachers identify at-risk students and decide on possible  interventions [2][3]. It also can provide near real-time feedback to  students, facilitating self-monitoring and self-assessment during  the entire period of the course [4]. This poster demonstrates the  dashboard implemented in Moodle and Open edX, particularly the   system architecture, analytic functions, visualizations, and  preliminary evaluations with teachers and students.   2. SYSTEM ARCHITECTURE  The dashboard on Moodle adopts the Model-View-Controller  (MVC) paradigm. The view is responsible for all visualizations  while the controller requests data from the server using AJAX and  feeds the data to the view. The server side is mainly responsible for  statistics calculation based on user activities recorded in Moodle.   The calculated results are stored in a database for efficient retrieval.  On edX, XBlock is the fundamental component to build online  sources. Our dashboard, XAct, is implemented as one of the  customized XBlocks. Similar to the Moodle dashboard, XAct  server can fetch data from the edX database and transfer analyzed  results to the client through AJAX. Through visualizations  generated with the D3 library, XAct supports students to check  his/her own online learning behaviours in near real-time.   3. ANALYTICAL FUNCTIONS  On both platforms, teachers first link course components (e.g.,  video, quiz) to course outcomes through a HTML block page   (Moodle) or XBlock edit mode (Open edX). On Moodle, student  actions including view, submit, create, start, review, and update  performed on linked components are counted. On Open edX, click  streams such as video interactions, page reading and quiz  answering recorded in the system are calculated for visualizations.  A students learning progress on a course component is calculated  in a relative manner, against a statistic upper limit of student  activities on this component. To avoid effects of outliers, the upper   limit is calculated as 3rd quartile + 1.5* interquartile range of  activity counts of all students. Students with an activity count  exceeding the upper limit will be given a full score (i.e., 100%).   The progress towards a learning outcome is the aggregation of  progress scores of all components linked to that outcome. To  prevent students from gaming the system, restrictions on  frequencies of actions performed on various types of components  (e.g., forums) are added.    4. VISUALIZATIONS   4.1 Progress Towards Learning Outcomes  On Moodle, the dashboard is shown as a Block on the course  homepage where a students progress towards each course learning  outcome is shown using a traffic-light metaphor: green for good  progress; yellow for fair and red for poor (Figure 1). This block also  contains a link to a detailed personal report page. The same  colour codes are applicable to all pages.      Figure 1. Students view of the Moodle block on course page   The personal report page lists all learning outcomes of the course.  For each outcome the students progress and the class average are  both shown for easy comparison (Figure 2).       Figure 2. Student activeness on each outcome (Moodle)    Permission to make digital or hard copies of part or all of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for third-  party components of this work must be honored. For all other uses, contact   the Owner/Author. Copyright is held by the owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029483    http://dx.doi.org/10.1145/3027385.3029483   Similarly, the XAct presents a vertical-bar chart to show each  students progress towards each learning outcome (Figure 3).     Figure 3. Student progress on each outcome (Open edX)    4.2 Progress on Course Components/Topics  On Moodle, clicking a learning outcome on a student report (Figure  2) leads to the students progress in each course component linked   to that outcome (Figure 4). Similarly, on Open edX, a chart  represents how much content on each topic a student has ever  interacted with (Figure 5). Instructors can configure which course  components are relevant to which topics. As Open edX is a video- centric learning environment, most topics include video and  quizzes. It is thus deemed that progress on topics would be more  informative. In addition, another component-based visualization is  implemented in the XAct dashboard (Figure 6), showing how a  student consumed different types of learning contents.      Figure 4. Student progress on course components (Moodle)     Figure 5. Student progress on course topics (Open edX)      Figure 6. Activities on main course components (Open edX)     4.3 Teachers Views  On Moodle, teachers also have access to visualizations for the   overall class progress. These visualizations show the class average  progress towards course learning outcomes, in forms like Figures 2  and 4. Instructors can also click into a list of individual students  progresses either on an outcome or a component (Figure 7) where  the student name is linked to that students individual report  (Figures 2 and 4). In this way, instructors can observe progresses  of the class as a whole and those of individual students. Particularly,  students with a below-average progress can be easily identified and  pedagogical interventions can be applied to them.      Figure 7. All-student report of progress (Moodle)   5. EVALUATION  The Moodle dashboard has been used in three general education  courses, in the areas of Political Science, Social Work, and  Education, each with about 120 students. A 4-minute instructional  video was played to the students in-class, teaching them how to use  the dashboard. To measure the effectiveness of the dashboard,  students were surveyed in the first and last lectures, and some are   invited to participate in follow-up interviews. The Open edX  version has been deployed in a technological general education  course towards the end of the course. 89 of the 120 students have  accessed the dashboard 711 times, within 8 days, and checked the  dashboard for 1.73 minute on average. Some students reported to  have used the dashboard to identify missed contents in the course.  Views of some peripheral (e.g. tutorial) videos have increased  significantly after introducing the dashboard. These indicate that   students have used this dashboard for self-checking of learning task  completions, and the dashboard has served its purposes. A user  survey has been conducted and data are currently under analysis.   ACKNOWLEDGMENTS   This work is partially supported by a Teaching Development Grant  from the University of Hong Kong.   6. REFERENCES  [1] Teaching and Learning. 2016. In The University of Hong   Kong: Teaching and Learning. Retrieved December 2, 2016,  from http://tl.hku.hk/tl/   [2] Ferguson, R. 2012. Learning analytics: drivers, developments  and challenges. International Journal of Technology  Enhanced Learning. 4, 5-6, 304-317.   [3] Siemens, G. 2010. What are Learning Analytics. Retrieved  November 29, 2016, from  http://www.elearnspace.org/blog/2010/08/25/what-are- learning-analytics/   [4] Elias, T. E. 2011. Learning Analytics: The Definitions, the  Processes, and the Potential. Retrieved December 1, 2016,  from  http://learninganalytics.net/LearningAnalyticsDefinitionsPro cessesPotential.pdf   http://tl.hku.hk/tl/ http://www.elearnspace.org/blog/2010/08/25/what-are-learning-analytics/ http://www.elearnspace.org/blog/2010/08/25/what-are-learning-analytics/ http://learninganalytics.net/LearningAnalyticsDefinitionsProcessesPotential.pdf http://learninganalytics.net/LearningAnalyticsDefinitionsProcessesPotential.pdf   "}
{"index":{"_id":"120"}}
{"datatype":"inproceedings","key":"Hu:2017:NFW:3027385.3029489","author":"Hu, Xiao and Yang, Chengrui and Qiao, Chen and Lu, Xiaoyu and Chu, Sam K. W.","title":"New Features in Wikiglass, a Learning Analytic Tool for Visualizing Collaborative Work on Wikis","booktitle":"Proceedings of the Seventh International Learning Analytics  Knowledge Conference","series":"LAK '17","year":"2017","isbn":"978-1-4503-4870-6","location":"Vancouver, British Columbia, Canada","pages":"616--617","numpages":"2","url":"http://doi.acm.org/10.1145/3027385.3029489","doi":"10.1145/3027385.3029489","acmid":"3029489","publisher":"ACM","address":"New York, NY, USA","keywords":"revision network, thinking orders, visualization, wiki","Abstract":"Wikiglass is a learning analytic tool for visualizing collaborative work on Wikis built by groups of secondary or primary school students. This poster presents new features of Wikiglass developed recently based on requests from teachers, including flexible selection of date range, revision network, and thinking order detection. Currently the new features are used and evaluated in two secondary schools in Hong Kong.","pdf":"New Features in Wikiglass, A Learning Analytic Tool for   Visualizing Collaborative Work on Wikis  Xiao Hu1 Chengrui Yang2    Chen Qiao1        Xiaoyu Lu2        Sam K. W. Chu1   1. Faculty of Education  2Department of Computer Science   The University of Hong Kong   {xiaoxhu, aaron102, cqiao, lxyu0405, samchu}@hku.hk   ABSTRACT  Wikiglass is a learning analytic tool for visualizing collaborative   work on Wikis built by groups of secondary or primary school   students. This poster presents new features of Wikiglass   developed recently based on requests from teachers, including   flexible selection of date range, revision network, and thinking   order detection. Currently the new features are used and evaluated   in two secondary schools in Hong Kong.       CCS Concepts   Human-centered computing  Visual analytics    Applied   computing  Collaborative learning   Keywords  Wiki; visualization; revision network; thinking orders   1. INTRODUCTION  Wikiglass is a Learning Analytics (LA) tool for supporting   secondary and primary school teachers and students to monitor   student group collaborations on Wikis [1]. By retrieving page   content and revision history from Wiki platforms, Wikiglass   visualizes statistics of student contributions. It is one of the first   LA tools supporting Chinese text processing and customized for   secondary and primary schools. Wikiglass is now used in two   secondary schools in Hong Kong where students collaborate in   groups on inquiry-based projects over a period of three to five   months. There are two visualization modes in Wikiglass: statistics   and timeline. The former allows teachers and students to compare   statistics of student groups in one class or individual students in   one group (Figure 1), such as revision counts and word counts.   The timeline mode displays statistics accumulated by dates on a   weekly basis, allowing teachers and students to monitor progress   of groups or individual students over time [1]. During classroom   implementation of Wikiglass, teachers and students opinions and   suggestions have been solicited through individual or focus group   interviews, based on which new features have been developed,   including flexible selection of date range, revision network, and   thinking order detection from sentences.   2. FLEXIBLE DATE SELECTION   By default, statistics shown in Wikiglass are accumulated from the   start of the project period to the current time. During classroom   implementation, there came a request for the flexibility of   selecting a date range to examine student contributions, for   example, during the winter holidays. A calendar was then added   into the interface to allow date specification which was taken into   all calculations in the class, group, and individual levels (Figure   1). It is noteworthy that the selection of date ranges is propagated   to all pages in the statistics mode until a new date range or the   option All Time is selected.      Figure 1. Data selection enabled in all pages.   3. REVISION NETWORK   The group page in statistics mode reveals how much each student   in a group contributed to the project (the pie chart in Figure 1).   However, it cannot tell the interactions among the students, or   whether they have built upon each others work. To fill this gap, a   network is visualized based on the page revision histories in a   Wiki. Figure 2 shows an example network where each node   represents a student. The edges are directed, starting from the   student who revised a page and ending at the student whose   writing was modified. The thickness of an edge is proportional to   the times of revisions one student did to another students   writings. Therefore, this network helps not only in illustrating the   collaborative relationships among students, but also in identifying   active and inactive students.      Figure 2. Revision network of one group      Permission to make digital or hard copies of part or all of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights   for third-party components of this work must be honored. For all other   uses, contact the Owner/Author.   Copyright is held by the owner/author(s).   LAK17, March 13-17, 2017, Vancouver, BC, Canada.   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029489           4. DETECTION OF THINKING ORDERS  Besides statistics on quantity of students contributions, the   teachers and students also raised a need for quality indicators of   students contributions. To meet the need, a new feature was   added to automatically detect the levels of thinking order (i.e.,   higher or lower levels) from student writings on Wiki.    4.1 Framework of thinking orders  Blooms taxonomy (BT) categorizes reasoning thinking into six   levels: knowledge, comprehension, application, analysis,   synthesis, and evaluation [2]. BT has been used in assessing   cognitive thinking levels in student writings [3] [4]. In this study,   we used the adapted BT in [3] to analyze student writings in   Wikis, as shown in Table 1.    Table 1. Levels of thinking order adapted from [3].   Thinking   Order   Bloom Taxonomy Possible purposes of   writing   High Evaluation   Synthesis   Reasoning,   Argumentation,   Evaluating, etc.   Medium Analysis   Application   Data interpretation,   Comparison, etc.   Low Comprehension   Knowledge   Definition,   Information statement,   Action description, etc.   4.2 Automated text categorization  As student writings are of a significant amount and generated in a   continuous manner, automated detection of thinking order from   writings is necessary. For this purpose, a text categorization   model was trained and evaluated on an annotated corpus which   was built from writings of previous students in one of the schools   using Wikiglass. It maximizes the generalizability of the model to   new writings of students in later cohorts, by using previous   students writings on the same subject and in the same school.    To build the categorization model, a range of text features were   extracted from each sentence, including lexical, syntactic, and   semantic features [5]. Multiple text categorization algorithms   were experimented and compared. The results showed that   Support Vector Machines (SVM) plus the randomized logistic   regression (RLR) feature election method achieved the best   performance (accuracy = 0.81; kappa = 0.58) which was then   integrated into Wikglass.   4.3 Visualizing thinking orders  Student writings on Wikis were first downloaded from Wiki   platforms. Once a new version of a page was recorded, the new   content on top of the last version was detected. Each sentence in   the new content then underwent the feature extraction and text   categorization process. The predicted levels of thinking orders   (i.e., high, medium, or low) were then saved into the database and   aggregated for visualizations. Statistics are shown in similar ways   as in Figure 1.    To enable closer examinations on the content of the sentences,   Wikiglass also allows teachers and students to view the actual   sentences and corresponding levels of thinking orders detected by   Wikiglass. In addition, teachers also have the authority to modify   the level of thinking orders if they think a predicted level is   inaccurate (Figure 3). The changes will not only be immediately   reflected on the screen, but also be taken as teachers feedback   and recorded by the system. The feedback will be used for   incrementally training the text categorization model, to further   improve the performance of categorization models over time [6].        Figure 3. Visualization of sentences and thinking orders   5. ONGOING WORK  Wikiglass is currently being used by two secondary schools in   Hong Kong, with a total of 602 students and 11 teachers. The   effectiveness of the tool and especially the new functions will be   evaluated through user survey, interviews as well as system log   analysis. Meanwhile, more schools are interested in adopting   Wikiglass in their classrooms. We are planning to evaluate the   efficacy of Wikiglass and LA in general with younger students in   primary schools.       ACKNOWLEDGMENTS  The work was partially supported by an Early Career Scheme   grant from the Research Grants Council of the Hong Kong Special   Administrative Region, China. (Project No. HKU 27401114).   6. REFERENCES  [1] Hu, X., Ip, J., Sadaful, K., Lui, G. & Chu, S. 2016.   Wikiglass: A Learning Analytic Tool for Visualizing   Collaborative Wikis of Secondary School Students, In   Proceedings of the 6th International Conference on   Learning Analytics and Knowledge (LAK '16). ACM, 550-  551. DOI: http://dx.doi.org/10.1145/2883851.2883966   [2] Bloom, B. S., Engelhart, M. D., Furst, E. J., Hill, W. H., &   Krathwohl, D. R. 1956. Taxonomy of Educational   Objectives: The Classification of Educational Goals.   Handbook 1: Cognitive Domain. London, WI: Longmans,   Green & Co. Ltd.   [3] Plack, M. M., Driscoll, M., Marquez, M., Cuppernull, L.,   Maring, J., & Greenberg, L. 2007. Assessing reflective   writing on a pediatric clerkship by using a modified Blooms   taxonomy. Ambulatory Pediatrics, 7(4), 285-291.   [4] Brierton, S., Wilson, E., Kistler, M., Flowers, J., & Jones, D.   2016. A Comparison of Higher Order Thinking Skills   Demonstrated in Synchronous and Asynchronous Online   College Discussion Posts. NACTA Journal, 60(1), 14-21.   [5] Che, W., Li, Z., & Liu, T. (2010). LTP: A Chinese language   technology platform. In Proceedings of the 23rd   International Conference on Computational Linguistics: pp.   13-16. ACL.   [6] Mitchell, T. (1997). Machine Learning, McGraw Hill,   Columbus, USA.     "}
