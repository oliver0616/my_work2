Revisiting Formative Evaluation: Dynamic Monitoring for  the Improvement of Learning Activity Design and Delivery   Griff Richards         Irwin DeVries  Thompson Rivers University   Open Learning  Kamloops, Canada   1.250.852.6866  griff@sfu.ca   idevries@tru.ca   ABSTRACT Distance education courses have a tradition of a formative  evaluation cycle that takes place before a course is formally  delivered. This paper discusses opportunities for improving online  and blended learning by collecting formative data during course  presentation. With a goal of overall improvement in instructional  effectiveness and identification of promising practices for  inclusion in a learning activities design library, we propose the  immediate and on-going monitoring of the effectiveness of  learning activities, tutor facilitation and learner satisfaction during  the course presentation. This has implications for constructively  involving the learners and facilitators in the course improvement  process. While originally conceived to reduce the time for pilot  evaluation of new courses and learning activities, the proposed  system could also be extended to individualized and blended  learning environments, and if implemented using semantic web  technologies, for research into the effectiveness of learning  activity patterns.   Categories and Subject Descriptors  H.5.m [Information systems]: Information Interfaces  and  Presentation. Miscellaneous.  General Terms  Algorithms, Management, Measurement, Documentation, Design,  Economics.   Keywords  Formative evaluation, Learning activities, Cohort-paced, Online  learning, e-learning Instructional Design.   1. INTRODUCTION Distance education has a long tradition of conducting formative  evaluation of instructional materials and learning activities before  the ongoing delivery of a course. The feedback from pilot testing   and expert evaluation enables course designers to catch and  correct any weaknesses detected. The lessons learned can be  incorporated into the professional design heuristics of the course  designers enabling promising design practices to be reused in new  courses, and disappointing practices to be redesigned or rejected.  In recent years there has been an influx of traditional face to face  delivery institutions to the online environment. Sometimes,  indeed often, they expect the instructor of a face-to-face course to   convert their course (or certain activities of their blended  courses) to on-line delivery with a minimum of instructional  design support, and there is little provision for observing which  learning activities work and which require improvement.  Typically course evaluation takes place at the end of the course,  after the final marks have been submitted, but before learners  receive grades. This delayed process does not capture immediate  responses and reflections in time to provide meaningful formative  evaluation that might enhance the learning experience of a course  in session. Thus for both blended and distance courses a case can  be made for a system for improving formative evaluation.   This paper looks at the potential for embedding formative  evaluation tools in both online and blended course delivery. Our  goal is to improve the quality of online and blended learning  experiences, along with facilitation and instructional design  practice, by stimulating ongoing reflective practices among  course designers and course facilitators such as professors,  instructors, mentors or tutors. We recognize that there may be  pitfalls to openly soliciting feedback from learners during a  course, and there may be governance and collective agreement  issues arising from providing feedback on the effectiveness of  learning activity facilitation. The proposals contained here are  work in progress, and an opportunity to open dialogue and critical  reflection on this topic. We are fully aware that every on-line  cohort establishes informal back channels where the learners  actively blog their opinions  possibly the only ones not in the  conversation on instructional effectiveness are the faculty  presenting the course.    2. FORMATIVE EVALUTION  Scrivens [1] coined the terms formative and summative  evaluation to distinguish between evaluation of educational  materials during their development and at the end of the  instructional cycle. Formative evaluation is intrinsic to  instructional systems design models [2][3] and it has been  ingrained into the development cycle of most distance learning  organizations that produce instructional media or course  packages. During the big media phase [4] when distance      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February  27March 1, 2011, Banff, Alberta, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.      157    education was dominated by centralized production facilities  turning out television shows and print packages, formative  evaluation was a key part of ensuring quality before printing  hundreds of copies for the warehouse or broadcasting on  television. Distance learning was in a sense asserting its rightful  place, and the best way to counter criticism of traditional  universities was to demonstrate the quality of the courses was as  good as if not better than the traditional offerings. Indeed, what  most distance learning courses lost in presence they more than  made up for in a systematic approach to development, the  alignment of course materials to instructional objectives, and the  thoroughness of content delivery and student assessment [5].   Formative evaluation was also an essential part of multimedia  development [6] and carried into web site development [7]. It was  evident in early on-line course development, again in response to  concerns about the quality of courses that simply shoveled content  onto the web [8][9] Various formative evaluation approaches  were suggested by Reigeluth and Frick [10] with the intent of  improving instructional design theories that then would translate  into improved theories and instructional design. However, as  online delivery became mainstream and blended with classroom  instruction, formative evaluation seemed to lose its earlier  attention in the literature. Perhaps course designs and instructional  activities became somewhat standardized, but probably the real  reason was that increasingly in the 2000s, web delivery had  become accepted as a credible indeed essential extension of the  academy. With the volume of courses to be transferred to the web  there were insufficient instructional design resources to conduct  formative evaluations. This period of adjustment was  characterized by the downsizing of resources for centralized  distance learning departments as faculties set up their own  distance programs, the growth of learning management systems  making it easier for individual instructors to load content online,  the rise of the dual mode university and in Canada the reduction  in the number of single mode distance universities [11] [12].  Traditionally neither formative nor summative evaluation has  seen a comfortable fit in the face-to-face classroom [13]. Courses  were taught by faculty who were expected to get the bugs out" in  two or three terms. As this same expectation is creeping into the  practice of online education, formative evaluation in online  learning has not seen a high profile in practice over the past  decade. Yet formative evaluation can strengthen both the  implementation of a program and the knowledge gained within it  [14].   3. CHALLENGES IN ONLINE      EDUCATIONAL PACTICE  In addition to changes in instructional development models for  online courses, the past twenty years of educational practice have  seen a change from objectivist philosophies and paradigms to  increasingly constructivist views [15] [16]. The internet is  increasingly seen less as a medium of delivery and more as a  medium of communication in which interactions can take place  among learners and instructors, and learners and the content [17].  Traditional models of distance education offered individual  delivery of content-based instructional materials. Alongside the  growth of channels for interaction, cohort paced courses have  been implemented. These require learners to interact in many  ways and to create new knowledge together. The resulting learner  engagement can promote both achievement and retention.   The need for formative evaluation of learning activities in online,  paced cohort courses is important in view of this shifting role of  the learner. Learners are active participants with rich and complex  experiences. The learners engagement in the collaborative  activities places them in the position of co-creators of knowledge  within the learning environment, as well as self-organizers of  their learning [18]. As described by Parrish, While IDs  [instructional designers] work to tame instruction into a  manageable, replicable process that begins by predetermining  outcomes to be measured through properly aligned assessments,  engagement describes that wild aspect of the process in which the  learner is as much or more in control of the activities as the  ID[19]. The situatedness of the learners and the contexts in  which they find themselves become meaningful realities in the  learning environment [20]. Development of community, shared  practices and reflection are important parts of learning activities.  The application of cooperative learning techniques to the design  of learning activities for cohort-paced e-learning can produce  engaging discussion, reflection and deeper processing of the  content. With instructor-facilitated cohort/collaborative  approaches providing such positive results, distance learning  course providers are abandoning investments in comprehensively  detailed content packages and elaborate instructional designs.  Institutions notice that these changes make a difference, and  cohort-paced distance learning courses have lower drop-out rates  than their self-paced counterparts, about 85% retention versus  65% for individualized delivery [21][22]. In a review of literature  Means, Toyama, Murphy et al. [23] noted significant effect sizes  for facilitated and collaborative online learning when compared  with individualized delivery for the same content, although they  were careful not to attribute this as a media effect noting that the  cohort modes often involve different activities and increased time  on task. At the same time, it is clear from the research that the  many of the types of activities included may be of little value. For  instance, they also noted that the provision of extra video clips  and chapter quizzes contributed little to student achievement,  while activities that provoke reflection and engage the learners  metacognitive processes can yield improvements in learning.  Richards [24] observed that trivial learning activities such as  knowledge level multiple-choice quizzes, or forum directions to  post your thoughts and reply to the thoughts of two other  learners led to a superficial understanding of the course content.  It is therefore important to continue formative evaluation in these  dynamic new learning environments, in order to determine which  activities are both valued and valuable and those that are little  more than "make work" projects. Feedback from learners in these  environments is necessary in gaining a better understanding of  these activities.  In this paper we strongly advocate for careful design of such  activities for cohort-paced e-learning, and suggest that if  formative evaluation is no longer conducted before the delivery of  a course, then it should be embedded into the course delivery.  This should be simple to implement. Finally, since the purpose of  formative evaluation is to inform practice and improve delivery,  the process should promote reflection on the part of learners,  instructors and designers as all have a role to play within the  learning experience, and all might benefit from an open  discussion on improving the learning environment.   158    4. OTHER BENEFITS OF FORMATIVE EVALUATION  Eijkman raises a series of questions we as educators need to  consider in our use of web-based learning and social  environments. For instance, what practices, habits, and patterns  of use emerge" and What changes need to occur in institutional  policies and technological practices in order to integrate the social  Web effectively into the educational mainstream [25].  Documentation or other forms of visualization of learning  activities and designs can help to capture emerging innovative and  expert practices [26] [27], and to gain a deeper understanding of  the user experience.    While not a primary focus of this paper, research and  development around reusability or adaptation of learning  activities and designs along with educational policy and practice  can also benefit from formative evaluation. If formative  evaluation of activities leads to their improvement over time due  to the use of this feedback in updating and maintaining courses,  these activities can be added to design libraries for re-use and  sharing. Further, analysis of the broader emerging patterns may be  incorporated into strategic and operational planning. The goal in  the end of improvement of learning activities and designs is  improved quality of instruction [28].     5. A SIMPLE MICRO MODEL OF AN  ONLINE LEARNING ACTIVITY   Fig. 1 diagrams three nested levels of the design and delivery of  an online learning activity. Level 1 is the Instructional Design  Level  the level at which instructional goals are aligned with  learning activities that are appropriate for the learners. Level 2 is  the Facilitation Level  and encompasses those roles, activities  and resources that come together during the conduct of a learning  activity. Level 3 represents the Learner Experience. Note that  each level has been allocated three phases of preparation,  enactment, and reflection. It is our belief that this is the simplest  depiction possible for our purposes and we recognize that learning  environments and learning activities may become extremely  crowded with multiple roles, players and resources. We fully  anticipate that other evaluators may want to expand this depiction  to be more explicit or to compact the phases to be more specific.  In some settings the design and facilitation roles may involve the  same individual(s). In some settings the facilitator may also be  consulted in the design process, while in others, the facilitator  may become involved years after the initial design, after a course  has run several times.    Online learning activities evolve to meet the needs of content,  audience and the constraints of the instructional system. The  model looks at a single activity, whereas a course is a strategy  of intentionally sequenced progression through a series of  learning activities. Some activities such as a lecture are well  structured, and others like a reading assignment are loosely- structured. It is also possible that parallel learning activities such  as study groups may be autonomously initiated and conducted by  a learner or group of learners as they form a learning community.   Whether these should be included in the scope of the Dynamic  Evaluation Model is left to the discretion of those conducting the  evaluation. Similarly, there may be others external to the  instructional process having a bearing on the results. While  Garrison and Anderson [29] only identify instructors, peers and   content in their interaction model for online learning the actual  educational environment may include professional faculty  developers, mentors, peers, friends, family and others  anyone  who influences the decisions and performance of any of the key  roles.      Fig. 1. A Conceptual Model for Dynamic Evaluation of  Learning Activities   6. ALIGNING THE MODEL WITH  ISNTRUCTIONAL DESIGN  METHODOLOGY  As discussed earlier, in a cohort-paced constructivist learning  activity not all learner activity is predictable since learners bring  their own experiences and contexts to the learning situation.  While situated in the design and execution of intentional learning  activities, the model also takes into account learners own  experiences of the activity. We use the term learning activity to  avoid confusion with the more technical terms learning design  and lesson plan which are expressions of learning activities.  The term learning activity encompasses any activity that brings  learners into planned contact with content, other learners, and  experiences that promote acquisition of skills, knowledge and  attitudes. This broad definition is congruent with similar  definitions [23]. While traditionally instructional design does not  include accidental nor incidental learning activities, in more open  ended learning environments learners might influence the learning  environment in unpredictable ways, and in their search for  alternate explanations may discover materials useful to others.   To the extent that instructional design is an intentional and  iterative process, we look at preparation (planning and alignment  of goals with activities), the design itself, and reflection on the  outcomes of the design. Preparation is included as part of  facilitation because so much success depends on the facilitators  skills and knowledge of facilitation techniques, their  understanding of the activity and their role in and commitment to  its success. Preparation is also important for learners in terms of  both prerequisite skills and knowledge and in terms of adequate  direction to participate the learning activity. We believe that  reflection is a part of all processes and, in terms of improving the  system, early reflection catches errors before they can become  deeply embedded in the teaching-learning system.      7. PRACTICAL ISSUES  The goal of formative evaluation is to improve the learning  experience. If evaluation of the learning activities is not   159    conducted until the end of the course or beyond, then no  remediation can take place if there is a problem. We propose the  following guidelines:     1. Formative evaluation should take place during or at the  end of each learning activity.   2. Formative evaluation should seek data and reflections  from learners, facilitators and designers.   3. Formative evaluation to seek both quantitative and  qualitative data.   4. The results of the formative evaluation should be open  to all participants.   5. If error correction is required, it should be considered  immediately    6. If activity re-design is required it should be embarked  upon so that it can be revised for the next course  offering.   7. If learning activities are to be evaluated for several  courses, then investment in an evaluation system to  gather and analyze the data should be considered.   8. The results of dynamic formative evaluations may have  value in explaining the findings of end of course  evaluations, and in the evaluation of generic learning  activity designs, including the training of facilitators,  and the directions provided to learners.     For purposes of brevity, we have not described the importance of  linking such a system to descriptive ontologies such as Learning  Object Context Ontology [30]. However, we believe that semantic  tagging will enhance the ability of researchers and designers to  better understand the patterns that may emerge from the data  collected, and raise the importance of both instructional activity  design and evaluation as part of organizational learning.      8. PROPOSED IMPLEMENTATION  Richards [24] embedded questions on the efficacy of cohort  learning activities into the Moodle site for a graduate distance  course in Instructional Design at Athabasca University. For each  activity, the learners were asked eight questions to rate their  experience (along a five point Likert scale) and provided an  opportunity to comment. This rating has been conducted a  number of years and Fig. 2 shows a typical result.   As the evaluation was constrained to a single course, a  questionnaire was used to present the questions and collate the  data. Unfortunately, with Moodle the raw data were not available  so neither is further analysis - even simple statistics such as the  standard distribution or the maximum and minimum values are  unavailable. For a more robust system capable of handling  multiple courses we propose to implement the dynamic evaluation  system external to the learning management system so that we  can have greater control over the data, and the results would be  then returned to the course participants through a web service.  The course instructors and course designers would also have  access to the participant comments. In a course with several class  sections or perhaps teaching assistants, additional questions could  be developed to link into each section and to pass back the  appropriate identifiers to and from the LMS.   If the function of the embedded questionnaires is to improve the  learning activities, then the most important question is what  suggestions the participant offers to improve the learning activity.   For research purposes, it will be useful to ferret out other  additional information for example on the role of the facilitator in  animating a learning activity. While it would be appropriate to  ask the learners if "the facilitator/ instructor/tutor contributed to  the success of this activity", it could only be interpreted in light of  facilitators own reflections about their preparation for the  activity, the amount of time devoted to the activity, and other  such factors.   Similar questions might be asked of the course designers when  they review the results of the activity. It is important to note that  it may be very easy or very difficult to pin down why a cohort  activity works or does not work. For example the questions used  in Fig. 2 take for granted that many preparatory steps had already  gone correctly: learners had the appropriate prerequisites, text  books had arrived, individuals had read the prescribed materials,  there were no untimely interruptions in internet services, and  other such assumptions. These are extrinsic factors. Intrinsic  factors are more within the realm of the course developers and  facilitators  was the activity relevant, was the group size  appropriate (what about the group make up), was the time  appropriate. The outcomes are the feeling of connectedness  (which is a vector on group cohesion), that all members of the  group contributed equally is in part a function of the balance  between individual and group accountability  group projects  generally do not work if there is no positive interdependence [31].  Finally, the achievement worthiness is important: was the activity  worthwhile and did the activity help with learning We can well  imagine learning activities that are well intentioned but involve  superficial treatment of the content and thus provoke little or no  deep learning and have little long-term effect on understanding or  retention.         Fig. 2. Typical results of a learning activity evaluation in  MDDE604   Dynamic formative evaluation seeks to gather data to ascertain  the effectiveness of a learning activity, if required remediate with  the current learners, and make adjustments as required in the  activity before the next class. The adjustments may be with the  content and materials, the directions to the facilitator role, or  directions to learners. However, as noted earlier, a significant  value of dynamic formative evaluation may be in generalizing the  lessons learned and formalizing the expertise so that it can be   160    shared with other course developers. This loftier purpose requires  the design of a data base that is semantically enriched so that  pattern description of the activities and the roles can be  generically described with ontologies such as the Learning Object  Context Ontology (LOCO) and extensions [30].  The semantic  tags will enable pattern analysis across several courses, initially to  allow course developers to locate and view how winning activities  are embedded in existing courses, but also in the long run to  identify and extract patterns into a library of successful practices.  This then brings us close to the ideals of Koper [28] in  documenting successful learning designs that can be reused in a  pragmatic manner. Before closing it is important to note that a  key implementation issue will be acceptance of the system by all  users. In distance delivery student response to end of course  questionnaires administered by administrative and marketing  groups can be as low as 10 per cent, while Richards [24] found  embedding the questions as part of the course brought a 100 per  cent response rate. For dynamic formative evaluation to be  effective it needs to be an active part of the learning experience   the questions should provide feedback to the learners on how their  perceptions and experience compared with that of others, and  there should be an active response to problem areas identified.   Going further, the dynamic evaluation system could also solicit  suggestions to update the learning resources that might be used to  help others learn  moving from a prescriptive to a constructive  learning environment has been a successful strategy in the  corporate learning context of the IntelLEO Project [32]. Similar  benefits should be obvious for facilitator and course designers in  improving the quality and efficiency of the online learning  experience.     9. SUMMARY In summary, the purpose of this paper was to provoke discussion  about the need to revisit formative evaluation of e-learning  activities and course designs. If e-learning and blended models  are the new reality of distance education, then formative  evaluation is more important than ever. Because of the  proliferation of distance education, much of it developed without  the assistance of an instructional design team, and the complexity  of constructivist learning design in cohort-paced courses, in many  cases formative evaluation needs to take place during early course  delivery. A dynamic process for formative evaluation on the  success of learning activities (whether designed or not) is  important in the creation of an informed community of practice.  Currently, because of back channel communications among the  learners, the only ones out of the feedback loop are the instructors  and course designers. A dynamic learning activity evaluation  system will help to close that gap.     10. REFERENCES [1] Scrivens, M. 1967. The Methodology of Evaluation. In:   Tyler, R.W., Gagne, R.M., Scrivens, M. (Eds.) Perspectives  of Curriculum Evaluation. Rand McNally & Co.    [2] Dick, W., Carey, L. 1978. The Systematic Design of  Instruction. Scott, Foresman, Glenview, IL.    [3] Reigeluth, C. 1983. Instructional Design: What Is It and  Why Is It In: Reigeluth, C. (Ed.): Instructional-Design Theories and Models: An Overview of their Current Status,  pp. 4-36. Laurence Erlbaum, Hillsdale, NJ.   [4] Schramm, W. 1962. Mass Communication. Annual Review of  Psychology 13, 251-284.   [5] Moore, M., Kearsley, G. 1996. Distance Education: A  Systems View. Wadsworth Publishing Company, Belmont  CA.   [6] Flagg, B.N. 1990. Formative Evaluation in the Performance  Context. Lawrence Erlbaum, Hillsdale, NJ.   [7] Preece, J., Benyon, D. 1993. Open University: A Guide to  Usability: Human Factors in Computing,  Addison-Wesley  Longman Publishing Co., Inc., Boston, MA.   [8] Harasim, L. 2000. Shift Happens: Online Education as a  New Paradigm in Learning. The Internet and Higher  Education 2, 41-61.   [9] Nobel, D. October 1997. Digital Diploma Mills.  http://www.handshake.ca/noble.html   [10] Reigeluth, C., Frick, T.W. 1999. Formative Research: A  Methodology for Creating and Improving Design Theories.  In: Reigeluth, C. (Ed.) Instructional Design Theories and  Models: A New Paradigm of Instructional Theory, vol. 2, pp.  633-652. Lawrence Erlbaum Associates, Inc., Publishers, NJ.   [11] Abrioux, D. 2001. Trends in Canadian Distance Education:  An Institutional Perspective. ICDE World Conference,  Dsseldorf.   [12] Abrioux, D. 2006. Strategic Issues in Single- and Dual-mode  Distance Education. Consultant's report,  http://www.col.org/resources/publications/consultancies/Pag es/2006-singleDualMode.aspx   [13] Cavanagh, R. 1996. Formative and Summative Evaluation in  the Faculty Peer Review of Teaching. Innovative Higher  Education 20, 235-240.   [14] Brown, J.L., Kiernan, N.E. 2001. Assessing the Subsequent  Effect of a Formative Evaluation on a Program. Evaluation and Program Planning 24, 128-143.   [15] Jonassen, D.H. 1990. Objectivism versus Constructivism: Do  We Need a New Philosophical Paradigm Educational Technology 30, 32-34.   [16] Cooper, P. 1993. Paradigm Shifts in Designed Instruction:  From Behaviorism to Cognitivism to Constructivism.  Educational Technology 33, 12-19.   [17] Garrison, D. R., Anderson, T., & Archer, W. 2000. Critical  inquiry in a text-based environment: Computer conferencing  in higher education. Internet and Higher Education, 2(2-3),  87 - 105.    [18] Rohse, S., Anderson, T. 2006. Design Patterns for Complex  Learning. Journal of Learning Design 1, 82-91.   [19] Parrish, P. 2007. Plotting a Learning experience. In: Botturi,  L, Stubbs, T. (Eds.) Handbook of Visual Languages for  Instructional Design: Theories and Practices. Information  Science Reference  Imprint of IGI Publishing, Hershey, PA,  91-111.   [20] Luckin, R., Cook, J., Clark, W., Day, P., Garnett, F.,  Ecclesfield, N., Whitworth, A., Hamilton, T., Akass, J.,  Robertson, J. 2011. Learner-Generated Contexts: A  Framework to Support the Effective Use of Technology for  Learning. In: Lee, M., McLoughlin, C. (Eds.): Web 2.0- Based E-Learning: Applying Social Informatics for Tertiary   161    Teaching. Information Science Reference (an Imprint of IGI  Global), 70-84.   [21] Anderson, T. 2009. Keynote: Beyond Learning  Management: Open Learning Support and Inspiration.  Canada MoodleMoot, Edmonton.   [22] Jakubec, M., Harrison, M., Enstrom, E. 2010. Evolving  Online: Perspective on Collaborative Learning. Canadian  eLearning Conference.   [23] Means, B., Toyama, Y., Murphy, R., Bakia, M., Jones, K.:  2010. Evaluation of Evidence-Based Practices in Online  Learning: A Meta-Analysis and Review of Online Learning  Studies. Revised September 2010. US Department of  Education, Office of Planning, Evaluation, and Policy  Development, Policy and Program Studies Service.   [24] Richards, G.  Moisey, S. 2009. Embedding collaborative   online learning activities. Moodle Moot. Edmonton.   [25] Eijkman, H. 2011. Dancing with Postmodernity: Web 2.0+  as a New Epistemic Learning Space. In: Lee, M.,  McLoughlin, C. (Eds.): Web 2.0-Based E-Learning:  Applying Social Informatics for Tertiary Teaching.  Information Science Reference (an Imprint of IGI Global),  343-364.    [26] Conole, G. 2008. Using Compendium as a Tool To Support  the Design of Learning Activities. In: Okada, S., Shum, B.,  Sherborne, T. (Eds.), Knowledge Cartography: Software  Tools and Mapping Techniques, London, Springer, 199-222.   [27] Dalziel, J. 2008. Learning Design: Sharing Pedagogical  Knowhow. In: Iiyoshi, T., Kumar, M. S. (Eds.) Opening Up  Education: The Collective Advancement of Education   through Open Technology, Open Content, and Open  Knowledge, pp. 375-388. The MIT Press, Cambridge, MA.   [28] Koper, R., Tattersall, C. 2005. Preface to Learning Design: A  Handbook on Modelling and Delivering Networked  Education and Training. Journal of interactive Media in  Education 18, 1-7.   [29] Anderson, T. 2003. Getting the mix right again: An updated  and  theoretical rationale for interaction.  The International  Review of Research in Open and Distance Learning, 4 (2).   [30] Knight, C., Gaevi, D., Richards, G. 2006. An Ontology- Based Framework for Bridging Learning Design and  Learning Content. Educational Technology & Society 9, 23- 27.   [31] Johnson, D., Johnson, R., Holubec-Johnson, E. 1994. The  New Circle of Learning: Cooperation in the Clasroom and  School: Alexandria, VA: Association for Supervision and  Curriculum Development.   [32] Siadaty, M., Jovanovi, J., Gaevi, D, Jeremi, Z.,  Holocher-Ertl, T. 2010. Leveraging Semantic Technologies  for Harmonization of Individual and Organizational  Learning. In  Martin Wolpers, Paul A. Kirschner, Maren  Scheffel, Stefanie Lindstaedt and Vania Dimitrova (Eds.)  Sustaining TEL: From Innovation to Learning and Practice  5th European Conference on Technology Enhanced  Learning, EC-TEL 2010, Barcelona, Spain, September 28 -  October 1, 2010. Proceedings 340-356.  http://www.springerlink.com/content/978-3-642-16019- 6#section=781059&page= 11& locus=15        162    