STEMscopes: Contextualizing Learning Analytics in a   K-12 Science Curriculum  Carlos Monroy   Center for Technology in Teaching  and Learning, Rice University   6100 Main St., MS 120  Houston, TX 77005  +1 713 348 5481   carlos.monroy@rice.edu   Virginia Snodgrass Rangel   Rice University Center for Digital   Learning and Scholarship  6100 Main St., MS 112   Houston, TX 77005  +1 713 348 5008   vsr@rice.edu   Reid Whitaker  Rice University Center for Digital   Learning and Scholarship  6100 Main St., MS 112   Houston, TX 77005  +1 713 348 3741   reid@rice.edu         ABSTRACT  In this paper, we discuss a scalable approach for integrating  learning analytics into an online K-12 science curriculum. A  description of the curriculum and the underlying pedagogical  framework is followed by a discussion of the challenges to be  tackled as part of this integration. We also include examples of  data visualization based on real student and teacher data. With  more than one million students and fifty thousand teachers using  the curriculum, a massive and rich dataset is continuously  updated. This repository depicts teacher and students usage of an  inquiry-based science program, and offers exciting opportunities  to leverage research to improve both teaching and learning. The  growing dataset, with more than a hundred million items of  activity in six months, also poses technical challenges such as data  storage, complex aggregation and analysis with broader  implications for pedagogy, big data, and learning.     Categories and Subject Descriptors  K.3.0 [Computers and Education]: General; H.4 [Information  Systems Applications]: Miscellaneous.   General Terms  Management, Design, Human Factors.   Keywords  Learning analytics, STEM education, online curriculum, big data.   1. INTRODUCTION  Computers and mobile devices offer new tools to support  learning, both inside and outside the classroom. This in turn opens  opportunities to understand and assess their impact on educators  and learners [42]. Indeed, a major advantage of online curricula  over traditional textbook-based curricula is the ability to capture  data about how teachers and students use these devices and  programs. In this vein, learning analytics is a relatively new, but  rapidly growing [17], discipline whose goal, according to the   Society of Learning Analytics, is understanding and optimizing  learning and the environments in which it occurs. Although most  learning analytics (LA) methods use data generated by the  interactions between learners and the Learning Management  Systems (LMS) they use [39, 57], in order to fully capture the  richness of learning, it is crucial to incorporate information  derived from hands-on activities, class discussions, and teachers  comments. This is of special relevance in science education where  students conduct real experiments and make observations not  captured by computers, or in other settings where students  complete activities using printed handouts. Achieving a holistic  approach to using learning analytics for curriculum improvement  is a challenge because it can be hard to make sense of and  contextualize the usage of data gathered by the LMS.   Ongoing research in educational data mining and learning  analytics suggests that harnessing analytics data can advance new  research methodologies in education, help educators better assess  their pedagogical practices, and devise innovative educational  methods, all with the goal of improving education [14, 38, 40, 47,  51, 57]. With the rapidly growing interest in and technical ability  to leverage these data in educational settings, there is a sense that  many recent educational technology and big data initiatives are  detached from what we know about learning and teaching [27,  46]. It is, therefore, imperative to ensure that the use of learning  analytics in the K-12 sector is grounded in relevant pedagogy and  that it incorporates the face-to-face interactive learning  experiences such as student inquiry, discussions, hands-on  activities, and so on, not captured by the digital artifacts. This type  of approach guarantees that data analyses in LA are not reduced to  the mere analysis of clicks and page visits [53].   The purpose of this paper, then, is to present a strategy to  incorporate learning analytics into one K-12 online science  curriculum and to discuss the challenges faced and options for  overcoming them. The strategy we have developed has two main  parts: the first is technical, and the second is qualitative. The  technical strategy comprises the architecture that enables our  analytics instruments and then the specific variables we are using  to answer questions about the curriculum. The second part, which  is qualitative, is more challenging to scale but necessary, we  argue, for designers and researchers to understand how the  curriculum is being used in classrooms. The qualitative strategy  has served to help us make sense of the analytics and to make  more meaningful recommendations for improvement.    We begin our discussion by first briefly describing the curriculum  examined here and its underlying pedagogical model. Then we  discuss the two parts of our strategy, including examples of   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   210    findings from the work we have done to implement the strategy  and improve the curriculum and its data dashboard. As part of  this, we present some examples of visualization techniques we  recently incorporated into STEMscopes based on teacher  feedback. Finally, we end with a discussion about the future of  data analytics in K-12 education, with a focus on the challenges  we need to overcome and the potential for educational  improvement.   2. THE 5E+I/A PEDAGOGICAL  FRAMEWORK  STEMscopes is a K-12, comprehensive, online science curriculum  that is both aligned to state standards and grounded in the 5E  science inquiry model. It offers hands-on inquiry activities,  intervention and acceleration materials, and additional supporting  resources for teachers. The 5E pedagogical model is an inquiry- based learning approach that aims to harness learners prior  knowledge while eliciting interest in new phenomena [8, 57]. This  instructional model was created and refined by the Biological  Science Curriculum Study (BSCS). The 5E acronym is an  abbreviation of five steps that each begins with the letter E:  engagement, exploration, explanation, elaboration, and evaluation.  In the Engagement phase, students draw on prior knowledge to  raise their interest in and activate their prior knowledge of the new  content. During the Exploration phase, students take part in  activities and experiments that allow them to experience and learn  new concepts and skills. The Explanation phase requires students  to explain those new concepts and skills in their own words. New  experiences in the Elaboration phase question and deepen  students understanding of the new concepts. Finally, learners  understanding is assessed in the Evaluation phase.    STEMscopes adds two steps to the basic 5E model: one for  intervention with students struggling to master concepts, and one  for acceleration for those students who have mastered the  concepts and are ready to extend and apply their learning [57].  Our curriculum also offers accompanying hands-on activity kits  that teachers can use with students for conducting science  experiments and make observations, thereby offering a deeper  science education experience. The curriculum is organized in  grades from Kindergarten to 12th grade. Each grade is comprised  of units called scopes. There are nearly four hundred scopes in  total. Scopes in turn, are classified in different categories. The  total of scopes per category are listed in table 1.      Table 1. Number of scopes per topic in STEMscopes.   Topic name   # of   scopes    Formation of Fossil Fuels                     74   Safety                                        62   Heredity                                      58   Classifying Matter                            38   Uses of Energy                                33   One Dimensional Motion                       29  Physical and Chemical Changes and   Properties  25    Prokaryotic and Eukaryotic Cells             24   Whole Number Place Value                     12   Measurement Conversions                      4   Geometric Figures                             3   Probability and Predictions                   3   Relationships in Data                         2   Figure 1 provides an example of what the 5E model looks like in  STEMscopes. Specifically, it depicts a 4th grade unit entitled,  Changes from Heat. Components for each of the seven  (5E+I/A) steps are presented in the column on the left. They  include slideshows, interactive games, and assessments, among  others. Figure 2 shows three elements of a scope, or unit: 1) an  overview of a units content in the context of the 5E model, which  provides a snapshot of all the components a teacher can find for  that scope. It is intended for class planning purposes; 2) an  interactive game in the Explain phase that offers an engaging and  stimulating activity for the student to revisit what they have  learned; and 3) a student journal sheet that can be used by students  when completing hands-on activities as part of the Engage and  Explore steps.    Figure 1. Screen shot of a science unit titled,   Changes from Heat. The column on the left   presents elements grouped by each 5E phase   (Engage, Explore, Explain, Elaborate, and   Evaluate).      211       3. TECHNICAL ANALYTICS STRATEGY  In this section, we lay out our strategy for using LA in  STEMscopes. The strategy addresses both the technical and sense  making challenges we face in the context of K-12 education.   3.1 Architecture   When addressing learning analytics for STEMscopes, we face an  environment where data is constantly generated and stored as  teachers and students use our curriculum. Curating, storing and  processing the large amount of data amassed on a daily basis,  nearly a quarter of a million interactions with the curriculum each  day, demands techniques from the area of databases, data  warehouse, and big data. We have designed a distributed  environment that allows us to scale the storage and processing of  user-generated data. Accordingly, we employ three separate high- end computer servers that handle regular STEMscopes requests,  usage data storage, and analytics data processing. Although this  infrastructure is robust enough for our immediate needs, we will  use different big-data methodologies as we introduce more  sophisticated analyses such as machine learning, data mining  algorithms, simulations, and computational linguistics. For  example, we recently began experimenting with Hadoop [41, 52],  an open source framework that enables distributed high-intensive  computing for processing extremely large amounts of data.  Although this type of methodology might not be considered  important in Learning Analytics, current processing times for  aggregating data in STEMscopes suggest it will be part of our  infrastructure in the future. Data aggregation by week and month  is in the order of three to four hours. It is true that these processes  are not executed frequently. However, we need to plan a scalable  infrastructure as we begin calculating content- and user-based  models derived from data spanning several years, across multiple  schools and districts along with recent user generated data.  Among the practical benefits of this big data approach to our K-12  curriculum are to allow large clustering and machine learning  algorithms for supporting personalized learning.   3.2 STEMscopes Dataset  STEMscopes serves about 2,400 schools with more than 50,000  teachers and over one million students in the state of Texas.  STEMscopes currently serves 40% of the states school districts,  making it the most used science curriculum in the state. During   the 2011-2012 school year, we gathered more than one hundred  million data points generated by districts, schools, and users, both  teachers and students. We are able to identify the science topic  covered, grade level, specific step in the 5E+I/A process,  materials used, and the interim assessment results. LA allows for  multiple kinds of analyses including basic correlation analysis,  regression analysis, path analysis, user modeling, domain  modeling, learning analysis, curriculum evaluation, trend analysis,  and longitudinal analysis [7]. In this way, LA can shed light on  program effectiveness, on the ways in which a program can be  improved, and on ways the curriculum managers can better  support the teachers using the curriculum.    Because LA is based on the measurement and analysis of data  generated by end users, it has the potential to be a powerful tool  for educational improvement [16]. LA can leverage education  through improved educational decision-making [1, 19, 36, 37],  clearer goal setting [23], more timely and frequent feedback for  student and teachers [1, 20, 21, 35, 48], individualization of  learning [6, 15, 24, 32, 49, 58], and the generation of a richer set  of data on student behavior and learning [5, 33].    Yet, in order for LA to have a sustained and positive impact on K- 12 education, it must be implemented in ways that both recognize  and draw on the existing education research. Indeed, while LA has  an established presence in the business sector and even in higher  education, its foray into K-12 education is particularly new, and is  being driven in large part by the start-up community and venture  capital [11, 12]. Although there is nothing wrong with these  drivers, educators are unlikely to buy into new technology and big  data initiatives if they are not immediately accessible and relevant  to their daily work [34, 54, 56]. This paper addresses this gap in  the literature by discussing the strategy for integrating LA into a  K-12 science curriculum.   3.3 Pedagogical Considerations  The STEMscopes curriculum is grounded in the 5E+I/A inquiry  model described above. This framework underpins not only the  development of the curriculum, but also the analysis of all data.  Without theory, the data patterns at best have no meaning, and at  worst are given the wrong meaning [2, 31]. The 5E models  underlying theory of learning, constructivism, is used to answer  important questions about how the curriculum is actually  implemented in classrooms and how students learn best [8]. For  example, constructivism and the 5E model contend that students  learn best by exploring concepts through experience before  engaging in any formal explanation. STEMscopes, however, is  flexible and allows teachers to use materials from the curriculum  in any order, or skip some of them.    The need for flexibilitywhich allows teachers to adapt the  available resources to their students needsis in tension with  what constructivism and the 5E model say about learning. In our  analyses, we therefore have to balance the expectation that  teachers will follow the instructional model with the reality that  teachers adapt all of their resources. Together, pedagogical and  implementation considerations prompt us to ask questions such as:  Is the curriculum used to scaffold student learning What are  different patterns that users follow through the curriculum, and  how are these patterns of use related to student outcomes    3.4 Data Collection  There are two main purposes for collecting and analyzing  analytics data in a K-12 curriculum. The first is to improve the  curriculum, and the second is to evaluate the impact of the   1 2   3   Figure 2. Various elements for one science topic: 1) a partial   summary of the content of one unit, 2) an interactive game,   and 3) a student journal.   212    curriculum on student learning. The collection of analytics data is  guided by the pedagogical considerations outlined above 5E+A/I model helps us form questions that we can answer using  the analytics data.   Two overarching questions we currently are investigating with teacher use of curriculum (frequency of use, kind of use, and  the order in which teachers use the steps embedded in the  curriculum) and the time teachers spent accessing Together, the answers to these questions begin to paint a picture  of how teachers are using the curriculum to teach science  Examining teacher use of the curriculum is quite complex and has  required us to continually refine what we want to know.  most basic question about frequency of use has many possible  answers that require different approaches for calculat and to accommodate all of the possible ways to conceive us For instance, we can capture overall use for each teacher who has  an account, but this tells us nothing about how frequently they are  using what; it might simply be that they are signing in  Therefore, we also calculated how frequently teachers are using  individual units (we call them scopes), different steps of the  model, and other components within the curriculum (e.g. teacher  background material). We also have drilled  weighting these statistics by the number of grades a teacher  teaches or the number of students a teacher has. In this way, we  have begun to deal with questions of data normalization, which  we will discuss more below.   Teacher use also deals with questions of how using the curriculum. Of interest to us are which parts of the  curriculum are teachers using and whether they are using the steps  in order. As part of a three-year study, we analyzed a non sample of 134 mostly elementary schoolteachers from a large,  urban district in Texas (of which we are in year one) examined which steps the teachers in our sample found that for these teachers, the most commonly used step was  Explore. The results are portrayed in figure 3. As with other use  questions, there are many ways to drill down into this finding,  such as creating different weights in order to take into account  how many grades or students a teacher has.   Figure 3. Use of each of the 5E+I/A steps.  The collection of analytics data is  guided by the pedagogical considerations outlined abovethe  5E+A/I model helps us form questions that we can answer using   investigating deal  (frequency of use, kind of use, and   the order in which teachers use the steps embedded in the  accessing the curriculum.   Together, the answers to these questions begin to paint a picture  teachers are using the curriculum to teach science.   Examining teacher use of the curriculum is quite complex and has  required us to continually refine what we want to know. Even the   about frequency of use has many possible  calculating variables   to accommodate all of the possible ways to conceive usage.  For instance, we can capture overall use for each teacher who has  an account, but this tells us nothing about how frequently they are  using what; it might simply be that they are signing in and out.  Therefore, we also calculated how frequently teachers are using  individual units (we call them scopes), different steps of the  model, and other components within the curriculum (e.g. teacher    down further by  weighting these statistics by the number of grades a teacher  teaches or the number of students a teacher has. In this way, we  have begun to deal with questions of data normalization, which   how the teachers are  using the curriculum. Of interest to us are which parts of the  curriculum are teachers using and whether they are using the steps   , we analyzed a non-random  schoolteachers from a large,   (of which we are in year one). We  examined which steps the teachers in our sample used most. We  found that for these teachers, the most commonly used step was   . As with other use  questions, there are many ways to drill down into this finding,  such as creating different weights in order to take into account   Questions of time are similarly complex. how long a teacher is logged in, when the first and last login were,  how long a teacher spends on each page, and on how many  different days a teacher actually accessed the website.  example, we investigated teacher user activity for  teachers from one district over the course of several months. We  found a great deal of variation in actual use: the span of time  during which teachers actively use their curriculum accounts  varies in length (from first login to last login for the period of  interest, which ranged from a span of one day to a span of 183  days), as does the number of days on which teachers  in to access content or materials. These data are summarized in  table 2 below. Teacher activity ranged from a low of one day on  which teachers logged in, to a high of 107 separate days on which  teachers accessed the site. We also adjusted this figure to take into  account the number of grades the teachers teach; some teach only  one grade, while others teach science to multiple grades.  we calculated the percent of the total time span teachers were actively logged into the website; this statistic  ranged from 0% to 100% of the time.      Table 2: Teacher data on visits to      These kinds of data help the curriculum designers understand if  and how often the curriculum is being used. We also will be able  to relate these data to student learning outcomes, both those  measured by assessments embedded in the curriculum  students can take online) and by district and state standardized  assessments. The strength and direction of these relationships can  shed light on the efficacy and effectiveness of the curriculum.  their own, however, these data have little meaning relate them back to the theory behind the curriculum complement them with qualitative data  3.5 Visualizing Data  The final part of our LA strategy is the incorporation of various  visualization techniques to help teachers and admini sense of data. One of the biggest challenges facing educators and  analysts alike is turning data into information  56]. Visualization is an important tool that, in concert with strong  theories, can help make sense of data visualization techniques we use are heat maps and timelines design of these visual interfaces is driven by comments and  feedback received from the various stakeholders. As a way of  example, teachers who participated in the focus gro in section 4.2) indicated that knowing what parts of the  curriculum have not been used would help as reminder of what  needs to be taught. Similarly, color intensity would suggest  sections most and least used, driving teachers attention to th sections.    Use of each of the 5E+I/A steps.   are similarly complex. The analytics can tell us  how long a teacher is logged in, when the first and last login were,  how long a teacher spends on each page, and on how many  different days a teacher actually accessed the website. For   ed teacher user activity for a sample of  one district over the course of several months. We   found a great deal of variation in actual use: the span of time  during which teachers actively use their curriculum accounts   rst login to last login for the period of  , which ranged from a span of one day to a span of 183   ), as does the number of days on which teachers actually log  These data are summarized in   activity ranged from a low of one day on  which teachers logged in, to a high of 107 separate days on which   We also adjusted this figure to take into  account the number of grades the teachers teach; some teach only   ile others teach science to multiple grades. Finally,  we calculated the percent of the total time span (in days) that  teachers were actively logged into the website; this statistic   visits to STEMscopes   These kinds of data help the curriculum designers understand if  and how often the curriculum is being used. We also will be able  to relate these data to student learning outcomes, both those  measured by assessments embedded in the curriculum (which  students can take online) and by district and state standardized  assessments. The strength and direction of these relationships can  shed light on the efficacy and effectiveness of the curriculum. On   these data have little meaning if we do not  relate them back to the theory behind the curriculum and if do not  complement them with qualitative data.    The final part of our LA strategy is the incorporation of various  visualization techniques to help teachers and administrators make  sense of data. One of the biggest challenges facing educators and  analysts alike is turning data into information [22, 25, 28, 29, 55,   . Visualization is an important tool that, in concert with strong  theories, can help make sense of data [13]. Two specific   are heat maps and timelines. The  design of these visual interfaces is driven by comments and  feedback received from the various stakeholders. As a way of  example, teachers who participated in the focus group (discussed  in section 4.2) indicated that knowing what parts of the  curriculum have not been used would help as reminder of what  needs to be taught. Similarly, color intensity would suggest   , driving teachers attention to those   213    We have identified some cases where visualization will play an  essential role in understanding teaching practices. Covering all of  them is beyond the scope of this paper, but here we describe two  of them. At the teacher level, for example, we want to find out  whether sequences of use are similar across scopes for the same  teacher, or just limited to scopes with similar themes or concepts.  The former implies a common teaching practice, whereas the  latter, practices can be attributed to particular content properties.  Another case is one where usage patterns for a given scope are  consistent in a high number of teachers in one district but different  when compared to other districts. One can speculate that perhaps  teacher training might have an impact on teaching practices.        3.5.1 Timelines  Timelines are used to understand sequence and pacing. In  timelines, events are plotted on a graph, allowing users to see  when topics were introduced, how long certain themes were  covered, any overlap of topics, and the time elapsed between  events. For example, a timeline might be used to investigate the   impact of the time elapsed between the moment when content was  covered and the assessment of that content and students learning  outcomes [18, 30, 44]. Figure 4 offers a partial snapshot of a  teachers use of STEMscopes. The number of Density labels in  the display is indicative of a teacher working on that scope for a  certain period of time. The presence of two different scopes at the  right end of the display: Adaptations and Circuits and Electricity,  might signify a shift to a different theme at that point in time  during the school year. Time-based visualization addresses the  when materials are accessed and for how long.   3.5.2 Heat maps  Heat maps allow us to study the way in which the curriculum is  presented. Colors illustrate the intensity of use across the 5E steps.  Figure 5 depicts a partial view of curriculum use for 3rd and 4th  grades in one school district (grade level is indicated by the  numeric prefix in the TEKS column). Each row corresponds to  one scope. Blank cells indicate resources not available in  STEMscopes. This graph show high use for most scopes in third  grade and from scopes 4.5A to 4.6A, while low use can be seen  from scopes 3.8D to 4.4A. The most used steps are Engage and  Explore, while Acceleration is the least used. The top used scopes  are 3.8CD  Space and 4.6A  Forms of Energy. Conversely, 4.3C   Models, is the least accessed. This type of visualization offers a  quick overview of what has been taught and to what degree.   3.5.3 Visualizing Student Performance  At any given time during the school year, educators need  information about student progress. In addition to the students  overall performance information, more detailed progress data  augment teachers understanding of each student on different   Figure 5. Heat map depicting a partial list of STEMscopes content use for one district. Scopes included are for 3rd   and 4th grades (indicated by the numeric prefix on the TEKS column).    Time in hours   Figure 4. Use of themes by a teacher, separation   among events indicates time elapsed (pacing) between   them.   214    topics or even concepts. Figure 6 depicts a dashboard with  information about fourth graders. Columns show scopes grouped  by topic. Upward green arrows represent increase in grades from  the previous assessment and downward red arrows illustrate drop  in grades. Students are clustered by degree of intervention or  acceleration they require, represented by the colored cells (three  different groups). Students in the intervention group (yellow- colored cells) are the ones that although advancing in the class,  need a moderate amount of help. Those in the intensive  intervention group (pink-colored cells), require more help because  their poor performance. Conversely, students in the acceleration  group (green-colored cells) can be challenged with more advanced  materials. This dashboard helps teachers to better understand the  needs of the students and schedule the most appropriate resources  from the Acceleration or Intervention components in the  curriculum. This case illustrates the potential of analytics for  achieving a more personalized teaching experience.   3.6 Identifying Uncommon Patterns  Another area where analytics can help is in the identification of  non-canonical sequences of usage. The following example  illustrates this case. STEMscopes offers a variety of assessments  instruments that teachers assign to students. These assessments  are designed to measure students knowledge at three different  points in time as follows: Pre-assessment, at the beginning of the   unit, Progress Monitoring, half way in the unit, and Standard  Assessment, at the end of the unit.    The expectation is for students to complete these assessments in  the order previously described; however, we have found  occurrences where students either do not finish some or all of the  assessments, or cases in which students execute them in different  and unintended sequence. Table 3 shows assessments of students  in 8th grade for the scope 8.5A Atoms. The first three students in  the table completed only Pre-assessment. The next three students  did both Pre-assessment and Progress Monitoring. The following  twelve students completed all three assessments. In all these  cases, students followed the expected completion sequence. The  last four students in the table show unusual completion patterns.  Hop skipped the Progress-monitoring; Tal completed the  Standard Assessment before the Progress Monitoring. Cia did  only Progress Monitoring while Ken skipped Pre-assessment.  This type of information can be used in the identification of those  students that are missing activities, since that could signal a bigger  problem beyond the scope under study. In instances where a  teacher shows an uncommon pattern consistently regardless of the  scope or grade taught, might indicate lack of training or  understanding in how to administer the assessment instruments,  thus requiring additional training and ultimately improving his or  her teaching practices.       Figure 6. Partial list of 4th-grade student progress for all scopes grouped by topic. Red arrows   indicate grade drop from previous assessment, while green arrows show grade increase from   previous assessment.   215    Table 3. Sequence of completion of student   assessments in one class.   4. QUALITATIVE DATA STRATEGY In this section, we discuss the second part of our data analytics  strategy. We argue that collecting qualitative data and using them  to complement the analytics data are necessary for both  improvement and evaluation. It can be difficult to scale a high  quality qualitative data collection strategy, but it is difficult, if not  impossible, to really understand how the online curriculum is  being used without spending time observing its use in classrooms  and talking to teachers about their use.    The need for a qualitative component reflects a key way in which  K-12 LA is different from LA in higher education. Specifically, it  reflects the difference in access to technology: while institutions  of higher education tend to have up-to-date technology, including  computer labs, laptops for loan, reliable internet connections, and  even well-wired students, the same cannot be said for K schools. There is much variation, for example, among schools  based on socioeconomic differences or based on urbanicity [ Much research has documented that a lack of access to computers  and other technology is a major barrier to the implementation of  online or computer-assisted learning. It also, therefore, is a major  barrier to the collection of reliable analytics data. with the teachers directly, it is possible to uncover details about  the context and quality of implementation. Below, we describe the  methods we have adopted as part of our qualitative strategy:  survey, focus groups, and observations.   4.1 Survey  The research team created and administered a survey to over 700  STEMscopes teachers in a local district as part of the st mentioned above. The survey was created based on existing  surveys about technology and data use and contains 93 items.  While some of these items are specific to the STEMscopes  curriculum and its particular offerings, other items are  generalizable to other technology or curricula. The survey asks  teachers about the following concepts: teacher notions of data and  data use, how teachers use STEMscopes, attitudes toward  STEMscopes and the data dashboard, challenges to using  STEMscopes and the dashboard, and district and school  for using STEMscopes and the dashboard. The survey was piloted  and validated in the fall of 2012, and then administered for the  purpose of our ongoing study; 210 teachers completed surveys.   Student Test 1 Test 2   Tay Pre-assessment     Kim Pre-assessment     Mar Pre-assessment     Bia Pre-assessment Progress Monitoring    Esm Pre-assessment Progress Monitoring    Kat Pre-assessment Progress Monitoring    Lia Pre-assessment Progress Monitoring Standard Assessment   Chr Pre-assessment Progress Monitoring Standard Assessment   Jas Pre-assessment Progress Monitoring Standard Assessment   Ash Pre-assessment Progress Monitoring Standard Assessment   Bet Pre-assessment Progress Monitoring Standard Assessment   Eli Pre-assessment Progress Monitoring Standard Assessment   Fer Pre-assessment Progress Monitoring Standard Assessment   Ken Pre-assessment Progress Monitoring Standard Assessment   Kar Pre-assessment Progress Monitoring Standard Assessment   Lau Pre-assessment Progress Monitoring Standard Assessment   Seb Pre-assessment Progress Monitoring Standard Assessment   Cou Pre-assessment Progress Monitoring Standard Assessment   Hop Pre-assessment Standard Assessment    Tal Pre-assessment Standard Assessment Progress Monitoring   Cia Progress Monitoring     Ken Progress Monitoring Standard Assessment   Sequence of completion of student   STRATEGY  In this section, we discuss the second part of our data analytics   collecting qualitative data and using them  to complement the analytics data are necessary for both   It can be difficult to scale a high  quality qualitative data collection strategy, but it is difficult, if not   lly understand how the online curriculum is  being used without spending time observing its use in classrooms   The need for a qualitative component reflects a key way in which  ducation. Specifically, it   reflects the difference in access to technology: while institutions  date technology, including   computer labs, laptops for loan, reliable internet connections, and  he same cannot be said for K-12   schools. There is much variation, for example, among schools  based on socioeconomic differences or based on urbanicity [36].  Much research has documented that a lack of access to computers   rier to the implementation of  assisted learning. It also, therefore, is a major   barrier to the collection of reliable analytics data. By interacting  with the teachers directly, it is possible to uncover details about   ity of implementation. Below, we describe the  methods we have adopted as part of our qualitative strategy:   The research team created and administered a survey to over 700  STEMscopes teachers in a local district as part of the study  mentioned above. The survey was created based on existing  surveys about technology and data use and contains 93 items.  While some of these items are specific to the STEMscopes  curriculum and its particular offerings, other items are   her technology or curricula. The survey asks  the following concepts: teacher notions of data and   , attitudes toward  and the data dashboard, challenges to using   district and school supports  The survey was piloted   , and then administered for the  purpose of our ongoing study; 210 teachers completed surveys.    And while there were many interesti discussing what we learned about teachers access to technology  in their schools and classrooms (see table  teachers (89%) have computers in their classrooms, but almost  half (47%) of those teachers have only of those is for the teachers exclusive use, meaning that students  have access to one to two computers for their own activities.  Another 45% of teachers have four to six computers in their  classrooms. The implication of these fin activities available to the students online is not likely; it simply is  not feasible for the vast majority of the teachers who responded to  the survey.   4.2 Focus Groups and Observations The research team also has conducted several focus groups and  observed over ten days of instruction to understand how teachers  useand want to usethe curriculum and data dashboard. example, in the summer of 2012, the research team conducted a  focus group with thirteen teachers and two science specialists to  gather feedback on the design of the data dashboard, which allows  teachers to view their students and assign work.  expressed interest in an option that would curriculum usage, in addition to seeing progress. Since the summer focus group, these elements have  been added to the dashboard and are in use currently 3).   The focus group participants also suggested the creation of a  feedback mechanism where they could write comments about  their experience using curriculum resources and sharing them with  other teachers. We believe that this mechanism will tremendously  improve our analytics strategy considering the nearly fifty  thousand teachers that presently use our curriculum. However,  analyzing the content of these open-ended comments requires the  adoption of techniques from natural language processing (NLP),  ontologies, digital libraries, and information retrieval.  Through classroom observations, we have seen how teachers and  students interactand do not interact curriculum. Observations, though time because they allow you to see and then compare and classify interactions. Teachers often unwittingly ove they use the curriculum, and observations can cut through the  problems of self-reporting and perceptions.   In the three classrooms we observed,  about the context and quality of implementation. For instance,  there were no more than two computers available to students, and  most of the teachers either printed materials from the website, or  projected the website onto a whiteboard. This finding can help  explain variation in teacher use. Similarly, we observed that whil  Test 3  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Progress Monitoring  Table 4: Access to computers  And while there were many interesting findings, it is worth  discussing what we learned about teachers access to technology  in their schools and classrooms (see table 4). A large majority of  teachers (89%) have computers in their classrooms, but almost  half (47%) of those teachers have only one to three. Usually one  of those is for the teachers exclusive use, meaning that students  have access to one to two computers for their own activities.  Another 45% of teachers have four to six computers in their  classrooms. The implication of these findings is that full use of  activities available to the students online is not likely; it simply is  not feasible for the vast majority of the teachers who responded to   and Observations  team also has conducted several focus groups and   observed over ten days of instruction to understand how teachers  the curriculum and data dashboard. For   n the summer of 2012, the research team conducted a  rteen teachers and two science specialists to   gather feedback on the design of the data dashboard, which allows  teachers to view their students and assign work. Participants   that would depict their own  ddition to seeing metrics on student   Since the summer focus group, these elements have  been added to the dashboard and are in use currently (see section   The focus group participants also suggested the creation of a  k mechanism where they could write comments about   their experience using curriculum resources and sharing them with  other teachers. We believe that this mechanism will tremendously  improve our analytics strategy considering the nearly fifty   rs that presently use our curriculum. However,  ended comments requires the   adoption of techniques from natural language processing (NLP),  ontologies, digital libraries, and information retrieval.   ations, we have seen how teachers and  and do not interactwith the online   Observations, though time-consuming, are important  and then compare and classify   interactions. Teachers often unwittingly over-report how often  they use the curriculum, and observations can cut through the   reporting and perceptions.    n the three classrooms we observed, we learned a great deal  about the context and quality of implementation. For instance,   were no more than two computers available to students, and  the teachers either printed materials from the website, or   the website onto a whiteboard. This finding can help  Similarly, we observed that while   : Access to computers.   216    some teachers followed the curriculum very carefully, others used  fewer of the activities and instead incorporated other available  resources. In talking with teachers, we also learned that because of  the dearth of computers, teachers often share accounts, which can  skew the data: one teachers user account may actually contain  data on several teachers use. We also observed that in two  classrooms, the students used their teachers authentication  information to log in, which can help explain outliers.   By visualizing how elements in the curriculum are used (and not  used), we have identified three implications for our learning  analytics strategy going forward. First, we will examine how  patterns of teacher curriculum usage relate to student outcomes in  science. These data can serve as validating the 5E+I/A  pedagogical model that underpins STEMscopes. Second, we can  investigate further why there are so many different usage patterns  by administering surveys and asking teachers more targeted  questions, including about their access to technology and training.  Finally, analysis of these data can shed light onto ways to improve  the user interface, such as how content and activities are written  and presented to the teachers and students.   5. CONCLUSION AND FUTURE WORK  LA has the potential to bring important changes to K-12 education  if implemented with a coherent, contextualized strategy, such as  the one briefly described in this paper. The STEMscopes LA  strategy has shed light on interesting finding and trends, such as  the fact that teachers use the Explore step the most and do not  have access to many computers in their classrooms. But it also has  raised new questions and highlighted several challenges facing the  LA community as it integrates further into the K-12 education  setting.   In some ways, the strategy we have developed creates as many  new questions as it answers. This point, of course, simply  highlights the complex nature of K-12 education. Two important  lessons that we have learned and that can inform similar efforts by  other researchers to incorporate LA at the K-12 level include the  need to drill down into variables to account for the various  dimensions of use and time, the need to undertake qualitative  research, and to talk to and observe teachers.   Future LA work that STEMscopes will undertake includes  examining use patterns and relating the various dimensions of use  to student assessments. These, no doubt, will present the team  with fresh challenges as the complexity of task becomes apparent.  There also are several hurdles that the LA community will have to  overcome or address to ensure that these new data collection and  analysis tools do, in fact, fulfill their promise. First, schools must  address the gap in access to technology. While some schools have  achieved one-to-one computing, most schools are not even close  to this goal, and this has profound implications for our ability to  collect reliable analytics data. Our data revealed that teachers and  students often share accounts, and that students are limited in the  activities they can complete online. This, in turn, means that the  analytics data we collect may not be reliable: for some teachers,  the usage analytics may accurately portray their use, while for  others, they may not.    A second and equally important challenge is time. Teachers often  do not have time to incorporate technology and online activities  into their regular instruction. It is time-consuming to plan for this  incorporation, especially because it often requires the teacher to  learn new software or strategies, and because it requires the  teacher to create new classroom and management routines. The  result of this hurdle is that the computer systems that should be   collecting analytics data are not fully implementedor are not  implemented at all, which degrades the quality of data we can  collect.   Though not a panacea for the challenges facing K-12, a strong LA  strategy can help educators make sense of and leverage the large  amounts of data generated by the increased use of mobile devices,  computers, and other technology in classrooms. With an improved  understanding of data, students can learn from their own mistakes.  Teachers can improve their lessons, work with students who need  extra help and provide extensions for those ready to move on.  Analysts can improve the curriculum and the resources available  to teachers and students. The strategy described in this paper was  developed to analyze detailed usage patterns of the online science  curriculum, STEMscopes. This approach enables analysis of large  amounts of data and opens the possibilities to complex  undertakings such as the creation of personalized learning  environments and targeted professional development programs for  educators.   6. ACKNOWLEDGMENTS  We would like to thank STEMscopes editorial and production  teams that make possible our science curriculum. Specials thanks  go to the teachers and science specialists that participated in the  focus group. Finally, thanks to Dan Hoyt and Lizzie Bell for  reviewing this manuscript.   7. REFERENCES  [1] Arnold, K. 2010. Signals: Applying Academic Analytics,   EDUCAUSE Quarterly 33, 1.  http://www.educause.edu/EDUCAUSE+Quarterly/EDUCAU SEQuarterlyMagazineVolum/  SignalsApplyingAcademicAnalyti/199385   [2] Atkinson, M. and Wiley, D. 2011. Learning analytics as  interpretive practice: applying Westerman to educational  intervention. Proceedings of the 1st International Conference  on Learning Analytics and Knowledge. ACM Press, New  York. 117-121.   [3] Baker, R., Corbett, A., Koedinger, K., and Roll, I. 2006.  Generalizing Detection of Gaming the System Across a  Tutoring Curriculum. In Proceedings of the 8th International  Conference on Intelligent Tutoring Systems. Berlin,  Heidelberg: Springer-Verlag, 402411.   [4] Baker, R., Corbett, A., Koedinger, K., and Wagner, A. 2004.  Off-Task Behavior in the Cognitive Tutor Classroom: When  Students Game the System. In Proceedings of the SIGCHI  Conference on Human Factors in Computing Systems (CHI  '04). New York, NY: ACM, 383390.   [5] Baker, S. and Yacef, K. 2009. The State of Educational Data  Mining in 2009: A Review and Future Versions. Journal of  Educational Data Mining, 1(1), 3-17.   [6] Beck, J. and Mostow, J. 2008. How who should practice:  Using learning decomposition to evaluate the efficacy of  different types of practice for different types of students. In  Proceedings of the 9th International Conference on   Intelligent Tutoring Systems, 353-362.   [7] Bienkowski, M., Feng, M., and Means, B. 2012. Enhancing  Teaching and Learning through Educational Data Mining  and Learning Analytics: An Issue Brief. Washington, DC:  SRI International.   [8] Bybee, R., Taylor, J., Gardner, A., Van Scotter, P., Powell,  J., Westbrook, A., and Landes, N. 2006. The BSCS 5E   217    instructional model: Origins and effectiveness. Science.  Colorado Springs.  http://science.education.nih.gov/houseofreps.nsf/b82d55fa13 8783c2852572c9004f5566/$FILE/Appendix%20D.pdf   [9] Chipman, S. 2010. Applications in Education and Training:  A Force Behind the Development of Cognitive Science.  Topics in Cognitive Science, 2(3), 386-397.   [10] Cogburn, D., Ramnarine-Rieks, A., Espinoza, F., and  Levinson, N. 2009. Learning Across Borders: Socio- Technical Strategies for Globally Distributed Teaching and  Learning. Proceedings of the 2nd International Conference  of Education, Research and Innovation, 1930-1941.   [11] Culatta, R. 2012. From Innovation Clusters to Datapalooza:  Accelerating Innovation in Educational Technology.  November 1, 2012:  https://www.educause.edu/ero/article/innovation-clusters- datapalooza-accelerating-innovation-educational-technology.   [12] DeSantis, N. 2012. A Boom Time for Education Start-Ups:  Despite Recession Investors See Technology Companies  Internet Moment. The Chronicle of Higher Education.  March 18, 2012: http://chronicle.com/article/A-Boom-Time- for-Education/131229/.   [13] Duval, E. 2011. Attention Please! Learning Analytics for  Visualization and Recommendation. Proceedings of LAK11:  1st International Conference on Learning Analytics and   Knowledge.   [14] Elias, T. 2011. Learning analytics: Definitions, processes,  and potential.    [15] Farzan, R. 2004. Adaptive socio-recommender system for  open-corpus e-learning. In doctoral consortium of the third  international conference on adaptive hypermedia and   adaptive web-based systems.   [16] Ferguson, R. 2012. The State of Learning Analytics in 2012:  A Review and Future Challenges. Technical Report KMI-12- 01. http://kmi.open.ac.uk/publications/pdf/kmi-12-01.pdf     [17] First International Conference on Learning Analytics and  Knowledge. Baniff, Alberta, Canada. February 27-March 1,  2011. https://tekri.athabascau.ca/analytics/   [18] Glenberg, A. 1976. Monotonic and non-monotonic lag  effects in paired-associate and recognition memory  paradigms. Journal of Verbal Learning and Verbal Behavior,  15, 1-16.   [19] Goldstein, P. and Katz, R. 2005. Academic Analytics: The  Uses of Management Information and Technology in Higher  Education, ECAR Research Study Volume 8.  http://www.educause.edu/ers0508   [20] Ha, S., Bae, S., and Park, S. 2000. Web mining for distance  education. In IEEE international conference on management  of innovation and technology, 715719).   [21] Hamalainen, W., Suhonen, J., Sutinen, E., and Toivonen, H.  2004. Data mining in personalizing distance education  courses. In World conference on open learning and distance  education.   [22] Heer, J. and Shneiderman, B. 2012. Interactive Dynamics for  Visual Analysis. Communications of the ACM, 55(4), 45-54.   [23] Hendricks, M., Plantz, M., and Pritchard, K. 2008.  Measuring outcomes of United Way- funded programs:  Expectations and reality. In J.G. Carman & K.A. Fredricks   (Eds.), Nonprofits and evaluation. New Directions for  Evaluation, 119, 1335.   [24] Heraud, J., France, L., and Mille, A. 2004. Pixed: an its that  guides students with the help of learners interaction log. In  International conference on intelligent tutoring systems   (workshop analyzing student tutor interaction logs to   improve educational outcomes), Maceio, 5764.   [25] Ingram, D., Seashore Louis, K., and Schroeder, R. 2004.  Accountability policies and teacher decision making:  Barriers to the use of data to improve practice. Teachers  College Record. 106(6), 1258-1287.   [26] Jagadish, H.  2012. Big Data: Its Not Just the Analytics.  May 2012 ACM SIGMOD Blog.  http://wp.sigmod.org/p=430.   [27] Junco, R. 2012. Most ed-tech startups suck! Heres where  they go wrong. http://venturebeat.com/2012/10/28/most-ed- tech-startups-suck-heres-where-theyre-going-wrong/.   [28] Kerr, K. A., Marsh, J. A., Ikemoto, G. S., Darilek, H., and  Barney, H. 2006. Strategies to promote data use for  instructional improvement: Actions, outcomes, and lessons  from three urban districts. American Journal of Education,  112(4), 496-520.   [29] Lachat, M. 2005. Practices that support data use in urban  high schools. Journal of Education for Students Placed at  Risk. 10(3), 333-349.   [30] Landauer, T.K. and Bjork, R. A. 1978. Optimum rehearsal  patterns and name learning. In M. M. Gruneberg, P. E.  Morris, & R. N. Sykes (Eds.), Practical aspects of memory,  625-632. London: Academic Press.   [31] Long, P.  and Siemens G. 2011. Penetrating the Fog:  Analytics in Learning and Education. EDUCAUSE Review,  46(5). http://www.educause.edu/ero/article/penetrating-fog- analytics-learning-and-education   [32] Lu, J. 2004. Personalized e-learning material recommender  system. In International conference on information  technology for application, 374379.   [33] Mazza R., and Dimitrova, V. 2004. Visualising student  tracking data to support instructors in web-based distance  education, Wide Web conference on Alternate track papers  & posters. New York, NY, USA: ACM Press, 154-161.  http://www.iw3c2.org/WWW2004/docs/2p154.pdf   [34] Means, B., Padilla, C., and Gallagher, L. 2010. Use of  Education Data at the Local Level: From Accountability to  Instructional Improvement. Washington, DC: U.S.  Department of Education, Office of Planning, Evaluation and  Policy Development.   [35] Merceron, A., and Yacef, K. 2005. Tada-ed for educational  data mining. Interactive Multimedia Electronic Journal of  Computer-Enhanced Learning, 7(1), 267287.   [36] Mossberger, K., Tolbert, C. J., and Gilbert, M. 2006. Race,  place, and informational technology. Urban Affairs Review,  41(5), 583-620.   [37] Norris, D., Baer, L., Leonard, J., Pugliese, L. and Lefrere, P.  2008. Action Analytics: Measuring and Improving  Performance That Matters in Higher Education,  EDUCAUSE Review 43(1).  http://www.educause.edu/EDUCAUSE+Review/EDUCAUS  218    EReviewMagazineVolume43  /ActionAnalyticsMeasuringandImp/162422   [38] Oblinger, D. and Campbell, J. 2007. Academic Analytics,  EDUCAUSE White Paper.  http://www.educause.edu/ir/library/pdf/PUB6101.pdf   [39] Pahl, C. 2004. Data mining technology for the evaluation of  learning content interaction. International Journal on E- Learning., 3(4).   [40] Pardo, A. and Delgado, C. 2011. Stepping out of the box.  Towards analytics outside the Learning Management  System, 163-167. In International Conference on Learning  Analytics.   [41] Pavlik, P., Cen, H., and Koedinger, K. 2009. Learning  Factors Transfer Analysis: Using Learning Curve Analysis to  Automatically Generate Domain Models. Proceedings of the  2nd International Conference on Educational Data Mining,  121-30.   [42] Pavlo, A., Paulson, E., and Rasin, A. 2009. A Comparison of  Approaches to Large-Scale Data Analysis. In Proceedings of  the 2009 ACM SIGMOD Conference.   [43] President's Council of Advisors on Science and Technology,  Prepare and Inspire: K-12 Education in Science, Technology,  Engineering, and Math (STEM) for America's Future  (Executive Office of the President, Washington, DC, 2010);  www.whitehouse.gov/sites/default/files/microsites/ostp/pcast -stem-ed-final.pdf.   [44] Reif, F. 2008. Applying Cognitive Science to Education:  Thinking and Learning in Scientific and Other Complex  Domains. MIT Press. Cambridge, MA.   [45] Roediger, H. and Karpicke, J. 2010. Intricacies of spaced  retrieval: A resolution. In A. S. Benjamin (Ed.), Successful  remembering and successful forgetting: Essays in honor of   Robert A. Bjork. New York: Psychology Press.   [46] Romero C. and Ventura, S. 2010. Educational Data Mining:  A Review of the State of the Art. IEEE Transactions on  Systems, Man and Cybernetics, Part C: Applications and   Reviews 40 (6): 601618.   [47] Siemens, G. 2012. Learning Analytics: Envisioning a  Research Discipline and a Domain of Practice. Proceedings  of the 2nd International Conference on Learning Analytics &   Knowledge.   [48] Suthers, D., Ravi, V., Medina, R., Joseph, S., and Dwyer, N.  2008. Beyond threaded discussion: representational guidance  in asynchronous collaborative learning environments.  Computers & Education, 50, 1103-1127.   [49] Talavera, L., and Gaudioso, E. 2004. Mining student data to  characterize similar behavior groups in unstructured  collaboration spaces. In Workshop on artificial intelligence  in CSCL. 16th European conference on artificial   intelligence, 1723.   [50] Tang, T., and McCalla, G. 2002. Student modeling for a  web-based learning environment: A data mining approach. In  Eighteenth national conference on artificial intelligence,  Menlo Park, CA, USA, 967 968.   [51] Tytler R. and Prain, V. 2009. A Framework for Re-thinking  Learning in Science from Recent Cognitive Science  Perspectives. International Journal of Science Education.  32(15), 2055-2078.   [52] Vatrapu, R., Tplovs, C., Fujita, N., and Bull, S. 2011.  Towards a visual analytics for teachers' dynamic diagnostic  pedagogical decision-making. Proceedings of the IEEE, 93- 98.   [53] Venner, J. 2009. Pro Hadoop. Apress. New York.   [54] Watters, A. 2012. Learning Analytics: Lots of Education  Data... Now What  http://hackeducation.com/2012/05/04/learning-analytics- lak12/   [55] Wayman, J. C. 2005. Involving Teachers in Data-Driven  Decision Making: Using Compuer Data Systems to Support  Teacher Inquiry and Reflection. Journal of Education for  Students Placed at Risk. 10(3).   [56] Wayman, J. C., and Cho, V. 2009. Preparing educators to  effectively use student data systems. In T. Kowalski (Ed.),  Handbook of data-based decision making in education, 89- 104.). New York: Taylor & Francis.   [57] Wayman, J., Cho, V., Jimerson, J., and Spikes, D. 2012.  District-wide effects on data use in the classroom. Education  Policy Analysis Archives, 20(25).   [58] Whitaker, J. R. 2012. Responding to the need for  intervention: Six easy steps prime students for mastery of  science concepts. Science and Children, 50(4), 75-79.      219      