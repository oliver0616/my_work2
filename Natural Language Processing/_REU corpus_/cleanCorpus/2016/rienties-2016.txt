The impact of 151 learning designs on student satisfaction  and performance: social learning (analytics) matters   Bart Rienties & Lisette Toetenel  Institute of Educational Teachnology   Open University UK  [First.Lastname]@open.ac.uk   ABSTRACT  An increasing number of researchers are taking learning design  into consideration when predicting learning behavior and  outcomes across different modules. This study builds on  preliminary learning design work that was presented at LAK2015  by the Open University UK. In this study we linked 151 modules  and 111.256 students with students satisfaction and performance  using multiple regression models. Our findings strongly indicate  the importance of learning design in predicting and understanding  performance of students in blended and online environments. In  line with proponents of social learning analytics, our primary  predictor for academic retention was the amount of  communication activities, controlling for various institutional and  disciplinary factors. Where possible, appropriate communication  tasks that align with the learning objectives of the course may be a  way forward to enhance academic retention.   CCS Concepts   Applied computing~Distance learning      Applied computing~E-learning    Keywords       Data analytics, Collaborative Learning, Distance Learning   1. INTRODUCTION  There is an increased interest in predictive modeling in education.  Beyond identifying students that require additional support, in the  Learning Analytics Knowledge (LAK) community many scholars  are interested in identify trends in learning and teaching from rich  data sources. In order to identify the meaning of some of these  trends, pedagogical information is required and this has often  been ignored to date [1]. Pedagogical knowledge or information  granted through Learning Design provides the context to the  quantitative analysis that Learning Analytics is able to provide.  Although several studies [2-4] and a specific LAK workshop [5]  on learning design have used principles of learning design to  unpack the complexities of learning analytics in the last four to  five years, few studies have empirically compared the impact of  different learning designs on learning processes and outcomes.   For example, Conole [6, p121] describes learning design as a  methodology for enabling teachers/designers to make more  informed decisions in how they go about designing learning  activities and interventions, which is pedagogically informed and  makes effective use of appropriate resources and technologies.  Learning design is widely studied in the Higher Education sector,  but the definition of this concept has various meanings in different  settings and similar work has been carried out under such names  as pedagogical patterns, learning patterns and pattern language  [3, p1441].  Learning design is implemented as a way to improve course  design [4, 7, 8], but few studies have empirically connected  learning designs of a substantial number of courses with learning  behavior in Learning Management Systems (LMSs) and learning  performance. This study builds on preliminary learning design  work that was presented at LAK2015 by the Open University UK  (OU). In this explorative study we indeed found that learning  design decisions made by teachers were related to learning  behavior of students in blended and online environments [9]. An  important finding of this study amongst 40 modules and 27K  students was that assimilative learning design activities (such as  reading, watching) were positively correlated to learner  satisfaction, but negatively to academic performance. Our current  study builds on this initial explorative study by focusing on an  extensive elaboration of the scope and reach of our data analysis,  whereby we linked 151 modules and 111K students with students  satisfaction and performance using multiple regression models,  whereby we were able to control for various institutional and  disciplinary factors to determine what the key drivers for learning  are, and whether our initial findings still hold with this richer  dataset.    1.1 Learning design meets learning analytics  While substantial progress has been made in the last 10 years in  conceptualising learning design [7, 8] by for instance using a  data-informed approach, relatively few studies have investigated  how educators in practice are actually planning, designing,  implementing and evaluating their learning design decisions.  Evaluating the success of a learning activity for instance by  analysing activity logs of student behavior is more informative  when compared to the overall pedagogy and design of the course.   By linking large datasets across a range of 40+ modules in online  and blended learning settings, both studies [9] point to the  important notion often ignored in learning analytics: by analysing  the impact of learning design on learner satisfaction and academic  performance across a range of modules, a cross-sectional study  may provide crucial (generalizable) insights beyond the specific  research findings within a single module or discipline.   In a recent study by Li, Marsh and Rienties [10], using logistical  regression modelling learner satisfaction data were analysed and  the findings indicated that these proxies of learning design had a   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883875       strong and significant impact on overall satisfaction. Similarly,  using logistic regression with a primary purpose of improving  aggregate student number forecasts Calvert [11] found 30  variables in five broad categorizations which broadly predicted  progression of students..  Although these studies provide (indirect) evidence that proxies of  learning design and individual student characteristics influence  learner satisfaction and academic retention, none of these studies  have identified across a large range of modules whether  objectively mapped learning designs of modules have an impact  of actual student behavior, learner satisfaction and academic  retention. In this follow-up study of our LAK 2015 paper, we aim  to address this gap by comparing the learning designs of 151  modules that were followed by over 110k online students at  different disciplines, levels, and programmes.    2. METHOD  2.1 OULDI Learning Design   The learning design taxonomy used for this article was developed  as a result of the Jisc-sponsored Open University Learning Design  Initiative (OULDI) [12], and was developed over five years in  consultation with eight Higher Education institutions. In contrast  to instructional design, Learning Design is process based [6];  practitioners make informed design decisions with a pedagogical  focus and communicate these to their colleagues and learners.  This is especially relevant for institutions which deliver distance  learning. The OU follows a collaborative design approach, based  upon almost a decade of academic and institutional research ([13].   Table 1: Learning design activities   Type of activity Example   Assimilative Attending to  information   Read, Watch, Listen,  Think about, Access.   Finding and  handling  information   Searching for and  processing  information   List, Analyse, Collate,  Plot, Find, Discover,  Access, Use, Gather.    Communication Discussing module  related content with at  least one other person  (student or tutor)   Communicate, Debate,  Discuss, Argue, Share,  Report, Collaborate,  Present, Describe.   Productive Actively constructing  an artefact   Create, Build, Make,  Design, Construct,  Contribute, Complete,.    Experiential Applying learning in a  real-world setting    Practice, Apply, Mimic,  Experience, Explore,  Investigate,.   Interactive  /adaptive   Applying learning in a  simulated setting    Explore, Experiment,  Trial, Improve, Model,  Simulate.    Assessment All forms of  assessment  (summarive, formative  and self assessment)    Write, Present, Report,  Demonstrate, Critique.   For a detailed description of the seven learning descriptions and  theoretical foundations, we refer to previous published work [9,  14]. Assimilative activities relate to tasks in which learners attend  to discipline specific information. These include reading text  (online or offline), watching videos, or listening to an audio file.  In terms of social learning analytics conceptualisations, the next   five categories describe different options available to teachers to  create an interactive, social learning environment [1, 15]. By  finding and handling information, for example on the internet or  in a spreadsheet, learners take responsibility for extending their  learning, and are therefore engaged in active learning.  Communicative activities refer to any activities in which students  communicate with another person about module content.  Productive activities refer to activities whereby learners build and  co-construct new artefacts. Experimental activities provide  learners with the opportunity to apply their learning in a real life  setting. Interactive activities endeavor to do the same, such as  simulations. Finally, assessment activities encompass all learning  materials focused on assessment to monitor (formative) progress  and/or traditional assessment for measurement purposes. Table 1  identifies the seven types of learning activity in the OULDI  model.   2.2 Setting  This study took place at the OU, the largest higher education  provider of online distance education in Europe. A process of  module mapping (i.e. analyzing and providing visualizations of  the learning activities and resources involved in a module) was  introduced as part of a university-wide learning initiative [9, 14]  which aims to use learning design data for quality enhancement.  The mapping process is comprehensive, but labour intensive;  typically taking between one and three days for a single module,  depending on the modules number of credits, structure, and  quantity of learning resources. A team of learning design  specialists reviewed all the available learning materials, classifies  the types of activity, and quantifies the time that students are  expected to spend on each activity.   Classifying learner activity can be subjective, and consistency is  important when using the data to compare module designs across  disciplines in the institution. Therefore, the learning design team  held regular meetings to improve consistency across team  members in the mapping process. Once the mapping process was  complete, the learning design team manager reviewed the module  before the findings were sent to the faculty. Academics had the  opportunity to comment on the data before the status of the design  was finalised. In other words, each mapping was at least reviewed  by three people, which enhanced the reliability and robustness of  the data relating to each learning design.   2.3 Instruments  2.3.1 Learning Design mapping  The learning design tool at the OU is a combination of graphical,  text-based tools that are used in conjunction with learning design  workshop activities, which were mandated at particular times in  the design process. In total 189 modules were mapped by the  learning design team in the period January 2014-October 2015.  Given that the OU offers multiple presentations of modules per  year, in total 276 module implementations were recorded, of  which we could link 151 modules with learning performance data  (see next section). In total 113.725 students were enrolled in these  151 modules, with an average module size of 753.15 (SD=  828.89). For each module, the learning outcomes were captured in  the Learning Design tools. Each activity within the modules  weeks, topics, or blocks was categorized according to the learning  design taxonomy (see Table 1). These categorizations were  captured in a visual representation in the form of an activity  planner (or blueprint).      2.3.2 Learner satisfaction  Our second core dependent variable is learner satisfaction. In the  past thirty years, the OU has consistently collected learner  feedback to further improve the learning experience and learning  designs. In line with other learner satisfaction instruments [16- 18], at the OU the Student Experience on a Module (SEaM)  questionnaire was implemented. The SEaM institutional survey  was introduced in 2012/13 combining two previous surveys using  a census approach; so inviting all learners on all modules to  participate. It consists of three themed sets of in total 40  questions: 1) The module overall (10 items), 2) Teaching, learning  and assessment (14 items) and 3) Feedback on the tutor (16  items). Following our analysis of key drivers amongst 65K  students learning experience [10], for this analysis we used the  aggregate scores of five core items that drive learner satisfaction.   2.3.3 Academic retention  Our first dependent variable is academic retention, which was  calculated by the number of learners who completed and passed  the module relative to the number of learners who registered for  each module. Academic retention is a key concern of many  institutions, and in particular at the OU. The academic retention  ranged between 34.46% and 100%, with an average of 69.35  (SD= 12.75). These figures do need to be read in the context of  the OUs mission to provide education for all, regardless of  entrance requirements [19].    2.3.4 Institutional analytics data  In line with previous studies [20-22], we included several  institutional analytics data that are known to influence the  students learning experience, such as the level of the course  (ranging from level 1 to level 4, which is roughly translated into  year 1 to post-graduate) [11], the specific discipline [23], the year  of implementation, size of the class or module [20-22]. In terms of  VLE engagement, the average number of minutes spent in the  VLE per week were used as proxy for engagement [24].     2.4 Data analysis  All data were collected on an aggregate, module level. As a first  step, we merged the learning design data with the LMS and  learner retention data based upon module ID and year of  implementation. In total 151 module implementations could be  linked with LMS learning behavior and learning performance  data. In order to correct for any selection-bias in terms of selecting  modules for these mapping activities, we compared these 151  module implementations vs. 1016 module implementations which  were not mapped in the Learning Design tool in 2014/2015.  Indeed significantly more level 0-1 and fewer post-graduate  modules were mapped, but no significant differences were found  in terms of academic performance or student experience (so  limiting selection bias). As the learning design team primarily  focused on large scale undergraduate modules, this result was  expected.   All data was anonymized by the first author, whereby names and  codes of modules and respective disciplines were replaced by  random codes to safeguard the identities of teachers and their  respective faculty. Follow-up regression analyses were conducted  using SPSS 21.    3. RESULTS  3.1 Relating learning design with learner  satisfaction  As a next step, we linked the learning design metrics with learner  satisfaction. On average, 80.85% (SD= 11.06) of the 26483  (28.99%) students who responded to the SEAM survey were  satisfied with their learning experience, with a range of 39-97%.  A significant positive correlation was found between assimilative  activities and Average SEAM (r = .333, p < .01), while negative  correlations were found in terms of finding information (r = -.258,  p < .01) and communication (r = -.224, p < .01).   Three separate regression analyses were conducted, whereby  learner satisfaction was significantly predicted by students who  followed the Level 0 access models, whom were significantly  more positive than other modules. Other institutional variables  such as disciplinary differences were mostly not significantly  predicting learner satisfaction, in line with previous findings [10]  that students at the OU have similar learning experiences  irrespective of disciplinary differences.   Table 2: Regression model of learner satisfaction and learning   performance predicted by institutional and learning design  analytics      Learning  Satisfaction   Learning  performance   Level0 .351** .005  Level1 .265 .017  Level2 -.212 -.004  Level3 -.018 .215  Year of implementation -.059 -.151*  Faculty 1 .213* .360**  Faculty 2 .045 -.189*  Faculty 3 .236* .069  Faculty other .051 .034  Size of module -.071 -.239**  Finding information -.294** -.154  Communication .050 .500**  Productive -.274** .133  Experiential -.105 .008  Interactive .221* -.049  Assessment -.221* .063   LMS engagement .117 -.190*  Learning Satisfaction  -.058  R-sq adj 31% 36%   n = 150 (Model 1-2), 140 (Model 3), * p < .05, ** p < .01  When we added the learning design activities, learner satisfaction  was significantly negatively predicted by finding information,  experiential and assessment learning activities, and positively  predicted by interactive activities (again with assimilative  activities as the reference point). Separate analysis with  assessment as reference point (not illustrated) indicated that  assimilative activities significantly and positively predicted  learner satisfaction, while the betas for the other three predicting     learning activities remained similar. Finally, when we added LMS  engagement the primary predictors remained the same, but  engagement in LMS did not predict learner satisfaction. The seven  learning activities explained 18% of variance, and when the  institutional analytics were included 12% of unique variance was  explained. The final, complete model is presented in Table 2. In  other words, learning design activities had a significant and  substantial impact on learner experience, whereby modules with  more assimilative and fewer inquiry and discovery-based learning  activities were perceived to lead to better learner experiences (for  at least those who complete the surveys).    3.2 Relating learning design with learning  performance  Three regression models were used to predicted academic  retention, whereby the final model is presented in Table 2.  Academic retention was significantly positively predicted by  students following Faculty 1 (relative to reference point of Faculty  4). Furthermore, academic retention was negatively predicted by  the overall size of the module and year of implementation. In  other words, modules that were relatively large in size, more  focused on natural sciences, and those that were taught in more  recent academic years 2014-2015 had relatively lower retention  rates than smaller modules and modules taught in academic years  2012-2013. When adding the average learning experience and  VLE engagement, no significant relations were found between  learner satisfaction, VLE engagement and academic retention.  Finally, when adding the seven learning design activities,  communication significantly and positively predicted academic  retention. LMS engagement negatively predicted academic  retention when the seven learning design activities were included,  which may counterbalance some of the effects of communication.  The seven learning activities explained 11% of variance, and  when the institutional analytics were included 6% of unique  variance was explained. Separate analyses (not illustrated) with  assessment rather than assimilative learning design activities as a  reference point indicated that assimilative had a negative but non- significant impact on retention when taking the other variables  into account. In other words, as illustrated in Figure 1 in simple  laymen terms, communication (as reported in percentages on Y- axis) seemed to be a key lever for retention (as reported from 0-1  on X-axis) in blended and online distance education at the OU.     Figure 1: Communication and academic retention (per   discipline).   4. DISCUSSION  Pedagogy and learning design have played a key role in computer- assisted learning in the last two decades [6], but research has not  extensively linked learning design to learner performance [23,  25]. Progress has recently been made in how (combinations of)  individual learning design elements (e.g., task design, feedback,  scaffolding, structure) influence learning processes and success in  experimental and natural settings within single modules. Building  on our first study [9], this study has provided strong empirical  evidence that learning design had a significant influence on  learner satisfaction and academic retention amongst 151 modules  followed by 113.725 students.   Our first and perhaps most important finding is that learning  design and learning design activities in particular strongly  influenced academic retention. A major innovation is that we were  able to move beyond simple correlation analyses to multiple  regression analyses, whereby we were able to control for common  institutional analytics factors and disciplinary differences. This  approach was useful, as our initial analysis with correlation  analysis presented at LAK2015 seemed to indicate that modules  with a heavy reliance on content and cognition (assimilative  activities) seemed to lead to lower completion and pass rates.  However, when controlling for the institutional data sources and  modelling the seven learning design activities simultaneously, the  negative link between assimilative learning design and academic  retention was no longer significant. The primary predictor of  academic retention was the relative amount of communication  activities. This is an important finding as most teachers are the  OU and across the globe have a tendency to focus on cognition  rather than social learning activities [21, 23, 26], while recently  several authors in the LAK field have encouraged teachers and  researchers to focus on the social elements of learning [1, 21].  Our second important finding was that learner satisfaction was  strongly influenced by learning design. Modules with assimilative  activities and fewer student-centred approaches like finding  information activities received significantly higher evaluation  scores. However, a crucial word of caution is in place here.  Although we agree with others [17, 18, 21] that learner  satisfaction and happiness of students is important, it is  remarkable that learner satisfaction and academic retention were  not even mildly related to each other in Table 2. More  importantly, the (student-centred) learning design activities that  had a negative effect on learner experience had a neutral to even  positive effect on academic retention.   Two possible explanations are available for the widely different  effects of learning design on learner satisfaction and academic  retention. First, although more than 80% of learners were satisfied  with their learning experience, as evidenced by several leading  scholars [25, 26] learning does not always needs to be a nice,  pleasant experience. Learning can be hard and difficult at times,  and making mistakes, persistence, receiving good feedback and  support are important factors for continued learning. Our findings  indicate that students may not always be the best judge of their  own learning experience and what help them in achieving the best  outcome.   Second, on average 72% of students who participated in these 151  modules did not complete the learner satisfaction survey. In  certain modules actual dropout was well above 50%, indicating  that students were voting with their feet when the learning  design and/or delivery did not meet their learning needs. An  exclusive focus on learner satisfaction might distract institutions     from understanding the true learning experiences and academic  retention. If our findings are replicated in other contexts, a crucial  debate with academics, students and managers needs to develop  whether universities should focus on happy students and  customers, or whether universities should design learning  activities that stretch learners to their maximum abilities and  ensuring that they eventually pass the module. Where possible,  appropriate communication tasks that align with the learning  objectives of the course may seem to be a way forward to enhance  academic retention.   5. CONCLUSION AND FUTURE WORK  A major innovation in comparison to our initial study is that we  were able to execute multiple regression analyses, whereby we  were able to control for common institutional analytics factors and  disciplinary differences, but it highly likely that additional factors  contribute to the satisfaction and retention to the factors included  in the model.  In the near future, we would be able to extend this  sample further when more data becomes available in order to  better understand the complex (inter)relations of learning design  on learning processes and outcomes as we will be able to combine  this with further data sets such as student and tutor comments.   In addition, combining this analysis with the learning outcomes  data allows sharing of good practice based upon robust analysis.  Furthermore, a particularly useful feature would be to integrate  this with demographic, individual and socio-cultural data about  students, so that subgroups can be analysed. This may influence  whether a learning design is suitable for a range of learners. In  terms of practical implications for LAK, researchers, teachers and  policy makers need to be aware of how learning design choices  made by teachers influence subsequent learning processes and  learning performance over time.   REFERENCES  [1] Ferguson, R. and Buckingham Shum, S. Social learning analytics:   five approaches. ACM, City, 2012.  [2] Thompson, K., Ashe, D., Carvalho, L., Goodyear, P., Kelly, N. and   Parisio, M. Processing and Visualizing Data in Complex Learning  Environments. American Behavioral Scientist, 57, 10 (October 1,  2013 2013), 1401-1420.   [3] Lockyer, L., Heathcote, E. and Dawson, S. Informing Pedagogical  Action: Aligning Learning Analytics With Learning Design.  American Behavioral Scientist, 57, 10 (October 1, 2013 2013), 1439- 1459.   [4] Lockyer, L. and Dawson, S. Learning designs and learning analytics.  In Proceedings of the Proceedings of the 1st International  Conference on Learning Analytics and Knowledge (Banff, Alberta,  Canada, 2011). ACM, [insert City of Publication],[insert 2011 of  Publication].    [5] Lockyer, L. and Dawson, S. Where learning analytics meets learning  design. In Proceedings of the Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (Vancouver,  British Columbia, Canada, 2012). ACM, [insert City of  Publication],[insert 2012 of Publication].    [6] Conole, G. Designing for Learning in an Open World. Springer,  Dordrecht, 2012.   [7] MacLean, P. and Scott, B. Competencies for learning design: A review  of the literature and a proposed framework. British Journal of  Educational Technology, 42, 4 2011), 557-572.   [8] Armellini, A. and Aiyegbayo, O. Learning design and assessment with  e-tivities. British Journal of Educational Technology, 41, 6 2010),  922-935.   [9] Rienties, B., Toetenel, L. and Bryan, A. Scaling up learning  design: impact of learning design activities on LMS behavior and   performance. ACM, City, 2015.   [10] Li, N., Marsh, V. and Rienties, B. Modeling and managing learner  satisfaction: use of learner feedback to enhance blended and online  learning experience. Decision Sciences Journal of Innovative  Education2016).   [11] Calvert, C. E. Developing a model and applications for probabilities  of student success: a case study of predictive analytics. Open  Learning: The Journal of Open, Distance and e-Learning, 29, 2  2014), 160-173.   [12] Cross, S., Galley, R., Brasher, A. and Weller, M. Final Project  Report of the OULDI-JISC Project: Challenge and Change in   Curriculum Design Process, Communities, Visualisation and   Practice. JISC, York, 2012.   [13] Cross, S., Galley, R., Brasher, A. and Weller, M. Final Project   Report of the OULDI-JISC Project: Practice, Challenge and Change   in Curriculum Design Process, Communities, Visualisation and   Practice. City, 2012.  [14] Toetenel, L. and Rienties, B. Analysing 157 Learning Designs using   Learning Analytic approaches as a means to evaluate the impact of  pedagogical decision-making. British Journal of Educational  Technology2016).   [15] Buckingham Shum, S. and Ferguson, R. Social Learning Analytics.  Journal of educational technology & society, 15, 3 2012).   [16] Marsh, H. W. SEEQ: a reliable, valid, and useful instrument for  collecting students' evaluations of university teaching. British Journal  of Educational Psychology, 521982), 77-95.   [17] Onwuegbuzie, A. J., Witcher, A. E., Collins, K. M. T., Filer, J. D.,  Wiedmaier, C. D. and Moore, C. W. Students Perceptions of  Characteristics of Effective College Teachers: A Validity Study of a  Teaching Evaluation Form Using a Mixed-Methods Analysis.  American Educational Research Journal, 44, 1 (March 1, 2007  2007), 113-160.   [18] Zerihun, Z., Beishuizen, J. and Os, W. Student learning experience as  indicator of teaching quality. Educational Assessment, Evaluation  and Accountability, 24, 2 (2012/05/01 2012), 99-111.   [19] Richardson, J. T. E. Approaches to studying across the adult life  span: Evidence from distance education. Learning and Individual  Differences, 26, 0 (8// 2013), 74-80.   [20] Arbaugh, J. B. and Duray, R. Technological and Structural  Characteristics, Student Learning and Satisfaction with Web-Based  Courses: An Exploratory Study of Two On-Line MBA Programs.  Management Learning, 33, 3 (September 1, 2002 2002), 331-347.   [21] Arbaugh, J. B. System, scholar, or students Which most influences  online MBA course effectiveness Journal of Computer Assisted  Learning, 30, 4 2014), 349-362.   [22] Marks, R. B., Sibley, S. D. and Arbaugh, J. B. A Structural Equation  Model of Predictors for Effective Online Learning. Journal of  Management Education, 29, 4 (August 1, 2005 2005), 531-563.   [23] Rienties, B., Kaper, W., Struyven, K., Tempelaar, D. T., Van Gastel,  L., Vrancken, S., Jasinska, M. and Virgailaite-Meckauskaite, E. A  review of the role of Information Communication Technology and  course design in transitional education practices. Interactive Learning  Environments, 20, 6 2012), 563-581.   [24] Tempelaar, D. T., Rienties, B. and Giesbers, B. In search for the most  informative data for feedback generation: Learning Analytics in a  data-rich context. Computers in Human Behavior, 472015), 157-167.   [25] Kirschner, P. A., Sweller, J. and Clark, R. E. Why Minimal Guidance  During Instruction Does Not Work: An Analysis of the Failure of  Constructivist, Discovery, Problem-Based, Experiential, and Inquiry- Based Teaching. Educational Psychologist, 41, 2 (2006/06/01 2006),  75-86.   [26] Koedinger, K., Booth, J. L. and Klahr, D. Instructional Complexity  and the Science to Constrain It. Science, 342, 6161 (November 22,  2013 2013), 935-937.          