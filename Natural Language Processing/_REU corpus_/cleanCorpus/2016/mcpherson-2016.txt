Student perspectives on data provision and use: Starting  to unpack disciplinary differences  Jen McPherson  Faculty of Business and Economics   Macquarie University  North Ryde, NSW, Australia    jen.mcpherson@mq.edu.au     Scott J. Fatt  Undergraduate Merit Scholar   Macquarie University  North Ryde, NSW, Australia   scott.fatt@students.mq.edu.au  Huong Ly Tong  Undergraduate Merit Scholar   Macquarie University  North Ryde, NSW, Australia   huong-ly.tong@students.mq.edu.au     Danny Y.T. Liu  Learning and Teaching Centre   Macquarie University  North Ryde, NSW, Australia  danny.liu@mq.edu.au   ABSTRACT  How can we best align learning analytics practices with  disciplinary knowledge practices in order to support student  learning Although learning analytics itself is an interdisciplinary  field, it tends to take a one-size-fits-all approach to the collection,  measurement, and reporting of data, overlooking disciplinary  knowledge practices. In line with a recent trend in higher education  research, this paper considers the contribution of a realist sociology  of education to the field of learning analytics, drawing on findings  from recent student focus groups at an Australian university. It  examines what learners say about their data needs with reference to  organizing principles underlying knowledge practices within their  disciplines. The key contribution of this paper is a framework that  could be used as the basis for aligning the provision and/or use of  data in relation to curriculum, pedagogy, and assessment with  disciplinary knowledge practices. The framework extends recent  research in Legitimation Code Theory, which understands  disciplinary differences in terms of the principles that underpin  knowledge-building. The preliminary analysis presented here both  provides a tool for ensuring a fit between learning analytics  practices and disciplinary practices and standards for achievement,  and signals disciplinarity as an important consideration in learning  analytics practices.   CSS Concepts   Information systems  Decision support systems  Applied  computing  Education  Human-centered computing   Collaborative and social computing  Social and professional   topics  User characteristics  Software and its engineering   Requirements analysis.   Keywords  Disciplinary differences; student needs; learning analytics;  knowledge; Legitimation Code Theory; sociology of education.   1. INTRODUCTION  If the aim of higher education is to initiate students into the  knowledge practices of knowledge societies [35; 40] through  developing students agency as professionals in their chosen  discipline [8], then what does this mean for learning analytics  practices Recent research in learning analytics points to the  importance of considering disciplinary differences in developing  predictive models [17; 42]. Further, recent research also  underscores the importance of both pedagogy and epistemology  [16; 21]. That is, if the purposes of learning analytics are to  [understand and optimize] learning and the environments in which  it occurs [39], then that purpose is better served when data  collection, measurement, and reporting are grounded within  existing educational research [16]. To date, educational psychology  and sociocultural theory have been valuable sources of theoretical  frameworks to inform learning analytics practices, although both  tend to overlook knowledge in favor of knowing or knowers [28].    A realist sociology of education provides a new lens through which  to view relations between knowledge practices and learning  analytics practices, and analytical and theoretical tools for  describing disciplinary differences. Research from this perspective  indicates that where some intellectual and educational fields tend  to progress through knower-building, emphasizing who you are  [27, p94, p110] (for example, marketing [1], media, and cultural  studies [27]), other fields tend to progress through knowledge- building, emphasizing what and how you know [27; 38] (for  example, law [12], accounting [31], physics, and economics [27]).  It is also possible for a field to progress through both knower- building and knowledge-building, with emphasis on both who you  are and what and how you know (for example, music [27],  English literature [19], and architecture [7]). Put simply, the  differences between sciences and the humanities could be described  as a code clash between knowledge and knower codes [27, p243],  although principles for knower-building and knowledge-building  are not homogenous within knower code and knowledge code  fields [27].    Importantly for learning analytics, principles for knower-building  and knowledge-building also define the dominant basis for  achievement within a discipline [27]. Achievement within knower- code disciplines tends to be defined in terms of learners  dispositions, including aptitude, attitude and personal expression  [27, p78]. Achievement in knowledge-code disciplines tends to be  defined in terms of disciplinary knowledge, skills, and procedures.  Learning analytics practices that take these principles into account  are more likely to support students in participating in learning   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883945     practices that align with disciplinary practices and standards for  achievement.    This paper will present an analytical framework or language of  description [5, p132] that makes disciplinary differences in the use  of analytics for curriculum, pedagogy, and assessment visible. The  analytical framework will frame different uses of data for  knowledge-building and knower-building practices, drawing on  data from a recent series of focus groups at Macquarie University.  The analytical framework is the first stage of a pilot study of  disciplinary differences in the use of data to support learning. The  focus groups were part of a larger project that emphasizes the  importance of student input into learning analytics, a perspective  that recent research suggests is sometimes overlooked in learning  analytics development [13; 14; 33].   The analytical framework will be developed through detailed  analysis of focus group transcripts, drawing on and extending a  framework developed by Chen [10] using Legitimation Code  Theory. Legitimation Code Theory provides analytical tools for  examining knowledge practices, allowing their organizing  principles to be conceptualized, and their effects to be explored  [27, p3]. Chens framework will be adapted to examine what  students say about the provision and use of data. Reflecting our  process in developing the framework for our pilot study, each  dimension of the framework will be presented in turn, supported by  examples from our focus group data. These examples reveal  differences in the provision and/or use of data in relation to  curriculum, pedagogy, and assessment that have significance for  learning analytics design and practice.   1.1 Learning practices, learning analytics  practices, and knowledge practices  As a practice, learning analytics condenses learning practices into  data which are then manipulated in symbolic relations and  relocalized [34] in learning practices, for example as prompts or  visual representations. Data are objects within a system of  representation, and as such, learning analytics creates a semiotic  system that is emergent from but not reducible to, interpersonal  exchange. Learning practices are located within broader networks  of practices which determine the properties of classroom teaching  and knowledge practices [11, p23]. The practice of learning  analytics is similarly located within broader networks of practices,  and it both enables and requires learners to engage in new kinds of  learning practices, for example, the interpretation and critical  analysis of data representations [22]. The effectiveness of learning  analytics systems rests on identifying actionable data that can  improve learning and teaching and course design, rather than  simply data that are easy to capture [37, p12]. At issue here is what  constitutes improvement, although it is noted that this is a question  that extends beyond learning analytics to educational research in  general.    Research within a realist sociology of education argues for a central  focus on knowledge, considering what is learned, as well as the  processes by which learners make shifts from congruent to more  abstract meanings. Legitimation Code Theory provides tools for  examining knowledge practices. Maton [27] provides a  comprehensive overview of two key dimensions of Legitimation  Code Theory: specialization and semantics, and outlines three other  dimensions of Legitimation Code Theory: autonomy, density, and  temporality. A brief summary of key principles in Legitimation  Code Theory can be found in Van Krieken et al. [41].    1.2 Specialization codes  This paper draws on the specialization dimension of Legitimation  Code Theory, which holds that:   [E]ducational practices and contexts represent messages as  to both what is valid to know and how, and also who is an  ideal actor (learner or teacher). That is, every practice or  knowledge claim is by someone (the subject) and is about,  or oriented towards, something (the object) [10, p131]  (italics in original).   The analysis presented here employs two key concepts that together  generate a range of specialization codes: epistemic relations, or  relations between practices and their object or focus and social  relations, or relations between practices and their subject [27,  p52]. The first refers to what can legitimately be described as  knowledge, and the second to who can claim to be a legitimate  knower [27, p52] (italics in original).    As is now standard within Legitimation Code Theory, epistemic  and social relations can each be described in terms of their relative  strength, from stronger (+) to weaker (-). As will become important  in the discussion of extracts from focus groups in Section 3, the  relative strength of epistemic relations and social relations actually  expresses the strength of classification and framingtwo concepts  that are central to Bernsteins [4; 5] work on pedagogic discourse.  Legitimation Code Theory builds explicitly on the work of  Bernstein, among other sociological theories [27]. Briefly,  classification (C) expresses power relations [5] and refers to  'relative strength of boundaries between contexts or categories' [27,  p29]. Framing (F) expresses control relations [5] and refers to 'the  locus of control within contexts or categories (where stronger  framing indicates greater control from above)' [27, p29]. Each can  be described in terms of relative strength: stronger or weaker  classification (+/-C) and stronger or weaker framing (+/-F).  Stronger epistemic relations (ER+) is therefore shorthand for  stronger classification (+C) and stronger framing (+F), and weaker  epistemic relations (ER-) are in effect weaker classification (-C)  and weaker framing (-F). Likewise, stronger social relations (SR+)  can be expressed as stronger classification and framing (+C/+F)  and weaker social relations (SR-) as (-C/-F) [27, p75-76].    Together, the relative strength of epistemic and social relations on  which our analytical framework is based generate four different  modalities or specialization codes. As defined by Maton [27, p30- 31]:     Knowledge codes (ER+, SR-) emphasize the possession of  specialized knowledge of specific objects of study  as the  basis of achievement and the attributes of actors are  downplayed;    Knower codes (ER-, SR+) emphasize the attributes of actors  ... as measures of achievement;    Elite codes (ER+, SR+) emphasize both specialized  knowledge and attributes of actors; and     Relativist codes (ER-, SR-) emphasize neither.   Knowledge code fields progress through knowledge-building,  knower code fields through knower-building, elite code fields  through both knowledge-building and knower building, and  relativist code fields through neither. As noted by Maton [27], in  any context, more than one code may be present and there may be  struggles over which is dominant (p77). There may also be code  clashes, for example between a learners disposition and the  dominant code of their educational context [9], or code shifts, such  as those at different stages of a curriculum [27].      2. METHODOLOGY  The analytical framework to be presented here has been developed  through detailed analysis of transcripts from a series of focus  groups with students at Macquarie University in mid-2015. The  focus groups were designed to gather information about learners  and their learning practices in order to investigate how meaningful  data can be made more accessible to students to improve their  learning. During focus groups, students were first provided with a  definition of learning analytics [39], shown everyday examples of  recommendation engines (eBay and Netflix), then asked what data  related to their learning they would like to have and why they would  like to have it. They were then asked how they would like to receive  this information, and to respond to examples of data dashboards  (Blackboard Analytics, a sample from Corrin and de Barba [13],  and Purdues Course Signals [3]). Although not discussed in detail  here, students were also asked what kind of data they would be  willing to share.    2.1 Participants  A total of 33 students (26 female and 7 male) participated in one  hour focus groups. Participants were recruited through several  channels including announcements posted through the universitys  learning management system, emails to students on established  mailing lists, advertisements posted on the universitys Facebook  page, posters, and flyers. Recruitment continued until data  saturation was reachedthat is, when an informationally  representative sample of students had participated in focus groups.  Participants included 24 undergraduate and 9 postgraduate students  from four out of the five faculties at Macquarie University: 9  students from the Faculty of Arts, 11 students from the Faculty of  Business and Economics, 10 from the Faculty of Human Sciences,  and 4 from the Faculty of Science and Engineering. There were no  participants from the universitys relatively new Faculty of  Medicine and Health Sciences. Although a statistically  representative sample was not required to achieve our research  objectives, the number of participants from each faculty is roughly  proportional to the distribution of students across faculties. Section  3 includes extracts from transcripts. In these extracts, pseudonyms  have been used to refer to participants. Extracts are from  undergraduate students unless stated otherwise.    2.2 Analysis  With students consent, focus groups were recorded and  transcribed, and transcripts were analyzed in NVivo. What students  said about the provision or use of data related to their learning was  examined for messages about what is valid to learn and who is an  ideal learner. The analysis builds on Chens [9, p83] external  language of description (summarized in Table 1) and explained  below. Where Chens framework was developed to analyze  students online learning experiences, it is extended here to  examine the realization of epistemic and social relations in what  students say about the provision or use of data in relation to  curriculum, pedagogy, and assessment practices.   A language of description enables a dialogue between empirical  data and theory, and once established, it makes the analysis visible  to and reproducible by other researchers [27, p137]. As shown in  Chens language of description in Table 1, an emphasis on  epistemic relations in her data was realized in what students said in  relation to content knowledge (curriculum), the teaching of content  knowledge (pedagogy), and explicit evaluative criteria  (assessment). An emphasis on social relations was realized in what  students said in relation to their personal knowledge and experience  (curriculum), personal dimensions of learning (pedagogy), and  self-evaluation (assessment). Following convention within   Legitimation Code Theory, the relative strength or degree of  emphasis on epistemic relations is indicated using ER+/- and the  strength or degree of emphasis on social relations is indicated using  SR+/-.    Table 1: Epistemic and social relations in the focus group data  (adapted from [9, p83]).   Concept Emphasis on   E p  is te  m ic     re la  ti o  n s   Curriculum Content knowledge ER+/-   Pedagogy Teaching of content  knowledge   ER+/-   Assessment Explicit evaluative criteria ER+/-   S o  ci a  l   re la  ti o  n s   Curriculum Learners personal  knowledge and experience   SR+/-   Pedagogy Personal dimensions of  learning   SR+/-   Assessment Learners self-evaluation SR+/-      3. RESULTS AND DISCUSSION  Our analytical framework or language of description was  developed to examine the realization of epistemic and social  relations in our focus group data. As is typical in this kind of  analysis, this involved repeated movements between theory and  data [27, p137]: in our case, this was achieved collaboratively  using Google Docs. Although based on what students have said  about their needs and interests in relation to the provision and/or  use of data, the analytical framework also provides a framework for  differentiating between learning analytics practices that support  knowledge-building and those that support knower-building. We  have used Chens [9] analytical categories (Table 1), but extended  the definition of each category to include what students say about  data or data-driven interventions in relation to curriculum,  pedagogy, and assessment practices.    Extracts from focus group transcripts will be provided to illustrate  each analytical category. Extracts have been selected from the  analysis that exemplify the nature and scope of each category, and  are intended to be informationally rather than statistically  representative of participants contributions to focus groups.    3.1 Curriculum  Following Chens [9] framework (Table 1), comments about the  provision or use of data in relation to curriculum were examined  for a focus on either content knowledge or learners personal  knowledge and experience.    3.1.1 Content knowledge  Student comments that focused on content knowledge were  categorized according to the strength of epistemic relations, or in  other words, the extent to which boundaries between disciplinary  categories are maintained (classification) and the extent to which  control over sequencing, selection, and pacing of content comes  from academic staff (framing). Those statements that associated the  provision and/or use of data with strong classification and framing  of content knowledge (+C, +F) were coded as ER+, while those that  associated the provision or use of data with weak classification and  framing of content knowledge (-C, -F) were coded as ER-.       Curriculum ER+  As shown in Table 2, the indicator for Curriculum ER+ in the focus  group data is the provision or use of data that emphasizes content  knowledge as defining the curriculum and/or legitimate educational  knowledge. Students identified a need for or interest in:    Data that would assist in finding resources that would assist in  understanding key concepts: youve always got lots of  readings to do and you need to know which readings are very   relevant to you, first to better understand the key concepts   covered in the unit and also to prepare us for the assignments  (Yuxi, postgraduate applied linguistics student, FG4);    Data that would help in finding assignment readings: [It  would] probably take the keywords of what your assignment is   about and be like, here's some suggestions for you to get   started (Lauren, first year arts and criminology student,  FG10);    Data that would support exam preparation: So I guess would it  be possible to collect data on things like that, keywords [in   sample exam papers] (Lauren, first year arts and criminology  student, FG10);    Resources used by high-achieving students:  compare which  resources and how many resources [correlate with] higher   scores and also which resources used how will benefit in what   way as well (Helen, first year psychology student, FG8).   Some of these needs could be met through discourse-centric  learning analytics that focus on the advancement of subject  knowledge [20, p188]. Examples of analytics identified by  students were at course level, and include recommendations that  rank relevance of readings based on key concepts in a subject, and  recommendations based on keywords in assignment topics or  sample exam papers.     Curriculum ER-  The indicator for Curriculum ER- in the focus group data is the  provision or use of data that downplays content knowledge in  defining the curriculum and/or what is legitimate educational  knowledge, as shown in the bottom row of Table 2. Here a  distinction is made between learning analytics practices that imply  that content knowledge is legitimate educational knowledge (ER-)  and the previous category where learning analytics practices  emphasize content knowledge as legitimate educational knowledge  (ER+).    In this analytical category, classification and framing are weaker:  objects of study are less bounded and clearly defined, and academic  staff are positioned as having less control over sequencing,  selection, and pacing of content. The needs or interests identified  by students included:    Data on graduate destinations that could be used to inform  subject choices: like how many people got a job in this field  using that degree . I reckon [having data on what subjects   people that got jobs did] would be like pretty helpful (Nadia,  first year arts media student, FG2);    Data on level of difficulty that could be used to inform subject  choices: knowing like the pass rate on like really, really hard  units If you knew the actual pass rate, the most popular grade   that people got, that would be really useful in choosing if I want   to do it or not (Nadia, first year arts media student, FG2).   In these examples, it can be seen that there is a sense of choice over  subject selection and curriculum, although the choices are informed  or mediated by linked data that condenses markers of academic  success within a discipline: getting a job in the field (first year arts  media student, FG2) or grades. Data that support subject choices  based on graduate destinations emphasize the idea that selection,  sequencing, and pacing in a curriculum are arbitrary, downplaying  the organizing principles of content knowledge as the basis for  selection, sequencing, and pacing.   Table 2: Curriculum and epistemic relations   Emphasis Strength Indicators   C on  te nt   k no  w le  dg e   ER+ Provision or use of data that  emphasizes content knowledge as  defining the curriculum and/or  legitimate educational knowledge.   ER- Provision or use of data that downplays  content knowledge in defining the  curriculum and/or what is legitimate  educational knowledge.      3.1.2 Learners personal knowledge and experience  Student comments about personal knowledge and experience were  categorized according to the strength of social relations. Here, the  strength of classification and framing is associated with the  primary experience of knowers [27, p29-30] and the attributes of  knowers, rather than knowledge. For example, in the first two  examples shown in Curriculum SR+, Jingyis emphasis is on the  judgement of students in choosing readings, rather than the content  of the reading, and Alices on the dispositions of students, or what  they are good at.    Curriculum SR+  Table 3 shows that the indicator for Curriculum SR+ in the focus  group data was the provision or use of data that condenses personal  experience, preferences, and opinions as legitimate educational  knowledge. The needs or interests identified by students in this  category included:    Data on popular or useful resources: I know that what kind of  articles other students are reading I think is very useful,   especially when we are doing our assignments (Jingyi,  postgraduate business and economics student, FG11);    Recommendations on subject selections:  it would be good  based on our grades that when we did really well [then we get   advice on] which units we should further undertake, like the   units that we're good at (Alice, second year finance student,  FG3);    Recommendations on subject selections or program choices  linked to interests: it might help with subject selection, other  degrees like if I was thinking about changing my degree or   something like that because you could see that hey look, you're   studying this degree but you spend a lot of your time looking at   this; would this maybe be appropriate for you (Vesna, fourth  year law and economics student, FG11);    Recommendations on subject selections based on similarity of  students experience:  it would be great if  they have a  database where students have done well in this subject and then   a lot of students will undertake this other subject as well, to     continue on because it's like similar (Alice, second year  finance student, FG3);    Representations of popular study pathways: You could do like  a pretty interactive map of the subjects people followed on with.   So if you kind of had your subject selected, it would be like a   bunch of people went over there, and then ... really bolded,   because lots of people went and followed and did [another   unit]. ... Because they're a paired subject, and you can see that.   Then, over here, lots of people went and did that. Then you'd   see a clear kind of path, people following on to these kind of   capstones (Isabelle, first year science and arts student, FG9).   In this category, recommendations at a personal level are based on  the choices of other students. Students frequently commented that  they would like data on what other students are reading. Several  equated popularity with usefulness, as in the first example above  from Jingyi, or as Nadia, a first year arts and media student,  explains: if you're going to write an essay and there's heaps of  resources that you don't know which one to choose, like you can   find the most popular one which will be like the most useful one  (FG2). As there could be other reasons for popularity unrelated to  disciplinary content (for example, popular readings may be short or  easy to read rather than relevant), these are coded as Curriculum  SR+. This is in contrast to recommendations that rank readings  based on relevance to key concepts in the data coded at Curriculum  ER+.   Recommendations at program/department level (for example, in  relation to student pathways) are linked to other data that condenses  students experience or attributes, producing recommendations  based on students strengths or interests. Students position  themselves as playing a significant role in determining what  legitimate knowledge is [9, p128], choosing a path through the  curriculum based on their own or other students experience or  preferences.     Curriculum SR-  The indicator for Curriculum SR- is the provision or use of data that  downplays personal experience, preference, and opinions,  distinguishing these from legitimate educational knowledge.  Comments coded in this category were student concerns about data  that presents personal experience, preferences, and opinions as  legitimate educational knowledge, rather than comments about  needs or interests. Peter, a mature-age postgraduate applied  linguistics student, for example doubted the status of what other  students have done: It may not be the best idea to work on the  basis of what other students have done, but something that I   suggested  was having discipline specific mentors that were able   to advise (FG6).    Table 3: Curriculum and social relations   Emphasis Strength Indicators   Le ar  ne rs   p er  so na  l  kn  ow le  dg e   an d   ex pe  rie nc  e SR+ Provision or use of data that  condenses personal experience,  preferences and opinions as legitimate  educational knowledge.   SR- Provision or use of data that  downplays personal experience,  preferences, and opinions,  distinguishing these from legitimate  educational knowledge.      3.2 Pedagogy  Student comments about the use or provision of data in relation to  pedagogy were examined for a focus on either the teaching of  content knowledge (epistemic relations) or the personal dimension  of the learning process and self-evaluation of the learning process  (social relations). Here, Chens [9] categories have been extended  to take into account the potential of analytics for self-evaluation of  the process of learning (pedagogy). Although Chens language of  description includes self-evaluation, this is limited to the products  of learning (assessment). Even though an assessment task may  assess learning processes as well as products, a basic distinction is  made here between self-regulation of learning (coded at pedagogy)  and learning processes and products associated specifically with  assessment (coded at assessment).    3.2.1 Teaching of content knowledge  As with student comments on content knowledge, student  comments on the teaching of content knowledge were categorized  according to the strength of epistemic relations. Distinctions within  this category are associated with the extent to which procedures for  disciplinary learning are explicit.     Pedagogy ER+  Table 4 shows that the indicator for Pedagogy ER+ is the provision  or use of data that emphasizes content knowledge (what) and makes  procedures for disciplinary learning (how) explicit to students. The  needs or interests identified in this category included:    Data that emphasize study habits appropriate to a discipline:   that would be useful as well, how much time they spend doing   readings, because it varies between subjects. I do a lot of law   subjects, so there's a huge emphasis on the readings whereas   my boyfriend is doing economics he doesnt have many   readings at all. So the way that we study has to be totally   different (Emma, second year psychology and law student,  FG4);    Data on students use of other discipline-specific resources that  emphasize procedures for learning content knowledge: for  exam prep, sometimes people post links to, oh this website was   really helpful, it had great multi choice practice especially in   psych where you dont really get past papers at all (Emma,  second year psychology and law student, FG4).    The needs and interests identified in this category can be  differentiated from those in Curriculum SR+ by an emphasis in  Pedagogy ER+ on explicit discipline-specific procedures for  learning; for example, how much time they spend doing readings  because it varies between subjects (Emma, second year  psychology and law student, FG4) as opposed to a focus on what in  Curriculum SR+.    Pedagogy ER-  The indicator for Pedagogy ER- is the provision or use of data that  emphasizes content knowledge (what) but where procedures for  disciplinary learning (how) are implicit to students, as shown in the  bottom row of Table 4. The needs or interests identified in this  category included data that help students to manage content  learning, with emphasis on what is learned. Examples of needs and  interests include:    Data that indicate activities completed:  back on a unit I did  in the first semester at uni they had actual tick boxes next to all   of thepretty much every resource. That was really fantastic   just to go, oh yeah, I've done that  So having that you need to   this, this is optional, but if you could do that that would be good,     just so that I dont have to write a zillion to do lists every time   I go to study (Emma, second year psychology and law student,  FG4);    Data that indicate activities to be completed: Ideally it would  be a tab in [the learning management system]. So it says, like,   semester two, and it has your list of subjects  you'd have a   semester overview  then you'd have just a running list,   starting with the subject name, ideally. You knowhave week   three reading done, and that would be something that the unit   convenor would set out  So you could have it kind of as a   running feed, as you went through. You'd be able to easily move   forward in a, kind of, auto-prioritized way, without just getting   stuck into one subject. Or you'd be able to see if you were   ignoring one subject, if it was just in one big list (Isabelle, first  year science and arts student, FG9).   While procedures in both of these examples are explicit in the sense  that they specify what students should do and when they should do  it, procedures for disciplinary learning are implicit because the data  provides prompts that are generic to any discipline. These can be  contrasted with examples in Pedagogy ER+ where procedures for  disciplinary learning are explicit.   Table 4: Pedagogy and epistemic relations   Emphasis Strength Indicators   Th e   te ac  hi ng   o f c  on te  nt    kn ow  le dg  e   ER+ Provision or use of data that  emphasizes content knowledge (what)  and makes procedures for disciplinary  learning (how) explicit to students.   ER- Provision or use of data that  emphasizes content knowledge (what)  but where procedures for disciplinary  learning (how) are implicit to  students.       3.2.2 Personal dimension of the learning process and  self-evaluation of the learning process   Student comments that emphasized the personal dimensions of  learning and self-evaluation of learning were examined for the  strength of social relations. Distinctions within this category relate  to the presence or absence of external criteria or standards against  which the personal dimension of the learning process can be  judged.     Pedagogy SR+  The indicator for Pedagogy SR+ is the provision or use of data that  condenses learners choices on how to study or study habits as can  be seen in Table 5. This can be contrasted with procedures for  disciplinary learning in Pedagogy ER+, given that students choices  about how to study may not necessarily match procedures for  disciplinary learning. Further, the emphasis in this category is on  how to study, rather than what and how in Pedagogy ER+.  Comments in Pedagogy SR+ relate to data that support the self- evaluation of learning processes, decisions about how to go about  learning, and social dimensions of learning. Needs and interests  identified in this category emphasize learners dispositions, without  reference to external criteria or standards against which learning is  measured. Examples of needs and interests at a personal level  include:    Data that support time management:  knowing how much  I've done and how that compares to where I should be. So just   setting that goal and making sure I fulfil all my study   commitments so that I can use my time properly (Emma,  second year psychology and law student, FG4);    Data that support scheduling: So if you could set up what you  want to do and having that schedule, personally for me that   would help, being able to go, okay it's Monday afternoon, it's   time to do torts (Emma, second year psychology and law  student, FG4);    Data on own or other students class attendance or online  lecture downloads: I kind of struggle between choosing  whether I should go to a lecture or just listen to it at home    So knowing which of the two is more useful and like which one   people go to the most would be more timesaving (Nadia, first  year arts media student, FG2);    Data on time spent on resources:  just being able to go, oh  yeah, I found this useful or this isn't reallyspend a lot of time   on this one or maybe just skip this (Emma, second year  psychology and law student, FG4);    Data that help in identifying like-minded students: It would be  nice if I can find the people who are reading similar things with   me and I know that he or she is interested in the same stuff as   me (Ailun, postgraduate applied linguistics student, FG3).   In the first quote in this list, Emma is looking to manage her time  against her own goals rather than an external standardin her  words, where I should be in relation to the goals she has set for  herself. This example suggests that from the learners perspective,  representations of her own activity in visual form can act as a proxy  for external standards. Making events and processes visible helps  to change those events [30, p2]. A numerical view is often regarded  as an objective one [32]: the numbers themselves are apparently  neutral, where in reality they are an interpretation of reality, one  that is heavily weighted in favor of what the [analyst] is able to  measure and chooses to measure [32, p480] depending on their  choice of measurement tool. Visual representations of data of this  type condense learning activity as 'things'semiotic objects which  take on a sense of objectivity.    In the shift towards representation, that is, when learning activity is  represented visually as a thing, truth becomes encoded in the  conventions of visual display. As Kress and van Leeuwen observe,  visual modality rests on culturally and historically determined  standards of what is real and what is not, and not on the objective  correspondence of the visual image to a reality defined  independently of it [23, p52]. Data has limits as a measure of  student performance or intention: Corrin and de Barba, for  example, note the partiality of data, and register concern about  heavy reliance on quantitative representations of student activity  [13, p1]. As noted by Sharples et al. [37, p13], students learning  behavior depends upon a complex interaction of personal,  emotional, social, and economic factors and these may not be  directly observable or measurable. As with representations of  interpersonal exchange in educational practice [29], visual or other  abstract representations of learning practices (e.g. models and  formulae) are partial because they only partially relocalize, and  cannot logically contain, the constellational identity [6, p.xix] of  learning interactions condensed within them.       Pedagogy SR-  The indicator for Pedagogy SR- in the data is the provision or use  of data that link study habits with grades or subject requirements to  provide an external standard for self-evaluation of learning, as  shown in the bottom row of Table 5. Needs and interests identified  by students in this category included:    Data on amount of time spent studying by high-achieving  students: So like, I dont know, if the person who topped the  unit spent X amount of time on the [learning management   system] page and made sure they did that, I reckon that would   be pretty useful to kind of make sure I'm aiming for that  (Emma, second year psychology and law student, FG4);    Data on study strategies of high-achieving students: I reckon  something that would be useful, too, is knowing what times the   top students like start working on assignments, like is it two   weeks before it's due, three weeks, as soon as they get it. That'd   be heaps useful (Nadia, first year arts media student, FG2)    Data on study pathways or load taken by high-achieving  students: I plan to move down from four to three subjects later  on, because it is going to be really full on and I've got other   things to do. To get an idea of whether thatI'm stepping out   of the herd and that's a bad idea. Or if I'm stepping away from   the high achievers in doing that. To be able to guide my plans.  (Isabelle, first year science and arts student, FG9)    Personalized warnings: Sometimes you fall behind and it's  good to have a bit of advance notice if you're not quite keeping   up, if you're falling behind on the lectures. It would be good to   have some kind of warning in place, I dont know, just for me  (Emma, second year psychology and law student, FG4)   As in the category Curriculum ER-, it can be seen that analytics  suggested by students in these examples are mediated by linked  data that condenses markers of academic success.    Table 5: Pedagogy and social relations   Emphasis Strength Indicators   Pe rs  on al   d im  en si  on  o  f  th  e  le  ar ni  ng  p  ro ce  ss  a  nd    se lf-  ev al  ua tio  n  of    le  ar ni  ng  p  ro ce  ss    SR+ Provision or use of data that  condenses learners choices on how  to study or study habits.   SR- Provision or use of data that links  study habits with grades or subject  requirements to provide an external  standard for self-evaluation of  learning.      3.3 Assessment  Student comments about the use or provision of data in relation to  assessment were examined for a focus on either explicit evaluative  criteria (epistemic relations) or self-evaluation of the products of  learning (social relations). As noted above, a distinction was made  between self-evaluation of the products of learning (in this  category) and self-evaluation of the process of learning (in  Pedagogy SR+/-), acknowledging that the process of learning may  also be assessable.   3.3.1  Explicit evaluative criteria  As in previous categories that focus on epistemic relations, student  comments on explicit evaluative criteria were also categorized   according to the strength of epistemic relations. Distinctions within  this category are associated with the presence or absence of  evaluative criteria such as marking criteria.     Assessment ER+  Moving on to Table 6, it can be seen that the indicator for  Assessment ER+ in the data is the provision or use of data that  emphasizes explicit evaluative criteria in judging products of  learning. The needs or interests identified in this category included:    Data that provide personalized feedback with reference to  marking criteria: But even if you click on it and just be like,  here's some feedback from the lecturer or your tutor here. Why   are you going good Why are you not going good Why have   you failed (Lauren, first year arts and criminology student,  FG10)    Data that provides personalized feedback with reference to  subject learning outcomes:  summative can help  [but] it's  really just how am I going and in terms of class average or   cohort average. It's how do I compare with others, which I   dont think is the best way of making a comparison. I think that   the best way of making a comparison is what do I expect   Where should I be in relation to what I should be learning If   I'm not at that stage, how can I get there (Peter, postgraduate  applied linguistics student, FG6)   As can be seen in the comments from Peter, a distinctive feature of  this analytical category is comparing performance against criteria  rather than against the performance of other students. In this  instance, while he starts out to say that he thinks the best point of  comparison is what do I expect which would be consistent with  Assessment SR-, he reformulates this as where he should be in  relation to what [he] should be learning. This statement has been  interpreted with reference to his other contributions to discussion,  and what I should be learning is taken to be an external standard.     Assessment ER-  As shown in the bottom row of Table 6, the indicator for  Assessment ER- in the data is the provision or use of data that  downplays explicit evaluative criteria. The needs or interests  identified by students in this category included:    Direct interventions based on performance:  you could use  that data to send automated things and say, just to let you know   that you would have to get an average mark of 89 per cent in   the next three assessments if you want to pass the subject; if   they were linking in your analytics, sayingyeah, like I've had   a look. Just want to remind you of where your average is   compared to the cohort. Just want to let you knowif it was   like an automated thingPS, you're on a track to fail (Isabelle,  first year science and arts student, FG9); Probably they could  send something like notifications if you're doing really badly in   the subject as well, to warn you (Tim, second year business  and economics student, FG2);    Indirect interventions based on performance: Maybe as an  option you can request your ranking and they'll send it to you   after you request it (Hongfei, first year business and  economics student, FG2); Some people don't want to check the  rankings (Ishrak, first year information technology student,  S06, FG2); Maybe if there was like an option on [the learning  management system], like when you go on the homepage you   can like click your profile or something like that then it's like   your grades and everything's just there (Nadia, first year arts  media student, FG2).     The terms direct and indirect are used here to differentiate  between targeted personalized interventions such as a warning  message by email or some other channel (direct), and less direct  interventions such as a data dashboard that students can choose to  access or not (indirect). Although a grade is a measure of success,  criteria for success are condensed into a single numerical figure,  differentiating this category from Assessment ER+.   Table 6: Assessment and epistemic relations   Emphasis Strength Indicators   Ex pl  ic it   ev al  ua tiv  e  cr  ite ria     ER+ Provision or use of data that  emphasizes explicit evaluative criteria  in judging products of learning   ER- Provision or use of data that downplays  explicit evaluative criteria      3.3.2 Self-evaluation of the products of learning  Student comments that emphasized self-evaluation of the products  of learning were examined for the strength of social relations.  Paralleling distinctions in self-evaluation of the process of learning  in Pedagogy SR+/-, the two strengths of social relations (SR+ and  SR-) in this category could be distinguished by the presence or  absence of external criteria or standards against which the products  of learning can be judged, or in other words, the presence of  external criteria (Assessment SR-) versus the absence of external  criteria or standards (Assessment SR+). As distinct from  Assessment ER+, this category includes comments related to self- assessment, rather than assessment by academic staff.     Assessment SR+  The indicator for Assessment SR+ in the data is the provision or  use of data that emphasizes learner beliefs in evaluating the  legitimacy of products of learning (Table 7). Comments coded in  this category included:    Beliefs about assessment criteria and criteria for academic  success: I think that there is more to academic success than  just getting numbers on a transcript. I think that that's - that's   come about because talking to friends at different universities,   it feels like sometimes the amount of work needed to get those   numbers varies across the board, and what that means  it's   definitely possible that there's not this one standard, one level   playing field that everyone is in (Vesna, fourth year law and  economics student, FG11);    Beliefs about evaluation: Because I know people say that a  particular unit's really hardlike [unit name] for example, it   has quite a high fail rate, but I managed to get a high distinction   in it (Jessie, third year commerce and arts student, FG5).    Assessment SR-  Finally, as can be seen in the bottom row of Table 7, the indicator  for Assessment SR- in the data is the provision or use of data that  allows for self-evaluation of products of learning with reference to  shared criteria external to the learner. The needs or interests  identified by students in this category included:    Comparisons with current cohort: Just something that I would  want to know. I want to know if my 60 grade that I thought okay   that's an alright pass, but I'd want to know if the class average   was 85 or 55  So whether that's relatively good or not  (Jessie, third year commerce and arts student, FG5);    Individual comparisons with previous cohorts:  you're  performing in the twentieth percentile. People in the twentieth   percentile last year got this final mark (Isabelle, first year  science and arts student, FG9);    Cohort level comparisons with previous cohorts:  the whole  cohort's data is consolidated and then they tell us, how well   your cohort did in comparison to other cohorts (Alice, second  year finance student, FG3).   In contrast to the examples in Assessment SR+, the emphasis here  is on shared criteria. In these examples, the shared criteria are the  standards set by the current or previous cohort. While external  standards in Pedagogy SR- are based on the performance of  particular groups of students (e.g. high-achieving students),  standards in this category refer to the performance of more general  cohorts of students. Here, students evaluate themselves with  reference to the standard set by that cohort. Where the emphasis in  the category Assessment ER- is on particular criteria for success  (grades and rankings), emphasis in Assessment SR- is on self- evaluation with reference to shared criteria (the standard set by the  cohort).   Table 7: Assessment and social relations   Emphasis Strength Indicators   Se lf-  ev al  ua tio  n  of    pr  od uc  ts  o  f l ea  rn in  g   SR+ Provision or use of data that  emphasizes learner beliefs in  evaluating the legitimacy of products  of learning   SR- Provision or use of data that allows for  self-evaluation of products of learning  with reference to shared criteria  external to the learner      4. CONCLUSIONS  A realist sociology of education brings knowledge into focus,  making visible principles of knowledge-building and knower- building within disciplines. These principles have significance for  learning analytics, because they define the primary basis for  achievement within disciplines.    Our paper has drawn on student perspectives on the provision and  use of data. As noted earlier, recent literature has identified gaps in  our understanding of student perspectives on learning analytics. A  small number of studies have begun to address this gap: Sclater  [36], for example, documents student participation in identifying  user requirements for a student-facing learning analytics app, while  Corrin and de Barba [13; 14] focus on students interpretation of  feedback delivered through dashboards. The latter study also fits  within a related strand of research literature that calls for a more  nuanced [15, p232] approach to analytics; for example, one that  seeks to understand learning analytics within disciplinary contexts  [17] and that moves beyond the predictive analytics paradigm [26].  We have sought to connect these two strands of research in an  analytical framework or language of description that differentiates  between different uses of data in relation to curriculum, pedagogy,  and assessment on the basis of their underlying epistemic or social  relations. Our theoretical approach parallels other recent work that  applies tools from Legitimation Code Theory in investigating the  integration of educational technology [18]. While the focus of  research by Howard et al. is on the use of laptops and information     and communication technologies in the classroom, their  recommendation that [r]elevant and appropriate technology uses  that match with underlying principles of subject areas, such as the  nature of success and relations to knowledge embodied in  technology-related practices, need to be identified and explicated  [18, p368] also has significance within the field of learning  analytics.    The analytical framework or language of description presented here  is a step towards a more granular understanding of learning  analytics practices with reference to principles of knowledge- building and knower-building practices. These principles are at the  center of disciplinarity: disciplinary differences in pedagogy,  curriculum, and assessment reflect core features, values and  underlying principles of subject areas [18, p362] (italics in  original). A language of description such as the one developed in  this paper exemplifies a knowledge-code (ER+, SR-): in making  the analysis visible to other researchers, who you are is  downplayed as the basis for legitimacy  Anyone who  understands the theory can see if the analysis is consistent with the  data and conclusions borne out by evidence [27, p137].    Our next step will be to apply this analytical framework to more  detailed examination of our focus group data to explore patterns of  disciplinary differences. Preliminary analysis suggests that student  perspectives on the provision or use of data frequently runs counter  to the dominant code suggested by their disciplinary affiliation.  These code clashes sound a warning for the field of learning  analytics, highlighting the potential of learning analytics practices  not only in the construction, reproduction, and transformation of  disciplinary identities, but also in their destruction.    Ideally, learning analytics would contribute to building students  capacity for reflexivity, or making conscious learning choices  within the context of their discipline. As sense-makers with  agency [25, p178] students can use data to support their inner  dialogues [24; 25] or internal conversations [2]. The practice of  learning analytics generates tools that enable students to self- regulate their learning, but at the same time, has the potential to  displace disciplinarity and create a vacuum of legitimacy [10].  Instead of a concern with knowledge-building consistent with their  discipline, students may be co-opted into utilitarian and  organizational discourses in which data-driven learning practices  are legitimated with reference to progress, rational economic  behavior, and quantification, and organizational demands for  quality, efficiency, and accountability. Depending on the degree of  autonomy over curriculum, that is, who controls curriculum  decisions and on what basis, it may be shaped by external drivers,  for example, discourses of employability which increasingly  [privilege] a knower code  with the emphasis on attributes and  ways of being [38, p6]. Without a more nuanced approach [15,  p232], learning analytics has similar potential to shape the  curriculum, through enabling new kinds of learning practices that  favor efficient and accountable ways of being over disciplinary  knowledge-building or knower-building.   5. ACKNOWLEDGEMENTS  This work was funded as part of a larger Innovation and  Scholarship Program project, supported by the Education Studio at  Macquarie University. H.L.T. and S.J.F. were supported through  Macquaries undergraduate Merit Scholars program. We wish to  thank the students who participated in our focus groups, and the rest  of the project team, for offering their insights. We would also like  to thank three anonymous reviewers for their valuable feedback.   6. REFERENCES  [1] Arbee, A., Hugo, W. and Thomson, C. (2014)   Epistemological access in marketing: A demonstration of the  use of Legitimation Code Theory in higher education.  Journal of Education, 59(39-63).   [2] Archer, M. (2003) Structure, agency and the internal  conversation. Cambridge University Press, Cambridge.   [3] Arnold, K. E. (2010) Signals: Applying Academic Analytics.  Educause Quarterly, 33(1), n1.   [4] Bernstein, B. (1977) Class, codes and control, Vol III: The  structuring of pedagogic discourse. Routledge, London.   [5] Bernstein, B. (2000) Pedagogy, symbolic control and  identity: Theory, research, critique.  Revised edition.  Rowman and Littlefield, Lanham.   [6] Bhaskar, R. (1975) A realist theory of science. Routledge.  [7] Carvalho, L., Dong, A. and Maton, K. (2009) Legitimating   design: A sociology of knowledge account of the field.  Design Studies, 30(5), 483-502.  http://dx.doi.org/10.1016/j.destud.2008.11.005   [8] Case, J. M. (2013) Researching Student Learning in Higher  Education: a social realist approach. Routledge, London.   [9] Chen, R. T.-H. (2010) Knowledge and knowers in online  learning: Investigating the effects of online flexible learning   on student sojourners. Thesis: University of Wollongong,  Wollongong.   [10] Chen, R. T.-H., Maton, K. and Bennett, S. (2011). Absenting  discipline: Constructivist approaches in online learning. In F.  Christie and K. Maton (Eds.), Disciplinarity: Functional  linguistic and sociological perspectives (pp. 129-150).  London: Continuum.   [11] Chouliaraki, L. and Fairclough, N. (1999) Discourse in late  modernity: Rethinking Critical Discourse Analysis.  Edinburgh University Press, Edinburgh.   [12] Clarence, S. L. (2013) Enabling cumulative knowledge- building through teaching: A legitimation code theory   analysis of pedagogic practice in law and political science.  Thesis: Rhodes University, Grahamstown, South Africa.   [13] Corrin, L. and de Barba, P. (2014) Exploring students  interpretation of feedback delivered through learning  analytics dashboards. In Proceedings of the 31st Annual  Conference of the Australasian Society for Computers in   Learning in Tertiary Education (ascilite 2014), Dunedin.  [14] Corrin, L. and de Barba, P. (2015) How do students interpret   feedback delivered via dashboards In Proceedings of the  Fifth International Conference on Learning Analytics And   Knowledge. http://dx.doi.org/10.1145/2723576.2723662  [15] Dawson, S., Gaevi, D., Siemens, G. and Joksimovic, S.   (2014) Current state and future trends: A citation network  analysis of the learning analytics field. In Proceedings of the  Fourth International Conference on Learning Analytics And   Knowledge, Indianapolis.  http://dx.doi.org/10.1145/2567574.2567585   [16] Gaevi, D., Dawson, S. and Siemens, G. (2015) Lets not  forget: Learning analytics are about learning. TechTrends,  59(1), 64-71. http://dx.doi.org/10.1007/s11528-014-0822-x   [17] Gaevi, D., Dawson, S., Rogers, T. and Gasevic, D. (2016)  Learning analytics should not promote one size fits all: The     effects of instructional conditions in predicting academic  success. The Internet and Higher Education, 28, 68-84.  http://dx.doi.org/10.1016/j.iheduc.2015.10.002   [18] Howard, S. K., Chan, A. and Caputi, P. (2015) More than  beliefs: Subject areas and teachers integration of laptops in  secondary teaching. British Journal of Educational  Technology, 14(2), 360-369.  http://dx.doi.org/10.1111/bjet.12139   [19] Jackson, D. (2014) Knowledge and knowers by Karl Maton:  A review essay. Journal of Education, 59, 127-146.   [20] Knight, S. and Littleton, K. (2015) Discourse-centric  learning analytics: Mapping the terrain. Journal of Learning  Analytics, 2(1), 185-209.   [21] Knight, S. J., Buckingham Shum, S. and Littleton, K. (2014)  Epistemology, assessment, pedagogy: Where learning meets  analytics in the middle space. Journal of Learning Analytics,  1(2), 23-47.   [22] Kop, R. and Fournier, H. (2011) New dimensions to self- directed learning in an open networked learning  environment. International Journal of Self-Directed  Learning, 7(2), 2-20.   [23] Kress, G. and van Leeuwen, T. (1990) Reading images.  Deakin University Press, Geelong, Victoria.   [24] Linell, P. (2009) Rethinking language, mind, and world  dialogically. Information Age Publishing, Charlotte, NC.   [25] Linell, P. (2014) Interactivities, intersubjectivities and  language: On dialogism and phenomenology. Language and  Dialogue, 4(2), 165-193.  http://dx.doi.org/10.1075/ld.4.2.01lin   [26] Liu, D. Y. T., Rogers, T. and Pardo, A. (2015) Learning  analytics - are we at risk of missing the point In  Proceedings of the 32nd ascilite conference, Perth.   [27] Maton, K. (2014) Knowledge and knowers: Towards a  realist sociology of education. Routledge, London.   [28] Maton, K., Carvalho, L. and Dong, A. (2016). LCT in praxis:  Creating an e-learning environment for informal learning of  principled knowledge. In K. Maton, S. Hood and S. Shay  (Eds.), Knowledge-building: Educational studies in  Legitimation Code Theory. London: Routledge.   [29] McPherson, J. (2014) Comparing apples with apples:  Professional accounting practices in university classroom   discourse. Thesis: University of Sydney, Sydney.  [30] Miller, P. (1994). Accounting as social and institutional   practice: An introduction. In A. G. Hopwood and P. Miller  (Eds.), Accounting as social and institutional practice (pp. 1- 39). Cambridge: Cambridge University Press.   [31] Mkhize, T. (2015) An Analysis of the Certificate of the  Theory of Accounting Knowledge and Knower Structures: A   case study of professional knowledge. Thesis: Rhodes  University, South Africa.   [32] Morgan, G. (1988) Accounting as reality construction:  Towards a new epistemology for accounting practice.  Accounting, Organizations and Society, 13(5), 477-485.  http://dx.doi.org/10.1016/0361-3682(88)90018-9   [33] Newland, B., Martin, L. and Ringan, N. (2015) Learning  analytics in UK HE 2015: A HeLF survey report: HeLF   Heads of e-Learning Forum. Available:  http://www.helf.ac.uk/    [34] Pennycook, A. (2010) Language as a local practice.  Routledge, London.   [35] Scardamaila, M. and Bereiter, C. (2006). Knowledge  building: Theory, pedagogy and technology. In K. Sawyer  (Ed.), Cambridge handbook of the learning sciences (pp. 97- 118). New York: Cambridge University Press.   [36] Sclater, N. (2015) What do students want from a learning  analytics app Available:  http://analytics.jiscinvolve.org/wp/2015/04/29/what-do- students-want-from-a-learning-analytics-app/   [37] Sharples, M., Adam, A., Ferguson, R., Gaved, M.,  McAndrew, P., Rienties, B., Walker, M. and Whitelock, D.  Innovating Pedagogy 2014: Open University Innovation   Report 3. The Open University, Milton Keynes, 2014.   [38] Shay, S. (2015) Curricula at the boundaries. Higher   Education, 1-13. http://dx.doi.org/10.1007/s10734-015-9917- 3   [39] Siemens, G. and Gaevi, D. (2012) Guest editorial-learning  and knowledge analytics. Journal of Educational Technology  & Society, 15(3), 1-2.   [40] Stehr, N. (1994) Knowledge Societies: The Transformation  of Labour, Property and Knowledge in Contemporary   Societies. Sage, London.  [41] Van Krieken, R., Smith, P., Habibis, B., Smith, P., Hutchins,   B., Martin, G. and Maton, K. (2010) Sociology: Themes and  perspectives. Pearson, Sydney.   [42] Wolff, A., Zdrahal, Z., Nikolov, A. and Pantucek, M. (2013)  Improving retention: predicting at-risk students by analysing  clicking behaviour in a virtual learning environment. In  Proceedings of the 3rd international conference on learning   analytics and knowledge, Leuven.  http://dx.doi.org/10.1145/2460296.2460324             