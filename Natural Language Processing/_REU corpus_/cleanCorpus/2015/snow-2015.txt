Youve Got Style: Detecting Writing Flexibility Across Time     Erica L. Snow   Arizona State University  Tempe, AZ   Erica.L.Snow@asu.edu     Cecile A. Perret  Arizona State University   Tempe, AZ  Cecile.Perret@asu.edu   Laura K. Allen  Arizona State University    Tempe, AZ  LauraKAllen@asu.edu   Matthew E. Jacovina  Arizona State University   Tempe, AZ  Matthew.Jacovina@asu.edu              Danielle S. McNamara   Arizona State University   Tempe, AZ   Danielle.McNamara@asu.edu      ABSTRACT  Writing researchers have suggested that students who are  perceived as strong writers (i.e., those who generate texts that are  rated as high quality) demonstrate flexibility in their writing style.  While anecdotally this has been a commonly held belief among  researchers, scientists, and educators, there is little empirical  research to support this claim. This study investigates this  hypothesis by examining how students vary in their use of  linguistic features across 16 prompt-based essays. Forty-five high  school students wrote 16 essays across 8 sessions within an  Automated Writing Evaluation (AWE) system. Natural language  processing (NLP) techniques and Entropy analyses were used to  calculate how rigid or flexible students were in their use of  narrative linguistic features over time and how this trait related to  individual differences in literacy ability and essay quality.  Additional analyses indicated that NLP and Entropy reliably  detected narrative flexibility (or rigidity) after session 2 and was  related to students prior literacy skills. These exploratory  methodologies are important for researchers and educators, as  they indicate that writing flexibility is indeed a trait of strong  writers and can be detected rather quickly using the combination  of textual features and dynamic analyses.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education] Computer-assisted  instruction (CAI); I.2.7 [Natural Language Processing] Text  analysis, discourse   General Terms  Measurement, Performance, Experimentation, Theory.    Keywords  Individual Differences, Writing Quality, Narrativity, Entropy,  Flexibility.   1. INTRODUCTION  One way in which people communicate ideas with others is  through effective writing. Strong writers successfully maneuver a  variety of subtasks while generating text: coordinating  information, setting goals, solving problems, and regulating their  knowledge sources [1]. The ability to effectively communicate  through written text is crucial for professional and academic  success [1, 2]. However, many students struggle with this  complex cognitive task [3]. Indeed, within the United States,  students struggle to master the required skills needed to  effectively write. In 2011, approximately only a quarter of  students in 8th through 12th grade scored at a proficient level on a  nationwide computer-based writing assessment [4]. These  findings demonstrate a need for researchers to understand the  writing process and skills that can be related to the mastery of this  complex task.    1.1 Narrativity  To better understand how students writing skills can be  improved, researchers have begun to examine the characteristics  of high quality writing [5, 6]. Recently, researchers have argued  that differences in writing quality can be detected based on the  linguistic features embedded within students essays. One such  linguistic feature that may be critical to essay quality is  narrativity. Narrativity has previously been identified as an  important component of writing through the use of natural  language processing techniques [7, 8]. Conceptually, the idea of a  text being highly narrative is straightforward: texts that are high in  narrativity have more story-like elements, including recurring  characters and unfolding events. Generally, highly narrative texts  are easier to understand and remember than texts that contain few  narrative elements (e.g., informational texts; [9, 10]). Readers  ease of processing narrative texts makes sense given that peoples  everyday conversations and other experiences with language are  replete with narrative elements [11, 12]. Educators have thus  made logical arguments that students writing (including non- fictional writing) should include narrative elements as a means to  increase reader engagement [13].   Although some students, in some contexts, would likely benefit  from including more narrative elements in their writing, research  has not supported the idea that successful essays typically have  higher levels of narrativity. One analysis, for example, found that  in a corpus of prompt-based, argumentative essays written by  college freshman, higher scores from expert graders were  associated with lower narrativity [14]. Although texts that were   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for the components of this work by others than ACM must be honored.  To copy otherwise, or republish, to post on servers or to redistribute to  lists, requires prior specific permission and/or a fee. Request permissions  from Permissions@acm.org.    LAK15, March 1620, 2015, Poughkeepsie, NY, USA.  Copyright 2015 ACM 978-1-4505-3417-4/15/03$15.00  http:/dx.doi.org/10.1145/2723576.2723592     194    higher in narrativity may have been easier to read, they may have  lacked informational content that is crucial for composing a  successful argumentative essay. However, not all successful  essays follow the same formula. In a different analysis, four  distinct linguistic profiles were identified for highly scored  argumentative essays [15]. Essays in one profile (described as  academic) seemed to be successful due to their structural  components, syntactic complexity, and their relatively infrequent  use of causal verbs and pronouns, which are often found in  narrative texts. Essays in another profile (described as accessible),  however, contained relatively more causal connectives and verbs  common to narrative writing. These two divergent profiles of  successful essays highlight the difficulty of understanding what  makes a successful writer.    1.2 Flexibility  Researchers have also begun to investigate how individual  differences may be related to writing quality [3, 16, 17, 18]. For  instance, Graham and Harris proposed that individual differences  in self-regulation might be related to writing ability. Similarly,  Shannon and Tierney found that students reading comprehension  ability was related to their ability to effectively write. Thus,  students who are better at comprehending text are also better at  generating it [18].    Another proposed characteristic of strong writers is their  flexibility in writing style [3]. Skilled writers do not simply reuse  the language or linguistic features from previous writing tasks;  instead, they assess each writing situation and adapt their style  accordingly [3]. However, one problem that researchers have  encountered when trying to examine the link between flexibility  and writing quality (or any other individual differences) is that  flexibility is hard to measure. Specifically, researchers have had  difficulty in finding a way to reliably measure the construct of  writing flexibility. Thus, even though flexibility is often assumed  to be an important characteristic of strong writers, few studies  have explicitly investigated this hypothesis.    Allen and colleagues recently completed one of the first empirical  investigations into writing flexibility [19]. By using NLP and  dynamic methodologies to investigate SAT style prompt-based  essays, they found that students who had higher literacy skills  generally used narrative elements flexibly across a series of SAT  style prompt-based essays. Allen and colleagues also showed that  narrative flexibility was significantly related to essay quality [19].  This work provides a starting point for researchers to begin to  understand the construct of writing flexibility. However, an  important question that has not yet been investigated is how  writing flexibility manifests and evolves across time. Specifically,  Allen and colleagues examined aggregated flexibility across 16  essays using a random walk distance score (see [20] for detailed  analysis of random walk analyses) [19]. Although this points to  the importance of flexibility, it does not reveal when flexibility  can be detected and how this skill evolves across time, both of  which must be understood to effectively guide instruction for  students who are inflexible in their writing.    One reason for the lack of studies that focus on the emergence of  writing flexibility across time is that research has predominantly  focused on traditional (summative) assessments of writing, which  lack the nuances required to investigate flexibility. Indeed, the  previously mentioned work is perhaps the sole writing study to  measure writing flexibility using online metrics (i.e., linguistic  features of essays). The current work builds upon that study by  using additional covert measures to capture nuanced changes as  they manifest across time.    1.3 Stealth Assessments  Traditional, summative assessments often take the form of tests or  graded assignments, and are intended to measure students  learning. Formative assessments, however, are intended to gauge  students current knowledge or skills in order to support more  successful instruction in the future. Formative assessment, when  well implemented, has been found to be beneficial to student  learning [21]. Thus, developing meaningful formative assessments  is an important, albeit difficult, goal for educators and is essential  for the success of computer-based, educational environments that  aim to guide instruction based on the needs of individual students  [22]. One way to capture students behaviors and knowledge  without disrupting their experience within a computer-based  system is through stealth assessment [23, 24]. Stealth  assessments, like formative assessments, are intended to measure  student characteristics (e.g., their current content knowledge or  level of engagement) with the ultimate goal of adapting  instruction and providing formative feedback based on those  characteristics; but importantly, stealth assessments are measured  through students normal use of an educational system. That is,  stealth measures do not require students to report on their  perceptions of their own behavior and knowledge, which are often  inaccurate when compared to students actual behavior and  knowledge [25].    Stealth assessments can take many forms, and can be used to  measure numerous constructs. For example, in Zoo U, an  educational game designed to assess young students social skills,  students clicking behavior in animated scenes is used to measure  impulse control [26]. This stealth assessment is built directly into  the normal interaction with the environment, not requiring  students to disengage from the activity to answer questions about  how impulsive they feel. Other research has used stealth  assessments to measure amount of exerted agency [27] and  gaming behaviors [28]. For a system that has students write  essays, attributes of writing ability can be assessed that go beyond  the traditional summative goal of measuring overall essay quality.    1.4 Current Study   This study builds upon and expands the work by Allen and  colleagues by proposing that a successful writer should flexibly  adapt across time, depending on the context of the writing task  and their literacy abilities [19]. We are hypothesizing that this trait  will become detectable relatively quickly. This study further  examines the extent to which the relation between narrative  flexibility and students literacy skills remains stable across time.  To test our hypotheses, we employ a stealth measure of writing  flexibility that emerges through the dynamic analysis of students  use of narrativity across multiple essays. The current study uses  natural language processing (NLP) and Entropy analysis to  investigate whether individual differences related to writing  proficiency are associated with students flexible use of linguistics  features across multiple prompt-based essays. The overarching  goal of this work is to begin to shed light upon the complex  interplay of flexibility, individual differences, and writing skill.  By identifying when students become more or less flexible,  instruction could be adapted for individual students (e.g.,  providing flexibility training for students who are found to be  rigid). The primary research questions of this study are listed  below.      195    1) At what point across multiple essays is writing  flexibility associated with individual differences in prior  literacy skill and essay quality     2) How many essays are required to detect the relation   between literacy skills and flexibility in writing style      1.5 Automated Writing Evaluation Systems  To answer the above questions, we employ the use of an  Automated Writing Evaluation (AWE) system. AWEs offer  students the opportunity to practice writing and automatically  receive scores and feedback on their essays [29, 30]. This form of  deliberate practice (i.e., practice with individualized feedback) is  critically important in order to develop strong writing skills; AWE  systems, therefore, can benefit students by providing them with  relevant feedback, which provides more beneficial writing  practice opportunities and simultaneously reduces burdens on  instructors. The algorithms that drive the scores provided by these  systems have demonstrated fairly high reliability and accuracy  (e.g., [31, 32]). The scores provided by expert human raters and  computers tend to correlate between r = 0.80 and 0.85, which is a  similar range to the range found between human raters [33, 32].  Similarly, they tend to report perfect agreement between 30-60%  and adjacent agreement (within one point of each other) between  85-100%. Overall, AWE systems are beneficial for students  because they can provide immediate summative feedback to  students on their essays  all without any input from the  instructor. Because of these features, a number of AWE systems  have been developed and are now being used in classrooms, such  as Criterion [31], MyAccess [33], WriteToLearn [34], and WPP  Online [35].    Relevant to the current study, Writing Pal (W-Pal) is an intelligent  tutoring system that was developed to provide high school and  entering college students with writing strategy instruction and  practice [36, 37]. W-Pal contains an AWE component that allows  students to write essays and receive both summative and  formative feedback automatically. The system includes a basic  word processor (see Figure 1), which allows students to write  essays based on a number of pre-determined prompts  (alternatively, teachers can add prompts for students to respond  to). When a student finishes writing an essay, it is submitted to the  W-Pal system and the students essay is provided with a holistic  score and formative feedback (see Figure 2 for a screenshot of the  feedback screen). Finally, after reviewing this feedback, students  have the opportunity to revise their essays.   An important aspect of the W-Pal system is the provision of high- level feedback. Specifically, the feedback in the W-Pal system  focuses on strategies that students can enact when they revise their  essays [38]. For example, if a student submits an essay to W-Pal  that contains no structure (i.e., it is only one paragraph long), the  feedback may focus on strategies that can improve essay  organization, such as the use of flow charts or outlines to visualize  the structure of an essay.   2. METHOD  2.1 Participants  The current study is part of a larger study (n = 86), which  compared a writing tutor (Writing Pal) to an Automated Writing  Evaluation (AWE) system [36]. The analyses presented here focus  solely on the participants who engaged with the AWE system (n =  45). All 45 participants were high-school students recruited from a   southwest urban environment located in United States. These  students were, on average, 16.4 years of age, with a mean reported  grade level of 10.5. Of the 45 students, 66.7% were female. In this  study, students self-reported ethnicity breakdown was 62.2%  were Hispanic, 13.3% were Asian, 6.7% were Caucasian, 6.7%  were African-American, and 11.1% reported other.    2.2 Study Procedure  The current study was comprised of a 10-session experiment.  During the first session, students completed a pretest that  contained measures of prior writing ability, prior world  knowledge, reading ability, and vocabulary knowledge. During  the following eight sessions, students wrote two essays per day  (for a total of 16) in the AWE system. Finally, during the last  session, students completed a posttest that contained measures  similar to the pretest.   2.2.1 Pretest   During session 1, all students completed a battery of individual  difference measures. This battery of measures took approximately  1 hour to complete and included demographics, prior world  knowledge, prior reading ability, prior vocabulary knowledge and  writing proficiency (25-minutes SAT-style essay).   2.2.2. Training  During sessions 2-9, students engaged in the training portion of  the experiment where each student practiced writing 25-minute  timed essays on SAT-style prompts. During each training session,  students wrote two prompt-based essays (see Figure 1 for  screenshot of W-Pal AWE prompt). This resulted in each student  writing a total of 16 essays across 8 training sessions (see Table 1  for prompt topic and order). After each essay, students received  automated formative feedback about their essay from the W-Pal  AWE system (see Figure 2 for a screenshot of the essay processor  and feedback messages). Students could then use this feedback to  revise their essay for approximately 10 minutes.   2.2.3 Posttest  During session 10, all participants completed a posttest that was  comprised of measures similar to the pretest, including a writing  proficiency test (25-minute SAT-style essay).     Figure 1. Screenshot of the W-Pal prompt   196      Figure 2. Screenshot of the W-Pal essay processor and   feedback screen     Table 1. Essay Prompt Order      2.3 Materials and Measures  2.3.1 Prior Reading ability  Students reading ability was assessed using the Gates-MacGinitie  (4th ed.) Reading Skill test [39]. The test included 20 multiple- choice questions that assessed students reading comprehension  ability by asking students to read short passages and then answer  two to six questions about the content of the passage. All students  were given 20 minutes to complete as many questions as they  could.    2.3.2 Vocabulary Knowledge  Students vocabulary knowledge was assessed using the Gates- MacGinitie (4th ed.) vocabulary test [39]. In this test, students  were shown 45 simple sentences, each with an underlined  vocabulary word. They were then asked to choose the word most   closely related to the underlined word within the sentence from a  list of five choices. All students were given 10 minutes to answer  as many questions as they could.    2.3.3 Prior World Knowledge  Students prior world knowledge was assessed using a 20-item  measure that covered the areas of science, literature, and history  [40].    2.3.4 Essay Quality  Students wrote a total of 18 essays (1 at pretest, 16 during  training, and 1 at posttest). Pretest and posttest essays were scored  using expert human raters. These raters used an SAT style rubric  that ranged from 1 to 6. Inter-rater reliability was reached at r=.70.  These raters also reached 100% adjacent agreement. Students  essay scores during training were measured through the use of a  hierarchical classification approach that provided automated  assessment (range of 1 to 6) of all the training essays.  This  algorithm uses features computed through the automated tools,  Coh-Metrix, the Writing Assessment Tool (WAT), and Linguistic  Inquiry and Word Count (LIWC).    2.4 Coh-Metrix  To assess the linguistic features of students essays (during  training), Coh-Metrix was utilized. Coh-Metrix [41, 8] is an  automated text analysis tool that calculates indices related to text  properties at the word, sentence, and discourse levels. This tool  was originally developed to provide more robust measures of text  difficulty [42]; its focus on the multiple levels of texts affords the  opportunity for this tool to provide more specific information  about the challenges and scaffolds that are contained within a  given text. The indices in Coh-Metrix range from basic text  properties to higher-level measures of cohesion and discourse.  The basic text indices within Coh-Metrix report statistical  information related to the length of specific discourse units within  the text, such as the number of words and paragraphs, as well as  the average length of words (average number of syllables per  word), sentences (average number of words per sentence), and  paragraphs (average number of sentences per paragraph). The  lexical measures in Coh-Metrix relate to the types of words that  are contained within a text, such as the overall level of word  specificity (hypernymy), the average frequency of the individual  words, and the diversity of words used throughout a given text.  Syntactic indices describe the complexity and properties of the  sentences within a text; this category of measure includes: the  number of modifiers per noun phrase within the text, the  similarity of the syntax among the sentences, and the frequency of  passive sentences in the text. Finally, Coh-Metrix reports on the  cohesion of a given text using a number of measures; some of  these indices include the incidence of connectives in a text as well  as word overlap measures, which indicate how frequently words  overlap amongst the sentences within a text.    Research with Coh-Metrix has suggested that there are multiple  dimensions within texts that work together to affect  comprehension [43]. Therefore, to account for these dimensions,  Graesser and colleagues developed the Coh-Metrix Easability  Components [44]. Coh-Metrix reports five primary Easability  components: Narrativity, Syntactic Simplicity, Word  Concreteness, Referential Cohesion and Deep Cohesion, which all  provide differential information about aspects of texts that can  influence comprehension (see [44] for more specific information  about these indices). Importantly, these Easability Components  are aligned within an existing multilevel framework of   Training Session # Essay Order   1 Planning   Originality   2 Winning   Loyalty   3 Patience   Memories   4 Heroes   Choices   5 Perfection   Optimism   6 Uniformity   Problems   7 Beliefs   Happiness   8 Fame   Honesty   197    comprehension [7]. Relevant to the current study is the Narrativity  Component. This component is discussed in further detail below.   2.4.1 Narrativity  The narrativity component score provided by Coh-Metrix assesses  the extent to which a text contains narrative versus informational  content. The overall narrativity of a text is representative of how  story-like it is  in that it uses characters, places, events, and  other discourse elements that may be more familiar to readers.  One of the primary characteristics of highly narrative texts is that  they contain more descriptions of actions and events (as opposed  to information texts, which contain less familiar words and a high  incidence of nouns); therefore, the Coh-Metrix narrativity  component includes information about the incidence of main  verbs, adverbs, and intentional events, actions, and particles  within a given text. Additionally, narrative texts tend to more  closely resemble oral language than informative texts [45], as they  contain simpler sentence constructions and a higher frequency of  familiar words and pronouns. Texts that are highly narrative are  typically considered easier to read, recall, and comprehend than  informative texts because they are engaging and more familiar to  readers [7, 46].   2.4.2 Flexibility  Students propensity to use narrativity in a flexible way across the  16 training essays was calculated using Shannon Entropy [47].  Shannon Entropy analysis is a statistical measure to capture  random, controlled, and ordered processes [48, 49, 50, 51]. In the  current study, Shannon Entropy is used to gain a deeper  understanding of how students use of narrativity fluctuates across  time. Previous work by Allen and colleagues used random walk  distance scores as a measure of flexibility across the set of 16  essays [19]. While this measure successfully captures changes in  students use of narrativity, it is not designed to capture fine- grained changes as they manifest and change overtime.  In the current study, students wrote 16 essays. Each essay was  analyzed by Coh-Metrix and assigned a narrativity percentage  score (ranging from 0 to 100), which represented the amount of  narrativity used within the essay. This resulted in students having  16 narrativity scores (1 per essay). These narrativity scores were  then separated into orthogonal quartiles (High Narrativity being  >75%, Medium-High Narrativity ranging between 50% and 75%,  Medium-Low Narrativity ranging between 25% and 50%, and  Low Narrativity being <25%).    Entropy was calculated using the orthogonal narrativity  categorizations of students essays. Equation 1 represents the  Shannon Entropy formula used within the current study. Within  this equation, P(xi) represents the probability of a given state. For  instance, the Entropy for student X is the inverse of the sum of  products calculated by multiplying the probability of each  category by the natural log of the probability of that category.  This formula captures whether students are ordered or flexible in  their use of narrativity across all 16 essays.      H(x)=- (!) log!(!) !  !!!                  (1)      Shannon Entropy produces a number that captures the amount of  consistency (or flexibility) present in a specific time series. Within  the context of the current study, low entropy scores suggest highly  rigid use of narrativity across all 16 essays, whereas high entropy   suggests flexibility in students use of narrativity across all 16  essays. These linguistic time series are produced based on the  narrativity score provided by Coh-Metrix. Students Entropy  scores (i.e., their narrative flexibility scores) reflect the amount of  flexibility that is present within their use of narrativity across all  16 essays. We also calculated a cumulative Entropy score for each  day of training. This afforded us the opportunity to investigate  how flexibility changes and evolves across time.    3. RESULTS  First, correlation and regression analyses were conducted to  examine the relations among students flexibility (Entropy scores)  in narrative writing style (narrative flexibility scores), average  essay quality, and three individual difference literacy ability  measures (vocabulary, reading ability, and prior knowledge)  across all essays. This analysis is intended to replicate previous  findings from Allen and colleagues that used a different measure  of narrative flexibility and found that it related to individual  differences in literacy skill and essay quality [19]. Descriptive  statistics are provided in Table 2.    Table 2. Descriptive Statistics   Variable M SD Range   Narrative Flexibility Score  .94 .40   0.00 1.59   Reading Comprehension  23.31 7.77 10.00 36.00   Prior World Knowledge  15.97  4.56   8.00 20.00   Vocabulary Knowledge  25.78 8.32   6.00 40.00    Average Essay Score    2.65 .62   1.63 4.06   Pearson correlations were conducted to examine the relation  between students narrative flexibility score and pretest measures  of their reading ability, prior knowledge, vocabulary skills, and  prior writing ability. Results from this analysis reveal that  students narrative flexibility scores were positively related to  their reading ability, prior knowledge, vocabulary skill, and prior  writing ability (see Table 3), thus suggesting that students with  higher literacy skills are also more flexible in their use of  narrative across time. To examine these relations further, we  conducted a stepwise regression analysis to examine the extent to  which literacy skills were predictive of narrative flexibility. One  variable was retained in the final model predicting 29% of the  variance in students narrative flexibility [F (1, 42) = 17.36 p<  .001; R2 = .29]: prior reading ability [B =.54, t(1, 43) = 4.17, p <  .001].    Table 3. Correlations Between Narrativity Flexibility and  Prior Literacy Ability   Individual Difference Measure Flexibility Score   Reading Comprehension .390**   Prior World Knowledge  .362*   Vocabulary Knowledge .307*   Pretest Essay Quality  .306*   p<.05*, p<.01**       198    Table 4: Cumulative Narrative Flexibility Scores       Table 5. Cumulative Flexibility Scores and Literacy Skills      Additional correlations were conducted to examine the link  between students narrative flexibility score and essay quality  during training and at posttest. Results revealed a significant  relation between average training essay score (r=.319, p=.03) and  flexibility. However, there was no relation to narrative flexibility  and posttest essay quality (r=1.08, p=.49). Thus, writing  flexibility seems to be an important factor during training but this  relation diminishes over time. These findings replicate the results  reported by Allen and colleagues [19]. This reveals that Entropy   and random walk distance measures are capturing similar  constructs. Importantly though, the current study employs Entropy  analysis which afford us the opportunity to examine how this skill  manifests across time. This is something that is not possible  through the use of random walk distance scores alone because this  metric needs multiple data points to calculate.   These initial results support the hypothesis that writing flexibility  is related to high literacy skills and essay quality. However, to  investigate these results at a more fine-grained level and examine  how prior skills impact flexibility across time, narrative flexibility  scores were calculated for each day of training, thus providing a  finite look at how flexibility manifests across time. For every  student, a cumulative narrative flexibility score was calculated for  each day of training. This score captures the cumulative amount  of flexibility that the student has exhibited in their use of  narrativity. For instance, on Day 2, students have completed two  essays; therefore, their cumulative narrative flexibility score will  be their narrative Entropy score on the Planning and Originality  essays.  Similarly, on Day 3 students flexibility score will be the  narrative Entropy score for the Planning, Originality, Winning,  and Loyalty essays. Table 4 provides the descriptive statics for  each days running Flexibility score.   Pearson correlations were conducted to examine the relation  between students cumulative narrative flexibility scores and  pretest measures of their vocabulary skills, reading ability, prior  knowledge, and prior writing ability. Table 5 shows the relation  between students cumulative narrativity score and their pretest  vocabulary knowledge, writing ability, world knowledge, and  writing ability. Results from this analysis reveal that the relation       between students cumulative flexibility score and vocabulary  skill, prior knowledge, and writing ability can be reliably detected  around Day 5 (10 essays written). These results demonstrate that  students vocabulary skill, prior knowledge, and writing ability  seem to be mostly related to their cumulative narrative flexibility  score for the later sessions. Thus, the impact that these skills have  on flexibility takes time to manifest.   Interestingly, however, the relation between students cumulative   Session Day (s) Range M SD   Day 1 (.00-0.69) 0.25 0.34   Days 1-2 (.00-1.32) 0.56 0.49   Days 1-3 (.00-1.39) 0.95 0.59   Days 1-4 (.00-1.33) 0.97 0.50   Days 1-5 (.00-1.67) 1.00 0.45   Days 1-6 (.00-1.64) 0.97 0.44   Days 1-7 (.00-1.60) 0.95 0.40   Days 1-8 (.00-1.59) 0.94 0.40   N= 45         Session Day Vocabulary Knowledge Reading Ability Prior Knowledge  Writing Ability     Day 1 .117 .199 .203 .106    Day 1-2     .416**     .464**   .306*     .431**    Day 1-3 .141     .417** .175        .291 (M)    Day 1-4 .178     .446**   .317*        .260 (M)    Day 1-5       .280(M)     .517**     .456**   .310*    Day 1-6   .301*     .543**    .454**    .340*    Day 1-7   .326*     .537**    .466**    .300*    Day 1-8   .326*     .543**    .466**    .306*    (M) p < .10, *p < .05; ** p < .01      199    narrative flexibility score and their reading ability was detectable  after Day 2 (with 4 essays written so far). Thus, reading skill  seems to be a reliable predictor of flexibility, after only a few  writing prompts. This suggests that students reading ability plays  a critical role (more than the other literacy skills) in their ability to  demonstrate narrative flexibility in their writing style.   4. DISCUSSION  A predominant assumption by many researchers is that strong  writers are more flexible in their writing style [3]. However, there  has been only one empirical investigation into relations between  writing quality and flexibility [19]. The current study replicates  and expands upon this work using an Entropy analysis to  investigate the manifestation of students narrative flexibility  across 16 prompt-based essays. Correlation analyses were used to  examine how essay quality, prior ability levels, and the writing  flexibility were related. Results from this study replicated the  work by Allen and colleagues by indicating that students who are  more flexible in their use of linguistic features also have more  knowledge about the world, are better readers, and have a higher  vocabulary [19].    These results reveal that the combination of linguistic features and  dynamical methodologies, such as Entropy, can be used to  provide stealth assessments of students flexibility in writing  style. This study is only a first step in the investigation of  flexibility in writing, and points toward complex relations  between these individual differences. There has been an abundant  amount of research that examined the characteristics of strong  writers. Some of this work has pointed toward individual  differences in self-regulation and reading comprehension ability.  The current study adds to this literature by revealing a relation  among individual differences in literacy skills, essay quality, and  narrative flexibility.    While the results presented here replicate previous work by Allen  and colleagues [19], we also contribute to the field by being  among the first to demonstrate that the relation between students  flexibility in writing style and their prior literacy skills can be  reliably detectable after only a few sessions (depending on the  literacy skill). The results presented here inform researchers on  individual differences that have the potential to influence narrative  flexibility and when that influence may be detectable. For  instance, the current results reveal that the relation between  reading comprehension skills and narrative flexibility was  detectable after Day two (4 essays). However, the relation  between prior knowledge, vocabulary skills, and writing quality  with students narrative flexibility took a bit longer to reliably  detect (around Day five). This finding suggests that flexibility is  related to reading comprehension, vocabulary, prior knowledge,  and writing ability and that these relations can be reliably detected  after only a few sessions (depending on the literacy skill  investigated) in an AWE system.     The novelty of the current work comes through the use of Entropy  analysis and NLP as a means to covertly capture the emergence of  students flexibility in writing style across time. These results are  the first to measure the emergence of writing flexibility with  Entropy analysis. Indeed, the combination of Entropy and NLP  provided us with a nuanced examination of writing flexibility that  would otherwise be missed through the use of summative  measures alone.    In sum, these exploratory methodologies have the potential to  afford educational researchers a means to quantify students  ability to exert flexibility in their writing style across time. The   current paper is designed to be a starting point for researchers  interested in the fine-grained assessment of writing flexibility  across time. In the future, these analyses will be built into the W- Pal AWE system as a means of monitoring writing flexibility as  students submit essays. The ultimate goal of this work is develop  practical ways to assess students behaviors and abilities in real- time. Thus, the stealth assessment of writing flexibility provided  here may be able to eventually act as a proxy for more traditional  methods of behavior assessment (i.e., self-reports) thereby  improving the student model of our systems and allowing for  individualized feedback based on a students demonstrated level  of flexibility.    5. ACKNOWLEDGMENTS  This research was supported in part by the Institute for Education  Sciences (IES R305A080589 and IES R305G20018-02). Ideas  expressed in this material are those of the authors and do not  necessarily reflect the views of the IES.    6. REFERENCES  [1] Flower, L. and Hayes, J. 1981. Identifying the organization   of writing processes. In L. Gregg and E. Steinberg (Eds.),  Cognitive processes in writing. Erlbaum & Associates,  Hillsdale, NJ, 3-30.   [2] Sharma, N., and Patterson, P. G. 1999. The impact of  communication effectiveness and service quality on  relationship commitment in consumer, professional services.  Journal of services marketing, 13(2), (1999), 151-170.   [3] Graham, S., and Perin, D. 2007. Writing next: Effective  strategies to improve writing of adolescents in middle and  high schoolsA report to Carnegie Corporation of New  York. Washington, DC: Alliance for Excellent Education.   [4] National Center for Education Statistics 2011. The Nations  Report Card: Trial Urban District Assessment Science 2009.  (NCES 2011452). Washington, D.C.: U.S. Department of  Education. Retrieved from  http://nces.ed.gov/nationsreportcard/pdf/dst2009/2011452.pd f   [5] McNamara, D. S., Crossley, S. A., and McCarthy, P. M.  2010. The linguistic features of writing quality. Written  Communication, 27, (2010), 57-86.    [6] Witte, S., and Faigley, L. 1981. Coherence, cohesion, and  writing quality. College Composition and Communication,  32, (1981), 189-204.   [7] Graesser, A. C. and McNamara, D. S. 2011. Computational  analyses of multilevel discourse comprehension. Topics in  Cognitive Science, 2, (2011), 371-398.   [8] McNamara, D. S., Graesser, A. C., McCarthy, P., and Cai, Z.  2014. Automated evaluation of text and discourse with Coh- Metrix. Cambridge: Cambridge University Press.   [9] Graesser, A. C., Olde, B., and Klettke, B. 2002. How does  the mind construct and represent stories In M. C. Green, J.  J. Strange, and T. C. Brock (Eds.), Narrative Impact: Social  and Cognitive Foundations.Lawrence Erlbaum Associates,  Mahwah NJ, 231-263.   [10] Haberlandt, K., and Graesser, A. C. 1985. Component  processes in text comprehension and some of their  interactions. Journal of Experimental Psychology, 114,  (1985), 357-374.   200    [11] Clark, H. H. 1996. Using language. Cambridge: Cambridge  University Press.   [12] Gerrig, R. J. 1993. Experiencing narrative worlds. New  Haven: Yale University Press.   [13] Newkirk, T. 2012. How we really comprehend nonfiction.  Educational Leadership, 69, (2012), 29-32.   [14] McNamara, D. S., Crossley, S. A., and Roscoe, R. D. 2013.  Natural language processing in an intelligent writing strategy  tutoring system. Behavior Research Methods, 45, (2013),  499-515.   [15] Crossley, S. A., Roscoe, R., and McNamara, D. S. 2014.  What is successful writing An investigation into the  multiple ways writers can write successful essays. Written  Communication, 31, (2014), 184214.    [16] Delisle, D., and Delisle, J. 2011. Building Strong Writers in  Middle School: Classroom-ready Activities that Inspire  Creativity and Support Core Standards. Minneapolis: Free  Spirit Publishing.   [17] Graham, S., and R. Harris, K. 2000. The role of self- regulation and transcription skills in writing and writing  development. Educational psychologist, 35(1), (2000), 3-12.   [18] Shanahan, T., and Tierney, R. J. 1990. Reading-writing  relationships: Three perspectives. In J. Zutell and S.  McCormick (Eds.), Literacy theory and research: Analyses  from multiple paradigms (Thirty-ninth yearbook of the  National Reading Conference), National Reading Conference  ,Chicago, IL, 13-34.   [19] Allen, L. K., Snow, E. L., and McNamara, D. S. under  review. The narrative waltz: The role of flexible style on  writing performance. Manuscript submitted to the Journal of  Educational Psychology.   [20] Snow, E. L., Likens, A., Jackson, G. T., and McNamara, D.  S. 2013. Students' walk through tutoring: Using a random  walk analysis to profile students. In S. K. D'Mello, R. A.  Calvo, and A. Olney (Eds.), Proceedings of the 6th  International Conference on Educational Data Mining,  (Memphis, Tennessee, July 6-9, 2013), Springer Berlin  Heidelberg, 276-279.   [21] Black, P., and William, D. 1998. Inside the black box:  Raising standards through classroom assessment. Phi Delta  Kappan, 80, (1998), 139-148.   [22] Gikandi, J. W., Morrow, D., and Davis, N. E. 2011. Online  formative assessment in higher education: A review of the  literature. Computers & Education, 57, (2011), 23332351.    [23] Shute, V. J. 2011. Stealth assessment in computer-based  games to support learning. In S. Tobias and J. D. Fletcher  (Eds.), Computer games and instruction. Information Age  Publishers, Charlotte, NC, 503-524.   [24] Shute, V. J., and Kim, Y. J. 2013. Formative and stealth  assessment. In J. M. Spector, M. D. Merrill, J. Elen, and M.  J. Bishop (Eds.), Handbook of Research on Educational  Communications and Technology (4th Edition). Lawrence  Erlbaum Associates, Taylor & Francis Group, New York,  NY, 311-323.   [25] McNamara, D. S. 2011. Measuring deep, reflective  comprehension and learning strategies: Challenges and  successes. Metacognition and Learning, 3, (2011), 1-11.   [26] DeRosier, M. E., Craig, A. B., and Sanchez, R. P. 2012. Zoo  U: A stealth approach to social skills assessment in schools.  Advances in Human-Computer Interaction, 2012, (2012), 22.   [27] Snow, E. L., Jacovina, M. E., Allen, L. K., Dai. J., and  McNamara, D. S. 2014. Entropy: A stealth assessment of  agency in learning environments. In J. Stamper, Z. Pardos,  M. Mavrikis, and B. M. McLaren (Eds.), Proceedings of the  7th International Conference on Educational Data Mining,  (London, UK, July 4 -7, 2014),Springer Berlin Heidelberg,  241-244.   [28] Baker, R. S. J. D., Corbett, A. T., Roll, I., and Koedinger, K.  R. 2008. Developing a generalizable detector of when  students game the system. User Modeling and User-Adapted  Interaction, 18, (2008), 287-314.   [29] Allen, L. K., Jacovina, M. E., and McNamara, D. S. in press.  Computer-based writing instruction. In C. A. MacArthur, S.  Graham, and J. Fitzgerald, (Eds.) Handbook of Writing  Research.   [30] Grimes, D., and Warschauer, M. 2010. Utility in a fallible  tool: A multi-site case study of automated writing evaluation.  Journal of Technology, Learning and Assessment, 8(6),  (2010). Retrieved from http://www.jtla.org.   [31] Attali, Y. and Burstein, J. 2006. Automated essay scoring  with e-rater v.2.0. The Journal of Technology, Learning  and Assessment, 4, (2006), (np).    [32] Warschauer, M., and Ware, P. 2006. Automated writing  evaluation: Defining the classroom research agenda.  Language Teaching Research, 10, (2006), 157-180.   [33] Rudner, L. M., Garcia, V., and Welch, C. 2006. An  evaluation of the IntelliMetricTM essay scoring system.  Journal of Technology, Learning, and Assessment, 4(4),  (2006).    [34] Landauer, T. K., Laham, R. D., and Foltz, P. W. 2003.  Automated scoring and annotation of essays with the  Intelligent Essay Assessor.  In M. Shermis and J. Bernstein  (Eds.), Automated Essay Scoring: A cross-disciplinary  perspective. Lawrence Erlbaum Publishers, Mahwah, NJ.   [35] Page, E. B. 2003. Project Essay Grade: PEG. In M. D.  Shermis and J. Burstein (Eds.), Automated essay scoring: A  cross-disciplinary perspective. Lawrence Erlbaum  Associates, Mahwah, NJ, 43-54.   [36] Allen, L. K., Crossley, S. A., Snow, E. L., and McNamara,  D. S. 2014. Game-based writing strategy tutoring for second  language learners: Game enjoyment as a key to  engagement. Language Learning and Technology, 18,  (2014), 124-150.   [37] Roscoe, R. D., Snow, E. L., Brandon, R. D., and McNamara,  D. S. 2013. Educational game enjoyment, perceptions, and  features in an intelligent writing tutor. In C. Boonthum- Denecke and G. M. Youngblood (Eds.), Proceedings of the  26th Annual Florida Artificial Intelligence Research Society  (FLAIRS) Conference , (St. Pete, Florida, May 22 -24, 2013),  The AAAI Press, Menlo Park, CA, 515-520.   [38] Roscoe, R. D., Varner, L. K., Crossley, S. A., and  McNamara, D. S. 2013. Developing pedagogically-guided  threshold algorithms for intelligent automated essay  feedback. International Journal of Learning Technology, 8,  (2013), 362-381.   201    [39] MacGinitie, W. H., and MacGinitie, R. K. 1989. Gates  MacGinitie reading tests. Riverside . Chicago, IL.   [40] OReilly, T., Best, R., and McNamara, D. S. 2004. Self- explanation reading training: Effects for low-knowledge  readers. In Proceedings of the 26th annual meeting of the  Cognitive Science Society, (Chicago, Illinois, August 4-7,  2004) Lawrence Erlbaum Associates, Mahwah, NJ, 1053- 1058.   [41] McNamara, D. S., and Graesser, A. C. 2012. Coh-Metrix: An  automated tool for theoretical and applied natural language  processing. In P.M. McCarthy and C. Boonthum-Denecke  (Eds.), Applied natural language processing and content  analysis: Identification, investigation, and resolution. IGI  Global, Hershey, PA, 188-205.   [42] Duran, N., Bellissens, C., Taylor, R., and McNamara, D. S.  2007. Qualifying text difficulty with automated indices of  cohesion and semantics. In D. S. McNamara and G. Trafton  (Eds.), Proceedings of the 29th Annual Meeting of the  Cognitive Science Society, (Nashville, Tennessee, August 1- 4, 2007) Cognitive Science Society, Austin, TX, 233-238.   [43] McNamara, D. S., Graesser, A. C., and Louwerse, M. M.  2012. Sources of text difficulty: Across genres and grades. In  J. P. Sabatini, E. Albro, and T. O'Reilly (Eds.), Measuring  up: Advances in how we assess reading ability. R&L  Education, Lanham, MD, 89-116.   [44] Graesser, A. C., McNamara, D. S., and Kulikowich, J. 2011.  Coh-Metrix: Providing multilevel analyses of text  characteristics. Educational Researcher, 40, (2011), 223-234.   [45] Biber, D. 1988. Variation across speech and writing.  Cambridge, England: Cambridge University Press.   [46] Haberlandt, K., and Graesser, A. C. 1985. Component  processes in text comprehension and some of their  interactions. Journal of Experimental Psychology, 114,  (1985) 357-374.   [47] Shannon, C. E. 1951. Prediction and entropy of printed  English. Bell system technical journal, 30(1), (1951), 50-64.   [48] Fasolo, B., Hertwig, R., Huber, M., and Ludwig, M. 2009.  Size, entropy, and density: What is the difference that makes  the difference between small and large real-world  assortments Psychology & Marketing, 26(3), (2009), 254- 279.   [49] Grossman, E. R. F. W. 1953. Entropy and choice time: The  effect of frequency unbalance on choice-response. Quarterly  Journal of Experimental Psychology, 5(2), (1953), 41-51.   [50]  Snow, E. L., Allen, L. K., Jacovina, M. E., and McNamara, D.  S. 2014. Does agency matter: Exploring the impact of  controlled behaviors within a game-based  environment. Computers & Education, 26, (2014), 378-392.    [51]  Snow, E. L., Jackson, G. T., and McNamara, D. S. 2014.  Emergent behaviors in computer-based learning  environments: Computational signals of catching up.  Computers in Human Behavior, 41, (2014), 62-70.                                        202      