Identifying Learning Strategies Associated with Active use  of Video Annotation Software   Abelardo Pardo  School of Electrical and Information   Engineering.   The University of Sydney, Australia  abelardo.pardo@sydney.edu.au     Yu Zhao   School of Electrical and Information  Engineering.    The University of Sydney,Australia  yzhao1450@sydney.edu.au   Negin Mirriahi  School of Education & Learning and   Teaching Unit  UNSW, Australia   negin.mirriahi@unsw.edu.au     An Zhao  Learning and Teaching Unit  University of South Australia   an.zhao@unisa.edu.au   Shane Dawson  Learning and Teaching Unit  University of South Australia    shane.dawson@unisa.edu.au     Dragan Gaevi  Schools of Education and Informatics   The University of Edinburgh  dgasevic@acm.org     ABSTRACT  The higher education sector has seen a shift in teaching  approaches over the past decade with an increase in the use of  video for delivering lecture content as part of a flipped classroom  or blended learning model. Advances in video technologies have  provided opportunities for students to now annotate videos as a  strategy to support their achievement of the intended learning  outcomes. However, there are few studies exploring the  relationship between video annotations, student approaches to  learning, and academic performance. This study seeks to narrow  this gap by investigating the impact of students use of video  annotation software coupled with their approaches to learning and  academic performance in the context of a flipped learning  environment. Preliminary findings reveal a significant positive  relationship between annotating videos and exam results.  However, negative effects of surface approaches to learning,  cognitive strategy use and test anxiety on midterm grades were  also noted. This indicates a need to better promote and scaffold  higher order cognitive strategies and deeper learning with the use  of video annotation software.   Categories and Subject Descriptors  J.1 [Administrative Data Processing]:Education; K.3.1 [Computer  Uses in Education] Computer-assisted instruction (CAI)   General Terms  Algorithms, measurements, human factors.   Keywords  Video annotation software, learning analytics, learning strategies.      1. INTRODUCTION  There is a long history of research in educational technology that  has aimed at identifying whether the introduction of a particular  technology has (or has not) improved student academic  performance. In the words of John Hattie it would appear for  education that everything seems to work [14]. That is, the  introduction of technology or for that matter any education-based  intervention generally improves student test scores. It is easy to  understand why research on educational technology and, more  recently learning analytics, has focused on assessment scores as  an outcome in lieu of, for example, the more complex approach of  determining change in student learning strategies [17]. Data on  assessment is readily accessible, easy to measure and amenable to  longitudinal studies. In contrast measuring learning processes  such as, self-regulated learning (SRL) is more complex with  measures commonly reliant on self-reports. However, more  recently the work of Winne [26], Azevedo [1] and others have  demonstrated the potential to establish indicators of SRL  proficiency through the analysis of student interactions with a  technology. That is, the analysis of trace data to provide insight  into the measurement of student judgment of learning and meta- cognitive monitoring. In this paper we describe a case study to  examine students strategic use of a video annotation tool as a  means to support their learning process.   The rest of the document is organized as follows. Section 2  presents the work in the area of video annotation in the context of  active learning scenarios. Section 3 describes the methodology  used for the case study. Section 4 presents the results obtained in  the study. The conclusion and lines for future work are described  in Section 5.   2. RELATED WORK  The use of pre-recorded lecture videos as a teaching strategy is  gaining increasing momentum across the education sector. This is  in part due to advances in video recording technology, increased  adoption and growing enthusiasm for the flipped classroom  teaching model. While the concept is only recently receiving  much recognition it is not a novel teaching strategy. The concept  commenced in the early 2000s when it was recognized as a means  for catering to different learning styles [18]. Since then, it has  been adopted in a variety of disciplines in order to make the best  use of the minimal amount of face-to-face time to enable more  collaborative activities and active learning that provide greater  opportunities for student and teacher interaction [15]. For   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.   ACM 978-1-4503-3417-4/15/03$15.00  http://dx.doi.org/10.1145/2723576.2723611  255    example, a recent study in a first year pharmaceutical course has  shown that the flipped classroom approach led to increased  attendance and improved exam performance as students  independently explored course material but applied their  understanding to active learning activities in class [19]. Similarly,  a study investigating third year engineering students perceptions  of a flipped classroom approach revealed that most students found  the online video lectures more convenient as this provided just in  time access to course content and students could rewind video  material to review difficult concepts as required [9].   An outcome of the increased adoption of blended or flipped  classroom models has been the greater use and integration of  video content. While traditionally, video has been a passive and  one way activity more recent advances in video technologies have  included time-stamped annotation features whereby students can  make comments and reflections for themselves as well as sharing  with peers and instructors [6, 23]. Studies investigating the  perceptions of pre-service teachers [5] and medical students [16]  annotating videos of their own teaching or simulated patient  consultation reveal that students perceive that video annotations  enhance their learning experience, critical reflection and provide  easy identification of areas for improvement.  While these studies report the benefits of the flipped classroom or  video annotation approaches, they have relied heavily on students  self-reports to determine the extent that students accessed the  video materials. This self-report approach can lead to inaccurate  data due to social desirability bias [2, 11] or incorrect recall of  prior behavior. In contrast, learning analytics and data-mining  approaches interrogate the trace data from students actual use of  the lecture videos and, hence, can provide more objective data for  analysis [12]. Recent studies have used learning analytics and  data-mining to investigate students actual engagement with  lecture videos by clustering students based on video tool access  [4] and how such objective data when coupled with results on  related quizzes [20] can inform instructors pedagogical  approaches and intervention strategies. Similarly, related studies  into students annotations have used logged data to investigate the  length of time-stamped annotations compared to hand-written  notes [21] and patterns in the linguistic and cognitive processes  inherent in the text of the annotations [10].  Although there are numerous studies reporting associations  between grades and variables obtained from data traces (for  example [24]), there is a lack of analysis to expand these  associations in order to unpack students approaches to learning to  understand the underlying associations. This lack of insight could  significantly impede the effectiveness of any instructive feedback  and recommendations that are ultimately provided to the student.  This is especially true when the use of technology is considered  part of the self-regulated learning process [25]. In such a process,  students are perceived to be active agents in their learning choices  and therefore make decisions about the tools they are going to use  for their learning. According to Winne [25], those decisions are  made based on external and internal conditions. External  conditions would include, for example, the graded use of a tool in  a course of study. While internal conditions would involve  metacognitive knowledge, motivation, and prior knowledge.  Specifically, the present study looks at the effect of internal  conditions (approaches to and motivational strategies for learning)  on the association between tool use and academic performance.  The study is conducted in the context of a flipped classroom that  embedded the use of video annotation software as part of the  instructional design.   3. METHOD  3.1 Study Design  The following case study collected data from a first year course at  a higher education institution deployed following a flipped  learning strategy. The data was collected from student use of a  video annotation tool, called the Collaborative Lecture Annotation  System (CLAS - a web-based video annotation application) [7,  23]. The study was carried out in the context of a natural  experiment as student participation was voluntary and beyond the  control of the researchers.   3.2 Materials  The study used three data sources. The first was data extracted  from CLAS. This tool offers two types of annotations. The first  form of annotation allows the insertion of a general comment  about the video. The second is a time-stamped annotation,  whereby a comment is made directly related to a specific point in  the video time-line. Both types of annotations can be made private  or public to a set number of users that have been previously given  access to a collection of videos associated with a course. For the  purposes of this study, data about the date, time and number of  annotations per student were analyzed. The number of annotations  is considered an estimation of the level of engagement of students  with the task as a more explicit engagement with the resource than  simply pressing the video play button.. This measure is applied in  lieu of the time the video is viewed, as that measure is subject to  inaccuracies that requires special heuristics [13].   The second data source related to the administration of the  Motivational and Self-Regulated Learning Questionnaire (MSLQ)  [22] and the Study Process Questionnaire (SPQ) [3] at the  commencement of the course. The questionnaire contained 64  questions presented as a seven level Likert scale. Lastly, the third  data source included the scores the students obtained on a  midterm examination consisting of 20 multiple choice questions.   3.3 Sample  The data was collected in the 2014 offering of a first year course  (n=300) at a large metropolitan University. Given the voluntary  nature of the study a total of 149 students participated comprising,  38 females and 111 males. The course followed a flipped  classroom pedagogical strategy whereby students completed a set  of tasks to prepare for the weekly face-to-face lectures. One of  these tasks required the use of CLAS. Students were invited to  write a general summary of the pre-recorded video. In addition,  students were requested to identify and provide three time- stamped annotations about the concepts presented in the video at  the specific time they were explained. The participation in these  tasks was optional and indirectly assessed through a collection of  problems with multiple-choice questions that must be submitted  before each face-to-face session. The videos provided in CLAS  aimed to assist student understanding of the concepts required to  solve the collection of problems. All the operations related to the  videos were stored in the logs produced by CLAS and used for the  study.  The case study reported in this paper focuses first on verifying  that a higher engagement with the video annotation tool correlates  with a higher academic achievement. We then explore which  aspects of learning explain a higher variation of the score in the  midterm exam together with the use of CLAS.    3.4 Variables and Measures  Independent Variables. The main independent variable used in the  study to test the effect of the use of CLAS is based on the number   256    of annotations (both general and time-stamped) submitted by each  student. The variable was re-encoded as a binary variable with the  value zero if a student did not submit any annotation and one if an  annotation was made. Furthermore, we used nine additional  covariates derived from the MSLQ and SPQ questionnaires as  independent variables. In the MSLQ, questions are grouped into  five scales: Intrinsic value (IVAL), Self-Efficacy (SEFF), Test  Anxiety (TANX), Cognitive Strategy Use (CSUS) and Self- Regulation (SREL). The value of each covariate is computed as  the average of the answers to the questions in the group.  Analogously, SPQ groups the questions into four scales: deep  motive (DM), deep strategy (DS), surface motive (SM) and  surface strategy (SS). The value of these variables is also the  average of the questions included in the corresponding category.  The rationale to use these two instruments relates to their  demonstrated capacity to provide indicators of academic  performance and learning process [3, 22]. These instruments can  therefore, assist in identifying the indicators most relevant for  explaining the adoption of video annotation.  Dependent Variable. The dependent variable MID is the score  obtained in the midterm exam that consisted of 20 multiple choice  questions about the material covered in the first five weeks of the  course. The scores on this exam range from zero to 20 (all  questions had the same value).   3.5 Data Analysis  Data analysis was performed by using linear regression  modelling. In our analysis, we created three models with the  midterm marks as the dependent variable. The first model had  only the binary variable representing the use of CLAS for video  annotations. This model was created to compare the increase in  the variability explained by the association between the use of  CLAS and the midterm marks compared to the variability  explained by the other two models. The second and third linear  regression models included the five MSLQ and four SPQ  variables, respectively, within the first model to investigate: i)  whether the association between CLAS use for video annotation  and midterm marks persisted over and above the MSLQ and SPQ  measures; and ii) whether the proportion of that association is  explained by the constructs measured by MSLQ and SPQ.  Unstandardized beta (B), standard error (SE) and standardized   coefficients for the independent variables and the R2 values are  reported for all three regression models. Results were considered  significant if p < .05. All statistical tests were performed using the  R software environment.   4. RESULTS  The descriptive statistics of each variable used in our study are  shown in Table 1. The first linear regression model shown in  Table 2 was obtained as a reference point to verify that a linear  relation between the midterm score and the level of engagement  with the video annotation tool was statistically significant. As it  can be seen, although the coefficient of the linear model is  statistically significant, it only accounted for slightly above 5%  (R2=0.052).   The second model presented in Table 3 was computed combining  the use of the video annotation tool with the values of the five  dimensions of the MSLQ test. The association between the tool  use and the performance was held even after controlling for the  MSLQ variables. The value of R2 indicated that the model  explained more than 13% of the variation. In addition, significant  and negative effects of the two covariates were observed   cognitive strategy and test anxiety. While the negative effect for  test anxiety was anticipated, the effect for cognitive strategy use   was not. Students with higher scores of self-reported cognitive  strategy use can be seen to be more hesitant or reluctant to  adopt a new learning tool or approach (e.g. annotating videos).  Moreover, we found a significant (yet weak) correlation (r=- 0.161, p=0.50) between the scores of cognitive strategy use and  the scores on the midterm exam. These findings suggest that the  instructional design needs to offer some further guidance to the  students on how to use the new tool effectively and provide a  clear rationale outlining the value that such tool use can play in  facilitating student learning and therefore, future academic  performance. In a recent study, Gaevi et al [10] demonstrated  that by altering the external conditions (i.e. graded versus non- graded) had a significant impact on the quality and quantity of  video annotations made in a course. The authors suggested that  the graded condition translated into a scaffolded structure that  essentially provided sufficient motivation and time for students to  develop an understanding of how the technology can be best  applied to aid their learning process. In this context, the  participatory nature of the current case study essentially lacked  the necessary impetus for students with established and effective  cognitive strategies to experiment and adopt an alternate process.   Table 1. Descriptive Statistics of the variables of the study   Variables  Mean (SD)   Entire sample  (N=149)   Mean (SD)  CLAS Users   (N=41)   Mean (SD)  CLAS Non-  Users  (N=108)   MID 14.09 (3.61) 15.43 (3.19) 13.58 (3.64)   CLAS USE 2.09 (4.28) 7.61 (4.97) 0.00 (0.00)   IVAL 5.31 (0.89) 5.40 (0.92) 5.27 (0.87)   SEFF 4.63 (0.99) 4.67 (1.01) 4.62 (0.99)   TANX 3.63 (1.36) 3.60 (1.47) 3.64 (1.32)   CSUS 4.80 (0.69) 4.70 (0.78) 4.84 (0.66)   SREL 4.66 (0.75) 4.59 (0.84) 4.69 (0.72)   DM 4.50 (1.00) 4.41 (1.07) 4.53 (0.97)   DS 4.50 (0.93) 4.38 (1.06) 4.54 (0.88)   SM 3.12 (1.21) 2.80 (1.26) 3.25 (1.17)   SS 3.72 (1.08) 3.58 (1.13) 3.77 (1.06)     Table 2. Model 1: Association between CLAS use and  midterm marks   Coeff  Unstandardized   coefficients  B  SE    Standardized	 coefficients	    t-value p-value   Intercept 13.58  0.34 - 40.03 <0.001  CLAS Use 1.85  0.65 0.23 2.86 <0.005   R2 = 0.0528, F(1, 147) = 8.195, p = 0.005, VIF = 1.00     Furthermore, strategies that reduce students test anxiety are  necessary, even in the context of flipped classrooms which offer  more opportunities to actively engage students. Further research  examining the association between academic performance,  cognitive strategy use, and tool use is needed. It may be the case  that there is an indirect effect of cognitive strategy use on  academic performance mediated by the tool use. In that  mediation, the count of activities (e.g. annotations) may not be the   257    best proxy of learning effort and motivation as shown in study by  Devolder et al. [8]. The authors demonstrated that the time spent  on self-regulated learning with an online learning system  mediated the effect of learning experience with the tool on  academic performance.     Table 3. Model 2: Association between CLAS use and the  midterm marks after controlling for the five MSLQ scales   Coeff  Unstandardized   coefficients   B  SE    Standardized	 coefficients	    t-value p-value  Intercept 17.92  2.34 - 7.655 <0.001  CLAS Use 1.60  0.64 0.20 0.6406 0.013  IVAL 0.32  0.46 0.08 0.696 0.488   SEFF 0.33  0.38 0.09 0.870 0.387   TANX -0.51  0.22 -0.19 -2.287 0.024  CSUS -1.21  0.61 -0.23 -1.989 0.049  SREL 0.03  0.54 0.01 0.059 0.953   R2 = 0.1341, F(6, 142) =3.664, p = 0.002, VIF =1.154     Table 4. Model 3: Association between the use of CLAS and  the midterm after controlling for the SPQ scales (DM, DS,   SM, SS).   Coeff  Unstandardized   coefficients   B  SE    Standardized	 coefficients	    t-value p-value  Intercept 17.59  1.77  9.947 <0.001 CLAS Use 1.65  0.64 0.21 2.570 0.011  DM -0.24  0.40 -0.07 -0.595 0.553   DS 0.09  0.42 0.02 0.217 0.829   SM -0.05  0.38 -0.02 -0.133 0.894   SS -0.85  0.42 -0.25 -2.028 0.044  R2 = 0.1287, F(5, 143) = 4.224, p = 0.001, VIF = 1.148      The final model represented in Table 4 was obtained by entering  the four dimensions of the SPQ questionnaire. In this case, only  one factor, surface strategy (aside from the use of CLAS), was  statistically significant with a negative coefficient. This model  accounts for slightly below 13% of the variation of the dependent  variable. The main conclusion derived from this model is that the  students who engaged with CLAS (e.g., made annotations) but  retained a surface strategy towards learning had lower midterm  scores. As in the case of Model 2, these associations suggest that  in order to increase midterm results, some measures to promote a  deeper approach to learning are required.   5. CONCLUSION  Video annotation tools to support learning are becoming common  in multiple learning experiences. Although the use of videos by  students should have a positive impact in their learning, there are  few studies that investigate the connection between video  annotation and student approaches to learning. The relation  between grades and quantitative data automatically collected has  been explored in the past. However, a more nuanced analysis is   needed that takes into account how students approach the overall  learning experience to put the data in the right context. This paper  has presented a case study establishing an association between  indicators of student strategies for learning and the use of a video  annotation tool, CLAS in a flipped learning environment. A  simple initial model showed a weak association between academic  performance in a mid-term examination and the use of video  annotations. However, when including the results of the MSLQ  and SPQ questionnaires, the models provide statistically  significant associations with some of the learning strategies. The  results highlight the potential of combining automatically  collected data with data self-reported by the students. The two  instruments used to collect self-reported information (MSLQ and  SPQ) are well established mechanisms to gauge the learning  strategies adopted by learners. The results discussed in the paper  offer insight on how to approach the use of video annotation  technology in a real class and provide students with the adequate  scaffolding to maximize its use.   As future research, we plan to widen the analysis in several  directions. First, a more comprehensive set of factors within the  course will be considered to study the effect of deploying the  changes suggested by these initial findings. Second, the study will  be extended to other learning contexts with different students and  not a flipped learning environment. Finally, we plan to include in  the study the effect of assessment strategies to see the influence in  student engagement.   6. REFERENCES   [1] Azevedo, R. et al. 2010. Measuring Cognitive and  Metacognitive Regulatory Processes During Hypermedia  Learning: Issues and Challenges. Educational  Psychologist. 45, 4 (Oct. 2010), 210223.   [2] Beretvas, S.N. et al. 2002. A Reliability Generalization  Study of the Marlowe-Crowne Social Desirability Scale.  Educational and Psychological Measurement. 62, 4  (Aug. 2002), 570589.   [3] Biggs, J. et al. 2001. The revised two-factor Study  Process Questionnaire: R-SPQ-2F. British Journal of  Educational Psychology. 71, 1 (Mar. 2001), 133149.   [4] Brooks, C et al. 2011. The Who, What, When, and Why  of Lecture Capture. Proceedings of the First  International Conference on Learning Analytics and  Knowledge (2011), 8692.   [5] Colasante, M. 2011. Using video annotation to reflect on  and evaluate physical education pre-service teaching  practice. Australasian Journal of Educational  Technology. 27, 1 (2011), 6688.   [6] Colasante, M. and Fenn, J. 2009. mat: A New Media  Annotation Tool with an Interactive Learning Cycle for  Application in Tertiary Education. World Conference on  Educational Multimedia, Hypermedia and  Telecommunications (2009), 35463551.   [7] Dawson, S. et al. 2012. Using technology to encourage  self-directed learning: The Collaborative Lecture   258    Annotation System (CLAS). Australasian Society for  Computers in Learning in Tertiatry Education (2012).   [8] Devolder, A. et al. 2012. Supporting self-regulated  learning in computer-based learning environments:  systematic review of effects of scaffolding in the domain  of science education. Journal of Computer Assisted  Learning. 28, 6 (Dec. 2012), 557573.   [9] Foertsch, J. et al. 2002. Reversing the  Lecture/Homework Paradigm Using eTEACH Web- based Streaming Video Software. Journal of Engineering  Education. 91, 3 (2002), 267274.   [10] Gaevi, D. et al. 2014. Analytics of the effects of video  use and instruction to support reflective learning.  Proceedings of the International Conference on Learning  Analytics and Knowledge (2014), 123132.   [11] Gonyea, R.M. 2005. Self-reported data in institutional  research: Review and recommendations. New directions  for institutional research. 2005, 127 (2005), 7389.   [12] Greller, W. and Drachsler, H. 2012. Translating learning  into numbers: A generic framework for learning  analytics. Educational Technology & Society. 15, 3  (2012), 4257.   [13] Guo, P. et al. 2014. How video production affects student  engagement: An empirical study of mooc videos.  Proceedings of the first ACM conference on Learning at  Scale (2014), 4150.   [14] Hattie, J.A.C. 2008. Visible learning: A synthesis of over  800 meta-analyses related to achievement. Routledge.   [15] Houston, M. and Lin, L. 2012. Humanizing the  classroom by flipping the homework versus lecture  equation. Society for Information Technology & Teacher  Education International Conference (2012), 11771182.   [16] Hulsman, R.L. et al. 2009. Reflective teaching of medical  communication skills with DiViDU: Assessing the level  of student reflection on recorded consultations with  simulated patients. Patient Education and Counseling.  74, 2 (Feb. 2009), 142149.   [17] Knight, S. et al. 2014. Epistemology , assessment ,  pedagogy: where learning meets analytics in the middle  space. Journal of Learning Analytics. 1, 1 (2014), 2347.   [18] Lage, M.J. et al. 2000. Inverting the Classroom: A  Gateway to Creating an Inclusive Learning Environment.  The Journal of Economic Education. 31, 1 (2000), 30.   [19] McLaughlin, J.E. et al. 2014. The Flipped Classroom: A  Course Redesign to Foster Learning and Engagement in a  Health Professions School. Academic Medicine. 89, 2  (Feb. 2014), 236243.   [20] Mirriahi, N. and Dawson, S. 2013. The pairing of lecture  recording data with assessment scores: a method of  discovering pedagogical impact. Proceedings of the  International Conference on Learning Analytics and  Knowledge (2013), 180184.   [21] Mu, X. 2010. Towards effective video annotation: An  approach to automatically link notes with video content.  Computers & Education. 55, 4 (Dec. 2010), 17521763.   [22] Pintrich, P.R. and de Groot, E. V. 1990. Motivational and  self-regulated learning components of classroom  academic performance. Journal of Educational  Psychology. 82, 1 (1990), 3340.   [23] Risko, E.F. et al. 2013. The Collaborative Lecture  Annotation System (CLAS): A New TOOL for  Distributed Learning. IEEE Transactions on Learning  Technologies. 6, 1 (2013), 413.   [24] Romero, C. et al. 2013. Predicting students final  performance from participation in on-line discussion  forums. Computers & Education. 68, (Oct. 2013), 458 472.   [25] Winne, P.H. 2006. How Software Technologies Can  Improve Research on Learning and Bolster School  Reform. Educational Psychologist. 41, 1 (2006), 517.   [26] Winne, P.H. and Hadwin, A.F. 2013. nStudy: Tracing  and Supporting Self-Regulated Learning in the Internet.  International Handbook of Metacognition and Learning  Technologies. R. Azevedo and V. Aleven, eds. Springer.  293308.       259      