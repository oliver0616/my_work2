What does student writing tell us about their thinking on social justice  Heeryung Choi School of Information University of Michigan heeryung@umich.edu  Christopher Brooks School of Information University of Michigan brooksch@umich.edu  Kevyn Collins-ompson School of Information University of Michigan kevynct@umich.edu  ABSTRACT In this work we investigate the use of deep learning for text anal- ysis to measure elements of student thinking related to issues of privilege, oppression, diversity and social justice. We leverage his- torical expert annotations as well as a large lexical model to create a more generalizable vocabulary for identifying these characteristics in short student writing. We demonstrate the feasibility of this approach, and identify further areas for research.  CCS CONCEPTS Social and professional topics Student assessment; Model curricula; Adult education; Computing methodologies Nat- ural language processing;  KEYWORDS Writing analytics, social work, social justice, natural language pro- cessing ACM Reference format: Heeryung Choi, Christopher Brooks, and Kevyn Collins-ompson. 2016. What does student writing tell us about their thinking on social justice. In Proceedings of Learning Analytics & Knowledge Conference, Vancouver, BC, Canada, March 13-17, 2017 (LAK 17), 2 pages. DOI: hp://dx.doi.org/10.1145/3027385.3029477  1 INTRODUCTION AND RELATED WORKS While individual courses are oen mapped to explicit learning out- comes, and course outcomes are aggregated to demonstrate higher levels of learning in a curriculum, it is also possible to consider thematic elements which run throughout a curriculum. In this work, we explore one such thematic element in the context of the discipline of social work at the University of Michigan. In particular, we are interested in nding evidence of student thinking about priv- ilege, oppression, diversity, and social justice (PODS). While these PODS skills are ill-dened, multifaceted, and highly contextual, the intent is that all courses within the discipline should strengthen student skills with respect top PODS thinking, regardless of the specic course objectives [4]. Due to brevity, we refer the interested reader to [4] for details about the PODS framework used.  Tomeasure elements of PODS, the University of Michigan School of Social Work engaged in an open coding activity, summarizing Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 17, Vancouver, BC, Canada  2017 Copyright held by the owner/author(s). 978-1-4503-4870-6/17/03. . .$15.00 DOI: hp://dx.doi.org/10.1145/3027385.3029477  student responses to short, topical scenarios related to social work practice for several years [1][3]. is was conducted primarily by manual coding of student writing: having an expert in the PODS framework go through writings of students accumulated over the years, identifying words demonstrating evidence of PODS thinking, and hence forming a short close vocabulary. In this poster we augment this approach with a machine-learned customized vocabulary which is supported by the work of the empath project [2]. Empath uses deep learning to form word associations from large textual datasets, and creates new lexical categories based on a few seed words.  Our aim in using empath is to boost the set of expert words provided into a broader set, increasing topic coverage and ideally increasing the generalizability of the historical lexicon. us the research question we explore is: Can we better detect elements of PODS in student writing through unsupervised machine learning  Our key ndings are as follows:  (1) ere is a low agreement between non-expert human coders on student writing, which shows the diculty of identify- ing elements of PODS thinking in student writing.  (2) ere is even a lower agreement between a historical expert- derived PODS lexical category and non-expert human coders on new student writing, which demonstrates the lack of generalizability of historical expert annotations.  (3) ere is a higher agreement between an empath-boosted lexical category based on historical expert annotations and non-expert human raters in the same task, which shows the potential generalization benets of using deep learning.  us the contribution of this work is evidence to support the use of unsupervised machine learning when aempting to identify elements of PODS thinking in student writing.  2 APPROACH To compare the ecacy and validity of this approach, three dier- ent comparison methods were constructed to assess students short answers. Each of these methods was related to PODS thinking, and hence was divided into four PODS classications: Privilege, Op- pression, Diversity, and Social Justice. In this work, we considered only the Social Justice category, since half of the questions directly asked students to write a paragraph describing Social Justice issues related to the vignee.  e rst method, which we call the expert set, was a vocabu- lary composed of expert-derived annotations based on manually choosing PODS-related words from student writing in historical iterations of the course.    LAK 17, March 13-17, 2017, Vancouver, BC, Canada Heeryung Choi, Christopher Brooks, and Kevyn Collins-Thompson  e second method, which we refer to as the boosted set, was generated through the empath application. e lexical category for this method was built from expert words using one of the empath default corpora (ction) and a word limit of n=200. All of the non unigram results returned by empath were converted into unigrams.  e third method was comprised of two vocabularies created by non-expert human coders (non-experts). ese coders read students responses and tagged terms related to the concept of Social Justice. e human coders were untrained graduate students who were not majoring in the eld of social work. roughout all approaches and analyses words were stemmed with a Lancaster stemmer.  3 DATASET e dataset used includes 472 responses to a writing prompt an- swered by 118 students taking introductory graduate-level social work courses. An average word count of the responses was about 90 words with a standard deviation of 57.87. e course was composed of ve sections taught by ve dierent instructors. All students were shown one of two dierent writing scenarios. e data used in this work was based on student responses to the following ques- tion: Write a paragraph describing the social justice actions that you believe would helpful to social workers in this [the writing scenario] case.  4 RESULTS We converted student responses into binary feature sets (dummy variables) and evaluated the three dierent approaches using Fleiss Kappa for inter-coder-agreement (). We found:  (1) ere is low agreement between the two human non- ex- perts for this task (=0.37, n=2).  (2) Despite the low agreement, this was higher than the agree- ment among the non-expert vocabularies and the vocabu- lary created by the expert on historical data (=0.21, n=3).  (3) Furthermore, the vocabulary created by the expert showed equally poor agreement with both non-experts (=0.06 and =0.08 respectively.)  (4) Amachine-learned vocabulary trained on an expert-derived historical data has low agreement with the expert-derived vocabulary (=0.21, n=2).  (5) At the same time, this machine-learned vocabulary has higher agreementwhen comparedwith human non-experts both as a group (=0.24, n=3) and pairwise than the vocab- ulary created by the expert on historical data (=0.17 and =0.17 in both cases).  5 DISCUSSION ese ndings have several ramications for future research. First, the general lack of agreement between non-expert coders shows  that the task is dicult for untrained human raters. Furthermore, the result that the low  value decreases when including an expert human coder based on historical data suggests that the issue is dicult to generalize. ese two ndings suggest a deeper under- standing of the domain is needed by raters, along with vocabulary which is broader than historical answers have provided.  e low  value between the boosted set and the expert set was somewhat expected: the method employed by empath is aimed at generalizing the vocabulary, and not in merely copying the seed words. What is surprising and encouraging is the performance of the boosted set when compared to the non-experts: an increasing .  greater than that found between the expert and the non-experts suggests that the machine-learned model has generalized the vo- cabulary for new data. us the topic boosting has leveraged new relationships from the empath corpus on the expert model.  6 FUTUREWORK An immediate concern of ours is to increase  values between hu- man coders by recruiting experts or engaging in more sophisticated analyses (e.g. discussion between coders on issues of disagreement). Simultaneously we will begin to explore the size of the boosted dataset. Empath uses cosine similarity to rank related terms, and we will consider the impact changing this parameter has on inter- coder agreement between the boosted dataset and the non-expert datasets. Finally, we will explore the impact dierent training cor- pora have on the accuracy of the deep learned model. e empath environment currently supports three dierent corpora, and we are investigating augmenting this with new, more discipline-specic documents.  ACKNOWLEDGMENTS We thank Dale Fitch, Beth Glover Reed, Tina Louise, and all of the instructors who allowed us to explore this issue with their students. is work was supported in part by the Michigan Institute of Data Science (MIDAS) learning analytics challenge initiative.  REFERENCES [1] Paula Allen-Meares. 2007. Cultural Competence: An Ethical Requirement. J.  Ethn. Cult. Divers Soc. Work 16, 3-4 (2007), 8392. [2] Ethan Fast, Binbin Chen, and Michael S Bernstein. 2016. Empath: Understanding  Topic Signals in Large-Scale Text. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (Feb 2016), 46474657.  [3] Dale Fitch and Beth Glover Reed. 2010. Automated Curricular Assessment Using alitative Data.  [4] University of Michigan School of Social Work. 2016. Section 4.01.03: Privilege, Oppression, Diversity and Social Justice (PODS) & Arma- tive Action. hp://ssw.umich.edu/msw-student-guide/section/4.01.03/325/ privilege-oppression-diversity-and-social-justice-pods-armative-action  http://ssw.umich.edu/msw-student-guide/section/4.01.03/325/privilege-oppression-diversity-and-social-justice-pods-affirmative-action http://ssw.umich.edu/msw-student-guide/section/4.01.03/325/privilege-oppression-diversity-and-social-justice-pods-affirmative-action  	Abstract 	1 Introduction and related works 	2 Approach 	3 Dataset 	4 Results 	5 Discussion 	6 Future work 	Acknowledgments 	References   