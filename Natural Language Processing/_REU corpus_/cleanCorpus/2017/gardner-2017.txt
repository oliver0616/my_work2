Integrating Syllabus Data into Student Success Models  Josh Gardner   School of Information  University of Michigan  jpgard@umich.edu    Ogechi Onuoha  School of Informatics   University of Edinburgh  oge.blessing@gmail.com   Christopher Brooks  School of Information   University of Michigan   brooksch@umich.edu     ABSTRACT  In this work, we present (1) a methodology for collecting,  evaluating, and utilizing human-annotated data about course  syllabi in predictive models of student success, and (2) an  empirical analysis of the predictiveness of such features as they  relate to others in modeling end-of-course grades in traditional  higher education courses. We present a two-stage approach to (1)  that addresses several challenges unique to the annotation task,  and address (2) using variable importance metrics from a series of  exploratory models. We demonstrate that the process of  supplementing traditional course data with human-annotated data  can potentially improve predictive models with information not  contained in university records, and highlight specific features  that demonstrate these potential information gains.   CCS Concepts   Applied computing ~ Education  Computing methodologies ~  Classification and regression trees   1. INTRODUCTION  The task of grade prediction is of interest to a variety of  stakeholders in higher education, including students, instructors,  advisors, and course designers. However, data that may be useful  in predicting these grades is often not collected by default and can  be resource-intensive to collect. This includes information about  pedagogy, course requirements, assignments, materials, and other  course features. If we can identify features that might be useful in  predictive modeling and a reliable, scaleable process for gathering  these data, we might improve the educational process for all of  these stakeholders. However, both tasks have received little  attention in prior learning analytics research to date [5].   2. DATA  Our analysis utilized 1,149 syllabi taken from courses in the  largest college within the University of Michigan. Using a two- stage process, we collected a set of 27 categorical features from  each syllabus. These human-annotated syllabus features were  combined with an existing student-course-term level dataset with  features representing student demographics (n = 11), student  academics (n = 1), and course features (n = 17; referred to as SIS  features as they come from the institutional student information  system and are different from syllabus features, but also refer to  courses). The combined dataset includes 27,935 observations  covering 596 unique courses, 49 departments, and approximately  20% of all semester grades.   3. ANALYSIS  Our analysis included a data collection and preprocessing phase in  which human-annotated syllabus features were collected and their  quality assessed, and an exploratory predictive modeling phase in  which we evaluated the relative importance of syllabus features.   3.1 Annotated Syllabus Features  As human annotation is resource-intensive, we devised a scaleable  two-stage annotation process that allowed us to minimize the  number of annotations while also evaluating their accuracy. In the  first phase, a sample set of 52 syllabi were annotated by up to five  raters each1. We calculated observed inter-rater agreement, using  Fleiss Kappa, for each feature in this initial sample to serve as a  measure of each features quality on the full set of syllabi, which  were annotated by only a single rater. For the 24 features having  {yes, no, unclear} as choices, we eliminated any ratings of  unclear as a proxy for rater noncompliance. There was  substantial variation in the level of inter-rater agreement across  features, ranging from k = 0.67 (substantial agreement) to below  zero (worse agreement than expected by chance), with mean 0.24  and standard deviation 0.18. Low- ( < 0.2) features  were excluded from most models to reduce potential noise. In the  second phase, the full set of 1,149 syllabi were sent for annotation  by individual raters. We received complete annotations for 934,  which were used in the exploratory modeling below.      3.2 Exploratory Modeling  We fit a series of classification trees on the full dataset comprised  of demographic, academic, SIS, and syllabus features to explore                                                                        1 Raters were obtained using Amazon Mechanical Turk.       Table 1: Selected syllabus features. Features marked with  asterisk (*) were rated on {0,1,2,3,4,5,6,7,8,9,10,10+} scale;  all others rated on {yes, no, unclear} scale.     Feature Description  Assignments*  How many assignments are there in the class    Attendance   Is attendance or participation graded    Curve  Is this class graded on a curve   Fees Does this course have any extra fees/costs   Instructors*  How many instructors does the class have    Late Policy  Is there a policy for late assignments    Picture   Is there a cartoon, comic, or other illustration  in this syllabus   Presentation  Are in-person student presentations required    Software Does the course require any software      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and  that copies bear this notice and the full citation on the first page.  Copyrights for third-party components of this work must be honored.  For all other uses, contact the Owner/Author. Copyright is held by the  owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029473       both the overall performance of models utilizing these features,  and the relative importance of individual features to determine  whether similar data collection efforts may be useful to  institutions interested in modeling or predicting student grades.  Classification trees were particularly well-suited to this task for  their ability to capture complex interactions between variables,  their handling of missing data and both categorical and continuous  features, and the useful variable importance metrics generated in  the process of recursive binary tree-fitting.   We utilized the caret package in R for model-building, which  generates trees by recursively partitioning the feature space based  on the highest-purity binary splits [4]. Models were fit using an  80-20 training-test split within courses (i.e., 80% of the data from  each course was used for training), with five repeats of 10-fold  cross-validation. A categorical variable with the official course  grade (A+, A, A-, etc.) was used as the outcome variable. In order  to isolate the predictiveness of each individual feature, to provide  a consistent baseline of performance, and to skim off variation  which could be explained by one of the best-known grade  predictors (students cumulative GPA), we first fit bivariate  models that used each individual feature in conjunction with  cumulative GPA to predict students final grades (i.e., we fit  models using p + GPA, for each of the p features). We then  utilized the variable importance metrics from each model to  provide a measure of the predictive efficacy of each feature.    Variable importance for a given feature is calculated as the sum of  the goodness of split measures for each split for which it was the  primary variable, plus goodness  adjusted agreement for all splits  in which it was a surrogate [2]. Essentially, this metric can be  thought of as a measure of the purity of splits obtained using this  variable, conditional on the other variables present in the model.    This bivariate approach (a) provided a common reference point,  cumulative GPA, for the importance (and therefore the  predictiveness) of individual features; (b) avoided penalizing sets  of correlated features, which are pruned out of classification trees,  by examining them separately; and (c) allowed us to examine how  the feature provided additional predictive power to explain only  residual variation from a well-known predictor  GPA  and avoid  examining a given feature in isolation, which might lead us to  overestimate its actual performance by allowing it to account for  variation easily explained by other well-known, readily available  predictors such as GPA. Similar approaches have been applied to  random forest models, but we adopted a CART model in favor of  its ability to more easily handle missing data [1]. Being mindful of  the high variance of CART trees, our robustness checks with  alternative specifications (described below) helped to limit model  variance as a confounding factor in this analysis [3].    We find several results relevant to both modeling and data  collection efforts. Our focus, being in the early stages of exploring  syllabus data, was on understanding features individually so as to  guide future data collection efforts. As expected, cumulative GPA  is the strongest predictor of academic performance, but several  syllabus variables also showed high importance including:  number of instructors, number of assignments, and indicator  variables for whether the course required in-person presentation(s)  or included a graded component for attendance or participation.  The results of the bivariate models for the 30 highest-importance  features are shown. As an additional robustness check, we fit a  series of models containing all of the variables within each group  of features (demographic, SIS, syllabus) plus cumulative GPA.  The results of these grouped models (omitted due to space   constraints) were consistent with those reported above, with rank  ordering of syllabus features exactly matching their relative  rankings in the bivariate data.         We do not find conclusive evidence that the human-annotated  syllabus features provide a substantial boost to model  performance relative to models containing all features except  syllabus features, but this may have been due to models  underfitting to these features. Future modeling efforts should  explore models with both more and less flexible functional forms,   hyperparameter optimization, and larger datasets to achieve  deeper understanding of syllabus features and in what contexts  they most augment the predictive performance of traditional  academic datasets.    4. CONCLUSIONS AND DIRECTIONS  Syllabus features have the potential to complement more  traditional feature sets in models predicting student grades. In this  work we report on (1) a method for both collecting these features,  as well as (2) evaluating their predictiveness. Future work can  advance the goals of this study by replicating this analysis with  similar and additional course features, exploring the predictive  potential of syllabus features with different modeling approaches  and datasets, and possibly conducting causal investigations into  the impact of such instructional features on student performance.   5. REFERENCES  [1] Archer, K. and Kimes, R. 2008. Empirical characterization of   random forest variable importance measures. Computational  Statistics & Data Analysis 52.4 : 2249-2260.   [2] Atkinson, E. and Therneau, T. 2015. An introduction to  recursive partitioning using the RPART routines. Rochester:  Mayo Foundation.   [3]  Breiman, L. 1996. Heuristics of instability and stabilization  in model selection. The annals of statistics 24.6: 2350-2383.   [4] Kuhn, M. et al. 2016. caret: Classification and Regression  Training. R package version 6.0-71.   [5] The Open Syllabus Project, http://opensyllabusproject.org                                     SBJCT CD == PSYCH N INSTRUCTORS == 1  PRMRY CRER DES == Rackham SBJCT CD == COMM SBJCT CD == MCDB SBJCT CD == CHEM  SBJCT CD == ANTHRCUL SBJCT CD == ORGSTUDY  CLASS LVL == 5 CLASS SCTN CD == 200 SBJCT CD == ENGLISH  CLASS LVL == 7 N ASSIGNMENTS == 10  SBJCT CD == ASIANLAN CLASS SCTN CD == 300  SBJCT CD == NURS SBJCT CD == MATH  PRESENTATION == Yes CLASS SCTN CD == 100  N INSTRUCTORS == 4 ATTEND/PARTCP GRADE == Yes  CLASS LVL == 6 N ASSIGNMENTS == 1 CLASS SCTN CD == 2 N INSTRUCTORS == 2  SBJCT CD == GERMAN STDNT CTZN STAT SHORT DES == U.S. Citzn  N ASSIGNMENTS == 6 CURVE == Unclear  FEES == Yes  10 20 30 Variable Importance Score (scaled to 100)  Fe at  ur e  Feature Group        SIS Features  Student Demographics  Syllabus Features  Variable Importance for All Features Relative to Cumulative GPA Top 30 Shown  Figure 1: Variable importance scores from bivariate  models (relative to cumulative GPA; top 30 shown).     