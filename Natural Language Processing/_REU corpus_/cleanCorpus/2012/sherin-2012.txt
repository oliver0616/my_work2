Using computational methods to discover student science  conceptions in interview data   Bruce Sherin  Northwestern University   2120 Campus Drive  Evanston, IL USA  1-847-920-9987   bsherin@northwestern.edu          ABSTRACT  A large body of research in the learning sciences has focused on  students commonsense science knowledgethe everyday  knowledge of the natural world that is gained outside of formal  instruction. Although researchers studying commonsense science  have employed a variety of methods, one-on-one clinical  interviews have played a unique and central role. The data that  result from these interviews take the form of video recordings,  which in turn are often compiled into written transcripts, and  coded by human analysts. In my teams work on learning  analytics, we draw on this same type of data, but we attempt to  automate its analysis. In this paper, I describe the success we have  had using extremely simple methods from computational  linguisticsmethods that are based on rudimentary vector space  models and simple clustering algorithms. These automated  analyses are employed in an exploratory mode, as a way to  discover student conceptions in the data. The aims of this paper  are primarily methodological in nature; I will attempt to show that  it is possible to use techniques from computational linguistics to  analyze data from commonsense science interviews. As a test bed,  I draw on transcripts of a corpus of interviews in which 54 middle  school students were asked to explain the seasons.   Categories and Subject Descriptors  H.3.3. [Information search and retrieval]: clustering; I.2.7  [Natural Language Processing]: Text analysis; J.1  [Administrative Data Processing]: Education; K.3.1 [Computer  Uses in Education] Computer-assisted instruction (CAI)).   General Terms  Algorithms, Experimentation.   Keywords  Learning Analytics, Conceptual Change   1. INTRODUCTION  Much of the recent interest in learning analytics has been driven  by the great surge in the amount and kinds of data that are  available. This paper, in contrast, applies learning analytic   techniques to a type of data that has a long history, and that  predates recent technological advances. For the last few decades,  a large body of research in the learning sciences has focused on  students commonsense science knowledgethe everyday  knowledge of the natural world that is gained outside of formal  instruction. Although researchers studying commonsense science  knowledge have employed a variety of methods, one-on-one  clinical interviews have played a unique and central role. The data  that result from these interviews take the form of video  recordings, which in turn are often compiled into written  transcripts, and coded by human analysts.    In my teams work on learning analytics, we draw on this same  type of data, but we attempt to automate its analysis. In this paper,  I describe one part of this work. The automated analyses I present  here are not intended to code the data using categories developed  by human analysts. Rather, these analyses are employed in an  exploratory mode, as a way to discover student conceptions in the  data. Furthermore, my goal in this paper is not to contribute new  results to research on commonsense science. Rather, my aims are  primarily methodological in nature; I will attempt to show that it  is possible to use relatively simple techniques from computational  linguistics to analyze the type of data that is typically employed  by researchers in commonsense science. As a test bed, I draw on  transcripts of a corpus of interviews in which 54 middle school  students were asked to explain the seasons.   It should be emphasized that it is not at all obvious that it should  be possible to analyze data of this sort using simple computational  techniques. Unlike some other applications in learning analytics,  the total amount of data I have is relatively small. Furthermore,  the speech that occurs in commonsense science interviews can  pose particular difficulties for comprehension. Student utterances  are often halting and ambiguous. Furthermore, gestures can be  very important, and external artifacts such as drawings are  frequently referenced. However, our analysis algorithms only  have access to written transcripts of the words spoken by  participants.   Even with all of this complexity, my general approach is to go as  far as possible with simple methods, before proceeding to more  complex methods. Thus, the analyses I describe here make use of  extremely simple methods from computational linguistics methods that are based on rudimentary vector space models and  simple clustering algorithms.    2. LITERATURE REVIEW  2.1 Commonsense Science  It is now widely accepted that many of the key issues in science  instruction revolve around the prior conceptions of students. This  focus on commonsense science leads to a perspective in which the  central task of science instruction is understood as building on,      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   188    adapting, and, when necessary, replacing students prior  knowledge. One outcome of this focus has been the growth of a  veritable industry of research on students prior conceptions. The  bibliography compiled by Pfundt and Duit  [1], which lists  literature on the science conceptions of teachers and students,  provides one measure of the scale of this effort. As of early 2009,  the bibliography had over 8300 entries, spanning a wide range of  scientific disciplines, including, for example, what students  believe about the shape of the earth [2], evolution [3], and  nutrition [4].   In discussing the literature on commonsense science, it has  become commonplace to distinguish two theoretical poles. At one  extreme is the theory-theory perspective. According to this  perspective, commonsense science knowledge consists of  relatively well-elaborated theories [5]. At the other extreme, is the  knowledge-in-pieces (KiP) perspective. In this perspective, it is  assumed that: (a) commonsense science knowledge consists of a  moderately large number of elementsa systemof knowledge  and (b) the elements of the knowledge do not align in any simple  way with formal science domains [6, 7].   I believe that the computational methods described in this paper  should be of interest to a broad range of researchers who study  commonsense science, and adopt a range of theoretical  perspectives. However, the exploration of computational methods  presented in this paper was biased by my own theoretical  perspective, which lies closer to the KiP pole. As I hope will  become evident, my exploration of computational methods has  been driven by a desire to get at the more basic knowledgethe  piecesthat I believe comprise commonsense science knowledge.  And I have attempted to capture the dynamics that unfold as  students construct explanations during an interview.   2.2 Vector space models and their  applications in education research  Generally speaking, the goal of this work is to attempt to use  computational techniques in order to see student conceptions in  transcripts of commonsense science interviewers. There are many  techniques from computational linguistics that could be employed  in this way. The techniques I will use are based primarily on a  type of vector space model [8]. In vector space models, the  meaning of a block of texta word, paragraph, essay, etc.is  associated with a vector, usually in a high dimensional space. So,  two blocks of text have the same meaning to the extent that their  vectors are the same. In this way, a vector space analysis makes it  possible to compute the similarity in meaning between any pair of  words or blocks of text. In Section 4, I will describe, in some  detail, the algorithms employed in the particular analyses used in  this work.   One particular variant of vector space model, Latent Semantic  Analysis (LSA), has had increasing prominence across a range of  disciplines and applications [9-11]. LSA incorporates several  innovations that distinguish it from the most basic form of vector  space analysis; most centrally, it makes use of an auxiliary  training corpus that provides information about the wider contexts  in which terms appear, and it reduces the dimensionality of the  vector space, which has the effect of uncovering latent relations  among terms.   Vector space methods have seen increasing use in educational  research. These applications have been greatly dominated by uses  of LSA. In fact, outside of information retrieval, some of the  earliest and most persistent uses of LSA have been in applications  related to education [12]. These applications have been of two   broad types. First, LSA has been used as a research tool by  educational researchersthat is, as a means of analyzing data, in  order to study thinking and learning. Second, LSA has been used  as a component of intelligent instructional systems. The majority  of these educational applications, across both types, have been  focused on the teaching of reading and writing.  For example, LSA-based systems have been employed to  automatically score essays written by students [10, 13]. In a  number of applications, students are asked to summarize a  passage or document that they have just read, and an LSA-based  system is used to evaluate these summaries. In one such  application, Shapiro and McNamara [14] had students read and  summarize portions of psychology textbooks. Using LSA, these  summaries were then compared both to the text the students read,  and to model essays composed by experts. Similarly, Magliano  and colleagues conducted a wide range of studies in which LSA  was used to assess the strategies employed by readers and their  reading skill, more broadly [15, 16].   In many of these uses of LSA, the data consisted of written text  produced by participants in the research. However, in some  instances, LSA has been applied to transcriptions of verbal data.  For example, in their study mentioned above Shapiro and  McNamara [14] found that LSA could be applied successfully  both to written summaries of the textbook and to transcriptions of  verbal summaries given by students. Similarly, Magliano and  Millis [15] applied LSA to think-aloud protocols that students  produced as they read passages of text.   As mentioned above, LSA has been used as a component of  intelligent instructional systems. For example, intelligent systems  have been constructed that provide feedback to students on  summaries that they write of a given text passage [17, 18]. One  LSA-based system, AutoTutor, is of particular interest here  because it has been applied to teach science-related subject matter  [19, 20]. AutoTutor teaches physics by first posing a problem or  question. The student responds by typing a response into the  system. The system then evaluates that response by using LSA to  compare the students text to a set of predefined expectations and  misconceptions; the expectations are pre-specified components of  a correct response and the misconceptions are possible erroneous  ideas that might be expressed by the student. Based on this  analysis, the system responds by posing further questions to the  student, either to help correct the misconceptions, or to draw out  more components of a complete answer to the original problem.   I want to say a bit about where the work described in the present  paper fits within the space of uses of vector space models in  education. First, in this work, SNLP is used as an analytic tool for  researchers; I will not be describing an LSA-based system that is  used by students. Second, I apply my analyses to verbal data. As  mentioned above, many applications of vector space models in  education use text that is typed by a student, either in the form of  an essay or short responses. Furthermore, prior research that has  worked with verbal data has employed data that is very different  than that employed in the present work. For example, the work by  Shapiro and McNamara [14] and Magliano and colleagues [15,  16], which I mentioned above, employed a more constrained type  of think-aloud protocol, focused on passages of text that were just  read. In contrast, the verbal data employed in this work consists of  relatively free-flowing discussions involving back-and-forth  between an interviewer and interviewee.   Third, in all these applications, answers given by students,  whether in written or verbal form, were evaluated by comparison  to a predefined model. This model might be, for example, some   189    portion of the text just read, or an ideal answer constructed by the  researcher. In contrast, as mentioned above, I will describe  techniques for automatically inducing a set of conceptions from  the data itself.   Finally, I want to emphasize one other respect in which this work  differs from prior work in education that made use of LSA;  namely, I am not using LSA! As noted above, I believe it makes  sense to begin with simpler techniques, and then to pursue more  sophisticated methods as it seems necessary.   3. THE INTERVIEWS  3.1 Subject matter and interview design  The data used in this work was drawn from a larger corpus  collected by the NSF-funded Conceptual Dynamics Project  (CDP).1 For the present work, I draw from a set of 54 interviews  in which students were asked to explain Earths seasons [21].    The seasons have long been a popular subject of study in research  on commonsense science, and a significant number of studies  have set out to study student and adult understanding in this area  [22-26]. Our seasons interview always began with the interviewer  asking Why is it warmer in the summer and colder in the  winter After the student responded, the interviewer would, if  necessary, ask for elaboration or clarification. The interviewer had  the freedom, during this part of the interview, to craft questions  on-the-spot in order to clarify what the student was saying.   Next, the student was asked to draw a picture to illustrate their  explanation. Then, once again, the interviewer could ask follow- up questions for clarification. Our interviewers were also prepared  with a number of specific follow-up questions to be asked, as  appropriate, during this part of the interview. Some of these  questions were designed as challenges to specific explanations  that students might give.   3.2 Overview of student responses  In prior work with our seasons data, Conceptual Dynamics  researchers have adopted a strongly KiP perspective [21]. We  assume that students possess a system consisting of many  knowledge elementsthe piecesthat may potentially be  drawn upon as they endeavor to explain the seasons. When a  student is asked a question during an interview, some subset of  these elements are activated. The student then reasons based on  this set of elements, and works to construct an assemblage of  ideas in the service of explaining the seasons. We refer to this  assemblage of ideas as the dynamic mental construct or DMC, for  short. For the purpose of the present work, it is not a bad  approximation to think of a DMC as a students current working  explanation of the seasons. So, throughout this manuscript, I will  use the terms DMC and explanation interchangeably.   The explanations of the seasons given by the students we  interviewed varied along a number of dimensions. But it is  helpful, nonetheless, to begin with a number of reference points,  in the form of a few categories of explanations (DMCs). The first  category, closer-farther, is illustrated by the diagram in Figure 1a.  In closer-farther explanations, the earth is seen as orbiting (or  moving in some other manner) in such a way that it is sometimes  closer to the sun and sometimes farther. When the earth is closer  to the sun then it experiences summer; when its farther away it  experiences winter.                                                                        1 NSF grant #REC-0092648. Conceptual dynamics in complex  science interventions (B. Sherin, PI).   The second category of DMC, side-based, is illustrated in Figure  1b. Side-based explanations are usually focused on the rotational  motion of the earth, rather than its orbital motion. In side-based  explanations, the earth rotates so that first one side, then the other,  faces the sun. The side facing the sun at a given time experiences  summer, while the other side experiences winter.    (a)     (b)     (c)  Figure 1. Closer-farther, side-based, and tilt-based DMCS.   The third and final category of DMC, tilt-based, is depicted in  Figure 1c. Tilt-based DMCs depend critically on the fact that the  earths axis of rotation is tilted relative to a line connecting it to  the sun. In a tilt-based explanation, the hemisphere that is tilted  toward the sun experiences summer and the hemisphere that is  tilted away experiences winter. This category includes the  normative scientific explanation, as well as some non-normative  explanations.   As discussed in Sherin et al. [21], during an interview, students  tend to move among DMCs. In some cases, students do begin the  interview with what appears to be a fully-formed explanation. In  other cases, a student might construct an explanation during the  interview, slowly converging on an explanation they find  reasonable. Finally, students can be to seen to shift from one  DMC to another, sometimes in response to a challenge from the  interviewer.   3.3 Example interviews  Now I will briefly discuss a few example interviews. These  examples will play a role as important reference points when I  discuss the automated analysis. In this first example, a student,  Edgar, began by giving an explanation focused on the fact that the  Earth rotates, and he stated that light would hit more directly on  the side facing the sun. He made the drawing shown in Figure 2,  as he commented:   E: Heres the earth slanted. Heres the axis. Heres the North  Pole, South Pole, and heres our country. And the suns  right here [draws the circle on the left], and the rays hitting  like directly right here. So everythings getting hotter over  the summer and once this thing turns, the country will be  here and the sun can't reach as much. It's not as hot as the  winter.   After a brief follow up question by the interviewer, Edgar seemed  to recall that the Earth orbited the sun, in addition to rotating. He  then shifted to a closer-farther type explanation:   E Actually, I don't think this moves [indicates Earth on  drawing] it turns and it moves like that [gestures with a  pencil to show an orbiting and spinning Earth] and it turns  and that thing like is um further away once it orbit around  the s- Earth- I mean the sun.   190    I Its further away   E Yeah, and somehow like that going further off and I think  sun rays wouldnt reach as much to the earth.   Thus Edgars interview illustrates a case in which a student began  with a side-based explanation and transitioned to a closer-farther  explanation. It is also worth noting that Edgars language was  halting, imprecise, and made significant use of gestures and his  drawings. These are features that might well pose difficulties for  automated analysis.     Figure 2. Edgar's drawing.   I want to briefly introduce interviews with two other students  from the corpus, both of whom gave variants of tilt-based  explanations. The first example is from an interview with Caden.   I:  So the first question is why is it warmer in the summer and  colder in the winter   C:  Because at certain points of the earths rotation, orbit around  the sun, the axis is pointing at an angle, so that sometimes,  most times, sometimes on the northern half of the  hemisphere is closer to the sun than the southern  hemisphere, which, change changes the temperatures. And  then, as, as its pointing here, the northern hemisphere it  goes away, is further away from the sun and gets colder.   I:  Okay, so how does it, sometimes the northern hemisphere  is, is toward the sun and sometimes its away   C:  Yes because the atIm sorry, the earth is tilted on its axis.  And its always pointed towards one position.   Note that, in Cadens explanation, the tilt of the earth affects  temperature because the hemisphere tilted toward the sun is closer  to the sun, and the hemisphere tilted away is farther from the sun.  (This is not correct.) In contrast, another student, Zelda gave a tilt- based explanation, but her explanation made use of the fact that  the tilt of the earth causes rays to strike the surface more or less  directly, and this is what explains the seasons.   Z: Because, I think because the earth is on a tilt, and then, like  that side of the Earth is tilting toward the sun, or its facing  the sun or something so the sun shines more directly on that  area, so its warmer.   Thus, Caden and Zelda both gave tilt-based explanations, but they  differed in how exactly the tilt of the earth affected the seasons.  For Caden the tilting causes one hemisphere or the other to be  closer to the sun. For Zelda, the tilting causes parts of the earth to  receive the suns rays more or less directly. This illustrates some  of the types of features we would like the automated analysis to  resolve.    4. VECTOR SPACE ANALYSIS OF THE  SEASONS CORPUS  In order to captured students conceptions expressed in the  seasons interviews, my team explored the use of techniques from  statistical natural language processing. In particular, we explored  the use of vector space models, augmented with cluster analysis.  These choices make sense for a number of reasons. As mentioned  above, one type of vector space model, LSA, has already been  employed, with some success, in applications that are in some   respects close to my own [10, 14-17, 19, 27].   In addition, initial attempts by Gregory Dam and Stefan  Kaufmann to apply LSA to my research teams data proved  promising, and thus justified further exploration [28]. Dam and  Kaufmann employed techniques based on one variant of LSA to  apply a given coding scheme to an earlier subset of this corpus.    The work described in this manuscript extends the work of Dam  and Kaufmann in several respects. First, Dam and Kaufmanns  analysis did not discover student conceptions in the data corpus.  Instead, it began with the conceptions identified by human  analysts and used those conceptions to code transcript data.  Second, unlike Dam and Kaufmann, I will be exploring the use of  simpler vector space models, rather than LSA.    Third, Dam and Kaufmann were primarily concerned with coding  at the level of students. Each student was coded by the computer  in terms of just one of three possible explanations of the seasons.  The success of this analysis was judged by comparison to an  analysis of these same transcripts by human coders, restricted to  the same set of three explanations. However, this type of analysis  represented a drastic simplification over our earlier qualitative  analyses of the corpus. As exemplified in the description of  Edgars interview above, the explanations given by students over  the course of an interview were quite clearly dynamic. Thus,  assigning a single code to each manuscript was often a dramatic  simplification. In this new work, all of my analysis is done at a  finer time scale; I look to identify student ideas only in small  segments of text.   In the rest of this section, I describe an exploratory analysis of our  data. Here, I restrict myself to one pathway through the analysis,  using one set of parameters and algorithms. In Section 5, I briefly  describe the results I obtain when employing different parameters  and algorithms.   4.1 The basics: Converting text to vectors  The central idea underlying any vector space model of text  meaning is relatively simple: Every passage of textwhether it is  a word, sentence, or essayis mapped to a single vector. The  direction in which this vector points is taken to be a representation  of the meaning of the passage. More precisely, the similarity  between two passage vectors is quantified as the cosine of the  angle between the two vectors (or, equivalently, the dot product of  the vectors if we assume the vectors are of unit length).   Table 1. Partial vocabulary and sample counts   sun 4 2.1  earth 2 1.7  side 0 0  away 2 1.7  tilted 1 1  closer 1 1  axis 2 1.7  day 0 0  farther 1 1  time 3 2.1   The question, of course, is how we go about converting a passage  of text to a vector. In the most rudimentary forms of vector space  models, this mapping is accomplished in a rather straightforward  manner. First, we look across the entire corpus of text that we  wish to include in our analysis, and we compile a vocabulary, that  is, a complete list of all of the words that appear somewhere in the   191    corpus. This vocabulary is then pruned using a stop list of  words. This stop list consists primarily of a set of highly common  non-content words, such as the, of, and because. For the corpus  used in this work, this resulted in a vocabulary consisting of 647  words. (The stop list used contained 782 terms.) If the vocabulary  is sorted from the most common to least common words, the top  10 words correspond to the list shown in the left hand column of  Table 1.   This vocabulary can be used to compute a vector for a passage  from an interview transcript as follows. First, we take the  transcript and remove everything except the words spoken by the  student. Any portion of the remaining text can now be converted  to a vector. To do so, we go through the entire vocabulary,  counting how many times each word in the vocabulary appears in  the text being analyzed. When this is done, we get a list of 647  numbers. If, for example, we process the portion of Cadens  transcript presented above, we obtain the values listed in the  middle column of Table 1 for the 10 most common words in the  larger corpus.   Finally, in most vector space analyses, the raw counts are  modified by a weighting function. In the analyses reported on in  this section, I replaced each count with (1 + log(count)). This has  the effect of dampening the impact of very frequent words. (Raw  counts of 0 were just left as 0.) Appropriately weighted values are  shown in the third column of Table 1.   4.2 Using passage vectors to discover  meanings in the data corpus  We now have a means of mapping a passage of text to a vector  consisting of 647 numbers. This capability can now be used to  discover units of meaning that exist across the 54 interviews that  comprise my data corpus. This process involves four steps which I  will now discuss: (1) preparing and segmenting the corpus, (2)  mapping segments to vectors, (4) clustering the vectors, and (5)  interpreting the results.    4.2.1 Preparing and segmenting transcripts  First, as discussed above, the transcripts are reduced to that they  include only the words spoken by the student during the  interview. Next, recall that, in our earlier analyses of this corpus  conducted by my research team, we found that students could be  seen to construct explanations of the seasons out of large number  of knowledge resources, and that their explanations could shift as  an interview unfolded. We thus need a way to attach meanings to  small parts of an interview transcript. This requires a means of  segmenting a transcript into smaller parts.    In keeping with my goal of using simple methods, I segmented the  transcripts by breaking each transcript into 100-word segments. In  order to lessen problems that might be caused by the fact that this  introduces arbitrary boundaries, I chose to employ overlapping  100-word segments, with the start of each segment beginning 25  words after the start of the preceding segment. So the first  segment of a transcript would include words 1-100, the second  words 26-125, the third 51-150, etc. When all of the 54 interview  transcripts were segmented in this manner, I ended up with 794  segments of text. These specific choices for segment size and step  size are, of course, somewhat arbitrary. In Section 5, I will briefly  present results with different values of these parameters.   4.2.2 Mapping segments to vectors  The next step in the analysis is to map each of these 794 segments  to a vector. To accomplish this, I employ precisely the method   described above. The result is 794 vectors, each consisting of a  list of 647 numbers.   However, here I must introduce one complication. There is one  inherent problem with applying vector space models to an  analysis of this sort of data. Vector space models such as LSA  were originally developed as a means to find documents in a large  corpus that pertain to a given topic. They were thus not developed  for finding fine distinctions in meaning among documents  pertaining to very similar topics. However, all of the documents  involved in my analysis are about very similar subject matter; they  all explain the seasons, and they almost all do so by talking about  the position and motion of the earth in relation to the sun.   In fact, the clustering analysis (described in the next section) does  not produce meaningful results if I use the raw document vectors  that are produced by the method described above. (I will say more  about this problem in Section 5.) Instead, I need a means of  modifying the vectors so that they highlight their more unique  featuresthe features that, on average, tend to differentiate the  segment from the other 793 segments of text.   For that purpose, I compute what I call deviation vectors. To  compute the deviation vectors for two vectors V1 and V2, I first  find their average, and then break each vector into two  components, one that lies along the average, and another that is  perpendicular to the average (refer to Figure 3). The perpendicular  components, V1' and V2', are the deviation vectors. If we use  these deviation vectors in place of the original vectors, the result  is that V1 and V2, have each been replaced by the component that  defines its unique piece  a piece that characterizes how it differs  from the average.    The same procedure can be employed with any number of vectors.  For the next steps of the analysis, I replaced the 794 segment  vectors in just this way; I found their average, and then replaced  each vector with its deviation from this average.     Figure 3. How to compute deviation vectors.   4.2.3 Clustering the vectors  Now each of the 794 segments has been mapped to a vector that  we understand as representing the meaning of that segment. The  next step is to identify common meanings amongst these  segments. To do that, we look for natural clusterings of the 794  vectors.   To cluster the transcript vectors, I employed the very general  technique called hierarchical agglomerative clustering (HAC). In  HAC, we begin by taking all of the items to be clustered, and  placing each of these items in its own cluster. Thus, we begin with  a number of clusters equal to the total number of items. Then we  pick two of those clusters to combine into a single cluster  containing two items, thus reducing the total number of clusters  by one. The process then iterates; we again pick two clusters to  combine, and the total number of clusters is decreased by one.  This repeats until all of the items are combined into a single  cluster. The result is a list of candidate clusterings of the data,   192    with each candidate corresponding to one of the intermediate  steps in this process.   A central issue in applying this algorithm is determining which  clusters to combine on each iteration. In practice, there are many  rules that can be applied. Throughout my discussions here, I will  present results that were obtained using a technique called  centroid clustering. At each step in the iteration, I first find the  centroid of each cluster (the average of all of the vectors currently  in the cluster). Then I find the pair of centroids that are closest to  each other, and merge the associated clusters. An explanation of  centroid clustering, including its application to vector space  models, can be found in [29].   4.2.4 Determining the number of clusters  The result of the clustering analysis can be thought of as a table  with 794 rows. At the top is a row in which each segment is in a  single cluster. At the bottom is a row in which all of the segments  are in a single cluster. Table 2 displays the results for just a part of  this large table. The bottom row, for example, shows the results  when the segments are grouped into three clusters that contain 271  segments, 279 segments, and 244 segments respectively. As you  move up the table the number of clusters grows, and the size of  each cluster shrinks. In each row of Table 2, clusters contain  segments that have been grouped together because, from the point  of view of our vector space model, they have similar meanings.  This means that each row in Table 2, constitutes a candidate  coding schemeit is a scheme for sorting segments into  categories. The puzzle, of course, is which row to select.    Table 2. Sizes of clusters for selected clusterings   10 19 72 9 68 140 62 44 122 136 122   9 19 72 68 62 44 122 136 122 149   8 19 72 68 44 122 136 122 211   7 72 68 44 122 122 211 155   6 68 44 122 122 211 227   5 68 122 122 211 271   4 122 122 271 279   3 271 279 244    Unfortunately, there is no simple answer to this question. In  general, there is a tradeoff. When the number of clusters is high,  we obtain a better fit to the data. However, we get this better fit at  the expense of a more complex model. Because each cluster is  described by a list of 647 values, each additional cluster  represents a dramatic increase in model complexity.2   Here, as elsewhere, I make my choice in a heuristic manner.  Across multiple analyses, I have found that working with a set of  about 7 clusters strikes a workable balance. With 7 clusters, it is  possible to resolve interesting features of the data, while  producing results (in the form of graphs) that are not overly  difficult to interpret.   4.2.5 What do the clusters mean  We now have grouped the 794 segments into 7 clusters, each                                                                        2 For this reason, if I use traditional measures for determining the   appropriate number of clusters (e.g., Bayesian information  criterion or Akaike information criterion), the terms  corresponding to the model complexity always dominate, and  the model with the smallest number of clusters prevails.   containing between 44 and 211 segments (refer to Table 2). The  next question we must answer is: What do these clusters mean  Each of the 7 clusters can be thought of as defined by its centroid  vectorthe average of all of the vectors that comprise the cluster.  These centroids each, in turn, are described by a list of 647  entries, each of which corresponds to one of the words in the  vocabulary. One way to attempt to understand the meaning of the  clusters, then, is to look at the words that have the largest value in  each centroid vector.    When this is done I obtain the results shown in Figure 4. For each  cluster, I list the 10 words that are most strongly associated with  that cluster, ignoring words that appeared less than 30 times in the  overall corpus. In addition, the second column in each table has  the value from the centroid vector corresponding to this word. The  third column in each table lists the total number of times that the  word appears across the entire corpus.    4.2.6 Interpreting the clusters based on the word lists  In many respects, the lists of words shown in Figure 4 clusters are  suggestive. First, several of the clusters seem to align with the  three broad classes of seasons explanations listed in Section 3. For  example, it seems natural to associate Cluster 1, which starts with  the words tilted, towards, and away, with tilt-based explanations.  Similarly, it seems natural to associate Cluster 4 (side, facing)  with side-based explanations, and Cluster 7 (farther, closer) with  closer-farther explanations of the seasons.   Cluster 1Cluster 1Cluster 1  tilted 0.767 82 towards 0.199 40 away 0.186 83 north 0.098 30 part 0.084 46 guess 0.077 31 closer 0.044 82 warmer 0.042 40 sun 0.03 545 farther 0.017 71  Cluster 2Cluster 2Cluster 2  earth 0.4 395 spinning 0.366 37 spins 0.2 38 time 0.198 65 axis 0.121 77 seasons 0.068 30 tilted 0.031 82 angle 0.017 31 north 0.014 30 chicago 0.006 45  Cluster 3Cluster 3Cluster 3  hemisphere 0.603 47 northern 0.522 31 colder 0.119 52 facing 0.106 46 closer 0.043 82 farther 0.035 71 warmer 0.023 40 axis 0.021 77 away 0.02 83 rays 0.018 33  Cluster 4Cluster 4Cluster 4  side 0.722 95 facing 0.091 46 earth 0.085 395 part 0.068 46 chicago 0.018 45 guess 0.008 31 seasons -0.008 30 time -0.01 65 heat -0.025 30 rotates -0.026 54  Cluster 5Cluster 5Cluster 5  rays 0.293 33 north 0.197 30 angle 0.194 31 light 0.188 41 chicago 0.163 45 sun 0.134 545 heat 0.076 30 towards 0.045 40 warmer 0.02 40 side 0.019 95  Cluster 6Cluster 6Cluster 6  day 0.415 75 moon 0.398 52 night 0.377 63 rotates 0.178 54 rotating 0.068 32 earth 0.055 395 spins 0.05 38 facing 0.048 46 light 0.046 41 seasons 0.046 30  Cluster 7Cluster 7Cluster 7  farther 0.413 71 closer 0.403 82 away 0.379 83 colder 0.216 52 sun 0.103 545 warmer 0.064 40 rotates 0.033 54 time 0.028 65 heat 0.02 30 rotating 0.013 32    Figure 4. Top words associated with each cluster.  But these clusters are not supposed to necessarily align with full- fledged explanations of the seasons. They are clusters of  segments, which it is hoped can align with smaller conceptual  units that, when combined, form the basis of a constructed   193    explanation. And, indeed, the additional clusters do seem to offer  the possibility of an analysis of that sort. For example, we should  expect tilt-based explanations to often be seen in concert with talk  about the Earths hemispheres (Cluster 3). And recall that tilt- based explanations invoke different mechanisms by which the  changing tilt of the earth impacts temperature. For example,  Caden argued that the tilting of the Earth causes parts of the earth  to be alternately closer or farther from the sun. In contrast, Zeldas  explanation focused on the impact of the Earths tilt and how it  impacts the angle and directness of the suns rays. We should thus  be able to see these ideas in combination, when we look at  individual interviews.   Similarly, we should expect to see side-based explanations  (Cluster 4) in tandem with clusters having to do with the rotation  of the Earth. Ideas about the rotation of the Earth seem to appear  in Cluster 2 and Cluster 6. Cluster 2 seems to truly be focused on  the spinning of the Earth. Cluster 6, in contrast, seems to be more  about day and night. Not surprisingly, talk about the rotation of  the Earth was often combined with talk about the day/night cycle.    4.3 Application to segmented transcripts  The clusters shown in Figure 4 thus seem to have reasonable  interpretations in terms of our understanding of the data corpus.  We have thus identified a set of common underlying ideas. A  next step I can take is to apply this set of ideas back to the original  transcripts. I want to use these ideasthese units of meaningto  interpret individual student interviews.   In order to accomplish this, I begin by preparing each of the  interview transcripts precisely as before; the transcripts are  reduced so that they include only the words spoken by a student,  then they are broken into 100-word segments using a moving  window that steps forward by 25 words. Next I compute the  vector for each of these segments, again using the same  techniques described earlier. Finally, each of these vectors for the  segments is compared to the 7 centroid vectors corresponding to  the 7 clusters (by taking the cosine of the angle between the  vectors and each centroid).   I begin my discussion of the results with Zelda, since her analysis  produces a graphic that is relatively easy to read. In Figure 5 we  see that Zeldas transcript has been broken into 5 overlapping  segments. Each of these segments is associated with 7 bars, one  bar for each of the 7 clusters. For all of the segments, Cluster 1 is  the clear winner. This makes sense since, as discussed earlier,  Zelda gave an answer that was very close to the accepted  scientific explanation of the seasons. Note, also, that the bar for  Cluster 5 is slightly elevated in three of the segments. Cluster 5  had to do with rays striking the earth at an angle. Again, this  makes sense given what we can read in Zeldas transcript.     Figure 5. Segmenting analysis of Zelda's transcript.   The interview with Caden provides an interesting contrast. When  Cadens transcript is analyzed using the segment centroids, we get  8 segments with the bars shown in Figure 6. Like Zelda, we  understood Caden as giving an explanation that emphasized the  tilt of the Earth. But, in Figure 6, we see that Cluster 3  dominatesthe cluster having to do with the Earths hemispheres   although there are hints of Cluster 1 (tilted-toward) in the  earlier segments. The predominance of Cluster 3 is not too  surprising. As I noted earlier, tilt-based explanations should be  closely associated with discussion of the two hemispheres of the  earth. Indeed, glancing at the portion of Cadens transcript  presented earlier, there is an emphasis on the different effects on  the northern and southern hemispheres.     Figure 6. Segmenting analysis of Caden's transcript.   Both Zelda and Caden were relatively stable in the explanations  that they gave. We would now like to see if this analysis can  capture shifts that occur as an interview unfolds. To see this, we  can now return to the interview with Edgar. Looking at Figure 7 it  seems clear that the interview has a two major parts. The first part  is dominated by Cluster 5, which has to do with rays striking the  Earths surface. The latter part is strongly dominated by Cluster 7,  which is the closer-farther cluster. Thus, once again, it seems  possible to interpret the automated analysis in a manner that is  consistent with our qualitative analysis of the interview.     Figure 7. Segmenting analysis of Edgars transcript   5. Alternative analysis methods  To this point, my exploration has been limited in a particular way;  I have looked at some of the results produced by my analysis, but  all of these results were produced by a single set of algorithms,  and with a single set of input parameters. In particular, my  analysis followed the following plan: (1) The transcripts were  pruned so that they only contained the words spoken by the  interviewee; (2) the resulting documents were broken into 100- word segments, with a step size of 25 words; (3) a vector was  computed for each segment, using a weight function of (1 +  log(count)), and ignoring words in my stop list; (4) the resulting  vectors were replaced with their deviation vectors; and (5) the  vectors were clustered using hierarchical agglomerative  clustering. These choices of algorithms and parameters were  chosen, in part, because they produced interpretable results. In this  section, I want to briefly give a sense for the results produced by  alternative approaches, including some that did not produce  interpretable results.   5.1 Alternative parameters  There are many ways in which the above analysis could be  altered, while still employing a method that is very similar in  outline. For example, the composition of the stop list could be  changed, and the transcripts could be pruned in a different  manner. For example, in pruning the transcripts, I needed to  decide what to do about word fragments, whether to leave them,  delete them, or complete them. More dramatically, I could have   194    opted to stem words, that is, to reduce them to their base or root.  For the most part, these smaller changes produced similar  interpretable results across a large range of variations.    For illustration, I will present the results obtained if the transcripts  are segmented into 50 word segments, with a step size of 10  words (rather than 100 and 25). When this is done, I end up with  2320 segments. When these segments are clustered, I obtain the  results shown in Table 4. Like Table 3, this table shows the sizes  of the clusters that are produced during some of the latter steps in  the clustering. Note that when the segments are grouped into 7  clusters, one of the clusters contains only 1 segment. For that  reason, it makes to sense to look at the next row in the table,  where the segments are grouped into 6 clusters. Figure 8 shows  the word lists associated with these clusters, which were produced  in the same method used to produce the lists in Figure 4.   Table 3. Sizes of clusters for selected clusterings   10 1 78 88 160 156 11 628 235 137 638   9 1 88 160 156 11 628 235 638 215   8 1 160 11 628 235 638 215 244   7 1 160 235 638 215 244 639   6 160 235 638 215 639 245  5 235 638 215 639 405   4 638 639 405 450  3 638 450 1044    Now we can compare the lists in Figures 8 and 4. Although there  are many differences, it is not difficult to discern an alignment.  Clusters 1, 3, 4, and 5 in the new analysis seem to be similar to the  corresponding clusters in the original analysis. Cluster 6 seems to  be similar to Cluster 7 in the original analysis. Finally, Cluster 2  in the new analysis is similar to both Cluster 2 and Cluster 6 in the  original analysis. (Note that it makes sense for Clusters 2 and 6 in  the original analysis to be grouped together since they both pertain  to the rotation of the Earth.) Thus, while there are certainly  differences, the qualitative picture produced by this new analysis  seems to bear a close resemblance to the original analysis.   Cluster 1Cluster 1Cluster 1  tilted 0.718 82 north 0.353 30 towards 0.185 40 part 0.106 46 away 0.076 83 guess 0.043 31 warmer 0.022 40 angle 0.015 31 chicago 0.0 45 hemispher e  -0.009 47  Cluster 2Cluster 2Cluster 2  earth 0.522 395 moon 0.298 52 day 0.218 75 rotates 0.206 54 night 0.197 63 axis 0.164 77 spinning 0.16 37 spins 0.1 38 rotating 0.069 32 time 0.061 65  Cluster 3Cluster 3Cluster 3  hemisphere 0.618 47 northern 0.433 31 facing 0.312 46 colder 0.096 52 part 0.033 46 rotating 0.029 32 light 0.018 41 away 0.015 83 axis 0.011 77 towards 0.008 40  Cluster 4Cluster 4Cluster 4  side 0.849 95 earth 0.048 395 seasons 0.023 30 facing 0.015 46 warmer 0.013 40 rotates 0.007 54 guess 0.004 31 chicago -0.013 45 part -0.016 46 northern -0.017 31  Cluster 5Cluster 5Cluster 5  chicago 0.6 45 light 0.35 41 rays 0.315 33 heat 0.113 30 time 0.071 65 towards 0.045 40 facing 0.041 46 sun 0.037 545 warmer 0.022 40 seasons 0.014 30  Cluster 6Cluster 6Cluster 6  sun 0.431 545 closer 0.305 82 farther 0.261 71 away 0.208 83 colder 0.09 52 angle 0.059 31 heat 0.038 30 warmer 0.014 40 guess -0.001 31 rays -0.024 33    Figure 8. Top words associated with each cluster.   5.2 Alternative algorithms  Of course, it is possible to make much more substantial changes to  the analysis presented in Section 4. Here I will consider changes  related to one unusual feature of my analysis, the use of deviation  vectors. Recall that I introduced the deviation vectors as a means  of addressing the fact that there was substantial overlap in the  vectors that are produced by my initial computation of document  vectors.    Table 4 shows the results that are produced when I do not  compute deviation vectors before clustering the segments. Note  that in each of the candidate clusterings shown, I obtain one very  large cluster, containing most of the segments, and several very  small clusters. When I look at earlier stages of the clustering, I see  the following behavior: initially the segments are all clustered into  a large number of relatively small changes, then the these small  clusters begin to agglomerate, one at a time, onto one large  cluster. In the final stages, which we see in Table 4, some small  remaining clusters are swept up into the large cluster. In short, this  analysis does not seem to discover a small number of moderately- sized clusters that we can associate with conceptions.   Table 4. Sizes of clusters for selected clusterings   10 2 2 3 4 3 4 9 6 11 750   9 2 2 3 4 3 4 6 11 759   8 2 2 3 4 3 4 6 770   7 2 2 3 3 4 6 774   6 2 3 3 4 6 776   5 2 3 3 4 782   4 2 3 4 785   3 3 4 787    The question remains as to whether there are other more standard  approaches that might improve on the results shown in Table 4. In  particular, it is standard practice to use judiciously-chosen  weighting functions as a means of accentuating the differences  among documents. Recall that the counts in my vectors were all  weighted by (1 + log(count)), where count is the number of  times a word appears in a given document. We can modify this  function so that it weights words that appear across many  documents less strongly than words that appear in in just a few  documents. I tried several such weighting functions, including  variants of the so-called tf-idf weighting. In all cases, I obtained  results that looked like Table 4.   6. Discussion  6.1 Summary  I began this paper with the observation that research on  commonsense science knowledge typically focuses on data  derived from one-on-one clinical interviews. To date, researchers  in this field have generally used humans as instruments for  analyzing this data. I believe that we have done so because of  some tacitly-held beliefs: we have tended to assume that, to make  sense of clinical interview data, it is necessary to have an  instrument with an ability to understand natural language. We  have also assumed that it is necessary to have access to as much  of the interaction as possible. We need not just the words spoken;  we also need gestures, facial expressions, drawings, etc. It also  seems to require the ability to make leaps that look across the  breadth of a data corpus.  The task of analysis is also, in some respects, complicated by the   195    theoretical position I adopted in this work. I believe that, in many  cases, it is simply not possible to understand a student as  expressing a single model of the seasons. Instead, students  construct and shift explanationsDMCsas the interview  unfolds. I want to capture this movement in explanations.    Nonetheless, this work set out to explore how much can be  accomplished with a relatively simple suite of techniques from  statistical natural language processing. Stated crudely, the  statistical techniques rely primarily on counting words.  Furthermore, from among the bag of words models that are  employed by linguists, I chose to begin with one of the simplest  possible models.   In short, there was every reason to think that the types of analysis  described here would not be very successful. Nonetheless, these  results are at least suggestive that these simple techniques can  give meaningful results. The clustering algorithm produced a set  of clusters that seemed to have meaningful interpretations interpretations that made sense given earlier qualitative analyses  of the same corpus. And, when these clusters were employed to  produce a segmented analysis of individual transcripts, they  produced a narrative analysis of the transcript that aligned with  the descriptions produced by human analysts.   That said, most of my presentation in this manuscript has had an  exploratory character. My goal has been only to begin to map the  boundaries of what might be possible with a family of relatively  simple computational techniques. In this final section of the paper,  I reflect on what we can conclude, and I discuss caveats.   6.2 What do these computational techniques  buy us  What role might these computational techniques play in the  toolkit of researchers, especially researchers who use clinical  interviews to study the conceptions of science students I  presented many results that were intriguing, but my results were  only intended to be about the methods themselves; I didnt use the  methods in the service of any scientific agenda. So what, in the  long term, might these techniques buy us   One question to ask is whether computational techniques can and  should replace human analysts, or at least reduce the work  required. Whether or not this may ultimately be possible, I should  be clear that the analyses presented in this paper were still highly  dependent on human interpretation. For example, I had to make a  judgment about the appropriate number of clusters to work with.  Even more importantly, the analysis required me to make sense of  the lists of words that were associated with each of the clusters.  Nonetheless, the computational techniques described here might  play a useful role in our toolkits, even in the short term. In  particular, I believe that the biggest and most immediate  contribution will be in the support computational techniques can  provide for traditional kinds of analysis. I believe that the primary  contribution of the computational techniques will be in their  ability to provide a type of triangulation that helps us to establish  the validity of our analyses. This point is worth some elaboration.  When two humans code the same data in order to establish the  reliability of a coding procedure, they are, in a fundamental way,  doing the same thing. Thus, when two humans code, we have two  sets of measurements, both essentially performed with the same  type of experimental apparatus. In contrast, if we can find a way  to obtain confirmatory results, using a very different type of  apparatus, then that should more profoundly increase our  confidence in the validity of our results. It is that type of support   that I believe is the biggest potential contribution of the  computational techniques.    6.3 Open issues and next steps  A large number of problems remain unsolved, some of which I  have been careful to highlight, others which I have glossed over.  Here I will mention a few.    6.3.1 Additional subject matter  One obvious next step is to try some of these same analyses on  different interview data, about topics other than the seasons. It is  possible that there is something special about the seasons as  subject matter. For example, it might be that, in this territory, a  small number of key words (e.g., tilt) can do a lot of the work of  discriminating among explanations.    6.3.2 Systematic comparison to human analysis  My presentation in this manuscript was, in places, selective and  anecdotal. I did not discuss the segmented analysis of every  transcript; I just selected a few to give the flavor of the analysis  produced, and I relied on the readers intuition to judge whether  the automated and human analyses were in accord. Thats in  keeping with the exploratory approach adopted in this manuscript.  But, ultimately, I want to perform an analysis in which I  systematically compare the output of the automated analysis to  codes produced by human analysts. As mentioned earlier, Dam  and Kaufmann [28] performed such an analysis in which each  transcript was given a single code. However, their analysis did not  capture the dynamic features of interviews. An analysis focused  on a smaller grain size is in greater accord with the theoretical  perspective I have adopted, and I believe it will be possible to  obtain good agreement between human and automated codes.   6.3.3 Systematic investigation of alternative analysis  methods.  In Section 5, I very briefly talked about the results I obtained  when using different methods and parameters. In future work, I  want to extend this exploration of alternative analysis methods so  that it is both more deep and more broad. I believe that it is  important to have a more deep understanding of why some  methods work and others do not. And I want to look more broadly  and systematically at alternative techniques, including some that  begin to depart significantly from the methods discussed here,  including latent semantic analysis [9-11], probabilistic latent  semantic indexing [30], and latent Dirichlet allocation [31].    6.3.4 Why does this work  Finally, perhaps the greatest puzzle raised by this research is the  question of why these techniques work at all. Where is the magic  In my view, this question is almost, on its own, worthy of a  program of research. Are gestures and diagrams really so  unimportant to understanding the explanations given in interviews  of this sort Are a few key words enough to understand what  students are saying Why did the clustering analysis pick out  precisely the same set of categories as our human coders  Answering these questions may do more than tell us something  about this new class of methods, it might lead to a deeper  understanding of the very phenomena about thinking and learning  that were are seeking to study. Ultimately this question will need  to be a focus of future research.    7. ACKNOWLEDGMENTS  This work was funded in part by NSF grant #REC-0092648.   196    8. REFERENCES  [1] Duit, R.  2009. Bibliography: Students' and Teachers'   Conceptions and Science Education. Leibniz Institute for  Science Education, Kiel, Germany.   [2] Vosniadou, S. and Brewer, W. F. 1992. Mental models of the  earth: A study of conceptual change in childhood. Cognitive  Psychol., 24, 4 (Oct. 1992), 535-585.    [3] Samarapungavan, A. and Wiers, R. W. 1997. Children's  thoughts on the origin of species: A study of explanatory  coherence. Cognitive Sci., 21, 2 (Apr.-Jun. 1997), 147-177.    [4] Wellman, H. M. and Johnson, C. N. 1982. Children's  understanding of food and its functions: A preliminary study  of the development of concepts of nutrition. J. Appl. Dev.  Psychol., 3 135-148.    [5] McCloskey, M. 1983. Naive theories of motion. In Mental   Models, D. Gentner and A. Stevens eds. Erlbaum, Hillsdale,  NJ, 289-324.    [6] Sherin, B. 2001. How students understand physics equations.  Cognition Instruct., 19, 4 479-541.    [7] Smith, J. P., diSessa, A. A. and Roschelle, J. 1993.  Misconceptions Reconceived: A Constructivist Analysis of  Knowledge in Transition. J. Learn. Sci., 3, 2 115-163.    [8] Salton, G., Wong, A. and Yang, C. S.  1974. A vector space  model for automatic indexing. Dept. of Computer Science  Cornell University, Ithaca N Y.   [9] Deerwester, S., Dumais, S. T., Furnas, G., Landauer, T. and  Harshman, R. 1990. Indexing by latent semantic analysis. J.  Am. Soc. Inform. Sci., 41, 6 (Sep. 1990), 391-407.    [10] Landauer, T., Foltz, P. W. and Laham, D. 1998. An  introduction to latent semantic analysis. Discourse Process.,  25, 2-3 259-284.    [11] Berry, M. W., Dumais, S. T. and O'Brien, G. W. 1995. Using  linear algebra for intelligent information retrieval. Siam Rev.,  37, 4 (Dec. 1995), 573-595.    [12] Haley, D., Thomas, P., De Roeck, A. and Petre, M. 2005. A  Research Taxonomy for Latent Semantic Analysis-Based  Educational Applications. In Proceedings of the International  Conference on Recent Advances in Natural Language  Processing (Borovets, Bulgaria, 2005).   [13] Landauer, T., Laham, D. and Foltz, P. W. 2003. Automatic  essay assessment. Assessment in Education: Principles,  Policy and Practice, 10, 3 295-308.    [14] Shapiro, A. M. and McNamara, D. S. 2000. The Use of  Latent Semantic Analysis as a Tool for the Quantitative  Assessment of Understanding and Knowledge. J. Educ.  Comput. Res., 22, 1 1-36.    [15] Magliano, J. P. and Millis, K. K. 2003. Assessing reading  skill with a think-aloud procedure and latent semantic  analysis. Cognition Instruct., 21, 3 251-283.    [16] Millis, K., Kim, H.-J. J., Todaro, S., Magliano, J. P.,  Wiemer-Hastings, K. and McNamara, D. S. 2004.  Identifying reading strategies using latent semantic analysis:   Comparing semantic benchmarks. Behav. Res. Methods, 36,  2 213-221.    [17] Wade-Stein, D. and Kintsch, E. 2004. Summary Street:  Interactive computer support for writing. Cognition Instruct.,  22, 3 333-362.    [18] Wiemer-Hastings, P. and Graesser, A. C. 2000. Select-a- Kibitzer: A Computer Tool that Gives Meaningful Feedback  on Student Compositions. Interact. Learn. Envir., 8, 2 (Jan 1  2000), 149-169.    [19] Graesser, A. C., Lu, S., Jackson, G. T. and Mitchell, H. 2004.  AutoTutor: A tutor with dialogue in natural language. Behav.  Res. Methods, 36, 2 (May 2004), 180-192.    [20] Graesser, A. C., Wiemer-Hastings, P. and Wiemer-Hastings,  K. 2000. Using Latent Semantic Analysis to Evaluate the  Contributions of Students in AutoTutor. Interact. Learn.  Envir. (Jan 1 2000), 129-147.    [21] Sherin, B., Krakowski, M. and Lee, V. R. 2012. Some  assembly required: How scientific explanations are  constructed during clinical interviews. J. Res. Sci. Teach.,  49, 2 166-198.    [22] Atwood, R. K. and Atwood, V. A. 1996. Preservice  Elementary Teachers' Conceptions of the Causes of Seasons.  J. Res. Sci. Teach., 33, 5 (May 1996), 553-563.    [23] Newman, D. and Morrison, D. 1993. The conflict between  teaching and scientific sense-making: The case of a  curriculum on seasonal change. Interact. Learn. Envir., 3 1- 16.    [24] Sadler, P. M. 1987. Alternative conceptions in astronomy. In  Proceedings of the Second international seminar on  Misconception and Educational Strategies in Science and  Mathematics (Ithaca, NY, 1987). Cornell University Press.   [25] Trumper, R. 2001. A cross-college age study of science and  nonscience students' conceptions of basic astronomy. Journal  of Science Education and Technology, 10, 2 192-195.    [26] Lelliott, A. and Rollnick, M. 2010. Big Ideas: A Review of  Astronomy Education Research 1974-2008. Int. J. Sci. Educ.,  32, 13 1771-1799. Doi 10.1080/09500690903214546.   [27] Foltz, P. W., Kintsch, W. and Landauer, T. 1998. The  measurement of textual coherence with latent semantic  analysis. Discourse Process., 25, 2-3 285-307.    [28] Dam, G. and Kaufmann, S. 2008. Computer assessment of  interview data using latent semantic analysis. Behav. Res.  Methods., 40, 1 (Feb. 2008), 8-20. Doi 10.3758/Brm.40.1.8.   [29] Manning, C. D., Raghavan, P. and Schtze, H.  2008.  Introduction to information retrieval. Cambridge University  Press, New York.   [30] Hofmann, T. 2001. Unsupervised Learning by Probabilistic  Latent Semantic Analysis, 42, 1 177.    [31] Blei, D. M., Ng, A. Y. and Jordan, M. I. 2003. Latent  Dirichlet Allocation. J. Mach. Learn. Res., 3, 4/5 (May 15  2003), 993-1022.         197      