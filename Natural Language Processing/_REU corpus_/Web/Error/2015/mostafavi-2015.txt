
Towards Data-Driven Mastery Learning

Behrooz Mostafavi
Department of Computer

Science
North Carolina State

University
Raleigh, NC 27695

bzmostaf@ncsu.edu

Michael Eagle
Department of Computer

Science
North Carolina State

University
Raleigh, NC 27695

mjeagle@ncsu.edu

Tiffany Barnes
Department of Computer

Science
North Carolina State

University
Raleigh, NC 27695

tmbarnes@ncsu.edu

ABSTRACT
We have developed a novel data-driven mastery learning
system to improve learning in complex procedural problem
solving domains. This new system was integrated into an
existing logic proof tool, and assigned as homework in a de-
ductive logic course. Student performance and dropout were
compared across three systems: The Deep Thought logic tu-
tor, Deep Thought with integrated hints, and Deep Thought
with our data-driven mastery learning system. Results show
that the data-driven mastery learning system increases mas-
tery of target tutor-actions, improves tutor scores, and low-
ers the rate of tutor dropout over Deep Thought, with or
without provided hints.

Keywords
Data-driven Mastery Learning, Problem Selection, Knowl-
edge Tracing, Logic Proof

1. INTRODUCTION
This paper describes a novel data-driven mastery learn-

ing system (DDML) that uses the knowledge tracing (KT)
of tutor actions in past student-tutor performance data to
regularly evaluate new student performance and select suc-
cesive structured problem sets. An overview of the DDML
system is shown in Figures 1 & 2. Problem sets are sepa-
rated into levels based on the rules or concepts required for
those problems, with the order of levels determined by the
course curriculum. Each level presents an opportunity for
mastery learning, requiring students to complete of a min-
imum number of problems containing the required rules or
concepts for that level in order to progress to the next. Be-
tween levels, student performance is assessed by the knowl-
edge tracing of all actions taken in the tutor (in the case
of this experiment, target-rule application), weighted by the
priority of those actions within the level. The resulting score
from the assessment is compared to a threshold value: the
average data-driven knowledge tracing (DKT) scores of cor-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org
LAK ’15 March 16 – 20, 2015, Poughkeepsie, NY USA
Copyright 2015 ACM 978-1-4503-3417-4/15/03$15.00
http://dx.doi.org/10.1145/2723576.2723622.

Exemplars (Historical)  

1st Interval Problems 

Knowledge Tracing of all steps in P1 to Pl 

P(G) 
P(S) 

P(L0) P(T) P(T) 

KCs KCs KCs 

Last 
Step Step Step 

 

 

 P1 P2 Pl  P1 P2 Pl 

2nd Interval Problems 

 Interval 
Performance 

Compare step to best possible step, update KCs 

KC 1 KC 2 … KC n 
Exemplar 1 0.35 0.12 … 0.67 
Exemplar 2 0.42 0.33 … 0.59 

… … … … … 
Exemplar m 0.37 0.28 … 0.73 
AVERAGE 0.46 0.26 … 0.71 

KC Thresholds sent to DDML 

Figure 1: The data-driven threshold builder. KCs
for each exemplar are updated using action steps
from an interval set of tutor problems. The KC score
averages at each interval are used as thresholds in
the DDML system.

responding problems solved by past students who have suc-
cessfully completed the entire tutor (exemplars). A score
value above or below the threshold determines the user’s
relative proficiency in the subject matter, and the difficulty
of the problem set given in the next level. This process con-
tinues through the end of the tutor. This is a new, novel
approach to achieve mastery learning.

For this experiment, we compared three versions of the
Deep Thought logic tutor: the original undirected proof
tool with un-ordered problem sets (DT0); DT0 with inte-
grated on-demand hints (DT1); and DT0 integrated with
data-driven mastery learning (DT2). We hypothesized that
DT2’s DDML system would result in higher learning gains
and lower tutor dropout than DT0 and DT1. Analysis of
the results indicate that students using DT2 had significant
improvement in full tutor completion by almost 3 times over
students using DT0. Students using DT2 also showed im-
provement over students using DT1 with hints, although
this was not statistically significant. The results show that
DT2’s data-driven mastery learning system is effective in
improving tutor percent completion for most students over
DT0 and DT1, confirming our hypothesis.

2. RELATED WORK
Cognitive tutors use knowledge tracing (KT) to track stu-

dents’ mastery of particular Knowledge Components (KCs)
based on their performance at each opportunity to apply

270



Level 1 

Level 2 

New Student 

P1.1 – P1.l 

Assessment 

Exemplar 
KC Thresholdi 

(i = KC1 – KCn) 

P2.1H – P2.lH 

P2.1L – P2.lL 

Level 3 

P3.1H – P3.lH 

P3.1L – P3.lL 
Assessment 

 High Proficiency High Proficiency 

Low Proficiency Low Proficiency 

Interval 
Performance + value 

- value 

Compare 
performance to 

interval thresholds, 
weight and sum 

difference. 
(+/-) 

Figure 2: The Data-Driven Mastery Learning system. At each level interval, new student KC scores are
calculated and then compared to exemplar thresholds for corresponding problem sets (see Figure 1).

that KC. The classic model used by Corbett & Anderson
uses Bayesian Knowledge Tracing (BKT) to predict proba-
bility of learned components based on learning, guess, and
slip parameters [3]. Once a KT model is created, it can be
used to select problems which best suit the student and what
he or she still needs to learn. BKT has been shown to be
generally effective for predicting success on next problems
(meaning it can be used for problem selection), although it
is more effective in some domains than others [7].

In tutoring systems with an intelligent problem selection
component, problem selection thrashing can occur. Thrash-
ing is a cycle that can occur in problem selection, where
students will be given a string of problems that focus on con-
cepts they have already mastered until they reach a prob-
lem with multiple concepts, including concepts they have
not yet mastered, inevitably get the incorrect answer, and
then are given a similar string of easy but unhelpful prob-
lems. Koedinger et al argued that thrashing was caused by
incorrect blame assignment in knowledge tracing systems;
when a student attempts a problem with multiple knowl-
edge components and gets an incorrect answer, knowledge
tracing works on the assumption that each knowledge com-
ponent must have been applied or executed equally incor-
rectly [5]. This can provide greater problems in domains
where knowledge components may overlap or conflict with
each other, such as deductive logic proof problem-solving.
The system presented in this paper was designed to avoid
problem selection trashing, in that we sought to intelligently
select appropriate problem sets by determining which knowl-
edge components each student needed to focus on and use
that determination to select appropriate problems.

2.1 Deep Thought
Fig. 3 shows the interface for Deep Thought, that displays

logical premises, buttons for logical rules, and a logical con-
clusion to be derived. Deep Thought was developed as a
practice tool for proof construction, divided into three sets
of problems.

In DT0 students were allowed to solve assigned problems
at will, in any order, though assignments were ordered. As
a student works through a problem, each step is logged in
a data file that records the current problem, the rule being
applied, any errors made (such as attempting to use a rule
that is logically impossible), completion of the problem, time
taken per step, and elapsed time taken to solve the problem.

Figure 3: A screen capture of the Deep Thought
tutor, showing given premises at the top, conclusion
at the bottom, and rules for application on the right.

Barnes & Stamper extended Deep Thought with a data-
driven hint-generation system to create DT1. DT1 used a
Markov decision process to automatically select hints for stu-
dents upon request, based on their individual performance
on specific problems [1]. In their research, Stamper et al.
observed student dropout in DT0 and DT1 were high [6].
Despite improvements to the student dropout rate from DT0
to DT1 with the addition of hints, there were still significant
numbers of students that did not complete the tutor. We
hypothesized that the DDML system we developed for DT2
would have higher percentage completion and lower student
dropout rates than DT0 and DT1 (see Table 2).

3. METHODS
DT2’s data-driven mastery learning system consists of

two major components: a mastery learning leveling com-
ponent; and an assessment component. Their processes are
described in this section.

3.1 Mastery Learning: DT2 Leveling Up
DT2 breaks the problem sets of DT0/DT1 into additional

levels while maintaining the same rule applications and the
same difficulty level of problems in DT0/DT1. Students’

271



cumulative performance on target-rule actions at the end of
a level determine whether they attempt the next level at
higher or lower proficiency, as shown in Figure 2. Level 1 of
DT2 contains three problems common to all students who
use the tutor, and provides initial performance assessment
data to the DDML model. Levels 2–6 of DT2 are each split
into two distinct sets of problems, labeled higher and lower
proficiency. The problems in the different proficiency sets
prioritize the same rules judged by domain experts to be im-
portant for solving the problems in that level. However, the
degree of problem solving difficulty between proficiency sets
is different, with problems in the low proficiency set requir-
ing fewer numbers of steps for completion, lower complexity
of logical expressions, and lower degree of rule application
than problems in the high proficiency set. Fig. 4 shows the
possible path progressions of students using DT2.

Figure 4: DT2 path progression. At each level, stu-
dents are evaluated and provided either the higher
or lower proficiency problem sets. Students can also
be switched from the higher to lower proficiency set
within a level.

Because the problem sets in DT2 are completely ordered
instead of random access, DT2 allows students to temporar-
ily skip problems within a level. Students who skip once in
the higher proficiency set are given the next unsolved prob-
lem in the set. Skipping twice in the higher proficiency set
will drop students to the lower proficiency problem set in the
same level, regardless of how many problems they solved in
the higher proficiency set (see Fig. 4). Students in the lower
proficiency set who skip a given problem are first offered an
alternate version of the same problem before moving them
to the next unsolved problem in the set. Students who re-
peatedly skip problems in the lower proficiency set will cycle
through the unsolved problems in the set.The system main-
tains a maximum of three required solved problems to con-
tinue to the next level, taking into account any problems
that may have been solved in the harder proficiency set,
and ensures that all rules required for demonstration are
presented to the student. Between the two proficiency sets,
and the alternate problems offered in the lower proficiency
set, there are 43 problems in the DT2 problem set
The DDML system can be applied to any procedural do-

main where quality of student solutions are difficult to eval-

uate. By limiting the number of problems students are re-
quired to solve, the DDML system ensures that students
do not spend an unreasonable amount of time solving prob-
lems in the current level, allowing them to move forward in
the tutor and be exposed to new concepts while receiving
practice with problems that have difficulty relative to their
performance.

3.2 Assessment: DKT Rule Score Updating
We hypothesize that students who have completed Deep

Thought in the past have acquired a coherant skill set for
proof problem solving and rule application. By splitting
the DT0/DT1 problem set into levels, DT2 allows regu-
lar evaluation of new students compared to the standard
of what exemplars were able to accomplish at corresponding
points in the tutor. DT2 uses data-driven knowledge trac-
ing (DKT) of past and current students to evaluate student
performance. For each student, a KT score ruleScorei for
each logical rule i in the tutor is created when a student first
logs into DT2, and these scores are maintained and updated
at each rule application made by the student.

The ruleScorei for a given rule i is initialized with a learn-
ing value p(L0) = 0.01, acquisition value p(T ) = 0.01, guess
value p(G) = 0.3, and slip value p(S) = 0.1. After each
observed application of rule i, ruleScorei is updated using
Bayesian knowledge tracing equations for inference and pre-
diction of individual skill knowledge [4].

3.3 Assessment: Proficiency Determination
By the time the student has completed a level of the tu-

tor, that student will have accumulated a set of rule scores
for each rule i calculated based on their performace. At the
end of each level, the DKT scores for each rule ruleScorei
are compared to a threshold value for that same rule. This
threshold value, ruleThresholdi, was calculated using data-
driven knowledge tracing of DT0 tutor logs from six 2009
Deductive Logic course sections. Only students who com-
pleted the entire DT0 assignment (n = 302) were used for
threshold calculation, since these exemplars demonstrated
proficiency for proof problem solving using all required rules.
DKT student scores were computed for problems in DT0,
and mapped to the corresponding levels in DT2. The scores
from DT0 for each rule score were averaged at each of these
break points and set as ruleThresholdi. Students using DT2
are therefore judged to have proficiency on a rule based on
how their performance compares to average student usage
from previous use of Deep Thought by past exemplars.

For each student in DT2, every rulei is assigned a positive
scoreSigni value if the rule score is above ruleThresholdi.
Each rulei receives a negative scoreSigni value if the score
is below the ruleThresholdi, meaning that they have not yet
shown the same level of proficiency in deciding which rules
to use as past students who successfully completed all the
problems in DT0. This positive or negative value is weighted
based on the priority of rule i in the current level. Rule
priority is determined by the set of rules deemed necessary
by domain experts to best solve the current proof problem.
Rules required for completion of the proof problems in that
level are weighted by 1. Rules not required for completion
of the proof problems in that level are weighted by 0.5.

The weighted scoreSigni values are summed, and the sign
of the sum determines whether a student is sent to the higher
proficiency path or the lower proficiency path

272



4. RESULTS AND DISCUSSION
DT2 was used as a mandatory homework assignment by

students in a philosophy deductive logic course (DDML group,
n = 47). Students were allowed to work through the problem
sets at their own pace for the entire 15-week semester. Prob-
lem Levels 1–6 were assigned for full completion of the tutor,
totalling 13–18 problems depending on proficiency path pro-
gression.
Data from DT2 was compared to data collected in two

prior semesters of the same philosophy course using DT1
and DT0, respectively. Both courses were taught by the
same instructor as the DT2-DDML group, and were assigned
the same problems for full completion of the tutor.
The Spring 2009 students used the version of Deep Thought

(DT1) with a hint-generation system developed by Barnes
& Stamper to aid students in solving proof problems (Hint
group, n = 48). Students using DT1 show a significant
increase in tutor completion over the DT0 system without
hints [2]. The Fall 2009 students used the original unaltered
DT0 (Control group, n = 43) with no hints.

4.1 Tutor Completion
Table 1(a) shows the number of total assigned problems

solved in tutor for the three groups. Students in the DT2-
DDML group solved 13 problems on average – the minimum
required for completion of the tutor – while students in the
DT1-Hint group completed 8 assigned problems out of 13,
and students in the DT0-Control group completed 6 out of
13.

Table 1: (a) Number of total assigned problems
solved in tutor. (b) Percentage of tutor completion.
* indicates significance over the control.

(a) DT2-DDML DT1-Hint DT0-Control
Mean 13.09 of 13–18 7.98 of 13 6.07 of 13
StDev 4.94 4.78 5.20

(b) DT2-DDML DT1-Hint DT0-Control
Mean 79.79* 61.38 46.69
Median 100.0 61.54 46.15
StDev 29.88 36.78 40.02

Table 1(b) shows that students from the DT2-DDML group
complete 33% more of the tutor on average than the DT0-
Control group, and 18% more of the tutor than the DT1-
Hint group, with lower variance in the final scores. The
median scores show that over half (55%) of the DT2-DDML
group completed the entire tutor (see Table 2 for tutor com-
pletion by group). A one-way ANOVA test on percent com-
pletion difference was performed on tutorial completion across
all three test groups, showing significance (F (2, 120) = 7.559,
p = 0.001). A Tukey post-hoc comparison shows a signifi-
cant improvement of the DT2-DDML group (M = 0.80, 95%
CI [0.70, 0.90]) over the DT0-Control group (M = 0.51, 95%
CI [0.40, 0.62], p <0.001, Cohen’s d = 0.84). Although the
DT2-DDML group had a higher mean percentage comple-
tion than the DT1-Hint group, the results were not signifi-
cant (p = 0.138, Cohen’s d = 0.35). However, this still in-
dicates that DT2’s data-driven knowledge tracing combined
with individualized problem set selection can improve stu-
dent percent completion as much as on-demand hints (DT1).

4.2 Student Dropout
Student dropout is defined as the termination of tutor ac-

tivity prior to completion of assigned problems (DT0/DT1)
or levels (DT2). Figure 5 shows the dropout trend of the
three data groups over the course of the tutor. The mark-
ers on the DT2-DDML line indicate the end of each level in
DT2, and the markers on the DT1-Hint and DT0-Control
lines indicate the end of each problem in DT1 and DT0.
The horizontal axis represents the percentage completion of
the tutor. The vertical axis represents the percentage of
all students in each group active in the tutor at the com-
pletion of each level (DT2) or problem (DT1, DT0). Both
the DT1-Hint and DT0-Control groups have higher dropout
than the DT2-DDML group across all problems. The DT0-
Control group also has greater dropout than the DT1-Hint
group, with a noted difference in higher level problems (cor-
responding to Levels 4–6 in DT2). This is consistent with
the results found by Stamper et al [6], that on-demand hints
help student retention in-tutor.

Figure 5: Rate of student dropout for the DT2-
DDML, DT1-Hint, and DT0-Control groups over
the course of the tutor.

Table 2 summarizes the number of students who com-
pleted and dropped out of the tutor across all three groups.
An overall chi-square test was performed to examine the re-
lationship between student dropout and group, showing sig-
nificance (?2(2, 123) = 11.75, p = 0.003). The DT2-DDML
group was significantly less likely (by 82%) to drop out
of tutor when compared to the DT0-Control group (?2(1,
84) = 11.50, p = 0.001, OR = 5.31). Although the DT2-
DDML group had lower student dropout than the DT1-Hint
group, the difference was not significant (?2(1, 86) = 3.23,
p = 0.072, OR = 2.21). The DT1-Hint group had a lower
dropout rate than the DT0-Control group, but not signifi-
cantly (?2(1, 76) = 2.74, p = 0.098, OR = 2.40).
As a whole, the DDML system in DT2 is as effective in im-

proving student in-tutor performance as much as on-demand
hints in DT1, and significantly better than DT0. The pur-
pose of on-demand hints is to aid and improve student learn-
ing, and tutor performance is a reflection of a student’s
knowledge and abilities in the subject matter. This result
is important for intelligent tutor design, indicating that the
DDML system presented here can achieve or outperform the
learning gain from on-demand hints, reducing tutor devel-

273



Table 2: Student completion of the tutor by group.
Dropped indicates that the student did not complete
Deep Thought.

Group Completed Dropped Total
DT0-Control 8 (18.6%) 35 (81.4%) 43
DT1-Hint 15 (31.3%) 33 (68.7%) 48
DT2-DDML 26 (55.3%) 21 (44.7%) 47
Total 49 (35.5%) 89 (64.5%) 138

opment time. The next section explores performance in the
application of rules in DT2.

4.3 Rule Score Threshold Comparison
Evaluation of the overall learning effect of the DDML

system model is determined by comparing final ruleScorei
scores for all i rules from the DT2-DDML group to end-
of-tutor ruleThresholdi scores from the DT0 logs. End-
of-tutor ruleThresholdi scores and the DT2-DDML group
scores were taken only from students who completed the
entire tutor, to measure successful student rule-application
performance.
The results are shown in Table 3. The rules listed in Table

3 are rules that were required for completion of the tutor.
The Difference column shows whether the DT2-DDML rule
scores are higher (+) or lower (?) than the ruleThreshold
scores. With the exception of modus tollens (MT) and con-
structive dilemma (CD), average rule scores for the DT2-
DDML group are higher than the respective ruleThresholdi
scores. This indicates that students using DT2 show a higher
overall awareness of proper individual rule usage by end of
tutor over students using DT0.

Table 3: Average rule DKT scores for DT2-DDML
group students who completed the entire tutor (n =
26), compared to end-of-tutor ruleThresholdi aver-
ages from DT0 (n = 302).

Rule ruleThresholdi DT2-DDML Difference
MP 0.649 0.743 +
DS 0.491 0.647 +

SIMP 0.734 0.947 +
MT 0.352 0.238 ?
ADD 0.426 0.747 +
CONJ 0.348 0.578 +
HS 0.455 0.669 +
CD 0.163 0.120 ?
DN 0.437 0.697 +
DEM 0.182 0.423 +
IMPL 0.463 0.703 +

TRANS 0.298 0.433 +
EQUIV 0.106 0.151 +

Students using DT2 complete more of the tutor on average
than students using DT0 or DT1 with hints. Students using
DT2 have a lower rate of tutor dropout than students using
DT0 and DT1, exposing more students to important domain
concepts. DT2-DDML students are also significantly more
likely to complete their entire assignment when compared
to the DT0-Control group and the DT1-Hint group. This
is particularly important, since success in Deep Thought is
correlated with course completion for introductory deductive

logic [6]. By solving more problems, DDML students have
more practice solving logic problems than students using
DT0 or DT1. These students also show higher proficiency
of tutor concepts.

5. CONCLUSIONS
This paper presented a holistic data-driven mastery learn-

ing system that uses knowledge tracing of domain concepts
in existing student-tutor performance data to regularly eval-
uate current student proficiency and select problem sets.
Analysis of the results show that this system can improve
tutor completion and proficiency of applied target-rules com-
pared to past exemplars. We conclude that the data-driven
mastery learning system presented in this paper significantly
aids mastery of core concepts, improves tutor scores, and
lowers rate of tutor dropout for a majority of students over
an undirected tutor, and at least as well as the same tutor
with on-demand hints. We expect that the integration of
hints with the DDML system will provide an even greater
effect than what was seen in this experiment, and this is the
first step of our future work.

6. ACKNOWLEDGEMENTS
This material is based on work supported by the National

Science Foundation under Grants 1432156 and 0845997.

7. REFERENCES
[1] T. Barnes and J. Stamper. Toward automatic hint

generation for logic proof tutoring using historical
student data. In Proceedings of the 13th International
Conference on Intelligent Tutoring Systems (ITS 2008),
pages 373 – 382, 2008.

[2] T. Barnes, J. Stamper, L. Lehmann, and M. J. Croy. A
pilot study on logic proof tutoring using hints
generated from historical student data. In Proceedings
of the 1st International Conference on Educational
Data Mining (EDM 2008), pages 197 – 201, 2008.

[3] A. T. Corbett and J. R. Anderson. Knowledge Tracing:
Modeling the Acquisition of Procedural Knowledge.
User Modeling and User-Adapted Interaction,
4:253–278, 1995.

[4] M. Eagle and T. Barnes. Data-Driven Method for
Assessing Skill-Opportunity Recognition in Open
Procedural Problem Solving Environments. In
Proceedings of the 15th International Conference on
Intelligent Tutoring Systems (ITS 2012), pages
615–617, 2012.

[5] K. R. Koedinger, P. I. Pavlik, J. Stamper, T. Nixon,
and S. Ritter. Avoiding Problem Selection Thrashing
with Conjunctive Knowledge Tracing. In Proceedings of
the 4th International Conference on Educational Data
Mining (EDM 2011), pages 91–100, 2011.

[6] J. Stamper, M. Eagle, T. Barnes, and M. J. Croy.
Experimental Evaluation of Automatic Hint Generation
for a Logic Tutor. In Proceedings of the 15th
International Conference on Artificial Intelligence in
Education (AIED 2011), pages 345–352, 2011.

[7] Y. Xu, F. Ave, and J. Mostow. Comparison of methods
to trace multiple subskills: Is LR-DBN best? In
Proceedings of the 5th International Conference on
Educational Data Mining (EDM 2012), pages 41–48,
2012.

274





