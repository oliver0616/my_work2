
Supporting Action Research with Learning Analytics 
A.L.Dyckhoff, V. Lukarov, A. Muslim, M.A. Chatti, U. Schroeder 

Learning Technologies Research Group, RWTH Aachen University 
52056 Aachen, Germany 

{dyckhoff, chatti,schroeder}@informatik.rwth-aachen.de,  
muslim@cil.rwth-aachen.de, and vlatko.lukarov@rwth-aachen.de 

 

  
ABSTRACT 
Learning analytics tools should be useful, i.e., they should be 
usable and provide the functionality for reaching the goals 
attributed to learning analytics. This paper seeks to unite learning 
analytics and action research. Based on this, we investigate how 
the multitude of questions that arise during technology-enhanced 
teaching and learning systematically can be mapped to sets of 
indicators. We examine, which questions are not yet supported 
and propose concepts of indicators that have a high potential of 
positively influencing teachers’ didactical considerations. Our 
investigation shows that many questions of teachers cannot be 
answered with currently available research tools. Furthermore, 
few learning analytics studies report about measuring impact. We 
describe which effects learning analytics should have on teaching 
and discuss how this could be evaluated.  

Categories and Subject Descriptors 
J.1 [Computer Applications]: Administrative Data Processing – 
Education. 

General Terms 
Documentation, Design, Human Factors, Theory 

Keywords 
Learning Analytics, Action Research, Indicators, Impact Analysis 

1. INTRODUCTION 
Teaching is a dynamic activity, which should constantly be 
monitored and adjusted to the demands of changing social 
contexts and needs of the learners, to ensure high quality. This 
implies that teachers need to be aware about teaching and learning 
processes. Moreover, they should constantly analyze, self-reflect, 
regulate and update their didactical methods and the learning 
resources they provide to their students.  

Reflection has been defined as a conscious activity, exploring 
ones experience, in order to gain new insight and appreciation [7]. 
It can foster learning, if reflection is embedded in a cyclical 
process of active experimentation, where concrete experience 
forms the basis for observation and reflection [36]. This is also 
true for action research. Action research is a method for reflective 
teaching practice that enables and guides teachers to investigate 
and evaluate their work [2]. Hinchey defines action research as "a 
process of systematic inquiry, usually cyclical, conducted by those 

inside a community rather than outside experts; its goal is to 
identify action that will generate improvement the researchers 
believe important" [31]. We believe that learning analytics tools 
could initiate and support action research, if they are designed to 
meet action research requirements, as also discussed in [16]. 
The field of learning analytics (LA) has been defined in several 
ways; see [21, 24, 33, 53]. In the context of our research, we 
understand learning analytics as the development and exploration 
of methods and tools for visual analysis and pattern recognition in 
educational data to permit institutions, teachers, and students to 
iteratively reflect on learning processes and, thus, call for the 
optimization of learning designs [39, 40] on the on hand and aid 
the improvement of learning on the other [14, 15].  

Although the goals behind action research and LA are very 
similar, a difference can be seen in the initial trigger of related 
study projects. While action research projects usually start with a 
research question that arises from teaching practice [2], learning 
analytics projects often evolve based on observations made with 
regard to already collected data. Action research projects, 
therefore, also often use qualitative methods to generate a holistic 
picture of the learning situation, while learning analytics are 
mostly based on quantitative methods (more details in table 3).  

In terms of LA, the creation of indicators has been controlled by 
and based on the data available in learning environments. Hence, 
the indicators might solely represent information that depends on 
the data sources used, e.g., by just making data visible that has 
been “unseen, unnoticed, and therefore unactionable” [13]. An 
important principle of action research is first to think about the 
questions that have to be answered before deciding about the 
methods and data sources [31]. Asking questions independently 
and putting aside the fact whether the necessary data is available 
or not, could provide more insightful information about the 
requirements analysis. This will improve the design of future LA 
tools and learning environments. 

The following assumptions and research questions let to the 
investigations at hand: 

Q1. Indicator-Question-Mapping: Important questions of 
teachers remain unanswered by LA tools: Current LA 
Implementations fail to answer several important 
questions of teachers, e.g., concerning correlations of 
student profile data with other sources. Which questions 
cannot be mapped to the available (sets of) indicators? 
Which indicators could deliver what kind of 
enlightenment? 

Q2. Teacher-Data-Indicators: There is a need for including 
teacher data into indicators: Current indicators do NOT 
explicitly relate teaching and teaching activities to 
student learning. Are there tools that explicitly correlate 
teacher data with student data? How should teacher data 
be correlated? 

 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
LAK’13, April 08–12, 2013, Leuven, Belgium. 
Copyright 2013 ACM 978-1-4503-1785-6/13/04…$15.00. 

 
 

220



Q3. Missing Impact Analysis: Current LA research focuses 
on supporting teachers and students in their tasks, but 
fails to prove the impact of LA tools on stakeholders’ 
behaviors. How could LA impact teaching? How could 
this impact be evaluated? 

The remainder of this paper proceeds as follows: Section 2 
(Methods) gives an overview on our research procedure and 
materials. Section 3 (Categorization of Indicators) structures the 
materials and the used related works. This is the foundation for 
section 4 (Analysis and Discussion). This analysis constitutes the 
consolidation of the previously described data, and serves as the 
basis for answering our research questions. We discuss our 
findings and limitations of the investigation. Finally, section 5 
(Conclusion) summarizes the main accomplishments of our study 
and gives an outlook on future tasks. 

2. METHODS 
We conducted our research in several steps. First, we used the 
results of a former qualitative meta-analysis, which investigated 
what kind of questions teachers ask while performing action 
research in their technology-enhanced learning (TEL) scenarios 
[22]. A summary of this study is provided below in section 2.1. 
Furthermore, we collected publications on LA tools and available 
indicators (section 2.2). For systematic reasons, two researchers 
from our research group independently developed a categorization 
scheme for the list of 198 collected indicators (section 3). The 
categories were merged into one scheme, which is presented in 
section 3. This categorization scheme helped us to get an 
overview of what is currently present and available in the research 
field. It supported the further analysis of LA tools and indicators. 
Last, the same two researchers performed and analyzed the 
mapping of teachers’ questions to sets of available indicators 
independently from each other and a second time together. The 
consolidation of our findings helped us to find questions that did 
not yet have corresponding indicators.  

2.1 Teachers’ Questions 
Dyckhoff structures the collected questions by the purposes and 
methods used to investigate them [22]. Table 1 shows the six 
resulting categories, including examples. 

Questions of category (A) mainly serve exploratory purposes. 
They can be investigated by qualitative methods, such as 
interviews with students or qualitative and quantitative surveys. If 
the learning environment offers rating features, these can be used 
as well. Questions of category (B) could be investigated by using 
quantifiable data from surveys and/or log files. Several of the 
questions fit into more than one category, due to the nature of the 
categorization. For instance, category (E) is a collection of 
questions investigating correlations, proportions and comparisons 
by combining data from different sources, like (A) and (B). 
Questions of the categories (C) and (D) seek to enlighten teachers 
on how diverse resources and activities might aid different groups 
of students in learning. Finally, category (F) refers to questions 
concerned with all kinds of performance measures [22]. 

2.2 Learning Analytics Tools 
We included the following LA projects in our study on indicators: 
LOCO-Analyst [1, 4], TADA-Ed [46], Data Model to Ease 
Analysis and Mining [38], Student Inspector [50], MATEP [56–
58], CourseVis [43, 45], GISMO [44], Course Signals [3], Check 
My Activity [25], Moodog [54, 55], TrAVis [41, 42], Moodle 
Mining Tool [48], EDM Vis [34], AAT [29], Teacher ADVisor 
[37], E-learning Web Miner [26], ARGUNAUT [30], Biometrics-
based Student Attendance Module [27], CAMera and Zeitgeist-
Dashboard [51, 52], Student Activity Meter [28], Discussion 
Interaction Analysis System (DIAS) [8–11], CoSyLMSAnalytics 
[49], Network Visualization Resource and SNAPP [5, 6, 17, 18], 
i-Bee [47], iHelp [12], and Participation Tool [32]. We limited our 
analysis on available indicators to these projects because they 
have been presented in literature as state-of-the-art LA-tools, 
which can already be used by their intended target users.  

(A) Qualitative evaluation 
 

How do students like/rate/value 
specific learning offerings? 

How difficult/easy is it to use the 
learning offering? 

Why do students appreciate the 
learning offering? 

(B) Quantitative measures of use / 
attendance 

When and how long are student 
accessing specific learning offerings 
(during a day)?  

How often do students use a learning 
environment (per week)? 

Are there specific learning offerings 
that are NOT used at all? 

(C) Differentiation between groups of 
students 

By which properties can students be 
grouped? 

Do native speakers have fewer 
problems with learning offerings than 
non-native speakers? 

How is the acceptance of specific 
learning offerings differing according 
to user properties (e.g. previous 
knowledge)? 

(D) Differentiation between learning 
offerings 

Are students using specific learning 
materials (e.g. lecture recordings) in 
addition or alternatively to 
attendance? 

Will the access of specific learning 
offerings increase if lectures and 
exercises on the same topic are 
scheduled during the same week? 

(E) Data consolidation / Correlation 
 

How many (percent of the) learning 
modules are student viewing? 

Which didactical activities facilitate 
continuous learning? 

How do learning offerings have to be 
provided and combined to with 
support to increase usage? 

(F) Effects on Performance 
 

How do those low achieving students 
profit by continuous learning with e-
test compared to those who have not 
yet used the e-tests?  

Is the performance in e-tests 
somehow related to exam grades? 

 

Table 1. Classification and examples of questions asked by teachers in TEL (cp. [22]) 

221



3. CATEGORIZATION OF INDICATORS 
On basis of our literature review, we developed a categorization 
scheme with the purpose to structure the indicators. Table 2 shows 
that five categories are related to the perspectives and six 
categories are related to the data sources that the indicators 
implement. The concept of perspective refers to the point of view 
a user might have on the same data. Our categorization 
distinguishes the perspectives into: individual student, group, 
course, content, and teacher. The user might, e.g., be interested in 
the usage behavior of a single student in relation to the whole 
group. Another perspective might be the content, e.g., how a 
specific resource was used. These examples show that the same 
data can be used to answer different questions and be in line with 
different perspectives. Note that our categorization of perspectives 
is similar to the work of Dimitracopoulou [20], who distinguishes 
four general points of view of interaction analysis indicators: 
individual, group, community and society. During our research, it 
became apparent that a high percentage of indicators rely on basic 
usage data, generated by students while using a single e-learning 

system. This is clearly not enough for creating a holistic picture of 
the students’ learning process. Hence, we additionally chose a 
categorization scheme related to data sources (student generated 
data, context/local data, academic profile, evaluation, 
performance, or course meta-data) to make sure that those 
indicators that include or even combine different data sources 
become more visible within our analysis. 

In the following two subsections we will introduce each category 
of table 2 by describing how teachers and students can use the 
related indicators and give concrete indicator examples from the 
literature. Note that all indicators relate to more than one category. 

3.1 Perspective Categories 
Individual Student: This category refers to indicators that are 
supposed to inspire an individual student’s self-reflection on 
his/her learning and academic success. Indicators process data and 
visualize information about a single student. For instance, the 
indicators could measure how active a student is. Activity is 
derived from the student’s generated data, and it can be divided 

 

                                                                                      Categorization 

 

 

 

       Indicator Examples 

Pe
rs

pe
ct

iv
e 

In
di

vi
du

al
 S

tu
de

nt
 

G
ro

up
 

C
ou

rs
e 

C
on

te
nt

 

Te
ac

he
r 

D
at

a 
so

ur
ce

 

St
ud

en
t-G

en
er

at
ed

 D
at

a 

C
on

te
xt

/L
oc

al
 D

at
a 

A
ca

de
m

ic
 P

ro
fil

e 

Ev
al

ua
tio

n 

C
ou

rs
e-

re
l. 

Pe
rf

or
m

an
ce

 

C
ou

rs
e 

M
et

a-
D

at
a 

Number of content pages viewed per student [54] 

 

X     

 

X      
Number of threads started per student [42, 45, 55]  X     X      
Number of assignments submitted per student [41, 54] X     X    X  
Relation between keywords and students [47] X     X      
Student risk group/status [3] X     X  X  X  
Social network derived from email exchange data per student [51] X      X     
Keyword analysis derived from email exchange data per student [51] X      X     
Resources watched (read) in a session per student (via webcam) [27] X     X X     
Students’ gender [26, 58] X  X     X    
Students’ academic level [58] X  X     X    
Students’ tags of learning resources [51] X   X     X   
Work amount (message dimension per user) per course phase [49] X          X 
Number of participants per group [25, 41]  X    X      
Number of files per group [41]  X    X      
Advice to groups concerning uncommunicative behavior [37]  X    X      
Global accesses to the course [45]   X   X      
Resources that have NOT been accessed (weekly, daily, hourly) [55]   X   X      
Learning paths analysis [51, 58]   X   X      
SNA: actor’s degree centrality per course phase [49]   X   X   X   
Students’ comprehension of topics (based on annotations) [1]   X         
Avg. overall quiz score [1]   X       X  
Clusters of students who made a (specific) mistake [46, 50]   X       X  
Advice to the teacher concerning excellent and weak students [37]   X       X  
Avg. number of contributions per course phase [49]   X   X     X 
Avg. number of incorrect answers per question in a quiz [1]    X      X  
Number of unique users per resource (weekly, daily, hourly) [55]    X  X      
Resources frequently used together (forum, mail, etc.) [27]    X  X   X   
Top5 pages/resources [58]    X  X      

Table 2. For illustration, this table shows a sample of the overall 198 indicators. Each indicator was categorized according to 
its perspective and related data sources as state in its respective publications. 

222



into receptive and active activities: Receptive activities are 
accessing or reading resources, listening to audio, or watching 
lecture videos. Active participation is based on contributions on 
the forums, discussion participation, editing wiki pages, uploading 
assignments, as well as sharing resources. The indicators also 
include summarized data from other students. This way, a student 
can compare his/her learning progress and success with the rest of 
the class. Most of the indicators of the individual student category 
can also support teachers in monitoring and assessing a single 
student’s learning process for the purpose of tutoring. They can be 
used for quick glances, or in-depth research on how a specific 
student is doing in a course. Sophisticated systems might even 
recommend learning activities, predict a student’s success or give 
advice. Examples of indicators are: Number of content pages 
viewed per student [58], Number of threads started per student 
[41, 45, 55], Number of messages read on forum per student [11, 
41, 45, 58], Number of messages read by the user in relation to 
number of messages available [12], Content currently read by one 
or more students [12], Relation between keywords and students 
[47], Frequency a student used each keyword [47], Student risk 
group/status [3], Advice to uncommunicative advanced students 
to help others [37]. 

Group: This category includes combinations of indicators 
dedicated to groups of students for the purpose of self-reflecting 
on their group’s behavior, learning, and academic success. 
Furthermore, items of this category can help the teacher to 
monitor and analyze each group’s activities, perform inter-group 
comparisons, or show the level of collaboration among students in 
a group. Examples include: Number of participants per group [25, 
41], Avg. number of posts per group [11], Number of messages 
quoted per group [41], Number of files per group [41], Avg. 
thread depths/weight [11], Relative activity regarding number of 
posts, number of types, number of initiated threads per group [11], 
Advice to groups concerning uncommunicative behavior [37]. 

Course: The course category is a collection of indicators that 
helps the teacher to monitor and analyze the overall course data. A 
course follow-up could also provide indicators for the teacher to 
review and compare several courses of the same type. Examples 
include: Global accesses to the course [45], Number of distinct 
users [58], Avg. visit duration [58], Learning paths analysis [51, 
58], Resources that have NOT been accessed (weekly, daily, 
hourly) [55], SNA: actor’s degree centrality per course phase [49], 
Learner isolation/students with limited connectivity [17, 18]. We 
did not find indicator related to course follow-up. 

Content: This set of indicators serves the purpose to present the 
students’ interactions with the content of a course. It explicitly 
takes the perspective of a resource, lesson, quiz, etc. Examples 
include: Number of unique users per resource (weekly, daily, 
hourly) [55], Number of revisits per lesson/quiz [1], View counts 
per resource (weekly, daily, hourly) [51, 55], Resources 
frequently used together (forum, mail, etc.) in each learning 
session [26], Top5 pages/resources [58]. 

Teacher: This class refers to indicators that allow the teacher to 
reflect upon his/her teaching, with the purpose of improving it. 
These indicators explicitly process teacher-generated data and 
correlate it with the student-generated data. We only found few 
indicators that corresponds to this category: Sociogram of 
interaction between teacher and participant [17, 18]. 

3.2 Data Source Categories 
Student-Generated Data: This set of indicators presents the 
students’ presence online, their usage behavior. These basic 

indicators take into consideration session information, number of 
visits, hits, duration, and other basic log data available. Other data 
generated by students are for example, messages posted to a 
discussion forum, files uploaded, etc. Other examples of 
indicators include: Course access by student per date [45], Overall 
time spent per student (weekly, daily, hourly) [26, 55, 58], Trends 
in students’ activity (based on time spent) [28]. 

Context/Local Data: These indicators use data that surround the 
student, such as, local or mobile data, and influence the learning 
context. These indicators might consider the location of the 
learner, co-learners nearby, calendar information, email exchange, 
homework assignments, deadlines, or exam dates. Furthermore, 
they might even use biometric data from webcams or fingerprints. 
Examples include: Social network derived from email exchange 
data per student [51], Keyword analysis derived from email 
exchange data per student [51], Resources watched (read) in a 
session per student (via webcam) [27].  

Academic Profile: These indicators consider the student’s 
academic profile. In the academic profile demographic 
characteristics of the student are included beside other data on 
academic progress. The demographic profile includes, e.g., 
gender, age, and mother language. The academic profile could 
include field of study, previous knowledge (as in finished courses 
and prerequisites), and grades from assignments, quizzes, exams, 
and mistakes. Examples are: Students’ gender [26, 58], Students’ 
academic level [58], Students’ age [26, 58]. We could not 
explicitly find indicators that correlate this data to other sources, 
besides: Student risk group/status [3]. 

Evaluation Data: This is a set of indicators, which consider the 
evaluation data provided by students, such as, survey question 
answers, annotations, tags of resources, ratings, course 
evaluations, one-minute feedback, or questionnaires. Examples 
include: Students’ comprehension of the studied topics (based on 
annotations) [1], Students’ tags of learning resources [51]. 

Course-related Performance: This category refers to indicators 
that use performance-related data, like grades from assignments, 
quizzes, and exams, or attempts per quiz questions, mistakes 
made, etc. Examples of indicators are: Avg. overall quiz score [1], 
Avg. number of incorrect answers per question in a quiz [1], 
Mistakes that often come together (a-priori): if students make 
mistake A, followed by mistake B, then later they make mistake C 
[46], Clusters of students who made a (specific) mistake [46, 50], 
Number of assignments submitted per student [41, 58], Advice to 
the teacher concerning excellent and weak students relative to the 
whole class [37]. 

Course Meta-Data: Lastly, this class refers to indicators that take 
into consideration the course structure, course goals, events, 
resources, resources allocation, teaching concept, and the rate of 
their implementation. Examples include: Work amount (message 
dimension per user) per course phase [49], Avg. number of 
contributions per course phase [49]. 

4. ANALYSIS AND DISCUSSION 
In the following, we discuss our findings. Our discussion is 
structured into three subsections, which are related to the three 
research questions Q1-Q3 mentioned in section 1. A fourth 
subsection refers to limitations of our study. 

4.1 Indicator-Question-Mapping (Q1) 
Our analysis showed that current LA implementations still fail to 
answer several important questions of teachers. In the first step of 

223



the research activity, we took the list of tools (see, section 2.2) 
and identified about 200 indicators. The second step of the 
research was to introduce the research questions (see, table 1) to 
the indicators (table 2). The third step of our research process was 
to map the available indicators to the corresponding questions by 
crosschecking them. The idea behind this mapping was that each 
indicator provides certain pieces of information that indicate how 
a question can be answered. With the help of this question-
indicator-mapping, we were able to identify which questions the 
indicators could support fully or in part.  

Most of the questions from category B (table 1) can be answered. 
They are concerned mainly with quantitative measures on the 
basis of student generated usage data. Several questions ask if, 
when, and how often students are learning online (accessing the 
learning environment). Diverse indicators can be used to answer 
these questions, such as the time spent online using the system or 
a specific resource (weekly, daily, hourly, per student or on 
average), the visit duration, and the number of sessions [1, 26, 55, 
58]. Since such log data is just an indicator for access of 
resources, researchers have also developed means for indicating 
that a student is reading online by using biometric data generated 
by webcams [27]. 

There are some other questions that could be answered as well. 
But it was not easy to find clear sets of indicators that correspond 
to them. During the mapping activity, we faced the difficulty of 
missing documentation about which indicators serve which 
purpose in which context. This made our work rather subjective, 
depending on our individual interpretation of each indicator.  
A few questions of category (B) cannot yet be answered, because 
they do not yet use specific student generated data, like mobile 
sensor data. Examples from this set of questions are: “Are 
students using specific learning offerings at home or mobile?” or 
“How often do students attend lectures?” 

It was not surprising for us that almost all questions of category 
A, which are concerned with the satisfaction and preferences of 
students, cannot yet be answered sufficiently. For example, we 
cannot calculate indicators on how students value specific 
learning offerings, or how satisfied the students are with specific 
learning offerings. Learning analytics tools would need to 
automatically collect and include, e.g., data from surveys and 
ratings, in order to support these questions. 
The same is true for even more complex questions. Some 
indicators could be used, but the resulting interpretations are not 
satisfying. One group of questions that cannot be related to any of 
the available indicators requires data correlation and combination 
from different data sources. Examples of these questions are: “Are 
students using specific learning materials (e.g. lecture recordings) 
in addition or alternatively to attendance?” or “How do learning 
offerings have to be provided and combined to with support to 
increase usage?” or “Which teaching activities increase learning 
activities (e.g. attendance in online discussions)?” LA indicators 
within the tools investigated in the study at hand have not 
explicitly addressed these questions. This might be due to data 
privacy issues. On the one hand, the more data sources we 
combine for indicator calculations, the more we will learn about 
the whole learning process. On the other hand, users’ privacy 
concerns legitimately increase. Since there are diverse contexts of 
teaching and learning, individual requirements have to be taken 
into account, when searching for compromise solutions. An 
overview on legal, ethical and related management issues 
surrounding analytics in the context of teaching, learning and 
research has been provided by [35].  

Furthermore, there is a set of complex questions, which require 
data from official student academic records/profiles. Examples for 
these questions are “How do those low achieving students profit 
by continuous learning with e-test compared to those who have 
not yet used the e-tests?” or “Is the performance in e-tests 
somehow related to exam grades?” or “Do native speakers have 
less problems with the learning offering than non-native 
speakers?” From our findings, we concluded that the most 
valuable questions still remain unanswered. Their related 
indicators require the usage of qualitative data, official academic 
profile data, teacher data, and their combinations/correlations.  

4.2 Teacher-Data-Indicators (Q2) 
We did not find tools or indicators that explicitly collect and 
present teacher data. Indicators that come closest to correlating 
teacher data, are indicators related to course phases (see, [49]), 
which relate to course events, or sociograms, which show 
interactions between teachers and students [17,18].  

There is some teacher data already stored in databases, such as 
activity logs. But other data is missing in most systems, like data 
on the quality of course materials. Missing data concerns meta-
data or information on events and activities outside the learning 
environment (e.g. lectures). LA tools should explicitly collect, 
analyze and present teacher data. We need to correlate this data 
with student behavior to show how and which teaching activities 
and events, have impact on student behavior and student learning. 
If teachers had indicators about their activities and online 
presence, they might be inspired and motivated to be more active 
in the online learning environment. Hence, their presence in 
discussions might stimulate students likewise to participate more 
actively and motivate them to share knowledge and ideas.  
In order to identify possible indicators that will present teachers’ 
data, we conducted a brainstorming session. We took into 
consideration the activities and duties teachers have while 
teaching a course. These teachers’ activities vary from grading an 
assignment or exam, preparation of learning offerings (lecture 
slides, notes), or providing and combining lecture resources 
online, writing emails, participating in online forums, etc. From 
these activities, we identified example indicators. Some of them 
are: “Time spent grading each assignment/quiz/exam?”, “Which 
learning offerings are preferably used to prepare or reinforce 
lecture topics”, “How do learning offerings have to be provided 
and combined with support to increased usage?”, “Usage diagram 
of the course, and showing specific milestones/events of the 
semester.”, “Simple intervention of teacher contacting a student, 
and does the behavior of the student changes?”, “Does teacher 
participation in online discussions, inspires students to be more 
proactive and present more in the discussion forums?”  

The following three use cases are supposed to provide ideas of 
how teachers could use indicators that are based on teacher data 
and how helpful the indicator could be for a certain task.  

Indicator use case 1: Teacher Forum Participation Indicator 
After the collection and analysis of students’ and teachers’ forum 
interaction data, the results are presented to the teachers, to show 
them their patterns of participation and forum presence. For 
example, it would be interesting to see whether in the periods 
when the teacher was more active and more present on the forum, 
students’ online presence and activities also increased. 

Indicator Use Case 2: Teacher Correspondence Indicator 
For this indicator, the system collects data and information about 
teacher’s personal correspondence with each of his students. The 
teacher notices that several students have low participation in the 

224



online discussions, and their assignment submission results are 
low. She decides to contact the students in order to investigate the 
reasons. After this intervention, the teacher will be curious to see 
whether the students’ behavior has changed for the better and 
whether her intervention was correlated to this. 
Indicator Use Case 3:  Average Assignments Grading Time  
The teachers publish, read and grade assignments online in the 
LMS. The data of the time spent on grading each assignment is 
logged by the system. From the dashboard, the teacher can see 
whether the time spent on each assignment is evenly distributed. 
If there are outliers from the standard deviation of the time spent, 
the teachers could be inclined to self-reflect why they were 
spending more time on certain assignments, while spending less 
time on other tasks. 
Of course teacher-data-indicators also raise privacy issues, as a 
single teacher’s data cannot be anonymized. One of several 
possible ways to solve this issue could be personalization, i.e., to 
present their own data only to the teachers’ themselves. 

4.3 Missing Impact Analysis (Q3) 
Regarding our last question, the following paragraphs discuss how 
LA could impact teaching and how this could be evaluated.  

The impact evaluation method described below is based on our 
assumption that learning analytics (LA) support action research.  
Table 3, which is based upon [14, 24, 31], opposes key 
characteristics of action research to LA to show similarities as 
well as differences of both approaches.   

Action research is a well-known method for improvement of 
teaching and professional development [31]. The LA tools 
considered in our analysis (see, section 2.2) try to achieve 
common purposes. Many studies aim at increasing awareness and 
reflection about the learning process; fostering improvement 
activities at best. Learning analytics are supposed to help teachers 

to reflect and draw conclusions on the quality of their learning 
content, pedagogical practice, and quality of interactions among 
students (see, [1, 29, 46, 50, 58]), while students are to be 
stimulated to self-reflect their learning behavior based on 
monitoring their own usage and interaction behavior as well as 
having comparative information at hand (see, [25, 32, 47, 51]). 

As presented in table 4, these overall goals have been detailed and 
formulated in several ways in the LA literature.  They can be 
divided into goals that 

a. explicitly inform the design of learning analytics tools 
b. involve a behavioral reaction of the teacher 
c. involve a behavioral reaction of the student 

How does an LA tool, which was carefully designed according to 
the above-mentioned requirements, influence the behavior of its 
users, and therefore, how does it influence practical learning 
situations? Strikingly, there are very few publications reporting 
about findings related to the behavioral reactions of (b.) teachers 
and (c.) students, i.e., few studies measure the impact of using 
learning analytics tools. What are the effects of the usage of LA? 
How do LA systems influence practical learning situations? How 
does a specific indicator help the user to reflect and change his 
behavior? Which behavioral changes are visible to us? How could 
we measure them?  

Evaluative research on LA tools focuses on functionality, 
usability issues (indicator design, comprehensibility, terminology) 
and perceived usefulness of specific indicators [1, 28, 43, 50]. 
Furthermore, possibly due to novelty reasons several projects 
have not yet published data about conducting reliable case studies 
or evaluations results at all (e.g., [26, 29, 48, 51]). Some have 
conducted rather small evaluations that limit generalization of 
conclusions; e.g., [56] tested MATEP with one teacher. Although 
[41] mentioned that their experimental study aimed at measuring 

 Action research Learning analytics 

Goals Professional development, finding answers to practical 
questions, improvement of teaching, and social justice 

Monitoring, analysis, prediction, intervention, assessment, feedback, 
adaption, personalization, recommendation, reflection and self reflection 

Process 
cycle 

Develop a question – formulate research plan – collect data 
– analyze – develop and implement action plan – record 
project in writing – share 

Data gathering (select, capture) – information processing (aggregate, 
report) – knowledge application (use, refine) and sharing 

Driving 
factor 

Human-driven: activities are centered around the person 
(group), who conduct the project 

Data-driven: process is based on large amounts of data that promise to 
reveal new information 

Advantages Individual, perfectly fitting to a specific scenario, answers 
exactly the questions a teacher asks, open for all questions: 
What do I want to learn about my teaching? Methods of 
data collection can be adjusted creatively/accordingly 

Standardized, general, suited for several scenarios, possibility to provide 
approved data analysis/visualization by research experts, developed for 
and well suited for TEL or distance learning, data privacy issues can be 
handled centrally 

Drawbacks 
 

Limited by time-constraints, teacher’s workload and action 
research know-how, data analysis error-prone due to human 
error, not optimized for TEL, data privacy and permissions 
need to be handled by the teachers themselves 

Limited by missing data, often restricted to quantitative data collection 
methods, interpretation difficulties, danger to answer only questions 
nobody is interested in, focused on questions like: What does the data tell 
us? Specific question cannot be studied, if data is missing. 

Mode Manually Automatically 

Impact Effecting reflection, motivation and teaching activities Assumed influence on users’ behaviors and reflective practice  

Context 
knowledge 

Knowledge about individual teaching situation (e.g., 
motives, teaching history, reasons) 

Only data on teaching activities that have been recorded (e.g. log files, 
teacher journals, IMS learning design) 

Instance Single instance (or rather courses by one teacher) Multiple instances of the same scenario possible 

Methods All kinds of qualitative and quantitative methods (e.g., 
surveys, interviews, video recording) 

Limited to quantitative data collection methods (mostly data that can be 
logged automatically on different devices) 

Table 3. Comparison of key characteristics of action research and learning analytics. 

225



impact on the learning situation, they could not make conclusions 
because of a low participation rate. Bratitsis and Dimitracopoulou 
[10] give some evidence for the effects of using learning analytics 
on teachers’ and students’ behavior. They concluded that 
discussion analysis indicators helped to increase students’ activity, 
which might affect the learning process by leading to more 
effective discussions and critical thinking. 

Current LA systems try to support teachers and students in their 
tasks. So far, none of the tools that we have analyzed has a strong 
proof for a beneficial impact on either teachers, or students, or 
both. However, these tools should not only be usable and 
interesting to use, but also useful in the context of the goals: 
awareness, self-reflection, improvement of teaching, and 
improvement of learning. Measuring the impact of learning 
analytics tools is a challenging task. One of the main problems is 
to find enough participants for evaluations. Therefore, we suggest 
using a combination of qualitative and quantitative methods. In 
the following paragraphs, we sketch one of several possible way 
how to measure impact. The following approach could be 
described as design-based research with a focus on uncovering 
action research activities.  

The first step of measuring the impact of Learning Analytics tools 
is to make the tools available to the users. These should be a 
representable group of non-expert teachers and students. To be 
able to draw conclusions about the impact of LA usage, we need 
to know the current status in a classroom, and set this status as a 
reference point. How is the teacher’s learning design, and the 
student’s learning process? In which way does he/she interact 
with the systems and students? What is going well/not-so-well? 
How are the participants addressing problems without the support 
of LA? How is the performance of students? Answers to these 
questions will be used later as a reference point on to compare and 
measure whether the presence of LA tools has certain impacts on 
the behavior of both, teachers and students. This can be achieved 
by doing qualitative and quantitative evaluation on teachers and 
students.  

Conducting interviews with teachers, or providing them with 
questionnaires will also help to grasp their motivation and 
qualitative sides of teachers’ status. As for the students’ side, 
online questionnaires in the LMS at the beginning of the semester 
could provide information and comprehend their mindset 
concerning their activities and interactions in their learning 
process. For quantitative analysis, students’ log data and 

performance data from previous semesters (if existing) could be 
extracted, and analyzed to serve as comparison basis with the 
collected data of the ongoing evaluating time period, which could 
also record teachers’ interactions with the LA tool. How did users 
use the LA tool? Can we find patterns of usage? How do they 
react upon the data, e.g., does a teacher write an email to his/her 
students after viewing an indicator?  

After we have created a clear picture of the current state, we have 
to identify, which activities are likely to be improved by the usage 
of the LA tool/indicators. This is where we pose our hypotheses 
how the usage of the tool will improve the behavior of students 
and teachers, their activities, and the learning process. It is 
important to explicitly distinguish between the beginning state of 
the learning process, and the predicted state of the system. This 
will support the impact claimed of the tool use and explicitly 
show in which direction the possible effects and influence of the 
tool on both teachers and students go. 

The final step of the impact evaluation is to conduct interviews 
and questionnaires with both teachers and students. These will 
help to take into consideration their personal feelings and opinions 
after using the tool in a given time period. Did a specific indicator 
puzzle them? Did they reflect upon data shown by the indicators? 
How did they react? Did they revise their action plan? 

The results of these interviews should be compared to the results 
of the questionnaires/interviews before the beginning of the 
impact evaluation. The quantitative analysis should consist of 
comparing the analysis of the current performance and log data, 
with the analysis of the data of the previous semesters. Results of 
these two comparisons are combined to create and present a list of 
changes in the activities of both teachers and students, changes in 
their behavior, as well as changes in the learning process and 
resulting performance. To finalize this step, the hypothesized 
changes are compared with the actual changes. This way, we can 
draw conclusions on how the hypothesized impact relates to the 
actual impact of the learning analytics tool.  

This proposed impact evaluation method has its own limitations. 
The process itself needs long periods of time (at least one 
semester including the exam phase). It requires a lot of effort and 
active participation from researchers and participants in the 
learning process (teachers and students). The analysis of the 
qualitative data from the questionnaires and interviews is always 
prone to personal interpretations and biased while making 
conclusions. Therefore, it is necessary that several domain experts 

a.  Learning analytics are supposed to b.  Educators are supposed to c.  Students are supposed to 

• track user activities 
• capture the interaction of students with 

resources / the interactions among students  
• gather data of different systems 
• provide educators / students with 

feedback/information on students’ activities 
• provide an overview 
• highlight important aspects of data 
• provide different perspectives 
• offer possibilities for (peer) comparison  
• draw the users attention to interesting 

correlations 
• pinpoint problematic issues 
• establish an early warning system 
• provide decision support 

• monitor learning process / way of learning / 
students’ effort 

• explore student data / get to know students’ 
strategies 

• identify difficulties 
• discover patterns 
• find early indicators for success / poor 

marks / drop-out  
• draw conclusions about usefulness of 

certain learning materials and success 
factors 

• become aware / reflect / self-reflect 
• better understand effectiveness of learning 

environments 
• intervene / supervise / advice / assist 
• improve teaching / resources / environment 

• monitor own activities / interactions / 
learning process 

• compare own behavior with the whole 
group / high performing students 

• become aware 
• reflect / self-reflect 
• improve discussion participation / learning 

behavior / performance 
• become better learners 
• learn 
 

Table 4. Goals of learning analytics concerning tools, educators, and students. 

226



interpret the qualitative data, and compare their findings. On the 
one hand, it will be a great challenge to predict and identify the 
key differences between the teaching and learning states before 
and after using the LA tool. Depended on the number of 
participants of the study and the collected data, it might not be 
possible to come to clear conclusions about the impact of LA 
tools. On the other hand, the suggested evaluation process delivers 
lots of information that can be used to iteratively inform future 
designs of LA tools, which might better support action research. 
Furthermore, as one reviewer of this paper wrote, LA is not yet 
part of the routine practice in teaching and learning. With more 
practice of LA in everyday life, other ways might appear to 
measure the impact of LA. 

4.4 Limitations of our Study 
We based our analysis on a former qualitative meta-analysis, 
which investigated what kind of questions teachers ask while 
performing action research in their technology-enhanced learning 
(TEL) scenarios. The meta-analysis was limited to case studies 
described in the conference proceedings of the German eLearning 
conference “DeLFI” [22]. This conference could be considered as 
representative for technology application in Germany at the time 
of writing. Future research could take into account questions from 
a more international context. These findings will probably differ 
from the teachers’ questions we based our study on, showing that 
specific contexts need specific approaches. It can also be assumed 
that the issues of education will change in the future, together with 
the teaching and learning scenarios. We conclude that practical 
LA designs should always consider and constantly update the 
concrete scenarios and take into account the characteristics of 
each target groups, which they are supposed to address. 

We based our analysis of indicators on 27 LA tools (section 2.2). 
There are many more studies on indicators; in particular there are 
more findings on higher-level indicators in the EDM field that 
will be able to solve complex questions of teachers in future LA 
tools. The challenge is, how to make them usable. 

Another limitation of our approach lies in the subjectivity of the 
personal interpretation of the questions and the indicators. For 
objectivity reasons, we tried to overcome this limitation, by 
conducting the indicator collection, categorization and question-
indicator-mapping separately by two researchers. The process of 
creating the mapping was not easy. Questions and indicators did 
not really fit. In order to provide teachers with insight and 
enlightening information about their students, LA researchers 
need to involve teachers’ interests in LA development projects 
and design and evaluate sophisticated indicators that are able to 
answer their questions. 

Another reason for the difficulties we faced was the absence of 
clear and explicit guidelines explaining which questions the 
presented indicators are trying to answer. If we, as researchers of 
the LA field, cannot clearly identify which questions each 
indicator answers, how can teachers and non-experts understand 
the purpose of an indicator? How can a teacher find the fitting 
indicators to his or her own individual questions? How can 
teachers be prevented from false interpretations? This calls for the 
necessity, that LA researchers should always provide, accessible 
guidelines of how to interpret each newly developed indicators, 
and give hints about which questions can be answered by which 
indicators in which learning context. At the same time, they also 
have to provide information about the limitations of each 
indicator. These arguments are especially meaningful for EDM-
based indicators. 

5. CONCLUSION 
Learning Analytics tools should be an integral part of TEL. The 
tools aim at having an impact on teachers and students. But the 
impact has not been evaluated. The concern we are raising is that 
LA tools should not only be usable, but also useful in the context 
of the goals we want to achieve.  

Creating diversified sets of indicators, according to the roles of 
and interests of users may be a promising idea [19]. An action 
research project may involve an expert researcher or mentor, who 
provides information, but leaves decisions to the practitioners of 
the field [31]. LA tools could take this position by informing and 
supporting action research tasks, such as choosing research 
questions and following a research plan, allowing the manual 
collection of data that otherwise would not be available (e.g., 
course attendance information), or recommending appropriate 
indicators for individual interests.  

Our investigation showed that the currently available research 
tools do not yet answer many questions of teachers. But 
appropriate and significant indicators would be important for 
initiating action research. At present, we have a set of indicators 
that mainly answer questions concerned with usage analysis. 
Complex questions that relate to qualitative analysis and data 
correlation from diverse sources cannot yet be answered with the 
existing indicators. This complies with findings from the field of 
interaction analysis in 2005, when most tools produced indicators 
of low interpretative value, e.g. percentage of participation or 
answers to messages [19]. The causes for these shortcomings are 
insufficient involvement of teachers in the design and 
development of indicators, absence of rating data/features, non-
used student academic profile data, and absence of specific 
student generated data (mobile, data usage from different 
devices), as well as missing data correlation and combination 
from different data sources. Furthermore, teachers’ data and 
indicators based on this teachers’ data are not easily visible in the 
current publication on implementations of Learning Analytics 
tools. And also, against this background of data consolidation, we 
need best practice examples to deal with data privacy issues. 

Future learning environments (no matter if LMS or PLE) will 
probably have rating features. Data collected by these functions 
should be used for LA tools. Furthermore, in order to fill in the 
gap between what the indicators do, and what the teachers 
actually need, researchers should actively involve teachers in the 
design and implementation of indicators. The identification of 
their requirements, as also noted by [19], will ensure that the 
implemented indicators will be tailored according to teacher 
needs. It is also noteworthy to mention that researchers should 
always provide guidelines for the teachers on how to interpret the 
indicators. It is imperative to give hints on which teachers’ 
questions can be answered with which sets of indicators. Also, LA 
researchers should clearly state the limitations of their newly 
developed indicator to prevent misinterpretations. Lastly, there is 
a necessity for creation of evaluation tools to measure the impact 
and effects of LA on the learning process. We need proofing 
mechanisms that will support and reassure the goals concerning 
increased awareness and reflections, and improved teaching 
processes. To conclude, we need better indicators that will serve 
answering the teachers’ questions, we need to collect and correlate 
data from different sources to support these new indicators, and 
we need to evaluate how these indicators impact teaching and 
learning processes.  
We discussed how LA tools could impact teaching and suggested 
how we could we measure this impact to initiate a discussion 

227



about this in the research community. Our own next steps are the 
enhancement of an existing LA tool, namely eLAT [23] and the 
evaluation of its impact. 

We would like to encourage the community of LAK researchers 
to discuss our findings to advance the field of LA and inform the 
development of improved and personalized tools that provide 
high-level indicators, which have a high potential of initiating 
reflective thinking processes. 

6. REFERENCES 
[1] Ali, L., Hatala, M., Gaševi?, D. and Jovanovi?, J. 2012. 

A qualitative evaluation of evolution of a learning 
analytics tool. COMPUT EDUC. 58, 1 (Jan. 2012), 470–
489. 

[2] Altrichter, H., Posch, P. and Somekh, B. 1996. Teachers 
investigate their work: An introduction to the methods of 
action research. Routledge. 

[3] Arnold, K. 2010. Signals: Applying Academic Analytics. 
EDUCAUSE Quaterly. 33, 1 (2010). 

[4] Asadi, M., Jovanovi?, J., Gasevic, D. and Hatala, M. 
2011. A Quantitative Evaluation of LOCO-Analyst: A 
tool for Raising Educators’ Awareness in Online 
Learning Environments. (2011), 1–17. 

[5] Bakharia, A., Heathcote, E. and Dawson, S. 2009. Social 
networks adapting pedagogical practice??: SNAPP. Proc. 
of ascilite (2009), 49–51. 

[6] Borgatti, S.P. 2002. NetDraw: Graph visualization 
software. Harvard: Analytic Technologies. (2002). 

[7] Boud, D., Keogh, R. and Walker, D. eds. 1985. 
Reflection: Turning Experience Into Learning. Routledge 
Chapman & Hall. 

[8] Bratitsis, T. and Dimitracopoulou, A. 2005. Data 
Recording and Usage Interaction Analysis in 
Asynchronous Discussions??: Proc. of the Workshop on 
Usage Analysis in Learning Systems (AIED’05) (2005), 
9–16. 

[9] Bratitsis, T. and Dimitracopoulou, A. 2008. Interaction 
Analysis as a Multi-Support Approach of Social 
Computing for Learning, in the Collaborative Era: 
Lessons Learned by Using the DIAS System. Proc. of the 
8th Int. Conf. on Advanced Learning Technologies 
(ICALT’08) (2008), 536–538. 

[10] Bratitsis, T. and Dimitracopoulou, A. 2008. 
Interpretation Issues in Monitoring and Analyzing Group 
Interactions in. INT J E-COLLABORATION. 4, 1 (2008), 
20–40. 

[11] Bratitsis, T. and Dimitracopoulou, A. 2006. Monitoring 
and Analyzing Group Interactions in Asynchronous 
Discussions with the DIAS system. 12th Int. Workshop 
on Groupware, GRIWG2006, Groupware: Design, 
Implementation and Use (2006), 54–61. 

[12] Brooks, C., Panesar, R. and Greer, J. 2006. Awareness 
and Collaboration in the iHelp Courses Content 
Management System. Proc. of the 1st Europ. Conf. on 
Technology Enhanced Learning (EC-TEL 2006) (2006), 
34–44. 

[13] Cator, K. and Adams, B. 2012. Enhancing Teaching and 
Learning Through Educational Data Mining and 
Learning Analytics: An Issue Brief. 

[14] Chatti, M.A., Dyckhoff, A.L., Schroeder, U. and Thüs, 
H. 2012. A reference model for learning analytics. 
IJTEL. 4, 5/6 (2012), 318–331. 

[15] Chatti, M.A., Dyckhoff, A.L., Schroeder, U. and Thüs, 
H. 2012. Forschungsfeld Learning Analytics. I-Com. 11, 
1 (Mar. 2012), 22–25. 

[16] Clow, D. 2012. The Learning Analytics Cycle??: Closing 
the loop effectively. Proc. of the 2nd International Conf. 
on Learning Analytics and Knowledge (LAK’12) (2012), 
134–138. 

[17] Dawson, S. 2010. “Seeing” the learning community: An 
exploration of the development of a resource for 
monitoring online student networking. BRIT J EDUC 
TECHNOL. 41, 5 (Sep. 2010), 736–752. 

[18] Dawson, S., Bakharia, A. and Heathcote, E. 2010. 
SNAPP??: Realising the affordances of real-time SNA 
within networked learning environments. Proc. of the 7th 
Int. Conf. on Networked Learning (2010), 125–133. 

[19] Dimitracopoulou, A. 2008. Computer based Interaction 
Analysis Supporting Self-regulation: Achievements and 
Prospects of an Emerging Research Direction. TICL. 
(2008). 

[20] Dimitracopoulou, A. 2005. State of the art of Interaction 
Analysis for Metacognitive Support & Diagnosis. IA. 
JEIRP Deliverable D.31.1.1. Kaleidoscope NoE. (2005). 

[21] Duval, E. and Verbert, K. 2012. Learning Analytics. 
eleed. 8, (2012). 

[22] Dyckhoff, A.L. 2011. Implications for Learning 
Analytics Tools: A Meta-Analysis of Applied Research 
Questions. IJCISIM. 3, (2011), 594–601. 

[23] Dyckhoff, A.L., Zielke, D., Bültmann, M., Chatti, M.A. 
and Schroeder, U. 2012. Design and Implementation of a 
Learning Analytics Toolkit for Teachers. EDUC 
TECHNOL SOC. 15, 3 (2012), 58–76. 

[24] Elias, T. 2011. Learning Analytics: Definitions, 
Processes and Potential. 

[25] Fritz, J. 2011. Classroom walls that talk: Using online 
course activity data of successful students to raise self-
awareness of underperforming peers. The Internet and 
Higher Education. 14, 2 (Mar. 2011), 89–97. 

[26] García-Saiz, D. and Pantaleón, M.E.Z. 2011. E-learning 
Web Miner: A data mining application to help instructors 
involved in virtual courses. Proc. of the 4th Int. Conf. on 
Educational Data Mining (EDM’11) (2011), 323–324. 

[27] González Agulla, E., Alba Castro, J.L., Argones Rúa, E. 
and Anido Rifón, L. 2009. Realistic Measurement of 
Student Attendance in LMS Using Biometrics. Proc. of 
the Int. Symposium on Engineering Education and 
Educational Technologies (EEET’09) (2009), 5–7. 

[28] Govaerts, S., Verbert, K., Duval, E. and Pardo, A. 2012. 
The student activity meter for awareness and self-
reflection. Proc. of the ACM SIGCHI Conf. on Human 
Factors in Computing Systems (CHI EA’12) (May. 
2012), 869–884. 

[29] Graf, S., Ives, C., Rahman, N. and Ferri, A. 2011. AAT: 
A Tool for Accessing and Analysing Students’ 
Behaviour. Proc. of the 1st Int. Conf. on Learning 
Analytics and Knowledge (LAK’11) (2011), 174–179. 

228



[30] De Groot, R., Drachman, R., Hever, R.R., Schwarz, B.B., 
Hoppe, U., Harrer, A., De Laat, M., Wegerif, R., 
McLaren, B.M. and Baurens, B. 2007. Computer 
Supported Moderation of E-Discussions: the 
ARGUNAUT Approach. Proc. of the Conf. on Computer 
Supported Collaborative Learning 2007 (CSCL’07) 
(2007), 165–167. 

[31] Hinchey, P.H. 2008. Action Research Primer. Peter Lang 
Publishing, Inc. 

[32] Janssen, J., Erkens, G., Kanselaar, G. and Jaspers, J. 
2007. Visualization of participation: Does it contribute to 
successful computer-supported collaborative learning? 
COMPUT EDUC. 49, 4 (Dec. 2007), 1037–1065. 

[33] Johnson, L., Adams, S. and Cummins, M. 2012. NMC 
Horizon Report: 2012 Higher Education Edition. 

[34] Johnson, M.W., Eagle, M.J., Joseph, L. and Barnes, T. 
2011. The EDM Vis Tool. Proc. of the 4th Int. Conf. on 
Educational Data Mining (EDM’11) (2011), 349–350. 

[35] Kay, D., Korn, N. and Oppenheim, C. 2012. Legal , Risk 
and Ethical Aspects of Analytics in Higher Education. 
JISC CETIS Analytics Series. 1, 6 (2012), 1–30. 

[36] Kolb, D. 1984. Experiential learning: experience as the 
source of learning and development. Prentice Hall. 

[37] Kosba, E., Dimitrova, V. and Boyle, R. 2005. Using 
Tracking Data to Build Student and Group Models and 
Generate Advice in Web-Based Distance Learning 
Environments. 

[38] Krüger, A., Merceron, A. and Wolf, B. 2010. A Data 
Model to Ease Analysis and Mining of Educational Data. 
Proc. of the 3rd Int. Conf. on Educational Data Mining 
(EDM’2012) (2010), 131–140. 

[39] Lockyer, L. and Dawson, S. 2011. Learning designs and 
learning analytics. Proc. of the 1st Int. Conf. on Learning 
Analytics and Knowledge (LAK’11) (2011), 153–156. 

[40] Lockyer, L. and Dawson, S. 2012. Where Learning 
Analytics Meets Learning Design. Proc. of the 2nd 
International Conf. on Learning Analytics and 
Knowledge (LAK’12) (2012), 14–15. 

[41] May, M., George, S. and Prévôt, P. 2011. TrAVis to 
enhance online tutoring and learning activities: Real-time 
visualization of students tracking data. Interactive 
Technology and Smart Education. 8, 1 (2011), 52–69. 

[42] May, M., George, S. and Prévôt, P. 2011. TrAVis to 
Enhance Students’ Self-monitoring in Online Learning 
Supported by Computer-Mediated Communication 
Tools. IJCISIM. 3, (2011), 623–634. 

[43] Mazza, R. 2006. Evaluating information visualization 
applications with focus groups. Proc. of the 2006 AVI 
workshop on BEyond time and errors: novel evaluation 
methods for information visualization (BELIV’06) (May. 
2006), 1–6. 

[44] Mazza, R. and Botturi, L. 2007. Monitoring an Online 
Course With the GISMO Tool: A Case Study. JILR. 18, 
2 (2007), 251–265. 

[45] Mazza, R. and Dimitrova, V. 2007. CourseVis: A 
graphical student monitoring tool for supporting 
instructors in web-based distance courses. INT J HUM-
COMPUT ST. 65, 2 (Feb. 2007), 125–139. 

[46] Merceron, A. and Yacef, K. 2005. TADA-Ed for 
Educational Data Mining. IME of CEL. 7, 1 (2005). 

[47] Mochizuki, T., Kato, H., Fujitani, S., Yaegashi, K., 
Hisamatsu, S., Nagata, T., Nakahara, J., Nishimori, T. 
and Suzuki, M. 2007. Promotion of Self-Assessment for 
Learners in Online Discussion Using the Visualization 
Software. User-Centered Design of Online Learning 
Communities. N. Lambropoulos and Z. Panayiotis, eds. 
Information Science Publishing. 365–386. 

[48] Pedraza Perez, R., Romero, C. and Ventura, S. 2011. A 
Java desktop tool for mining Moodle data. Proc. of the 
4th Int. Conf. on Educational Data Mining (EDM’11) 
(2011), 319–320. 

[49] Petropoulou, O., Retalis, S., Siassiakos, K., Karamouzis, 
S. and T., K. 2007. Helping Educators Analyse 
Interactions within Networked Learning Communities: A 
Framework and the AnalyticsTool System. Proc. of the 
6th Int. Conf. on Networked Learning (2007), 317–324. 

[50] Scheuer, O. and Zinn, C. 2007. How did the e-learning 
session go? The Student Inspector. Proc. of the Conf. on 
Artificial Intelligence in Education (AIED’07) (Jun. 
2007), 487–494. 

[51] Schmitz, H.-C., Scheffel, M., Friedrich, M., Jahn, M., 
Niemann, K. and Wolpers, M. 2009. CAMera for PLE. 
Proc. of the 4th Europ. Conf. on Technology Enhanced 
Learning (EC-TEL’09) (Oct. 2009), 507–520. 

[52] Schmitz, H.-C., Wolpers, M., Kirschenmann, U. and 
Niemann, K. 2009. Contextualized Attention Metadata. 
Human Attention in Digital Environments. C. Roda, ed. 
Cambridge University Press. 186–209. 

[53] Siemens, G. 2010. What are Learning Analytics? 

[54] Zhang, H. and Almeroth, K. 2010. Moodog: Tracking 
Student Activity in Online Course Management Systems. 
JILR. 21, 3 (2010), 407–429. 

[55] Zhang, H., Almeroth, K., Knight, A., Bulger, M. and 
Mayer, R. 2007. Moodog??: Tracking Students’ Online 
Learning Activities. Proc. of World Conf. on Educational 
Multimedia, Hypermedia and Telecommunications (ED-
Media’07) (2007), 4415–4422. 

[56] Zorrilla, M., García, D. and Álvarez, E. 2010. A decision 
support system to improve e-learning environments. 
Proc. of the EDBT/ICDT Workshops (2010), 1–8. 

[57] Zorrilla, M.E., Marín, D. and Álvarez, E. 2007. Towards 
Virtual Course Evaluation Using Web Intelligence. Proc. 
of the 11th Int. Conf. on Computer Aided Systems Theory 
(EUROCAST’07) (2007), 392–399. 

[58] Zorrilla, M.E. and Álvarez, E. 2008. MATEP: 
Monitoring and Analysis Tool for E-Learning Platforms. 
Proc. of the 8th IEEE International Conf. on Advanced 
Learning Technologies (ICALT’08) (Jul. 2008), 611–613.  

 

229





