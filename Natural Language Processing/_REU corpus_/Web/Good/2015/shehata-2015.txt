
Measuring Student Success Using Predictive Engine 
Shady Shehata
D2L Corporation

151 Charles Street West, Suite 400
Kitchener, ON, Canada, N2G 1H6

1­519­772­0325 ext. 3257
Shady.Shehata@d2l.com

Kimberly E. Arnold
University of Wisconsin

1305 Linden Drive
Madison, WI 53706

T608­263­9443 
kimberly.arnold@wisc.edu

ABSTRACT
A basic  challenge  in  delivering  global  education  is  improving
student success. Institutions of education are increasingly focused
on improving graduation and retention rates of their students. In
this  poster, we  describe Student  Success  System (S3)  that  can
measure student performance starting from the first weeks of the
semester  and  the  adoption  process  for  S3  by  University  of
Wisconsin System (UWS). 

Categories and Subject Descriptors
G.3 [PROBABILITY AND STATISTICS]

General Terms
Terms:  Algorithms,  Management,  Measurement,  Performance,
Design, Experimentation.

Keywords
Learning Analytics, Data Mining, Machine Learning, Predictive
Modeling, Regression Analysis, Algorithms, Student Success.

1. Introduction 
Advancements  in  data  analysis  and  predictive  modeling  have
tremendous  potential  to  improve  student  success  by  enabling
colleges and universities to build powerful predictive models that
predict student behavior. S3 is an Early Intervention System that
empowers  institutions  with  predictive  analytics  to  improve
student success, retention, completion, and graduation rates.  S3
provides  educators  with  early  indicators  and  predictions  of
student success and risk levels. Predictions generated by S3 are
based on predictive models that are created by applying machine
learning  algorithms  on  historic  course  data  (usually  prior
offerings  of  the  same  course  for  which  predictions  are  to  be
generated).  The  predictive  models  are  adaptable  and
customizable to the instructional approach of each course, as well
engagement and

____________________________

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies  bear  this  notice  and  the  full  citation  on  the  first  page.
Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author. 

Copyright is held by the owner/author(s).

LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA

ACM 978-1-4503-3417-4/15/03.

http://dx.doi.org/10.1145/2723576.2723661

achievement  expectations.  The  system  does  not  currently
recommend and deliver content based on the analysis of a user’s
performance or learning style.  Section 2 describes  the Student
Success  System  predictive  modeling.  Section  3  describes  the
process  of S3 adoption at  the  University of Wisconsin  System
(UWS).

2. Student Success System
S3 is developed by D2L Corporation and is currently available in
production. The core component of S3 System is the predictive
engine that is able to generate customized predictive model for
an  individual  course.  The  S3  predictive  engine  generates
regression models for each course. Figure 1 shows the main page
of the student success system where instructors can monitor the
status of each student in terms of their predicted success index. 

The success index is expressed as score on a scale of 0-10, and
trend indicated by the color and shape of an associated symbol:
At  Risk  (red  triangle),  Potential  Risk  (yellow  diamond),  and
Successful  (green  circle).  The  levels  are  determined  based  on
thresholds on the predicted grade. The defaults are: 0%-60% for
At-Risk,  60%-80%  for  Potential  Risk,  and  80%-100%  for
successful.  The success index is broken down into five success
predictors, a.k.a. “Domains” as shown in Figure 2 win-loss chart.
The predictive model can be configured by the S3 administrator
to include or exclude each of these predictors. When selected, the
individual success predictors are combined to provide the overall
value of the success index.  The importance (weight)  associated
with  individual  predictors  is  automatically  determined  by the
predictive  model.  The  success  index  that  is  generated  by
regression model can be aggregated either  at the domain or the
variable  levels.  The  model  aggregation  determines  the  type  of
aggregation  for  calculating  the  success  index.  The  domain
indicators represent the success/risk outcome based on the set of

Figure 1. Student Dashboard.
.

416

http://dx.doi.org/10.1145/2723576.2723661


measurements related to each domain. An alternative aggregation
level is for the success index to be determined based on variables
without the pre-defined grouping of variables them into domains.

At domain  aggregation,  the  success  index  is  generated  in  two
steps.  First,  each domain model generates predictions based on
the  domain  variables.  Then,  the  overall  domain  generates  the
overall success index based on the output of each domain model
as shown in Figure 3. At variable aggregation, the success index
is generated in one step where the variables of all  domains are
used to generate the overall success index as show in Figure 4.

3. UWS Adoption of S3
The University of Wisconsin System (UWS) was awarded a grant
to  begin  exploring  data  analytics  to  improve  student  success.
UWS was looking for a comprehensive solution that would allow
instructors  access  to new insights  about  how to better  support
their students. Since the composition of UWS totals over 150,000
unique  individuals  each year, UWS was  looking for a solution
that was robust enough to support exceptionally diverse learners.
A  rich  source  of  contextualized  data  was  available  from  a
decade’s use of the Desire2Learn learning management system.

Therefore,  UWS  partnered  with  D2L  to  begin  piloting  the
Student Success System.  In the pilot period, five campuses and
roughly  6,500  students  have  experienced  S3.  Since  S3  is
designed  on  a  premise  of  probabilistic,  predictive  modeling,
UWS  felt  strongly  that  the  information  serve  as  a  proactive
catalyst  for  instructors.   For  these  reasons,  pilot  faculty were
selected  based  on  evidence  of  iterative  student  intervention.
Instructor  interest  was  gauged  and  if  the  instructor  chose  to
participate, a stipend was awarded to ensure sufficient time was
dedicated to the pilot. One of the most appealing features of S3
was  the  course-level  models  for  student  success.  However,
predicting risk can lead to adverse effects on students, especially
those early in their post-secondary career. For this reason, UWS
dedicated  significant  resources  to  evaluate  how models  were:
configured  and  selected,  behaved  over  time  (semester),  and
impacted  individual  students.  For  each  pilot  course,  model
criteria  and  historical  course  data  was  gathered,  domain
configurations were created, and models were built in simulation
mode.   A model matrix  was then created for each pilot course.
Each model matrix  laid out between 18 and 25 possible  model
configurations  along  with  aggregate  error  measures  (Mean
Squared  Error  and  Average  Percent  Correct),  and  error  for
students with unsuccessful outcome (grade of D or F). After the
matrices  were  created,  each  matrix  was  presented  to  the  pilot
faculty member, and up to three models were selected for more
detailed  review.  Aggregate  error  measure  and  other  associated
metrics  can  be  very  helpful  in  delineating  between  potential
models.  However,  in  UWS’s  estimation,  aggregate  measures
alone were not sufficient to select a model for high-risk decision
making.   Therefore, after faculty selected up to three models for
more detailed review, and student level previews were generated
(as shown in Figure 5).   These renderings allow for a student-
level view of how a model acts across the weeks in a semester,
providing an additional level of information to help the instructor
and  S3  administrator  make  the  best  model  selection.  Of  key
importance  at  this  juncture  is  to  evaluate  various  model
configurations  for  base-level  disparate  student  impact.
Additionally, the individual  student-level preview allows for an
easy  visual  cue  for  many  instructors  about  when  a  model
stabilizes  (week  3  versus  week3,  for  example)  to  help  them
design their intervention strategy.

After the instructor evaluated  the model matrix  for her  course,
followed by the student level-preview, the instructor, selected the
model that she felt would best allow her to support the students
in her  class.  Instructors relied on the error measures  and other
metrics,  along  with  which  model  would  supplement  their
intervention and support strategies.  The ultimate model selection
was always left to the discretion of the course instructor. S3 then
automatically generated new predictions every seven days.

Figure 5. Sample S3 Student-Level Preview.
.

Figure 3. Domain Aggregation.
.

Figure 4. Variable Aggregation.
.

Figure 2. Win-Loss Chart.
.

417





