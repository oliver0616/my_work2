
Combining Observational and Experiential Data  
to Inform the Redesign of Learning Activities 

Abelardo Pardo1 
abelardo.pardo@sydney.edu.au  

Robert A. Ellis2 
robert.ellis@sydney.edu.au 

Rafael A. Calvo1 
rafael.calvo@sydney.edu.au 

 
1School of Electrical and Information Engineering 

2Institute for Teaching and Learning 
The University of Sydney NSW 2006 Australia 

 

ABSTRACT 
 A main goal for learning analytics is to inform the design of a 
learning experience to improve its quality. The increasing 
presence of solutions based on big data has even questioned the 
validity of current scientific methods. Is this going to happen in 
the area of learning analytics? In this paper we postulate that if 
changes are driven solely by a digital footprint, there is a risk of 
focusing only on factors that are directly connected to numeric 
methods. However, if the changes are complemented with an 
understanding about how students approach their learning, the 
quality of the evidence used in the redesign is significantly 
increased. This reasoning is illustrated with a case study in which 
an initial set of activities for a first year engineering course were 
shaped based only on the student’s digital footprint. These 
activities were significantly modified after collecting qualitative 
data about the students approach to learning. We conclude the 
paper arguing that the interpretation of the meaning of learning 
analytics is improved when combined with qualitative data which 
reveals how and why students engaged with the learning tasks in 
qualitatively different ways, which together provide a more 
informed basis for designing learning activities..  

Categories and Subject Descriptors 
K.3.1 [Computer Uses in Education]: Computer-assisted 
instruction 

General Terms 
Algorithms, Design, Human Factors, Measurement, Performance. 

Keywords 
Learning analytics, active learning, mixed methods analysis, 
approaches to learning, interventions. 

 

 

1. INTRODUCTION 
The combined areas of Learning Analytics and Educational Data 
Mining have emerged with the potential to improve university 
student learning. In contexts other than education, some authors 
claim that using massive amounts of data should stop the search 
for models and rely solely on correlation [1]. Does this apply to 
learning? To explore this question, a key objective guiding this 
study is whether insights from the digital footprint left by students 
engaging in a learning experience can be meaningfully elaborated 
by understanding qualitative differences in how students 
approach their learning in the same experience, enabling a more 
nuanced interpretation of the digital footprint. We postulate that 
learning contexts are highly complex ecosystems in which a large 
number of factors are intertwined and shape how different 
stakeholders perceive their effectiveness. Although technology 
now offers the possibility of capturing massive amounts of data 
about student events, the quality of data interpretation and the 
effectiveness of the derived interventions are still areas subject to 
debate. This paper investigates a strategy to address this challenge 
through exploring observational data collected while students 
interact in their technology-enabled environment, and inferring 
the meaning of patterns within this data set informed by smaller 
scale phenomenographic data. We argue that the interpretation of 
the observational data can be improved by complementing its 
analysis using data which describes the student experience of 
learning, how, what and why they learn. Finding the right 
framework to combine these two data sources will translate into 
deeper insight and understanding of the student experience and 
help to improve the design of effective interventions. The 
objective is to increase the understanding of the digital footprint 
in the learning environment by complementing the observational 
data with a description of how students approach their learning, 
the strategies they adopt and why they adopt these strategies. The 
rest of this paper is organized as follows. Section 2 describes the 
work related to the space explored in the document. Section 3 
describes the proposed approach to study the combination of the 
two data sources. Section 4 includes the description of the case 
study used to validate the proposed framework. Section 5 
summarizes the paper and offers lines for future research. 

2. RELATED WORK 
Learning analytics and educational data mining have emerged as 
research areas that focus on the use of recorded interactions 
occurring in a learning experience to enhance the understanding 
and improve learning and the environment in which it occurs [4]. 
In the last years numerous products, applications and research 
contributions have appeared to tackled issues such as retention 
[3], student success [6, 7], career recommendations [5], etc. The 
Educational Data Mining space, although closely related to 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be 
honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. Request permissions from 
Permissions@acm.org. 
LAK’15, March 16 - 20, 2015, Poughkeepsie, NY, USA 
Copyright is held by the owner/author(s). Publication rights licensed to 
ACM. 
ACM 978-1-4503-3417-4/15/03…$15.00  
http://dx.doi.org/10.1145/2723576.2723625 

305



learning analytics, has been focusing primarily on the algorithms 
to be applied to large data quantities to automatically produce 
models to predict academic achievement [2, 14].  

In parallel with these contributions, several researchers have 
identified a so-called middle space where the concerns derived 
from a focus on aspects of learning and aspects of analytic 
techniques meet [16]. Knight and Buckingham-Shum [8] provided 
a more pragmatic view and suggested to take into account 
literature in the fields of epistemology, pedagogy and assessment 
to help delimit this middle space. Their argument derives from the 
recognition of assessment as an essential part of learning, but at 
the same time, one of the most controversial. The abundance of 
data provided by technology about how students behave in a 
learning experience has logically prompted the question of how 
can this improve student achievement in general (not simply 
assessment results). But there are other stakeholders that need to 
be brought to the forefront of the discussion about how this 
middle space is shaped: the instructor, the learning designer and 
the students. Lockyer et al. [9] argued that in order for learning 
analytics to gain widespread adoption, there is a need to establish 
a framework connecting the data collected in a learning 
environment and the design of learning experiences. Learning 
designs can clearly benefit from big data collected while being 
enacted so that they can be refined in a continuous improvement 
cycle To do this, some research has suggested combining 
methodologies informed by the social sciences in order to 
understand the students’ perspective on the digital footprint they 
leave behind [19]. 

In this study, we have integrated learning analytics evidence with 
evidence from a social science orientation known as the Student 
Approaches to Learning (SAL) framework (e.g. [11–13]). At a 
high level of description, the SAL framework has identified a 
number of variables which account for qualitative variation in the 
student experience of learning at university; student approaches to 
learning, student conceptions of learning and student perceptions 
of the their learning environment. Qualitative variation in any of 
these variables has been shown to be positively and logically 
related to qualitative variation in the other variables. For example, 
deep approaches to learning have been shown to be positively and 
significantly associated with coherent conceptions of learning, 
positive perceptions of aspects of the learning environment such 
as assessment and workload, and relatively higher levels of 
academic achievement (e.g. [12]). Similarly, surface approaches 
to learning have been shown to be positively associated with 
fragmented conceptions of learning and negative perceptions of 
the learning environment (e.g. [15]). 

By combining evidence from both learning analytics and the SAL 
framework, we hope to add some depth to key issues that exist in 
the ‘middle space’ between the two areas that starts to explain 
why some students experience learning relatively more 
successfully than others.  

3. DATA TO SUPPORT ACTIVITY 
DESIGN 
To date, collected data has mostly been used to create reports and 
predictive models. However, the interpretation leading to changes 
in a design  has received much less attention from researchers [17, 
18]. Thus, one of the most challenging areas to capitalize the 
potential of ‘big’ data in learning environments is the connection 
between data and actions to modify those environments. In this 

paper we argue that successful changes in a learning design 
require a thorough understanding not only of the footprint, but 
also of the approaches that students experience in their learning.  

3.1  Design informed by digital footprints 
Current approaches to learning analytics collect extensive data 
about events occurring while learning and apply various 
algorithms to produce a model for such learning. For example, 
Romero et al. [15] collected a set of student attributes recorded in 
blended course hosted in the Moodle Learning Management 
System. A clustering algorithm was then used to produce a set of 
rules used to classify future students. These rules can be used to 
propose different tasks to the students depending on the category 
estimated by the model.  
This example shows how data provides previously unknown 
knowledge (the categories in which students can be divided). If 
we use a ‘big data’ approach alone, the instructor does not 
understand why the students adopted the strategies they did with 
the technologies, why some were more successful than others and 
what strategies they used.  

3.2 Design informed by qualitative data 
A strength of the SAL approach is the detailed investigation of the 
student perspective about what they think they are learning (their 
conceptions of learning), how they go about their learning and 
why, (their approaches to learning), and how qualitative variation 
in their conceptions and approaches are related to qualitative 
variation in academic achievement as measured by variables such 
as task marks and course marks. 

Challenges for the SAL approach in digital learning environments 
involving large numbers of students is getting a sense of, and 
evidence to confirm, variation in the quality of learning across 
multiple tasks and hundreds if not thousands of students. We 
postulate that this shortcoming can be complemented successfully 
with analytics and educational mining methodologies. 

3.3 Combining both approaches to inform the 
design of learning activities 
The premise used in this study is that a mixed-methods approach 
using both ‘big data’ and SAL methodologies will enable 
instructors to design better informed and student-centered 
activities than if they consider the two sources of information 
separately. 

4. CASE STUDY 
The aim of the study was to investigate why some students were 
more successful than others in a first year electrical engineering 
course focusing on computer engineering. The number of students 
enrolled is slightly over 300. Students attend one weekly 2 hour 
lecture, one weekly 2 hour tutorial, and one weekly 3 hour lab 
session. The design of the course is based on active learning 
principles. For the lectures, students have to prepare the session 
interacting with a number of activities that include visualizing 
videos, answer multiple choice questions, and solve several 
problems. The following course events (and their identifier for the 
analysis) are recorded and included in the digital footprint dataset 
for every student: 

• Visualization of any page of the course notes (RV) 
• Access to specific sections of the course notes (ACE) 
• Multiple choice questions answered correctly (EQC) 

306



• Multiple choice questions answered incorrectly (EQI) 
• Video visualization events (EV). 
• Incorrect answers to problems sequence (EA) 
• Dashboard showing student participation (DV) 

 
The course included a mid-semester exam consisting of 20 
multiple choice questions and counting 20% of the final course 
score. The score for this exam (MID) is considered as the 
dependent variable in the analysis described below. 

The methodology in this study consists of three stages. The first 
considers only the digital data set, a model derived from it, and 
the derived changes in the learning experience. The second stage 
is an analysis of student self-reported data about the experience of 
engaging with the course activities. The third phase consists on 
revisiting and reassessing the changes when combining the two 
data sets. The steps followed for this analysis are: 

4.1 Model derived from the digital footprint 
The model derived from the digital footprint data set has been 
obtained using all-subsets statistical regression. The descriptive 
statistics of the previously described factors are shown in the 
Table 1: 

Table 1 Summary of variables used in the regression 

Var Min 1
st 

Quart Median Mean 
3rd 

Quart Max 

ACE 16 143 214 252.6 329 935 
RV 29 238 387 417.6 549.0 1315 
DV 0 0 8 21.96 28.0 210 

EQC 0 28 70 95.39 138.0 390 
EQI 0 18 45 60.66 83 368 
EA 0 331 437 430 541 1385 
EV 0 44 113 184 269 1925 

MID 3 11 14 13.48 16 20 
 
The statistical analysis was carried out with the R software 
platform. Given the small number of factors, all-subsets linear 
regression procedure was applied. The model with the highest R-
squared value included only three of them: incorrect answers to 
problems (EA), number of multiple choice questions answered 
correctly (EQC), and number of multiple choice questions 
answered incorrectly (EQI). The coefficients for the model are 
shown in Table 2. Normality was established for the dependent 
variable with the Shapiro-Wilk test (W = 0.9703, p<1e-5). The 
residuals of the linear model reported a value of W = 0.9838 
(p<0.005) for the same test. 

Table 2 Coefficients for the Derived Linear Model 
Variable Coeff Std error t value p 
Intercept 10.9312 0.4089 26.732 <1e-15 

EA 0.012 0.0033 3.687 <1e-3 
EQC 0.0036 0.0054 6.607 <1e-10 
EQI -0.0397 0.0074 -5.389 <1e-6 

R-squared = 0.2598, F(3, 297) = 34.76, p-value < 1e-15 
 

4.2 Actions derived from the Digital Model 
The interpretation of the linear model is that the mid-term score is 
more likely to be higher for those students with a higher 
difference between correct and incorrect answers to the sequence 
of problems required to prepare the lecture. Additionally, the 

second and third coefficients suggests that those students that 
answer the embedded questions successfully more often are more 
likely to get a higher score in the mid-term. This result points to 
the fact that both the formative and summative assessment 
included in the course to prepare the lecture are having the 
expected effect in students. The robustness of the model is 
validated by the high value of R-squared. More than 25% of the 
variation of the score in the mid-term is explained by the linear 
combination of the three factors. After reviewing the model, the 
following actions were considered: 

1. Use the linear equation to estimate the midterm score and 
suggest changes in course engagement. 

2. Increase the number of questions in the course notes. 
3. Improve the problems by providing feedback when an 

incorrect answer is introduced. 

4.3 Student qualitative data set 
To probe the students’ experience of engaging in activities related 
to the linear model, a survey containing the following questions 
was administered to the students shortly after the midterm exam. 
140 students answered the survey, with an average age of 19.5 
years. 

1. What was the purpose of the pre-lecture course materials 
(course notes, multiple choice questions, videos, sequence 
problems and participation dashboard)? 

2. How did you approach using the videos? What did you do 
and why? 

3. How did you approach using the problem sequence? What 
did you do and why? 

4. How did you approach using the dashboard? What did you 
do and why? 

The first question investigated the students’ conceptions of the 
course materials used for the flipped learning design. Questions 
two to four investigated how the students approached the use of 
the videos, the problem sequences and the dashboard. The follow 
up questions in each of these asked them about their strategies 
(what they did) and their intention (why they did those things). 
Student responses to the questions went through an iterative 
classification process amongst the researchers following a 
phenomenographic methodology (Prosser and Trigwell, 1999). 
The process involved classifying student responses into 
qualitatively different categories that offered some explanation as 
to differences in how students reported understanding about the 
purpose of the course materials, how they approached the flipped 
design in the course.  

4.3.1 Conceptions of course materials 
Not all students held the same conceptions of the course 
materials. Some students (n=72) reported conceptions which 
revealed an awareness of how the materials were meant to prepare 
them for the lecture in ways that would improve their 
understanding. These were categorized as coherent conceptions of 
the course materials.  
“The online course materials were to help us learn material 
before the lecture, which helps us understand the lecture more. 
The videos were an interactive way of engaging with students and 
it could help students understand some of the more difficult parts. 
Participation dashboard was to help us increase the amount 
we’ve done by comparing us to the rest of the class.” 

307



In contrast, another group of students (n= 68) thought that the 
online course materials just repeated the same information in the 
lecture and were more focused on getting higher marks by using 
the materials than preparing and understanding. These were 
categorized as fragmented conceptions. For example; 
“The course materials are used to just reinforce information 
taught in videos and placed in lecture notes. Prep-questions help 
improve final semester marks and also make sure we keep up to 
date with course material.” 

4.3.2 Approaches to videos 
One group of students (n=96) reviewed the videos, taking notes 
and engaging in a studious way. These were categorized as deep 
approaches to engaging with the videos.  
“I watched the videos and took notes. I tried to understand the 
notes. If I was confused, I would refer to course material notes. 
Tested understanding with multiple choice questions. Very helpful 
because I can constantly watch it until I understand.” 
A second group of students (n= 44) did not seem to engage with 
the videos in ways intended by the instructor. They did not report 
an intention to engage or test understanding, tending to adopt a 
disengaged approach. These were labelled as surface approaches 
to engaging with the videos, such as; 
“I watched the video whilst doing other stuff. Mainly because the 
video are lengthy and my attention span is short.” 

4.3.3 Approaches to problem sequences 
The idea behind the problem sequences was to help the students 
go through a process of problem solving that would be useful in 
the context of computer engineering. One group of students 
(n=64) saw connections between the other parts of the course and 
problem sequence. Such approaches were categorized as a deep 
approach to the problem sequences.  
“I worked the problem sequences and then answered the 
questions accordingly. If I got a question wrong, I liked how I was 
able to rework it and see what I did that was wrong. Being able to 
do this helped me understand what I had misconceptions about. 
Also, the fact that you got the mark regardless of the attempts was 
helpful as I was more open to going ahead and trying the 
questions and learning as I go, rather than worrying about losing 
marks because of misunderstandings in the content I just 
learned.” 
 
Another group of students (n=76) did not display a similar 
awareness of how to engage meaningfully with the problem 
sequences. They seemed to drill the questions in the problem 
sequence with little reflection, mostly focusing on getting the 
right answer. These were categorized as surface approaches.  
“I answered each question of the problem sequences one by one, 
to get the participation mark... I repeated all the questions until I 
reached 100% correct.” 

4.3.4 Quantitative analysis of the qualitative 
categories 
A SAL approach to the student experience of learning looks at the 
relatedness amongst the reported conceptions and approaches to 
see if there are any associations amongst the categories that 
warrant further research in more empirically designed studies 
(Prosser and Trigwell, 1999). In the following analyses, cross-
tabulations were used to investigate if there were associations 

between the conceptions held, and approaches adopted. A link 
between the different aspects of the experience could be helpful in 
understanding how to adjust misconceptions or relatively poor 
approaches. Associations were identified between conceptions of 
course materials and approaches to video, approaches to problem 
sequences and approaches to video and approaches to problem 
sequences and the mid-term mark. 
Table 3 Associations between conceptions of course materials 

and approaches to videos 
Conception of 

course materials
Approach to videos Total Deep surface 

Coherent 56 16 72 
Fragmented 40 28 68 

Total 96 44 140 
?? =5.83, phi = .20, p<0.05 
Table 3 shows a statistically significant association between 
conceptions of course materials and approaches to videos, 
suggesting that a coherent conception of the course materials is 
related to a deep approach to videos and a fragmented conception 
of course materials is related to surface approaches to videos 
(phi=.20, p<.05). 
Table 4 shows a statistically significant association between 
approaches to problem sequences and approaches to videos, 
suggesting that a deep approach to problem sequences is related 
to a deep approach to videos and a surface approach to problem 
sequences is related to surface approaches to videos (phi=.20, 
p<.05) 

Table 4 Associations between approaches to problem 
sequences and approaches to videos 

Approach to video
Approach to problem 

sequence Total 
Deep Surface 

Deep 51 45 96 
Surface 14 30 44 
Total 65 75 140 

?? =5.50, phi = .20, p<0.05 
Table 5 shows a statistically significant association between 
approaches to problem sequences and the mid-term mark, 
indicating that deep approach to the problem sequences is related 
to a relatively higher mark in the mid-term than surface 
approaches to the problem sequences and a fragmented 
conception of course materials is related to surface approaches to 
videos (t=4.1, p<.001, es = 0.72) 

Table 5 Associations between approaches to problem 
sequences and mid-term mark 

Approaches to 
problem 

sequences 

Final Mark 

Mean Standard Deviation Effect size 

Deep 15.98 2.6 
0.72 

Surface 13.88 3.2 

T test: T= 4.1** 
N=140, **p<.001, #mark out of 20 
 

308



4.4 Actions derived from the combined data 
The authors recognized that the design changes derived from the 
digital footprint did not reveal how we should advise students to 
engage more with the material, nor did it suggest changes to helps 
students adopt more effective approaches to learning. The 
changes would have been based on the assumption that by 
increasing the factors in the model, students improve their 
learning experience. 

The SAL analysis adds another dimension to the investigation of 
the student experience which can improve the interpretation of the 
digital footprint. Firstly, just engaging more with any of the 
events does not seem to be a guarantee that a student will improve 
their experience. In each of the activities, there appeared to be a 
qualitative difference in how the students approached their 
learning. For some students, answering the multiple choice 
questions in the videos was part of a deeper engagement which 
also involved taking notes, going over the videos to improve 
understanding and referring to course notes as a type of cross-
reference. For other students, viewing the videos was a passive 
experience, not really paying attention and often doing other 
things. 

When advising students about their levels of participation, not 
only can the instructor suggest ideas about increasing the quantity 
of their participation, the advice can also model and provide 
suggestions on how to improve the quality of their engagement 
based on the experience of past successful students. The advice 
can take the form of explicit strategies for more meaningfully 
engaging with the videos, problem sequences and other events 
designed to link with these activities.  

For the course materials, the instructor can use the students’ 
description of their experience to inform their redesign. The 
instructions surrounding the videos can reinforce the types of 
ideas sent to students not sufficiently engaging with them. 
Rubrics and other learning materials can scaffold the type of note-
taking and reflective strategies that the more successful students 
reported adopting. Similarly, the problem sequences can be 
redesigned to emphasize that repeating the sequences is useful, 
but only if the students are doing in in ways to rectify 
misconceptions or confirm accurate ones. 

5. CONCLUSIONS 
The paper has explored the benefits of designing activities for a 
learning environment by considering the digital footprint of 
students and contextualizing interpretations from that data with 
Student Approaches to Learning framework. Using only data 
from their digital footprint, the information for the types of 
interventions focused on are limited and may misdirect the efforts 
of teachers. Integrating an understanding of the students’ 
approaches using the SAL framework exposes a more nuanced 
reality of why students interact with the activities and the type of 
strategies that they use. We argue that the combination of these 
two paradigms provide a richer evidence-base for instructors 
seeking to design interventions which address the needs of the 
students. The question as to whether an elaborated intervention 
based on both data sets is more effective for the quality of the 
student experience is beyond the scope of this study and provides 
motivation for further studies. Here we offer the arguments and 
evidence as a basis for a richer evidence-base from which to 
design interventions which are likely to help. 

 

6. REFERENCES 
[1] Anderson, C. 2008. The End of Theory: The Data Deluge 

Makes the Scientific Method Obsolete. Wired Magazine. 
[2] Antunes, C. 2010. Anticipating student’s failure as soon as 

possible. Handbook of Educational Data Mining. C. Romero, 
S. Ventura, M. Pechenizkiy, and R.S.J. d. Baker, eds. CRC 
Press. 353. 

[3] Arnold, K.E., Hall, Y., Street, S.G., Lafayette, W. and Pistilli, 
M.D. 2012. Course Signals at Purdue?: Using Learning 
Analytics to Increase Student Success. International 
Conference on Learning Analytics and Knowledge (2012), 
267–270. 

[4] Baker, R. and Siemens, G. 2014. Educational data mining and 
learning analytics. The Cambridge Handbook of the Learning 
Sciences. R.K. Sawyer, ed. Cambridge University Press. 

[5] Bramucci, R. and Gaston, J. 2012. Sherpa: increasing student 
success with a recommendation engine. International 
Conference on Learning Analytics and Knowledge (2012), 
82–83. 

[6] Essa, A. and Ayad, H. 2012. Improving student success using 
predictive models and data visualisations. Research in 
Learning Technology. 5, (2012), 58–70. 

[7] Essa, A. and Ayad, H. 2012. Student success system: Risk 
Analytics and Data Visualization using Ensembles of 
Predictive Models. Proceedings of the 2nd International 
Conference on Learning Analytics and Knowledge - LAK ’12 
(New York, New York, USA, Apr. 2012), 158–161. 

[8] Knight, S., Shum, S.B. and Littleton, K. 2014. Epistemology , 
assessment , pedagogy?: where learning meets analytics in the 
middle space. Journal of Learning Analytics. 1, 1 (2014), 23–
47. 

[9] Lockyer, L., Heathcote, E. and Dawson, S. 2013. Informing 
Pedagogical Action: Aligning Learning Analytics With 
Learning Design. American Behavioral Scientist. 57, 10 (Mar. 
2013), 1439–1459. 

[10] McAfee, A. and Brynjolfsson, E. 2012. Big Data. Harvard 
Business Review. October (2012), 60–69. 

[11] Pintrich, P.R. 2004. A Conceptual Framework for Assessing 
Motivation and Self-Regulated Learning in College Students. 
Educational Psychology Review. 16, 4 (Dec. 2004), 385–407. 

[12] Prosser, M. and Trigwell, K. 1999. Understanding Learning 
and Teaching. The Experience in Higher Education. Society 
for Research in Higher Education & Open University Press. 

[13] Ramsden, P. 2003. Learning to Teach in Higher Education. 
Routledge. 

[14] Romero, C., López, M.-I., Luna, J.-M. and Ventura, S. 2013. 
Predicting students’ final performance from participation in 
on-line discussion forums. Computers & Education. 68, (Oct. 
2013), 458–472. 

[15] Romero, C., Ventura, S. and Garcia, E. 2008. Data mining in 
course management systems: Moodle case study and tutorial. 
Computers & Education. 51, 1 (Aug. 2008), 368–384. 

[16] Suthers, D. and Road, E.W. 2013. Learning Analytics as a 
“Middle Space.” Proceedings of the International Conference 
on Learning Analytics and Knowledge (2013), 2–5. 

[17] Verbert, K. and Duval, E. 2012. Learning Analytics. 
Elearning and Education. 8 (2012). 

[18] Wise, A.F. 2014. Designing Pedagogical Interventions to 
Support Student Use of Learning Analytics. Proceedings of 
the International Conference on Learning Analytics and 
Knowledge (2014).  

309





