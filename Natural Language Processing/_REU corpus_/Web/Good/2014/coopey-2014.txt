
Collaborative Spatial Classification

Eric Coopey
Tufts University

eric.coopey@tufts.edu

R. Benjamin Shapiro
Tufts University

ben@cs.tufts.edu

Ethan Danahy
Tufts University

ethan.danahy@tufts.edu

ABSTRACT
Interactive technologies have become an important part of
teaching and learning. However, the data that these sys-
tems generate is increasingly unstructured, complex, and
therefore difficult of which to make sense of. Current com-
putationally driven methods (e.g., latent semantic analysis
or learning based image classifiers) for classifying student
contributions don’t include the ability to function on multi-
modal artifacts (e.g., sketches, videos, or annotated images)
that new technologies enable. We have developed and im-
plemented a classifcation algorithm based on learners’ in-
teractions with the artifacts they create. This new form of
semi-automated concept classification, coined Collaborative
Spatial Classification, leverages the spatial arrangement of
artifacts to provide a visualization that generates summary
level data about about idea distribution. This approach has
two benefits. First, students learn to identify and articulate
patterns and connections among classmates ideas. Second,
the teacher receives a high-level view of the distribution of
ideas, enabling them to decide how to shift their instruc-
tional practices in real-time.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous

General Terms
Collaboration; Classification; Spatial Arrangement

Keywords
Audience Response System, Minimum Spanning Tree, In-
teraction Techniques, Clustering

1. INTRODUCTION
The modern classroom is an increasingly data rich environ-
ment. This information stems from two main sources, which
are the creation of multimodal learning technologies and the
development of student thinking centered pedagogies. Such

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
LAK’14 , March 24 - 28 2014, Indianapolis, IN, USA.
ACM 978-1-4503-2664-3/14/03 $15.00.
http://dx.doi.org/10.1145/2567574.2567611

pedagogies are explicitly intended to evoke verbal or mate-
rial creation of evidence concerning how students conceptu-
alize and operationalize course content [8] [12]. This infor-
mation is meant to enable teachers, students, and students’
peers to asses, reflect on, and discuss student thinking. Stu-
dent centered pedagogies focus on creating representations
and learner interactions that extend well beyond simple mul-
tiple choice questions or fill in the blank answers.

Making sense of this diverse set of multimodal input is one of
the biggest challenges to successful implementation of tech-
nology in the learning process. The volume of data gen-
erated goes beyond what a single teacher can be expected
to consume, let alone make actionable, real-time decisions
concerning the direction of their teaching [6]. Teachers need
tools which assist in summarizing and categorizing the ideas
generated by their students.

Latent Semantic Analysis (LSA) is popular for classifying
student responses according to the words they contain [9].
LSA can also be effective in learning analytics, support-
ing formative assessment by giving teachers tools to visu-
ally identify and understand participation and discourse pro-
cesses and information flows [13].

Computational image classifiers have the ability to classify
an image according to similarity of the objects they contatin
[3]. Learner based methods require sufficient training data to
generate a classifier, and the parameters of the classification
algorithm must be fine-tuned [2].

Both LSA and image classification work on a narrow scope
of student work, and in many implementations require a sig-
nificant amount of labeled training data. Training classifiers
and pre-labeling response types is infeasible in the “bloom-
ing, buzzing” classroom [14]. With the limitations of these
purely content driven classifiers in mind, we propose a new
classification algorithm driven by user interactions, which is
flexible enough to operate on any form of input. Involving
learners in the process requires students to engage with their
peers’ ideas and identify the patterns and relationships con-
tained within them, an engineering practice encouraged by
the Next Generation Science Standards [10].

To this end, we present a new form of semi-automated clas-
sification coined Collaborative Spatial Classification, which
highlights how new forms of user interaction information
can be used to provide insight into group-wide understand-

138



Figure 1: Response System Framework

ing. Utilizing the spatial arrangement of artifacts across all
participants, we classify artifacts into groups and produce
several high level visualizations of the underlying data.

2. PROBLEM OF PRACTICE
Audience Response Systems are popular for their ability to
quickly give the teacher a high level overview of student un-
derstanding [7]. Colloquially referred to as “clickers”, they
excel at identifying the distribution of responses. The rep-
resentation they produce, the bar chart, is easy to interpret.
Many practitioners find the feedback from these systems use-
ful in their teaching, especially when utilizing peer instruc-
tion [5].

The main drawback is that audience response systems only
work with multiple choice questions (MCQ). Many have
noted the limitations of multiple choice questions in assess-
ment, including the influence of guessing [1] and the need
for a large and well-tested question bank [15].

After analyzing the current state of audience response sys-
tems, we identified two main concepts that were crucial to
their success which we intent to leverage in our work:

1. Mechanical Evaluation - Individual responses can be
evaluated without a judgment intervention by a human

2. Ease of Visualization - The standard bar chart is sim-
ple for a wide range of users to understand

We propose the Response System Framework pipeline (see
Figure 1) for capturing and processing participant input.
This framework is flexible enough to incorporate current
MCQ systems as well as new forms of input that generate
increasingly complex forms of data.

First, determine the type of question a teacher wants to ask
and the type of data that it will produce. The system must
transform each response into a mathematical representation
and store the results of all participants. Using the informa-
tion from the entire group, analyze the data and produce
an abstracted representation which captures elements of all
participants. Finally, create an abstracted visualization of
the information in a meaningful way that allows teachers
(and students) to easily interpret the results and act upon
them.

2.1 Metacognitive Reflection
The Next Generation Science Standards [10] encourage the
instruction of authentic scientific practices, with a focus on
activities that promote skills related to analyzing and inter-
preting data from multiple sources and identifying patterns
between them. We propose a pedagogical change, facilitated

by technology, that engages students in metacognitive reflec-
tion and encourages them to interact with their peers’ ideas
by grouping similar artifacts together.

Our classification algorithm (detailed in Section 4) leverages
class-wide spatial arrangement meta-data, with a high level
flow diagram depicted in Figure 2.

1. Students submit an artifact in response to a prompt,
ideally an open ended conceptual question where there
isn’t just a single correct answer

2. Students analyze the artifacts of their peers’ and ar-
range them spatially, grouping artifacts they deem sim-
ilar closer together

3. The algorithm takes each spatial arrangement gener-
ated by the class and determines the number of groups
students have identified. The number of clusters can
also be manually specified.

4. Groupings are visualized, either as a list or graph

5. The teacher uses the groupings to drive a classroom
discussion by pointing out differences or similarities
between the different clusters

Figure 2: High Level Flow of Algorithm

This process is also captured by our Response System Frame-
work, with the steps shown in Figure 3.

Figure 3: Spatial Classification Framework

3. EXPERIMENT
3.1 Context

139



InterLACE (https://int.erlace.com) is a newly developed com-
puter supported collaborative learning system designed for
use in the modern classroom [4]. A full description is outside
the scope of this paper, but one feature that InterLACE af-
fords is a whiteboard like collaboration space, which allows
users to physically arrange individual ideas on the screen in
two dimensions. When student save their arrangement, they
are required to reflect on how they arranged the artifacts.
Other participants can see the annotation, and load their
peer’s arrangement for inspection. Users noted that this af-
fordance helped them “organize their thoughts” and “group
ideas together.” They were applying the Gestalt theory of
proximity to objects they deemed similar, and moved them
closer together on the screen to express this equality.

This observation sparked the idea that it might be possi-
ble to capture the semantic intent of multiple users spa-
tial arrangement, creating a group-wide distance function
between each pair of ideas. Would it be possible to use
multiple users spatial arrangement metadata to generate an
abstracted view of the original data?

3.2 Methods
Data was collected in a high school physics classroom that
was beginning a unit on analyzing graphs. The teacher gave
an open ended task of creating a graph that a real function
might make. This activity was performed in 3 sections with
a total of 72 students, producing 80 different graphs and 70
save arrangement events. InterLACE is designed to allow
the same activity to be run across multiple sections, however
students only see posts for their section. We will examine
the data of one section in detail, which contained 24 students
working in pairs and 23 save arrangement events.

Figure 4 shows a saved arrangement for the creation of posi-
tion verses time graphs, with the annotation “the first group
went away from the origin and then came back. The second
group all had times where the object was at rest and the
last graph is not a function.”

3.3 Algorithm
Collaborative Spatial Classification takes as input the set
of saved arrangements for an entire class, which for each
student contains the (x,y) coordinates of each artifact on
the screen.

The first step is to calculate average euclidean distance be-
tween every pair of artifacts, across all participants, shown in
Algorithm 1, lines 3-9. This creates a cumulative distance
table measured in pixels, which represents an undirected,
fully connected graph between all artifacts. Kruskal’s algo-
rithm [16] creates a minimum spanning tree, line 13, joining
all artifacts with the least cost, where cost is measured in
distance.

If a user has specified the number of clusters they want
(num), the algorithm simply removes the num-1 longest
edges in the minimum spanning tree. If no number was
specified, it attempts to determine the number of clusters
by removing any edge with a distance longer than two times
the standard deviation (lines 16-25).

The end results is a sparse graph, with num groups and

Figure 4: Student Arrangement

num-1 edges. Nodes that exist in the same tree are consid-
ered to be part of the same group.

4. RESULTS
4.1 Visualizations
The simplicity of data visualization in traditional clicker sys-
tems was identified as an important design principle. There-
fore one representation should be as simple as possible, with
the goal of accessibility trumping other attributes.

The List View can be seen in Figure 5, which shows the
calculated groupings from our classification algorithm run
on the classroom data. The percentage in the header of
each grouping is the number of artifacts that fall into that
group.

In this instance, the algorithm automatically determined
that there were three separate groups. The first set ap-
pears quasi-linear, with instances where velocity is zero (the
position line is flat). The largest set at 42% of responses,
are graphs that are parabolic in nature. The final grouping
are wave function graphs. The list view provides both struc-
ture and summary level information about the distribution
of ideas in the classroom.

One drawback of a list view is that it fails to convey any
sense of relative distance. For example, even though two ar-

140



Data: X = Collection of size n participants of object A =
(x,y) screen coordinates for each post

Result: Undirected Sparse Graph G

initialization1
Let Y [][] = Table holding cumulative pairwise distance33
for x ? X do4

for i = 1 to n do5
for j = i+1 to n do6

Y [i][j] += EuclideanDistance(x[i], x[j]);7
end8

end9

end10
G = Kruskal(Y )1212

? = Calculate standard deviation of edge length in G1414

if num is set then15
/* Create num clusters */
Pop the longest num-1 edges from G1717

else18
/* Calculate how many clusters to create */

for e ? G[E] do19
if e > 2? then20

Remove e from G21
end22

end23

end24
Algorithm 1: Collaborative Spatial Classification(X,
num)

tifacts are determined to be in the same group, their relative
distance may still be quite far. This could be the result of
limitations to the clustering algorighm, or a user manually
specifying fewer groups than exist.

The Graph View encodes the relative distance between each
artifact as the length of the edge between them (Figure 6).
Tightly clustered items are more similar than items that
have long edges between them. For illustrative purposes a
separate dataset that is purely text is used in Figure 6, as
the graph images had a tendancy to obscure some of the
graph edges.

4.2 Teacher Feedback
The most challenging aspect to getting feedback on the vi-
sualizations was explaining to teachers what the data rep-
resented, which may imply that the visualization was too
complex. We feel this is a major barrier to increased use
and acceptance of advanced analytics in the classroom, and
certainly not unique to our implementation.

The most effective method was a short description supported
by the high level overview (Figure 2), where the students
arrangements went into a “black box” and out came a list.
They seemed to grasp for concepts related to the “average”
arrangement, which is conceptually close enough for our pur-
poses.

This challenge has opened our eyes to the fact that re-
searchers need to be acutely aware of the best ways to con-
vey their results to practitioners. The success of your work
is potentially less dependent on the complexity of your al-

Figure 5: List View

Figure 6: Graph Cluster View - Text

gorithm, and more dependent on how you frame the output
and design interventions that best take advantage of the af-
fordance.

Three teachers used grouping of artifacts as part of an ac-
tivity. The visualizations were created after the fact and
discussed separately with the teachers. In general, they re-
sponded positively to the automated grouping and summa-
tive information. They all agreed that the process of having
students engage with each others ideas and find patterns in
the data was beneficial.

Their interpretations of the visualization varied. The list
view was identified as easy to read and providing useful in-
formation about idea distribution. The graph view was a
point of confusion, as the groups were difficult to identify,
and the goal of encoding relative distance within the visual-
ization had little added benefit to the teachers.

141



4.3 Future Work
A more strictly scaffolded activity could potentially provide
better results for automated grouping. For instance, provide
in the prompt “Group the posts according to slope (positive,
negative, zero).”

Cluster evaluation techniques such as the Rand Index [11]
could be applied between students to identify which artifacts
are most commonly present in different groupings. Addi-
tional metadata is captured during the process of sorting,
and leveraging information such as how many times each
artifact was moved could provide deeper insight into the
process of arrangement. Both could give the teacher addi-
tional information in real time of which ideas the students
were unsure.

Assessment support based on spatial grouping is also of in-
terest. If the teacher provdies a ground truth arrangement,
it becomes possible to train a classifier on that input by
copying and slightly manipulating this representation. Us-
ing this trained classifer on student responses could provide
a quick way of identifying student input that is likely correct.

5. CONCLUSION
As the capabilities of the technology inside the classroom im-
prove, the complexity of the data students generate dramat-
ically increases. Purely computational techniques to make
sense of the dynamic creation of artifacts and interactions
lack the flexibility to work on multi-modal representations.
By leveraging a user-interaction driven method of concept
classification, we are able to generate summary level infor-
mation about idea distribution, allowing teachers to adjust
their teaching in real time. Including students as part of the
sensemaking process has a two-fold benefit. First, students
engage with their peers’ ideas and work to identify patterns
in the data, as prescribed by the Next Generation Science
Standards. Second, the meta-data they produce as they ar-
ticulate connections between ideas can be leveraged to create
high level abstractions of the data. The techniques intro-
duced could be extended to other complex forms of inter-
action data generated in the classroom. Finally, researchers
must strive to make the results of our work more accessible
to practitioners by ensuring that our methods are clearly
communicated using easily understood constructs.

6. ACKNOWLEDGMENTS
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 1119321. Any
opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the views of the National Science Foun-
dation.

7. REFERENCES
[1] Y. BerebyMeyer, J. Meyer, and O. M. Flascher.

Prospect theory analysis of guessing in multiple choice
tests. Journal of Behavioral Decision Making,
15(4):313–327, 2002.

[2] O. Boiman, E. Shechtman, and M. Irani. In defense of
nearest-neighbor based image classification. In
Computer Vision and Pattern Recognition, 2008.
CVPR 2008. IEEE Conference on, pages 1–18, 2008.

[3] A. Bosch, A. Zisserman, and X. Munoz. Representing
shape with a spatial pyramid kernel. In Proceedings of
the 6th ACM international conference on Image and
video retrieval, pages 401–408, 2007.

[4] E. Coopey, E. Danahy, and L. Schneider. InterLACE:
interactive learning and collaboration environment. In
Proceedings of the 2013 conference on Computer
supported cooperative work companion, CSCW ’13,
pages 11–14, New York, NY, USA, 2013. ACM.

[5] C. H. Crouch and E. Mazur. Peer instruction: Ten
years of experience and results. American Journal of
Physics, 69:970–977, 2001.

[6] E. Duval. Attention please!: learning analytics for
visualization and recommendation. In Proceedings of
the 1st International Conference on Learning
Analytics and Knowledge, LAK ’11, pages 9–17, New
York, NY, USA, 2011. ACM.

[7] C. Keller, N. Finkelstein, K. Perkins, S. Pollock,
C. Turpen, and M. Dubson. AIP Conference
Proceedings, 951(1):128–131, Nov. 2007.

[8] D. S. Knowlton. A theoretical framework for the
online classroom: A defense and delineation of a
student-centered pedagogy. New Directions for
Teaching and Learning, 2000(84):5–14, 2000.

[9] T. K. Landauer, P. W. Foltz, and D. Laham. An
introduction to latent semantic analysis. Discourse
processes, 25(2-3):259–284, 1998.

[10] NGSS. Next generation science standards: For states,
by states. The National Academies Press., 2013.

[11] W. M. Rand. Objective criteria for the evaluation of
clustering methods. Journal of the American
Statistical Association, 66(336):846–850, Dec. 1971.

[12] M. Scardamalia and C. Bereiter. Knowledge building:
Theory, pedagogy, and technology. The Cambridge
handbook of the learning sciences, pages 97–115, 2006.

[13] L. Sha, C. Teplovs, and J. van Aalst. A visualization
of group cognition: semantic network analysis of a
CSCL community. In Proceedings of the 9th
International Conference of the Learning Sciences -
Volume 1, ICLS ’10, pages 929–936. International
Society of the Learning Sciences, 2010.

[14] M. Sherin, V. Jacobs, and R. Philipp. Mathematics
teacher noticing: Seeing through teachers’ eyes. Taylor
& Francis US, 2010.

[15] E. Ventouras, D. Triantis, P. Tsiakas, and
C. Stergiopoulos. Comparison of oral examination and
electronic examination using paired multiple-choice
questions. Computers & Education, 56(3):616–624,
Apr. 2011.

[16] C. Zahn. Graph-theoretical methods for detecting and
describing gestalt clusters. IEEE Transactions on
Computers, C-20(1):68–86, 1971.

142





