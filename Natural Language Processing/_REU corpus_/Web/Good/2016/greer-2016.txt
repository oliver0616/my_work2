
Learning Analytics for Curriculum and Program Quality

Improvement (PCLA 2016)

Jim Greer
University of Saskatchewan

E232 Administration
University of Saskatchewan

Saskatoon, Canada
jim.greer@usask.ca

Marco Molinaro
University of California, Davis

3470 Trousdale Parkway,
WPH-601A

Davis, CA, USA
mmolinaro@ucdavis.edu

Xavier Ochoa
Escuela Superior Politécnica

del Litoral
Vía Perimetral Km. 30.5

Guayaquil, Ecuador
xavier@cti.espol.edu.ec

Timothy McKay
University of Michigan

450 Church Street
Ann Harbor, MI, USA

tamckay@umich.edu

ABSTRACT
This workshop on Learning Analytics for Curriculum and
Program Quality Improvement investigates how LAK can
drive improvements in teaching practices, instructional and
curricular design, and academic program delivery. This work-
shop brings forward research and examples of how LAK can
help build the case for instructional, curricular, or program-
matic change and further how LAK can be used to foster
acceptance of change processes by teachers, administrators,
and other stakeholders in the educational enterprise.

CCS Concepts
•Applied computing ! Education;

Keywords
ACM proceedings; LATEX; text tagging

1. MOTIVATION
Much of the research in LAK to date has been a?A?IJs-

tudent facinga?A?I?, that is, using data to better understand
learners and their need or to create interventions that di-
rectly support or influence learners. This workshop takes the
perspective on how LAK can drive improvements in teach-
ing practices, instructional and curricular design, and aca-
demic program delivery. While this does influence student
outcomes in the long term, the data gathered and evidence
generated is more instructor and administrator facing. We
have seen examples of how LAK can help build the case for
instructional, curricular, or programmatic change and fur-

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
LAK ’16 April 25-29, 2016, Edinburgh, United Kingdom

c? 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4190-5/16/04. . . $15.00
DOI: http://dx.doi.org/10.1145/2883851.2883899

ther how LAK can be used to foster acceptance of change
processes by teachers, administrators, and other stakehold-
ers in the educational enterprise. When successful, these
kinds of changes are often associated with educational re-
form or culture shifts in educational practice.

2. OBJECTIVES
This workshop o?ers those in the LAK community an op-

portunity to share and explore how educational data, its
analysis and visualization, and the evidence derived can
change/improve the context of learning. The main research
and practice questions to be addressed in this workshop are:

1. how to provide relevant and actionable information
to faculty, teaching assistants, departmental and col-
lege administrators to encourage a greater emphasis on
student learning and use of evidence-based practices,
thus encouraging a continuous improvement approach
to teaching and learning.

2. how to create visualization and data collection tools
and approaches that encourage a community of in-
structors and administrators to engage in making deci-
sions based on data to improve student learning whether
at the activity, lesson, course, series, department, col-
lege or university-wide levels.

3. how to extract information from the multiple modali-
ties used in instructional environments and help cap-
ture, represent and evaluate faculty instructional ap-
proaches, student-faculty engagement, student-student
interactions and student-technology interactions.

4. how to represent, summarize and mobilize data from
human interactions and student-technology interactions
to motivate change and quality improvement. Is there
a way to make the data and representations more use-
ful for promoting sustainable change?

5. how to change the evaluation of instructional activities
to be more formative, actionable and multi-dimensional



in nature, emphasizing individual and group improve-
ment rather than a one-size fits all student survey by
which instructors or courses are judged and compared.
Ideally such evaluation systems would go well beyond
student satisfaction as the sole measure of good teach-
ing and should include learning outcomes, utility of
outcomes, applicability to future learning, match or fit
between learner and instructor, and more.

This is the first time that a workshop focused on Learn-
ing Analytics for Curriculum and Program Improvement has
been held.

3. DISSEMINATION STRATEGY
Whether in face-to-face, blended, or fully online instruc-

tional situations, learning analytics for instructional and
program improvement makes sense. Researchers, practition-
ers, faculty and administrators interested in improvement in
learning environments would be interested in this workshop.
This workshop invites members of the LAK community in-
terested in this topic and as well as those interested in data-
informed approaches to institutional change in teaching and
learning. The proceedings from this workshop, a collection
of peer-reviewed papers, will appear in the CEUR Workshop
Proceedings.

4. WORKSHOP FORMAT
After peer review, approximately half a dozen papers were

accepted for presentation at this half-day workshop. The
format follows a model of short presentations by authors of
the papers followed by round-table smaller-group discussions
of issues arising from these short presentations. A ”World-
cafe” of discussion on emerging themes will close the work-
shop. We expect that an introduction to the proceedings
will be written by the workshop organizers as a preamble to
the published prodceedings.

5. PROGRAM COMMITTEE AND PARTIC-
IPANTS

In addition to the four organizers, a program committee
was established to provide peer reviews of the various pa-
pers. Three reviews were invited for each submission.

• Christopher Brooks, University of Michigan

• Katherine Chiluiza, ESPOL, Ecuador

• Cristian Cechinel, UFPel, Brazil

• Stephanie Frost, University of Saskatchewan

• Vive Kumar, Athabasca University

• Phil Long, University of Texas at Austin

• Leah McFayden, Unversity of British Columbia

• Emily Miller, Association of American Universities

• Christopher Pagliarulo, University of California, Davis

• Craig Thompson, University of Saskatchewan

• Catherine Uvarov, University of California, Davis

• Katrien Verbert, KULeuven, Belgium

6. SYNOPSIS OF PAPERS PRESENTED
The accepted papers all aim at improving decisions about

learning and learning environments based on data-driven
methods. Some aim at providing enhanced information to
instructors to better understand their students and how
their teaching is impacting student learning [4] [5] [1]. Oth-
ers are more directly aimed at instructional designers, to
learn from successful design strategies in certain courses and
to apply that knowledge to other course-design scenarios [2]
[7]. The third type of paper is aimed at providing actionable
information to administrators for better managing curricu-
lum and program change [3] [6].

Issues include instructional designs that lead to optimal
use of learning management systems, developing metrics for
better analysing curricula, risk mitigation approaches to cur-
ricular change, and visualization methods for improving de-
cision making about academic programs. In order to ad-
dress these types of issues, the ”data literacy” of instructors,
instructional designers, and administrators is a point of con-
cern. It is important that users of program and curriculum
learning analytics (PCLA) tools understand the capabili-
ties and limitations of these tools and the outputs they pro-
vide. Appropriate interpretations of statistical models or
predictions are an important prerequisite to e?ective use of
data-driven approaches. Poor statistical literacy and mis-
understanding of probabilities can lead to bad decisions,
systematic discrimination, and prejudicial abuse of learners.
Along with powerful analytics tools, there is a responsibility
to ensure that those using and interpreting the results from
such tools are suitably trained to use them well.

The reference list below contains the the accepted work-
shop papers, which will appear in the CEUR Proceedings.

7. REFERENCES
[1] D. G. Blazenka Divjak and M. Maretic. Assessment

analytics for peer-assessment: A model and
implementation. In Program and Curricular Learning
Analytics Workshop, 2016.

[2] J. Fritz. Lms course design as learning analytics
variable. In Program and Curricular Learning Analytics
Workshop, 2016.

[3] J. Greer, C. Thompson, R. Banow, and S. Frost.
Data-driven programmatic change at universities:
What works and how. In Program and Curricular
Learning Analytics Workshop, 2016.

[4] D. Y. Liu, C. E. T. a ndAdam J. Bridgeman, and
K. Bartimote-Au?ick. Empowering instructors through
customizable collection and analyses of actionable
information. In Program and Curricular Learning
Analytics Workshop, 2016.

[5] M. Molinaro, M. Steinwachs, Q. Li, and
A. Guzman-Alvarez. Promoting action from instructors
to departments via simple, actionable tools and
analyses. In Program and Curricular Learning
Analytics Workshop, 2016.

[6] X. Ochoa. Simple metrics for curriculum analytics. In
Program and Curricular Learning Analytics Workshop,
2016.

[7] W. Y. Wong and M. Lavrencic. Using a risk
management approach in analytics for curriculum and
program quality improvement. In Program and
Curricular Learning Analytics Workshop, 2016.



