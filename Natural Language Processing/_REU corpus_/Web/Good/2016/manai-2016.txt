
Real-time indicators and targeted supports: Using online 
platform data to accelerate student learning 

Ouajdi Manai 
The Carnegie Foundation for the 

Advancement of Teaching 
51 Vista Ln, Stanford,  

CA 94305 USA 
manai@carnegiefoundation.org 

Hiroyuki Yamada 
The Carnegie Foundation for the 

Advancement of Teaching 
51 Vista Ln, Stanford,  

CA 94305 USA 
yamada@carnegiefoundation.org 

Christopher Thorn 
The Carnegie Foundation for the 

Advancement of Teaching 
51 Vista Ln, Stanford,  

CA 94305 USA 
thorn@carnegiefoundation.org 

 
 

ABSTRACT 
Statway® is one of the Community College Pathways initiatives 
designed to promote students’ success in their developmental 
math sequence and reduce the time required to earn college credit. 
A recent causal analysis confirmed that Statway dramatically 
increased students’ success rates in half the time across two 
different cohorts. These impressive results were also 
obtained   across  gender and race/ethnicity groups. However, 
there is still room for improvement. Students who did not succeed 
in Statway often did not complete the first of the two-course 
sequence. Therefore, the objective of this study is to formulate a 
series of indicators from self-report and online learning system 
data, alerting instructors to students’ progress during the first 
weeks of the first course in the Statway sequence.     

CCS Concepts 
Applied computing ? Education ? Learning management 
systems   

Applied computing ? Education ? Computer-assisted 
instructions 

Keywords 
Learning analytics; hierarchical linear modeling; cognitive and 
non-cognitive factors; online engagement; community college 
developmental mathematics; networked improvement community 

1. OBJECTIVE 
The objective of this study is to formulate a series of indicators, 
available during the first weeks of the course, to alert instructors 
to their students’ engagement and progress in a timely manner for 
possible targeted interventions. These indicators are distinct from 
other early warning systems in that they integrate conceptual 
knowledge, productive dispositions, and academic behaviors by 
leveraging self-report and online learning platform data to monitor 
students’ engagement and progress. In this paper, we report 
preliminary results obtained from Fall 2013 Statway courses and 
describe our plan of analysis. 

 

2. THORETICAL FRAMEWORK 
Approximately 60 percent of the nation’s incoming community 
college students are referred to at least one developmental math 
course, of which 80 percent will not earn college-level math credit 
within three years [1]. Without achieving college math credit, they 
cannot transfer into four-year degree programs or qualify for entry 
into preparation programs in a wide range of occupational-
technical specialties. As a result, millions of students each year 
fail to acquire essential mathematics skills and are unable to 
progress toward their career and life goals.  

To address this national problem, the Community College 
Pathways (CCP) program was developed and implemented 
through Networked Improvement Communities (NICs) involving 
college faculty, administrators, researchers, designers, and content 
experts [5, 6, 9]. Statway is one of the CCP initiatives designed as 
a year-long course that allows students to simultaneously 
complete their developmental mathematics and college-level 
statistics requirements to receive college credit. A causal analytic 
study, using a propensity score matching technique [16] with a 
hierarchical linear modeling (HLM) approach [12, 15], confirmed 
that Statway tripled student success in half the time across two 
different cohorts, and that this effect held across gender and 
race/ethnicity groups [26].  

However, there is still room to improve students’ success in 
Statway. A recent study suggested that over  half of non-success 
cases across the two-course sequence stemmed from students 
failing the first course and not proceeding into the second course 
[18]. Our approach for addressing this issue is to leverage existing 
data, formulate indicators that identify students in need of support, 
and alert instructors to their students’ engagement and 
understanding in a timely manner for possible targeted 
interventions [27]. In particular, we focus on the first several 
weeks of the course, during which the majority of students who 
enroll in developmental math courses fail to attend class [1]. 

To formulate a series of indicators that track students’ progress 
over an academic term as early as possible, we leverage three sets 
of data. One set of self-report data is used to measure what we call 
productive persistence, a set of non-cognitive factors thought to 
affect community college developmental math student success 
[17, 27]. For instance, many students have had negative prior 
math experiences, which lead them to develop the belief that “I 
am not a math person” [10]. These beliefs can trigger anxiety and 
poor learning strategies when students are faced with difficult or 
confusing math problems [2, 3, 11]. This phenomenon is 
compounded for some students (e.g., women, African Americans) 
who are members of groups that have been stereotyped as “not 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM mustbe 
honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. Request permissions from 
Permissions@acm.org. 
LAK '16, April 25-29, 2016, Edinburgh, United Kingdom. 
© 2016 ACM. ISBN 978-1-4503-4190-5/16/04…$15.00 
DOI: http://dx.doi.org/10.1145/2883851.2883942 
 



good at math” [7, 19, 24]. Also, for students who question 
whether they belong in a class or  college, it can be difficult to 
fully commit to the behaviors that may be necessary for success, 
such as joining study groups or asking professors for help [22, 
23].  

A second set of data assesses cognitive factors in the form of 
students’ mathematical conceptual knowledge (MCK). Research 
shows that when students learn math concepts, achievement gains 
are visible not only in their conceptual understanding but also in 
their procedural skill; however, gains in procedural skill do not 
necessarily translate into conceptual understanding [4]. In fact, it 
is possible that students are able to execute procedures adeptly 
without having sound knowledge of the concepts that underlie 
them. For this reason, and since existing college math placement 
tests students take before entering community college (i.e., 
Accuplacer, Compass, MDTP) almost exclusively assess 
procedural skill, it is important to seek a better understanding of 
the conceptual knowledge (or lack thereof) of students entering 
Statway. 

Lastly, we leverage data from an online learning system. These 
data represent any aspects of students’ interaction with the online 
platform, such as reading pages, practicing problems, and 
completing assignments, which can be used to operationally 
define students’ engagement and understanding [21]. We focused 
on students’ patterns of deviation from a standard pattern or 
“norm”, which is a promising predictor of failure [14]. The norm 
can be defined as a classroom norm or a norm that commonly 
emerged across colleges. 

Our aim is to integrate all three sources of data into an efficient 
system of indicators to provide instructors with actionable insights 
in a timely manner. 

The technical infrastructure supporting this work aims at 
collecting students’ data from multiple sources -- An online 
portal, a survey application, and an online instructional platform. 
The portal serves as a gateway and integrates with the other 
components through API calls and LTI launches, the online portal 
orchestrates the business logic responsible for sequencing various 
content of the course. A data pipeline is also in place to capture all 
students’ (and faculty) productions longitudinally. Figure 1 
summarizes this whole process. 

 

Figure 1: System topology for data collection. 

3. METHODS/DATA SOURCES  
In this paper, we present preliminary results using data available 
from Fall 2013 Statway courses taken by approximately 1600 
students from 26 colleges. However, our plan is to develop and 
refine our indicator system with data from 2015 Statway courses 
in the near future. 

The first source of data is a self-report survey, which is 
administered twice. At Week 1, self-report data on productive 
persistence are collected along with student background 

characteristics (e.g., demographics). At Week 4, only productive 
persistence items are administered. 

Data on MCK are also collected during Week 1. To avoid possible 
priming effects of MCK completion on responses to productive 
persistence items, students complete the MCK assessment after 
the week 1 survey. The current version of MCK consists of 21 
items, measuring key constructs that have the potential to impede 
students’ learning of mathematics and statistics and have been 
demonstrated to be difficult for developmental students: meaning 
of fractional and decimal values, proportional reasoning, basic 
algebraic notation, and reasoning about the effects of 
mathematical operations. A Rasch scale score [25] is computed to 
represent students’ knowledge of mathematical concepts at the 
outset of  Statway. 

The third source of data is from the online learning platform. 
Statway courses in Fall 2013 were hosted by Carnegie 
Mellon’s Online Learning Initiative (OLI) platform. The Statway 
curriculum consists of 12 modules, the first six of which are 
taught during the first course. Within each module, there are 
material pages to review, practice activities (i.e., Learn By Doing 
and Did I Get This?), and checkpoints that are graded for 
students’ understanding. These platform components generated 
over 12 million rows of data, which  were used to measure 
students’ engagement and proficiency as part of the indicator 
system.  

To develop effective and efficient indicators [27] from self-report, 
conceptual knowledge, and system use data, we leveraged two 
types of student end-of-term outcome variables: (a) a summative 
assessment administered to all Statway students across colleges 
and (b) students’ course grades. Students’ performance on the 
summative assessment, which was a sort of final exam, was 
factored into their grades by faculty. The correlation between 
summative assessment scores and grades (on a four point scale) 
was approximately .60 [20]. We wanted to  ensure the 
development of quality indicators using these two types of 
outcomes. 

Our analytic plan for creating indicators is as follows. First, we 
develop a Week 1 indicator using data on productive persistence 
and MCK by running analyses with the end-of-term outcomes 
described above. Under productive persistence, we focus on 
students' self-perceptions as math learners and doers, such as fixed 
mindset and stereotype threat. To measure fixed mindset, students 
were asked to respond to the statement, "Being a 'math person' or 
not is something about you that you really can't change. Some 
people are good at math and other people aren't", by indicating 
how much they agree or disagree with the statement. Responses 
were coded as: "Strongly agree" = High; "Agree" and "Mostly 
agree" = Medium; and "Mostly disagree", "Disagree", and 
"Strongly disagree" = Low. The survey item assessing stereotype 
threat asked students whether they thought others at their school 
would be surprised or not surprised if people like them succeeded 
in school. Responses were coded as: "Extremely surprised" and 
"Very surprised" = High; "Moderately surprised" and "Slightly 
surprised" = Medium; "Not surprised at all" = Low. We also use 
the first module’s checkpoint performance as an intermediate 
outcome. We consider each module checkpoint as key milestone 
for students, and accordingly examine checkpoint data whenever 
appropriate. 

Second, we leverage data from the online learning platform to 
track student progress on a daily basis with a focus on any 
patterns of students’ deviations from a normative pattern in 



engagement and understanding. Their engagement patterns in a 
given module will also be submitted to analyses with the 
corresponding module checkpoint.   

Third, we examined productive persistence data collected during 
Week 4, with a particular focus on students' perceptions of their 
own classroom setting as a mathematical learning environment. 
By the fourth week of class, we expect students to have developed 
their perceptions of the classroom they belong to. Hence, data 
measuring, for instance, belonging uncertainty, will be 
informative for identifying students in need. In fact, earlier 
research confirmed this hypothesis: those who reported a high 
level of belonging uncertainty were significantly more likely to 
drop out of the course [27]. To assess their level of belonging 
uncertainty, students were asked, "When you think about your 
college, how often, if ever, do you wonder: 'Maybe I don't belong 
here?'" Responses were coded as: "Always" = High; "Frequently" 
and "Sometimes" = Medium; "Hardly ever" and "Never" = Low. 

Fourth, we continue monitoring data from the online platform to 
detect any patterns of deviations. A prior study suggests that 
students who attempted a checkpoint after failing were more 
likely to achieve a higher score on the summative assessment than 
those who did not [13]. Accordingly, we monitor this type of 
attempt behavior for each checkpoint as part of our indicator 
system. 

We apply a hierarchical linear modeling approach [15] to the 
above analyses due to the hierarchical nature of our data (i.e., 
students nested within classrooms, which are further nested within 
colleges). By taking this approach, we want to appropriately 
capture variation at student, classroom, and college levels. 

4. RESULTS 
Our preliminary results are informative toward developing our 
indicator system. Table 1 presents the descriptive statistics of the 
summative assessment administered at the end of the fall term in 
2013 by each of the productive persistence construct groups. As 
expected, students’ productive persistence has a strong 
relationship with their end-of-term outcomes. More specifically, 
lower levels of fixed mindset, stereotype threat, and belonging 
uncertainty during the first and fourth weeks of class are 
positively associated with summative assessment scores at the end 
of term. Consistent with the results in Table 1, Figure 2 
demonstrates that the student success rate of Statway 1 (i.e., the 
first course in the Statway sequence) is associated with the level 
of each productive persistence construct. 

Table 1: Descriptive Statistics of Summative Assessment 
Rasch Scale Score Per Productive Persistence Construct 

Note. High, Medium, and Low represent the levels of each 
productive persistence construct. For the definitions of the levels, 
see 3. METHODS/DATA SOURCES. 
M = mean, SD = standard deviation. 

Figure 2: Student success rate of Statway 1 per productive 
persistence construct. High, Medium, and Low represent the 
levels of each productive persistence construct. For the 
definitions of the levels, see 3. METHODS/DATA SOURCES. 

Figure 2 presents the student success rate of Statway 1 by the 
level of math conceptual knowledge (MCK) and the level of 
online engagement during the first module. Engagement was 
defined as number of student attempts at practice activities (i.e., 
Learn By Doing and Did I Get This?) and checkpoints. 
Interestingly, there are interaction effects between  the level of 
MCK and the level of online engagement on student success rate. 
That is, students who demonstrated low levels of math conceptual 
knowledge but engaged with the online platform during the first 
module tended to achieve success rates comparable to students 
who demonstrated higher levels of math conceptual knowledge 
but never engaged with the online platform. 

 

Figure 3: Student success rate of Statway 1 by level of online 
engagement during the first module and MCK group. No 
engagement means students have attempted 0% of practice 
activities and checkpoints. Some engagement means students 
have done some (between 1-99%) of these activities. Full 
engagement means students have done 100% of these 
activities. 

5. SIGNIFICANCE OF THE STUDY 
This study demonstrated the ways in which we can leverage both 
self-report and online learning system data to identify students in 
need of support. For instance, we identified a few self-report 
survey items measuring non-cognitive factors as promising 
predictors of student outcomes, suggesting that we can inform 
actionable insights with only a few items’ data. More specifically, 
if we identify students who show higher levels of fixed mindset 
and/or stereotype threat during the first week of class, we can 
promote a growth mindset (as opposed to a fixed mindset) by 
engaging them in growth mindset activities (formulated by the 

 Fixed 
mindset 

at Week 1 

Stereotype 
threat 

at Week 1 

Belonging 
uncertainty 
at Week 4 

Level M SD M SD M SD 
High 0.55 0.68 0.54 0.67 0.42 0.84 
Medium 0.69 0.77 0.61 0.77 0.63 0.78 
Low 0.85 0.82 0.90 0.79 0.85 0.80 



networked improvement community [NIC] members) and 
providing them with feedback that establishes high standards 
while assuring student that they are capable of meeting them [8]. 
For instance, if students exhibit higher levels of belonging 
uncertainty during the fourth week of class, we can provide group 
activities that facilitate building a learning community for all 
students in the classroom (interventions are formulated by the 
NIC members). Furthermore, the results suggest that the use of the 
online learning platform may provide scaffolding for students 
with low levels of math conceptual knowledge. 

The results shown in this paper are still descriptive and 
preliminary. As aforementioned, we will apply a hierarchial linear 
modeling approach to better formulate our indicator system 
because of the hierarchical nature of our data (i.e., students nested 
within Statway courses within colleges). We also plan to work on 
more real-time analytics at more granular levels (e.g., how 
students’ math conceptual knowledge, productive persistence, and 
online engagement interact with each other to manifest in patterns 
of success or failure, when students start deviating from classroom 
norms, whether they attempt a module checkpoint right after 
failing it). In this paper, we defined 1-99% attempts as “some 
engagement” to differentiate it from no or full engagement (see 
Figure 3); however, we plan on dividing this intermediate level of 
engagement into more meaningful sub-levels for further 
actionable insights. In the future, we plan on expanding the 
current analysis plan by including data beyond the first month. 
For example, which aspects of productive persistence can we 
leverage as data to identify students in need after the first month? 
Which online behavioral patterns or deviations should we focus 
on to alert instructors of  targeted supports for students? Overall, 
we aim to establish the best practices for leveraging different 
kinds of data over time to provide support for students success. 

6. ACKNOWLEDGMENTS 
This program of work is supported by Carnegie Corporation of 
New York, The Bill & Melinda Gates Foundation, The William 
and Flora Hewlett Foundation, The Kresge Foundation, and 
Lumina Foundation in cooperation with the Carnegie Foundation 
for the Advancement of Teaching. 

7. REFERENCES 
[1] Bailey, T., Jeong, D. W., & Cho, S.-W. (2010). Referral, 

enrollment, and completion in developmental education 
sequences in community colleges. Economics of Education 
Review, 29, 255–270. 

[2] Beilock, S. L., Gunderson, E. A., Ramirez, G., & Levine, S. 
C. (2010). Female teachers' math anxiety affects girls' math 
achievement. Proceedings of the National Academy of 
Sciences, USA, 107, 1060-1063. 

[3] Blackwell, L. S., Trzesniewski, K. H. & Dweck, C. S. 
(2007). Implicit theories of intelligence predict achievement 
across an adolescent transition: A longitudinal study and an 
intervention. Child Development, 78, 246-263. 

[4] Boaler, J. (1998). Open and closed mathematics: Student 
experiences and understandings. Journal for Research in 
Mathematics Education, 29, 41–62. 

[5] Bryk, A. S., Gomez, L. M., & Grunow, A. (2011). Getting 
ideas into action: Building networked improvement 
communities in education. In M. T. Hallinan (Ed.), Frontiers 
in sociology of education (pp. 127-162). New York, NY: 
Springer. 

[6] Bryk, A. S., Gomez, L. M., Grunow, A., & LeMahieu, P. G. 
(2015). Learning to improve: How America's schools can get 
better at getting better. Cambridge, MA: Harvard Education 
Press. 

[7] Cohen, G. L., Garcia, J., Purdie-Vaughns, V., Apfel, N., & 
Brzustoski, P. (2009). Recursive processes in self-
affirmation: Intervening to close the minority achievement 
gap. Science, 324, 400-403. 

[8] Cohen, G. L., Steele, C. M., & Ross, L. D. (1999). The 
mentor’s dilemma: Providing critical feedback across the 
racial divide. Personality and Social Psychology Bulletin, 25, 
1302–1318. 

[9] Dolle, J. R., Gomez, L. M., Russell, J. L., & Bryk, A. S. 
(2013). “More than a network: building communities for 
educational improvement.” In B. J. Fishman, W. R. Penuel, 
A. R. Allen, & B. H. Cheng (Eds.), Design-based 
implementation research: Theories, methods, and exemplars. 
National Society for the Study of Education Yearbook. New 
York, NY: Teachers College Record. 

[10] Dweck, C.S. (2006). Mindset. New York, NY: Random 
House. 

[11] Haynes, T. L., Perry, R. P., Stupnisky, R. H., & Daniels, L. 
M. (2009). A review of attributional retraining treatments: 
Fostering engagement and persistence in vulnerable college 
students. In Smart, J. C. (Ed.), Higher education: Handbook 
of theory and research (pp. 227-272). New York, NY: 
Springe 

[12] Hong, G., & Raudenbush, S. W. (2005). Effects of 
kindergarten retention policy on children’s cognitive growth 
in reading and mathematics. Educational Evaluation and 
Policy Analysis, 27, 205-224.  

[13] Krumm, A. E., D'Angelo, C., Podkul, T. E., Feng, M., 
Yamada, H., Beattie, R., Hough, H., & Thorn, C. (2015, 14-
15 March). Practical measures of learning behaviors. Paper 
presented at the ACM Conference on Learning @ Scale 
(L@S'15), Vancouver, BC, Canada. 

[14] Krumm, A. E., Waddington, R. J., Teasley, S. D., & Lonn, S. 
(2014). Using learning analytics to support academic 
advising in undergraduate engineering education. In J. A. 
Larusson & B. White (Eds.). Learning Analytics: From 
Research to Practice (pp. 103-119). New York: Springer. 

[15] Raudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear 
models: Applications and data analysis methods (2nd ed.). 
Thousand Oaks, CA: Sage Publications. 

[16] Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of 
the propensity score in observational studies for causal 
effects. Biometrika, 70, 41-55. 

[17] Silva, E., & White, T. (2013). Pathways to improvement: 
Using psychological strategies to help college students 
master developmental math. Carnegie Foundation for the 
Advancement of Teaching. Stanford, CA. 

[18] Sowers, N., & Yamada, H. (2015). Pathways Impact Report. 
Stanford, CA: Carnegie Foundation for the Advancement of 
Teaching. 

[19] Steele, C. M., & Aronson, J. (1995). Stereotype threat and 
the intellectual test performance of African-Americans. 
Journal of Personality and Social Psychology, 69, 797-811.  



[20] Strother, S., & Sowers, N. (2014). Community College 
Pathways: Summative Assessments and Student Learning. 
Stanford, CA: Carnegie Foundation for the Advancement of 
Teaching. 

[21] U.S. Department of Education. (2013). Expanding evidence 
approaches for learning in a digital world. Washington, DC: 
Author. 
www.ed.gov/edblogs/technology/files/2013/02/Expanding-
Evidence-Approaches.pdf 

[22] Walton, G. M., & Cohen, G. L. (2007). A question of 
belonging: Race, social fit, and achievement. Journal of 
Personality and Social Psychology, 92, 82-96.  

[23] Walton, G. M., & Cohen, G. L. (2011). A brief social-
belonging intervention improves academic and health 

outcomes among minority students. Science, 331, 1447–
1451. 

[24] Walton, G. M., & Spencer, S. J. (2009). Latent ability: 
Grades and test scores systematically underestimate the 
intellectual ability of negatively stereotyped students. 
Psychological Science, 20, 1132-1139.  

[25] Wright, B. D., & Masters, G. N. (1982). Rating scale 
analysis: Rasch measurement. Chicago: MESA Press. 

[26] Yamada, H. (2014). Community College Pathways’ Program 
Success: Assessing the First Two Years’ Effectiveness of 
Statway®. Stanford, CA: Carnegie Foundation for the 
Advancement of Teaching.   

[27] Yeager, D., Bryk, A., Muhich, J., Hausman H., & Morales, 
L. (2013) Practical measurement. Carnegie Foundation for 
the Advancement of Teaching. Stanford, CA.   

 



