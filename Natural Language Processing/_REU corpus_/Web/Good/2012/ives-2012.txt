
LAK 2012 Panel 

Building a Data Governance Model for Learning Analytics 
Sabine Graf 

Computing and Information Systems 
Athabasca University, Canada 

sabineg@athabascau.ca 

                  Cindy Ives 
Acting Associate Vice President (Learning 
Resources), Athabasca University, Canada  

cindyi@athabascau.ca 
  

Lori Lockyer  
Faculty of Education, University 

of Wollongong 

Paul Hobson  
Director, Enterprise 

Architecture, Information 
Technology, University of 

British Columbia  

Doug Clow  
Institute of Educational 
Technology, The Open 

University UK  

PANEL SUMMARY  
This international panel presentation aims to explore and 

discuss the issues that emerge when an educational institution 

decides to develop learning analytics initiatives. While learning 

analytics may provide data that lead to improvements in the 

quality of teaching and learning design, and therefore has the 

potential to enhance the overall quality of education, the 

successful development and implementation of tools and 

processes for learning analytics are complex and problematic. 

In this panel, data governance considerations will be discussed 

from organizational, ethical, learning design, and technical 

points of view. 

Categories & Subject Descriptors 

 J.1 [Administrative Data Processing] Education; K.3.1 
[Computer Uses in Education] Collaborative learning, 

Computer-assisted instruction (CAI), Computer-managed 

instruction (CMI), Distance learning 

General Terms 

Management. 

Keywords 

 Learning Analytics, Ethics. 

1. LIST OF PANELISTS 
Chair: Cindy Ives, Acting Associate Vice President 

(Learning Resources), Athabasca University 

Discussants:  

• Sabine Graf, Assistant Professor, Computing and 
Information Systems, Athabasca University 

• Lori Lockyer, Professor, Faculty of Education, 
University of Wollongong 

• Paul Hobson, Director, Enterprise Architecture, 
Information Technology, The University of British 

Columbia 

• Doug Clow, Lecturer, Interactive Media 
Development, Institute of Educational Technology, 

The Open University UK 

Format: 

After a brief introduction to the topic and the presenters, each 

discussant will offer an opening statement about their 

perspective on the factors affecting learning analytics projects 

with which they are familiar. Each will then address the 

following specific questions related to data governance: 

• Who owns the data that are being analyzed? Students, 
instructors, administrators, learning designers? What 

permissions are necessary for data access? What 

documentation is required? 

• To what extent do student and instructor privacy 
concerns determine the nature and scope of a learning 

analytics project? What consultations are necessary 

with stakeholders? 

• Should the analysis of learning and teaching data be 
considered as research? To what extent should 

learning analytics projects be subject to the 

guidelines and controls of research ethics boards? 

• For student facing analytics projects such as 
dashboards and identification of learners at risk, 

when does responsible facilitation of learning cross 

the line to be seen as an intrusion on learners’ 

privacy? 

• From a technical point of view, what types of data can 
be tracked and used in learning analytics? How does 

this correlate with the types of data that should be 

tracked in order to inform appropriate conclusions? 

Audience input will be encouraged with a view to engaging an 

open discussion of these and other issues. 

 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. 

LAK’12, 29 April – 2 May 2012, Vancouver, BC, Canada. 
Copyright 2012 ACM 978-1-4503-1111-3/12/04…$10.00 

21



2. TOPIC OF THE PANEL 
This panel brings together representatives from four 

universities from different countries and with distinct mandates 

to discuss factors to be considered during the implementation of 

a learning analytics initiative. Indeed, we will propose that the 

issues are best discussed broadly across the institution in 

advance of a learning analytics project, and should be framed in 

the context of data governance. 

The promise of analytics for higher education institutions has 

been well articulated for several years  [3], but researchers have 

noted that actual applications are limited  [6].  Recent initiatives 

 [2] in organizational level academic analytics focusing on 

student retention, such as at the American Public University 

System  [5], and on providing learners with dashboards to chart 

their progress through online courses, such as the Course 

Signals program (Purdue University, 2009), have great 

potential to inform systemic improvements to online learning 

experiences. Other approaches emphasize extracting evidence 

of student behaviors inside learning systems, with a view to 

informing iterative improvements in course design  [4]. This 

methodology is analogous to the immediate and informal 

feedback of face-to-face instruction, and has the added potential 

benefit of making teaching and learning practice more 

transparent, leading to the design of qualitatively different 

online learning environments. 

Methodological considerations are critical when considering 

data analysis projects  [1]. Issues of data quality, 

interpretability, ethics and privacy need to be understood and 

addressed in order to draw valid and reliable inferences, before 

applying this new knowledge in practical ways. The skills to 

interpret data are not trivial. Neither are the skills to design 

and develop tools for identifying and accessing data of interest. 

As well, in order for learning analytics tools to contribute to the 

improvement of pedagogical practices, evaluation measures and 

feedback mechanisms are needed, where the results of the 

analytic tool are measured in terms of whether and how much 

they benefit instructors and course designers, and whether and 

how much they lead to improved learning designs. 

Furthermore, such new or revised teaching and learning design 

practices must be evaluated with respect to whether they really 

enhance student learning. Users of data also need mechanisms 

that communicate whether the implemented learning analytics 

techniques are useful for them and what further information 

would help them to improve the overall quality of their 

practice. 

A wide variety of factors affect decision making about data 

analysis in general, and learning analytics in particular, among 

them data security and access, governance policies and 

procedures, data volumes and visualization, data accuracy and 

comprehensiveness, as well as user needs for information, 

strategies for units and levels of analysis, and feedback 

mechanisms for evaluating usefulness and interpretability. The 

complexity of the relationships among organizational processes, 

analytic tools and interpretive methods suggests careful design 

and planning of academic analytics projects to ensure 

appropriate, ethical and useful benefits. 

3. REFERENCES 
[1]. Boyd, D. (2010, April). Privacy and Publicity in the 

Context of Big Data. WWW. Raleigh, NC. Retrieved 

February 17, 2012 from 

http://www.danah.org/papers/talks/2010/WWW2010.html  

[2]. Brown, M. (2011). Learning Analytics: The Coming Third 

Wave. In Educause Learning Initiative Brief. Retrieved 

January 6, 2010, from 

http://www.educause.edu/Resources/LearningAnalyticsTh

eComingThir/227287 

[3]. Goldstein, P. J., & Katz, R. N. (2005).  Academic 
analytics: The uses of management information and 

technology in higher education. ECAR Research Study (vol 

8). Retrieved January 22, 2012,from 

http:/www.educause.edu/ers0508 

[4]. Graf, S., Ives, C., Rahman, N., & Ferri, A. (2011). AAT – 
A tool for accessing and analysing students’ behaviour 

data in learning systems. In Proceedings of the 

International Conference on Learning Analytics and 

Knowledge (LAK2011) (pp. 174-179). ACM Press, 

February, Banff, Canada.  

[5]. IBM (2012). Using data to boost student engagement and 

retention. In Campus Technology. Retrieved January 22, 

2012, from 

http://campustechnology.com/whitepapers/2011/12/ibm_us

ing-data-to-boost-student-engagement-and-retention.aspx  

[6]. Romero, C., & Ventura, S. (2007). Educational data 

mining: A survey from 1995 to 2005. Expert Systems with 

Applications, 33(1), 135-146. 

 

 

 

 

22





