
Connecting Data with Student Support Actions in a 
Course: A Hands-on Tutorial 

Abelardo Pardo 
The University of Sydney 

Abelardo.pardo@sydney.edu.au 
 

Jurgen Schulte 
University of Technology Sydney 

Jurgen.schulte@uts.edu.au 
 

Jing Gao 
University of South Australia 

Jing.gao@unisa.edu.au 

Roberto Martínez-Maldonado 
University of Technology Sydney 

roberto.martinez-
maldonado@uts.edu.au 

Simon McIntyre 
University of New South Wales 

s.mcintyre@unsw.edu.au 
 

George Siemens 
University of Texas at Arlington 

gsiemens@gmail.com 

Simon Buckingham Shum 
University of Technology Sydney 

Simon.BuckinghamShum@uts.edu.au 
 

Dragan Gaševi?  
The University of Edinburgh, 
University of South Australia 
dragan.gasevic@ed.ac.uk

ABSTRACT 
The amount of data extracted from learning experiences has 
grown at an astonishing pace both in depth due to the increasing 
variety of data sources, and in breath with courses now being 
offered to massive student cohorts. However, in this emerging 
scenario instructors are now facing the challenge of connecting 
the knowledge emerging from data analysis with the provision of 
meaningful support actions to students within the context of an 
instructional design. 
The objective of this tutorial is to give attendees a set of 
hypothetical scenarios in which the knowledge extracted from a 
learning experience needs to be used to provide frequent 
personalized feedback to students. 

CCS Concepts 
• Applied computing~Computer-assisted instruction 

Keywords 
Learning analytics, feedback, predictive analytics, instructional 
design, student support. 

1. BACKGROUND 
In recent years we have witnessed a formidable increase in the 
technological mediation present in learning experiences. Learners 
in general may now have access to a wider set of resources, 
through a larger variety of formats, and interact with peers and 
instructors through an increasingly large number of channels. 
These new affordances have the potential of providing a more 
effective experience, but at the same time the risk of increasing 
the complexity of the design and deployment stages due to the 
sheer increase in options for the learner. 

The technology used in these learning scenarios offers an 
unprecedented number of data sources such as clickstreams, 
student location, physiological measurements, affect, social 

connections etc. 

A number of recent research contributions in the area of learning 
analytics have highlighted the need to focus more on learner 
support actions deployed in a learning experience after a 
comprehensive data set has been analyzed with state of the art 
data mining or analytics algorithms [1]. Recent advances in data 
collection and student modelling [2], text analytics [3], the use of 
dashboards [4], etc. have translated into a rich set of potentially 
useful tools. However, instructors are still facing significant 
challenges when trying to integrate them properly as part of an 
instructional design and assess their impact in every-day teaching 
and learning activities [5]. 

A learning experience with a rich variety of format, platforms and 
communication channels poses a more complex environment for 
students as well. The perception and evaluation of the learning 
environment is related both with approaches to study and the 
quality of the learning outcomes [6]. In other words, a 
technologically richer learning environment may require 
additional support to facilitate its perception by the students. 
Other research areas such as behavioral economics have identified 
the notion of choice architecture, or how small features in 
interactive environments can have significant effects in people’s 
behavior [7]. The idea was discussed in the context of learning 
experiences in relation to address the issues of retention. Students 
faced numerous decisions during their career and specifically 
targeted nudges or reminders may support them through this 
process. Subtle interventions to influence student behavior are 
more effective in situations with a large number of choices, when 
there are experts that can provide appropriate advise, and when 
individual preferences can be estimated [7]. Ideally, instructors 
should be able to influence student behavior through interventions 
to suggest the best approach to maximize the overall quality of a 
learning experience. But these interventions are usually 
challenging to deploy, specially at scale. 

Massive open online courses (MOOCs) provide learning 
experiences for cohorts of thousands of students. In these 
scenarios, information about learners is now at unprecedented 
levels of depth (detailed accounts of their actions in online 
environments) and breath (for large number of students). The data 
sets extracted from these courses have prompted the study of 
techniques to collect information, summarize and display it in 
more amenable formats, and use it to estimate student behavior. 

Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of this work must be honored. For all other 
uses, contact the Owner/Author.  
Copyright is held by the owner/author(s). 
LAK '17, March 13-17, 2017, Vancouver, BC, Canada 
ACM 978-1-4503-4870-6/17/03. 
http://dx.doi.org/10.1145/3027385.3029441 



Additionally, these new scenarios have push for the exploration of 
new mechanisms to automate as many tasks as possible. 

The problem of how to support instructors in the decisions they 
make while facilitating a learning experience has not received the 
attention it deserves. There is an increasing disconnection between 
the role of the instructor, the increasingly complex learning 
scenarios, and the rich set of data collected during a learning 
experience. Instructors now face the challenge of finding the best 
way to embed the potential of data-supported decision making 
within their instructional designs as well as day-to-day teaching 
and learning activities. From the point of view of the students, the 
provision of feedback has been identified as one of the most 
problematic aspects when supporting their learning [8]. We are 
facing a paradox in which students recognize the importance of 
feedback, complain about its lack of quality, and in some cases 
they don’t engage with it [9]. How is possible that in the age of 
ubiquitous data we are still struggling to provide students with 
better support? 

Hattie [10] proposed a model of teaching and learning that was 
based on three postulates, and one of which being that student 
achievement in a learning experience is enhanced as a function of 
feedback. A more refined model proposed that feedback can be 
provided at four levels (task, learning process, self-regulation and 
self) and is most effective at the learning process and self-
regulation levels [11]. Ironically, feedback in the context of higher 
education is usually perceived as inherently problematic [12] or an 
administrative chore [13] - despite being one of the aspects with 
large potential to translate into significant improvements in a 
learning experience. 

The aim of this tutorial is to propose the combination of these two 
elements. On one hand, the manipulation of comprehensive data 
sets that provide detailed account of the events occurring in a 
learning experience, and on the other, the need to apply a sense-
making stage to identify the personalized feedback that can be 
provided to students with respect to their learning process and 
self-regulation aspects. The hands-on exercises will assume that 
the audience has detailed knowledge of a learning environment 
and the need to provide frequent, personalized and relevant 
comments to students while they participate in a learning 
experience. 

2. OBJECTIVES 
The tutorial will last for half day and will be hands-on with a set 
of activities conceived to guide the attendees to design 
personalized interventions in a real learning experience. The topic 
in the tutorial will appeal to researchers and practitioners in the 
areas of learning analytics, educational data mining, instructional 
design, and any academic interested in improving the timeliness 
and relevance of feedback and support for their students. 
Attendees are not required to have experience with any 
programming language but need to be familiar with the 
conventional data management procedures required in a course 
with a large number of students such as for example combining 
scores and observations from multiple sources. The objectives of 
the proposed tutorial are to: 

• Understand the use of exploratory data analysis to 
summarize data sets derived from a learning experience 

• Identify student support actions to be deployed while 
the experience is being delivered 

• Define a set of low latency actions (those reaching 
students within a day) that are personalized to each 
student 

• Express the connection between data and actions in a 
formalism suitable to be deployed at scale. 

The tutorial will use the platform OnTask (available at 
ontasklearnin.org) and various synthetic data sets to allow 
attendees to explore the connections between the data available in 
a realistic learning environment, the sense-making procedures 
required by instructors, and the deployment of frequent and highly 
personalized student support actions. 

3. REFERENCES 
[1] A. F. Wise, "Designing pedagogical interventions to support 

student use of learning analytics," in International Conference 
on Learning Analytics and Knowledge, 2014, pp. 203-211. 

[2] R. Pelánek, J. Rihák, and J. Papoušek, "Impact of data 
collection on interpretation and evaluation of student models," 
presented at the International Conference on Learning 
Analytics and Knowledge, Edinburgh, UK, 2016. 

[3] S. A. Crossley, K. Kyle, and D. S. McNamara, "The tool for 
the automatic analysis of text cohesion (TAACO): Automatic 
assessment of local, global, and text cohesion," Behavior 
Resesearch Methods, pp. 1-11, Sep 28 2015. 

[4] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. L. Santos, 
"Learning Analytics Dashboard Applications," American 
Behavioral Scientist, vol. 57, pp. 1500-1509, 2013. 

[5] A. Bakharia, L. Corrin, P. de Barba, G. Kennedy, D. Gasevic, 
R. Mulder, et al., "A Conceptual Framework linking Learning 
Design with Learning Analytics," in International Conference 
on Learning Analytics and Knowledge, S. Dawson, C. Rosé, 
and H. Drachsler, Eds., ed. Edinburgh, UK: ACM, 2016, pp. 
329-338. 

[6] K. Trigwell and M. Prosser, "Improving the quality of student 
learning: the influence of learning context and student 
approaches to learning on learning outcomes," Higher 
education, vol. 22, pp. 251-266, 1991. 

[7] R. H. Thaler and C. R. Sunstein, "Nudge. Improving 
Decisions About Health, Wealth, and Happiness," 2008. 

[8] N. E. Winstone, R. A. Nash, J. Rowntree, and M. Parker, "‘It'd 
be useful, but I wouldn't use it’: barriers to university 
students’ feedback seeking and recipience," Studies in Higher 
Education, pp. 1-16, 2016. 

[9] C. Withey, "Feedback engagement: forcing feed-forward 
amongst law students," The Law Teacher, vol. 47, pp. 319-
344, 2013. 

[10] J. Hattie, "Influences on Student Learning," in 
Inaugural Lecture: Professor of Education, ed. University of 
Auckland, 1999. 

[11] J. Hattie and H. Timperley, "The Power of Feedback," 
Review of Educational Research, vol. 77, pp. 81-112, 2007. 

[12] R. Higgins, P. Hartley, and A. Skelton, "Getting the 
Message Across: The problem of communicating assessment 
feedback," Teaching in Higher Education, vol. 6, pp. 269-274, 
2010. 

[13] D. Hounsell, "Toward more sustainable feedback to 
students," in Rethinking Assessment in Higher Education: 
Learning for the Longer Term, D. Boud and N. Falchikov, 
Eds., ed London and New York: Routledge, 2007. 

 



