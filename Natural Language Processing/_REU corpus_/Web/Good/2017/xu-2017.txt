
Person-Centered Approach to Explore Learner’s 
Emotionality in Learning within a 3D Narrative Game 

Zhenhua Xu 
University of Toronto 

252 Bloor St W, Toronto, ON., Canada, M5S 1V6 
zhenh.xu@mail.utoronto.ca

Earl Woodruff 
University of Toronto 

252 Bloor St W, Toronto, ON., Canada, M5S 1V6 
earl.woodruff@utoronto.ca 

 
 

ABSTRACT 
Emotions form an integral part of our cognitive function. Past 
research has demonstrated conclusive associations between 
emotions and learning achievement [7, 26, 27]. This paper used a 
person-centered approach to explore students’ (N = 65) facial 
behavior, emotions, learner traits and learning. An automatic facial 
expression recognition system was used to detect both middle 
school and university students’ real-time facial movements while 
they learned scientific tasks in a 3D narrative video game.  

The results indicated a strong statistical relationship between three 
specific facial movements (i.e., outer brow raising, lip tightening 
and lip pressing), student self-regulatory learning strategy and 
learning performance. Outer brow raising (AU2) had strong 
predictive power when a student is confronted with obstacles and 
does not know how to proceed. Both lip tightening and pressing 
(AU23 and AU24) were predictive when a student engaged in a 
task that requires a deep level of incoming information processing 
and short memory activation. The findings also suggested a 
correlational relationship between student self-regulatory learning 
strategy use and neutral state. It is hoped that this study will provide 
empirical evidence for helping us develop a deeper understanding 
of the relationship between facial behavior and complex learning 
especially in educational games.  

CCS Concepts 
• General and reference?Empirical studies • Applied 
computing?Interactive learning environments 

Keywords 
Emotion; Affect; Facial expression recognition; Learner traits; 
Game-based learning; Complex learning; Scientific reasoning; 
Educational video games 

1. INTRODUCTION 
Emotion plays a critical role in complex learning. In recent years,  
 
 
 

 
 
studies on the link between emotions and cognition in the field of 
educational psychology [4, 25, 26], neuroscience [22] and artificial 
intelligence education [e.g., 2, 5, 6, 7, 36] have made significant 
contribution in helping us understand emotion and its’ role in 
students’ learning processes [27, 34]. Consequently, we have 
witnessed a variety of methods being used to assess students’ 
affective experiences across different age groups and learning 
platforms [2, 6, 3, 7]. In particular, recent advancements in 
computer facial recognition technology within which a 
combination of sophisticated algorithms and non-intrusive 
webcams are used obtain reliable measures of student emotions in 
real-time [14, 23, 35]. Nevertheless, there is a paucity of empirical 
work exploring the role of emotion in complex learning with 
educational video games. To our knowledge, there are two major 
areas of research: one focusing on examining the difference in 
learners’ emotional experience when learning from an educational 
game in comparison to learning with intelligent tutoring systems 
[e.g., 32], the other using self-report approach or computational 
models to predict special emotions such as confusion and its 
relationship to learning and performance. 

Different from the aforementioned studies, this present study used 
a person-centered approach to examine the direct relationship 
between emotion, appraisal processes and learning outcomes by 
using multi-channel data, which includes students’ real-time facial 
data, questionnaires and computer log data. Person-centered 
approach refers to placing the individual at the focal point of the 
analysis because it enables us to investigate how psychological 
constructs relate to emotions and learning outcomes. Specifically, 
we asked the following research questions: 
(1) What were the commonly occurring facial movements and 
emotions during learning in an educational video game?  
(2) Could the frequency of the facial movements predict students’ 
learning gains and to what extent did the facial behavior predict 
learning? 
(3)  Is there a relationship between learner traits and emotions 
and/or facial movements?  

1.1 A Description of 3D Game-Based 
Learning Environment: CRYSTAL ISLAND  
CRYSTAL ISLAND is a 3D game-based learning environment 
designed to develop students’ problem solving, scientific 
reasoning, and literacy skills (See Figure 1). In the game, a 
participant is informed to play the role of a medical detective to 
identify the epidemic that has spread amongst a group of scientists 
on a remote island. The participant explores the virtual environment 
from a first-person perspective: s/he is asked to engage in a series 
of activities that involve generating hypotheses based on the clues 
s/he gathered from virtual characters (i.e., patients, a lab technician, 
a nurse and a chef), reading books and articles, viewing posters and 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for 
components of this work owned by others than the author(s) must be 
honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions@acm.org. 
LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada. 
Copyright is held by the owner/author(s). Publication rights licensed to 
ACM. 
ACM 978-1-4503-4870-6/17/03…$15.00 
DOI: http://dx.doi.org/10.1145/3027385.3027432  
 



running lab tests to identify the spreading disease’s transmission 
sources. At the end of gameplay, the participant must submit a 
correct diagnosis. In CRYSTAL ISLAND, the reading material has 
concept matrices used to assess participant’s understanding of 
scientific concepts. The matrices are designed in a multiple-choice 
format. When a participant finishes reading every book or research 
paper, s/he has to compete the questions in the concept matrix. 

2. THEORETICAL FRAMEWORK 
In this study, we applied Ekman and Friesen’s theory of universal 
facial expressions of emotion [11] and Scherer’s component 
process model (CPM) [31] to guide our understanding of the 
relationships between emotion, appraisal factors and learning. 
Ekman and Friesen hypothesized that universals are to be found in 
the relationship between distinctive movements of the facial 
muscles and particular emotions [11]. They indicated that the 
difference in facial behaviors is the function of the evoking events 
rather than the reflection of the difference in facial muscles. Ekman 
and Friesen’s study of facial expressions of emotions has lead them 
to develop the Facial Affect Scoring Technique, which enabled 
them to study deception, emotion recognition and in-depth facial 
expression of emotions. In recent years, research has suggested that 
facial expression is a useful approach to examine affect because 
facial expression is ubiquitous, and it is convenient as facial 
representation can be used to model learning-centered affective 
states [18]. 

Component Process Model is the most comprehensive model of 
emotion [1]. Scherer indicated that emotion is “an emergent, 
dynamic process based on an individual’s subjective appraisal of 
significant events” [31]. In a nutshell, CPM demonstrates three 
central notions about the emotion processes [32, 15] which are: 1) 
People appraise events through sequentially processed evaluation 
such as novelty, intrinsic pleasantness, goal conduciveness, et.al. 
[15, 16]; 2) The effects of the appraisal evaluation, that is, how the 
result of the appraisal evaluation changes the state of emotion and 
ensures that it is adaptive to the event and;  3) The pattern of an 
emotional response is the cumulative result of all appraisal induced 
changes in the emotion components [16].  

3. METHODOLOGY 
3.1 Participants 
Sixty-five students participated in this study. Forty-four 
undergraduate students (77% females) were recruited from a 
leading Canadian university (mean age of 22 years) and twenty-one 
middle school and high school students (76% males, mean age of 
13 years) were recruited from a sport club and Canadian public 
schools. The majority of the undergraduate students (44%) were in 
their fourth year of study representing a variety of disciplines. In 
the teenage group, 61% of the students in grade seven and eight. 

3.2 Measures/Materials  
3.2.1 Survey Questionnaires 
Two questionnaires were used to gather participants’ information: 
1) Demographic information (e.g., age, gender, ethnicity, year of 
school enrollment and prior knowledge on life science) was 
obtained from all participants. 2) The 30 items from the Motivation 
Strategies for Learning Questionnaires (MSLQ) [31, 9] were 
modified and adapted to measure students’ motivation and self-
regulated learning strategies using a 7-point Likert scale. In 
addition, a content pre-test (19 items) was also administrated to all 
participants. It was used to measure students’ understanding of 
microbiology concepts.  

3.2.2 Facial Data and CRYSTAL ISLAND Logs 
Multi-channel process data were collected when participants 
played CRYSTAL ISLAND: 1) software log files and; 2) real-time 
participants’ facial data using the webcam video corpus (See Figure 
2). The log-file data captured students’ interactions with the game 
environment, including timestamp, action type, location, object, 
and characters involved in the interaction. In this study, we only 
focused on using reading task related variables for the data analysis. 
These variables were treated as learning outcome variables. 
Namely, they were: the number of books completed, the number of 
questions answered incorrectly, the number of answers being 
corrected and the number of attempts made in order to obtain the 
correct answers before submitting to the system.   

Students’ facial data was generated through an automatic facial 
expression monitoring software (iMotions® n.d.), which relies on 
a video data stream of learners’ facial expressions to track their 
affective states and fluctuations. This software draws on Ekman 
and Friesen’s (1978) Facial Action Coding System (FACS) to 
detect and track participants’ affective states through action units 
(AUs) that are corresponding to individual facial motor muscles 
[1]. The facial expression monitoring software analyzes the video 
recording at 30 frames per second. It classifies seven basic 
emotions (i.e., joy, anger, sadness, surprise, fear, contempt, disgust) 
three learner-centered emotions (i.e., frustration, confusion and 
neutral state) and 19 facial action units (AUs).  

The video corpus used in the present study consisted of fifty-eight 
video recordings with an average of 52,300 FACS-annotated video 
frames per video recording. Both basic and learning-centered 
emotions and facial action units were used for data analysis in this 
paper. 

3.3 Data Analysis  
3.3.1 Analysis of Leaner Variables  
Prior to data analysis, reliability tests were conducted to examine 
the internal consistency of the three instruments (i.e., MSLQ, a 
content pre-test and a scale measuring students’ prior knowledge). 
The results suggested that the reliability of all the measures were 
satisfied: Cronbach’s alpha value for MSLQ motivation scale was 
.80; the Cronbach’s alpha value for MSLQ learning strategy scale 
was 0.75. The Cronbach value for the content pre-test was .88. For 
the prior knowledge measure, the Cronbach’s alpha value was .81.  
Following the reliability tests, six composite variables were created 
based on the adapted 30 MSLQ items that pertained to self-efficacy, 
goal orientation, task values and self-regulatory strategies on a 7-
point Likert scale. One composite variable was also created based 
on the scale that measures students’ prior knowledge.  
 

3.3.2 Task Variables 
Task variables selected for the data analysis were: 1) students’ 
content pre-test scores and, 2) reading related variables extracted 
from the CYRSTAL ISLAND video game log files including the 
number of books completed by each student, the number of 
questions in concept matrix answered incorrectly, the number of 
attempts made to obtain the right answers and the frequency of 
answers being corrected before submitting final answers to the 
system.  
 

3.3.3 Analysis of Facial Data 
Students’ facial data selected for this study were the seven basic 
emotions (i.e., joy, anger, sadness, surprise, fear, contempt, 
disgust), the three learner-centered emotions (i.e., frustration, 
confusion and neutral state) and the 19 facial action units (AUs). 



For the purpose of this study, the frequency values for each emotion 
and AU were computed and used for the data analysis. That is, the 
percentage of each emotion and AU was registered as existing for 
each participant and was used to answer the first two research 
questions. 

4. RESULTS 
To answer the first research question, we ran descriptive statistics 
to identify the occurring emotions and facial movement action units 
in the gameplay. Following that, we applied R software to extract 
the number of raw evidence output values that were beyond 
threshold values, and then, we computed the values by dividing 
them by the total number of output values for each participant. The 
frequently occurring emotions and facial action units were 
identified and they were: four basic emotions (joy, surprise, sad, 
anger), three learning-centered emotions (confusion, frustration 
and neutral) and eight facial action units such as outer brow raising 
(AU2), brow lowering (AU4), upper lid raising (AU5), upper lip 
raising (AU10), lip corner depressing (AU15), lip tightening 
(AU23), lip pressing (AU24) and eyes closing (AU43).  

Table 1: Selected sample images of commonly occurring facial 
action units1 

Facial Muscle 
Action Units Description Example Image 

AU2 Outer Brow Raiser 
 

AU4 Brow Lowered 
 

AU5 Upper Lid Raiser 
 

AU23 Lip Tightener 
 

AU24 Lip Pressor 
 

 

Before answering the second research question, multiple 
correlation analyses were conducted to explore the relationship 
between emotions, facial movements and task variables. The 
findings suggested that three basic emotions (e.g., surprise, fear and 
anger) were associated with the number of books completed (r = -
.30, p < .05; r = -.32, p < .05; r = -.30, p < .05). Two learning-
centered emotions such as frustration and neutral were correlated 
with the number of attempts students made to answer the questions 
(r = .32, p < .05) and the content pre-test (r = .30, p < .05). Sad was 
negatively associated with the content pre-test and student prior 
knowledge level (r = -. 36, p < .001; r = -.30, p < .001, respectively). 
In terms of the facial action units, upper lid raising (AU5) was 
significantly associated with the number of questions in concept 
matrix that were answered incorrectly (r = .57, p <.001) and the 
number of attempts made to answer the questions (r = .71, p < .001). 
Both lip tightening (AU23) and pressing (AU24) were also 
significantly correlated with the number of attempts made to 
answer the questions and the number of times corrected the wrong 
answers (r = .50, p < .001; r = .46, p < .001; r = .40, p < .001; r = 
36, p < .001, respectively). Lip puckering (AU18) was negatively 
associated with students’ prior knowledge level (r = -.33, p < .05). 
Eyes closing (AU43) was negatively correlated with the number of 
                                                                    
1 The facial action unit (AU) descriptions and sample images were 

adapted from The Facial Action Coding System (FACS) [13]. 

books completed, the number of incorrect answers and the number 
of attempts students made to answer the questions.  

Linear regression models were constructed to examine the 
predictive effects of frequently occurring facial action units and 
emotions on learning performance. The findings indicated that lip 
tightening (AU23) and lip pressing (AU24) were strong predictors 
of the number of books completed by each student (R2 = .13, F(2, 
56) = 3.68, p <.05). Outer brow raising (AU2) was a strong 
predictor of the number of questions in concept matrix that were 
answered incorrectly (R2 = .11, F(1, 57) = 6.00, p <.05) and the 
number of answers corrected before submitting the correct answers 
to the system (R2 = .16, F(1,57) = 9.18, p < .05) and the number of 
attempts students made to answer all the questions that were listed 
in the concept matrix (R2 = .09, F(1, 57) = 4.82, p < .05). Both fear 
and frustration were best predictors of the number of books 
completed (R2 = .17, F(2, 56) = 5.32, p < .05).  

 To answer the third research question, correlation analyses were 
conducted and the results indicated that disgust was statistically 
significantly associated with self-efficacy (r = .34, p < .05). Anger 
was significantly associated with intrinsic goal orientation. 
Contempt, disgust and sad were correlated with extrinsic goal 
orientation (r = .26, p < .05; r = .30, p < .05; r = .28, p <.05, 
respectively). Sad was negatively correlated with students’ prior 
knowledge level (r = -.30, p < .05).  Neutral emotional state was 
correlated with self-regulatory learning strategies (r = .34, p < .05). 
contempt was negatively correlated with both task value and 
students’ prior knowledge level (r = -.34, p < .05; r = .40, p < .001, 
respectively). 

5. CONCLUSION 
This study used an automated real-time facial data to address 
empirical yet complementary questions about the relationship 
between learner traits, facial behavior, emotions and learning 
performance with a complex educational game.  

The results highlighted the strong statistical relationship between 
specific facial movements and learning outcomes. For instance, 
outer brow raising (AU2) was a strong predictor of the number of 
questions answered incorrectly in the concept matrix, the number 
of attempts made to answer the questions and the number of times 
the wrong answers being corrected before submitting the correct 
answers to the system. In past research, AU2 was identified as 
being associated with frustration, surprise and anxiety [6, 18]. 
According to Ekman, Friesen and Hager [13], the combination of 
AU2 and AU4 was called “fear brow”. When an individual is 
confronted with obstacles (e.g., consistently asked to correct the 
wrong answers in the concept matrix) and does not know how to 
proceed, s/he is more likely to experience negative emotions (e.g., 
frustration). We also found that lip tightening (AU23) and lip 
pressing (AU24) were strong predictors of the number of books 
completed by each student. However, lip tightening and lip 
pressing can be interpreted as a state of deep concentration. As 
students were working on the reading materials in CYSTAL 
ISLAND and knowing that they have to answer the concept matrix 
questions right after the completion of each reading, they were 
more likely to engage in a deep level of incoming information 
processing and activate short-term memory.  
Furthermore, this study also investigated the relationship between 
learner trait variables, emotion and facial expressions. One of the 



interesting findings was the correlational relationship between 
neutral state and students’ self-regulatory learning strategy use. 
Past research suggested neutral state can be treated as a state of flow 
or cognitive equilibrium. However, there remains much to discover 
about the role of neural or confusion facial expressions in learning-
centered emotions.  
This study is the first of its kind to provide empirical evidence for 
relations between facial movements, emotions, learner traits and 
learning performance within a narrative video game environment, 
using a person-centered approach. As researchers have suggested, 
the automated facial action unit tracking allows for close 
examination of persistent cognitive-affective states [18], it is hoped 
that the present study will help us develop a deeper understanding 
of the role of facial behavior in learning, and therefore, to help us 
create more emotionally-adaptive learning environments. 
 

 
Figure 1. Screenshot of CRYSTAL ISLAND interface: 

Infirmary interface. 
 
 

 
 

Figure 2. Real-time emotion classification of facial expression 
while learning with CRYSTAL ISLAND using the automatic 

facial expression monitoring software (iMotions® n.d.) 
 

6. ACKNOWLEDGMENTS 
Our thanks to the North Carolina State University Center for 
Educational Informatics (Dr. James Lester and his research team) 
for allowing us to use the CRYSTAL ISLAND game-based 
learning platform and for their technical support in the process of 
data collection. We would also like to thank our research assistants 
(Ruilin Li, Esther Zheng and Anna Chu) for helping us with the 
initial data preparation.   

7. REFERENCES 
[1] Azevedo, R., Taub, M., Mudrick, N., Farnsworth, J. and 

Martin, S. A. 2016. Interdisciplinary Research Methods Used 
to Investigate Emotions with Advanced Learning 
Technologies. In M. Zembylas and P.A. Schutz (Eds.), 

Methodological Advances in Research on Emotion and 
Education. doi:10.1007/978-3-319-29049-2_18 

[2] Calvo, R. A. and D’Mello, S. 2010. Affect detection: An 
interdisciplinary review of models, methods, and their 
applications. IEEE Transactions on Affective Computing, 
1(1), 18-37. 

[3] Conati, C. and Maclaren, H. 2009. Empirically Building and 
Evaluating a Probabilistic Model of User Affect. User 
Modeling and User-Adapted Interaction, 19, 267-303. 

[4] Csikszentmihalyi, M. 1990. Flow: The psychology of optimal 
experience. New York, NY: HarperCollins. 

[5] D' Mello, S. K. 2013. A Selective Meta-analysis on the 
Relative Incidence of Discrete Affective States during 
Learning with Technology, Journal of Educational 
Psychology, 105(4), 1082-1099. 

[6] D’Mello, S. and Graesser, A.C. 2010. Multimodal semi-
automated affect detection from conversational cues, gross 
body language, and facial features. User Modeling and User 
Adapted Interaction, 20 (2), 147-187. 

[7] D’Mello, S. and Graesser, A.C. 2011. The Half-Life of 
Cognitive-Affective States during Complex Learning. 
Cognition and Emotion, 25(7), 1299-1308. 

[8] D’Mello, S. K., and Kory, J. 2015. A review and meta-
analysis of multimodal affect detection systems, ACM 
Computing Surveys, 47(3). 

[9] Duncan, T. G. and McKeachie, W. J. 2005. The making of 
the Motivated Strategies for Learning Questionnaire. 
Educational Psychologist, 40(2), 117-128. 

[10] Ekman, P. and Friesen, W.V. 1978. Facial Action Coding 
System. Palo Alto: Consulting Psychologist Press. 

[11] Ekman, P. 1970. Universal facial expressions of emotion. 
California Mental Health Research Digest, 8(4).  

[12] Ekman, P. 1999. Basic emotions. In T. Dalgleish and M. 
Power (Eds.). Handbook of Cognition and Emotion. Sussex: 
John Wiley & Sons. 

[13] Ekman, P., Friesen, W. V. and Hager, J.C. 2002. Facial 
Action Coding System: Investigator’s Guide. A Human Face. 

[14] Geller, T. 2014. How do you feel? Your computer knows. 
Communications of the ACM, 57(1), 24-26. 

[15] Gentsch, K., Grandjean, D. and Scherer, K. R. 2014. 
Coherence explored between emotion components: Evidence 
from event-related potentials and facial electromyography. 
Biological Psychology, 98, 70-81. 

[16] Gentsch, K., Grandjean, D. and Scherer, K. R. 2015. 
Appraisals generate specific configurations of facial muscle 
movements in a gambling task: Evidence for the component 
process model of emotion. PLoS ONE, 10(8). 
doi:10.1371/journal.pone.0135837 

[17] Grafsgaard, J. F., Fulton, R. M., Boyer, K. E., Wiebe, E. N. 
and Lester, J. C. 2012. Multimodal Analysis of the Implicit 
Affective Channel in Computer-Mediated Textual 
Communication. Proceedings of the 14th ACM international 
conference on Multimodal interaction, 145-152. 
doi>10.1145/2388676.2388708 

[18] Grafsgaard, J. F., W. J. B., Boyer, K., Wiebe, E. N. and 
Lester, J. C. 2013. Automatically recognizing facial 
indicators of frustration: A learning-centric analysis. 2013 



Humaine Association Conference on Affective Computing 
and Intelligent Interaction, 159-165. 
doi>10.1109/ACII.2013.33 

[19] Grafsgaard, J., Boyer, E. K. and Lester, J. C. 2011. 
Predicting Facial Indicators of Confusion with Hidden 
Markov Models. Proceedings of the Fourth International 
Conference on Affective Computing and Intelligent 
Interaction, (Memphis, the United States, October 09 – 12, 
2011). 97-106. 

[20] Grandjean, D. and Scherer, K. R. 2008. Unpacking the 
cognitive architecture of emotion processes. Emotion, 8, 341-
351. 

[21] Harley, J. M. 2014. Measuring Emotions with an Agent-
based Learning Environment (Doctoral dissertation). 
Retrieved from McGill University Library. 

[22] Immordino-Yang, M. H. and Damasio, A. 2007. We feel, 
therefore we learn: The relevance of affective and social 
neuroscience to education. Learning Landscapes, 5(1), 115-
131. 

[23] Kodra, E., Senechal, T., McDuff, D. and Kaliouby, R. 2013. 
From Dials to Facial Coding: Automated Detection of 
Spontaneous Facial Expressions for Media Research. 
Proceedings 2013 IEEE International Conference.  

[24] Lester, J. C., McQuiggan, S. W., & Sabourin, J. L. (2011). 
Affect recognition and expression in narrative-centered 
learning environments. In R. A. Calvo & S. D’Mello (Eds.), 
New perspectives on affect and learning technologies. New 
York, NY: Springer. 

[25] Mandler, G. 1975. Mind and Emotion. New York, NY: 
Wiley.  

[26] Pekrun, R., Elliot, A. J. and Maier, M. A. 2006. Achievement 
goals and discrete achievement emotions: A theoretical 
model and prospective test. Journal of Educational 
Psychology, 98(3), 583-597. 

[27] Pekrun, R. and Linnenbrink-Garcia, L. 2012. Academic 
emotions and student engagement. In S. L. Christenson, A. 
L. Reschly & C. Wylie (Eds.), Handbook of emotions in 
education. New York, NY: Routledge. 

[28] Pintrich, P. R. 2000. The role of goal orientation in self-
regulated learning. In M. Boekaerts, P. R. Pintrich & M, 
Zeidner (Eds.), Handbook of self regulation. New York: 
Academic Press. 

[29] Rodrigo, M. M. T. and Baker, R. S. J. D. 2011. Comparing 
learners’ affect while using an intelligent tutor and an 
educational game. Research and Practice in Technology 
Enhanced Learning. 6 (1), 43–66. 

[30] Sabourin, J. L. and Lester, J. C. 2014. Affect and 
engagement in game-based learning environments. IEEE 
Transactions on Affective Computing, 5(1), 45-56.  

[31] Scherer, K. R. 2009. The dynamic architecture of emotion: 
Evidence for the component process model. Cognition and 
Emotion, 23(7). 1307-1351. 

[32] Scherer, K. R. 2000. Emotions as episodes of subsystem 
synchronization driven by nonlinear appraisal processes. In 
M. D. Lewis, & I. Granic (Eds.), Emotion, Development, and 
Self-organization: Dynamic Systems Approaches to 
Emotional Development. New York: Cambridge University 
Press. 

[33] Scherer, K. R. 2009. The dynamic architecture of emotion: 
Evidence for the component process model. Cognition and 
Emotion, 23 (7), 1307-1351. 

[34] Sabourin, J. L. and Lester, J. C. 2014. Affect and 
engagement in game-based learning environments. IEEE 
Transactions on Affective Computing, 5(1), 45-56. 

[35] Terzis, V., Moridis, C. N. and Economides, A. A. 2013. 
Measuring instant emotions based on facial expressions 
during computer-based assessment. Journal of Personal and 
Ubiquitous Computing, 17(1), 43-52. doi>10.1007/s00779-
011-0477-y 

[36] Whitehill, J., Bartlett, M. and Movellan, J. 2008. Automatic 
facial expression recognition for intelligent tutoring systems. 
2008 IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition Workshop, 1-6. doi: 
10.1109/CVPRW.2008.4563182 

 

 



