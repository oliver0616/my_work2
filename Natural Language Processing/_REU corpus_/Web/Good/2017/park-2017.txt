
Detecting Changes in Student Behavior
from Clickstream Data

Jihyun Park
Department of Computer Science

University of California, Irvine
Irvine, CA 92697

jihyunp@ics.uci.edu

Kameryn Denaro
Teaching and Learning

Research Center
University of California, Irvine

Irvine, CA 92697
kdenaro@uci.edu

Fernando Rodriguez
School of Education

University of California, Irvine
Irvine, CA 92697

fernanr1@uci.edu

Padhraic Smyth
Department of Computer Science

University of California, Irvine
Irvine, CA 92697

smyth@ics.uci.edu

Mark Warschauer
School of Education

University of California, Irvine
Irvine, CA 92697

markw@uci.edu

ABSTRACT
Student clickstream data can provide valuable insights about
student activities in an online learning environment and how
these activities inform their learning outcomes. However,
given the noisy and complex nature of this data, an on-
going challenge involves devising statistical techniques that
capture clear and meaningful aspects of students’ click pat-
terns. In this paper, we utilize statistical change detection
techniques to investigate students’ online behaviors. Us-
ing clickstream data from two large university courses, one
face-to-face and one online, we illustrate how this method-
ology can be used to detect when students change their pre-
viewing and reviewing behavior, and how these changes can
be related to other aspects of students’ activity and perfor-
mance.

CCS Concepts
•Information systems? Data mining; Web log anal-
ysis; •Computing methodologies ? Machine learn-
ing approaches; •Applied computing? Learning man-
agement systems;

Keywords
Student clickstream data; Change detection; Regression; Pois-
son models

1. INTRODUCTION
One of the major goals in educational data mining (EDM)

is to use student clickstream data to describe and under-
stand students’ behavioral patterns. While past findings

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

LAK ’17, March 13-17, 2017, Vancouver, BC, Canada
c© 2017 ACM. ISBN 978-1-4503-4870-6/17/03. . . $15.00

DOI: http://dx.doi.org/10.1145/3027385.3027430

have advanced our understanding what we can learn from
clickstream data, one of the remaining challenges involves
devising statistical techniques that help us identify students
who are changing behavior in the middle of a term. There
are a number of reasons motivating this problem; one is
to identify students who are in need of assistance during
the course, another is to identify reasons that students are
changing their behavior so that a course could be improved
overall. The analysis of clickstream data within a course can
also provide invaluable information to course instructors and
to education researchers, and there is a need to be able to
both summarize and visualize the results in a straightfor-
ward manner.

In this paper we will focus on clickstream data from two
courses at a large university: one face-to-face course and one
online course, both from the 2015-2016 academic year. For
each course, clickstream data is obtained through a course
management system in the form of {student ID, time stamp,
activity}. The types of activities recorded correspond to
broad categories of student behavior, such as previewing
lecture notes, submitting assignments, or posting and re-
sponding to discussion board questions. For instance, one
of the courses we examine in this paper had 377 registered
students who generated approximately 380,000 click events
over a 10-week period. Figure 1 displays each of the indi-
vidual student clickstreams over the 85 days of the course,
with each row corresponding to a student. While the plot
shows some general increases in click activities around quiz
and exam dates, it is not easy to see much else, nor to un-
derstand how individual student behaviors are related to the
overall population due to significant variability in students’
click patterns. Furthermore, we are unable to determine
whether students change their click behaviors in any signif-
icant way, or whether or not these behaviors are correlated
with course performance.

As discussed in more detail in the next section, student
clickstream data has been the subject of a number of prior
studies, such as the investigation of potential predictive re-
lationships between online student activity and student out-
comes (such as course grades). Here we focus instead on
detecting changes in individual student activity over time,
relative to the activity of the class of a whole. In particular

http://dx.doi.org/10.1145/3027385.3027430


DAYS

S
T
U
D
E
N
T
S

0

1

Figure 1: A plot of student clickstream activity in
the 10-week face-to-face course over time, where
each row represents an individual student and each
column represents a day. A black marker in cell i, t
indicates clickstream activity for student i on day t.

we investigate the use of statistical change detection tech-
niques (e.g., [9]) to automatically detect changes in activity
over time for each student. We model the activity of each
student relative to the aggregate activity of all students in
the class and compare two models on a per student basis; a
model where there is no change in student activity versus a
model where there is a significant change in activity at some
unknown point during the period of the course. Likelihood-
based techniques are used to fit both models on a per student
basis and model selection criteria is implemented in order to
determine whether each student is best modeled under the
“change” or “no-change” model.

The paper proceeds as follows. In Section 2 we discuss re-
lated work. Section 3 outlines the change-detection method-
ology that we propose, and Section 4 provides illustrative re-
sults on simulated data sets. Section 5 discusses the course
data sets that provide an illustration of the methods dis-
cussed in Section 3 and Section 6 describes the results of
applying our change-detection methodology to these data
sets. The paper concludes with discussion and conclusions
in Section 7. The primary novel contribution of this work is
the development of a systematic quantitative approach for
detecting significant changes in a student’s clickstream over
time.

2. RELATED WORK
Clickstream data analysis in an educational setting has fo-

cused on what the clickstream can say about the students in
terms of learning behavior through a variety of features de-
rived from the clickstream. Much of the prior work on click-
stream data analysis for understanding student behavior has
occurred in the context of Massive Open Online Courses
(MOOC) setting. Many of these analyses have focused on
using the clickstream data to predict MOOC completion (for
example in [5]) and to predict learning outcomes within a
MOOC. For example, the relationship between the number
of posts and the learning gains of the students [16] has been
investigated, as well as how discussion forum views are po-
tentially related to learning outcomes [1]. There has also
been research focused on improving predictions of learning
outcomes by incorporating clickstream events as well as sum-

maries of the clickstream [3].
A secondary research topic has focused on describing stu-

dents with similar clickstreams (e.g., [15]), the activities that
the students are engaging in, and in understanding the stu-
dent’s typical online interaction within a class. As an exam-
ple, clickstream data analysis was used to better understand
whether or not students were following a defined learning
path [6]. In other work, students’ clickstreams were grouped
into similar plans of action to better understand learning
pathways [14]; how discussion forums and other activities in
the MOOC were related to country and culture [12]; and ex-
amined whether engagement on discussion forums increased
based on the type of video a student watched [2]. All of
these clickstream analyses have an underlying goal of de-
scribing student behaviors through the clickstream and to
draw meaningful conclusions about those students.

MOOCs are typically used by people as a way to learn
new skills or keep up-to-date with current ones. Because
most MOOCs do not offer formal degrees, there are no se-
rious consequences for doing poorly or dropping out. In
contrast, college course grades determine whether students
succeed or fail (whether they advance to the next course, re-
main in their intended major, or graduate). Thus, findings
from MOOC clickstream studies cannot offer broad expla-
nations about student learning experiences in higher educa-
tion settings. So while MOOCs and college courses share
some similarities, in terms of course management systems
and clickstream data, studying college courses may require
a different set of goals and statistical techniques.

For instance, one important area of higher education re-
search focuses on student engagement. Studies find that
students who are not engaged with the learning process—
that is, students who do not put in the time and energy into
purposeful learning—are at greater risk for failing courses
and dropping out of college [10]. While this finding is not
new, understanding how to quickly identify these students,
especially at the course-level, remains a significant challenge.

Clickstream data has the potential to address this since
the data is obtained in real time. Researchers can provide
instructors with immediate insights how students are engag-
ing with the course management system. This is especially
important in courses with large enrollments, where problems
with student engagement can often go unnoticed [13]. Some
recent work has found that student engagement with the
course management system, as indexed by number of days
students visited the site relative to their peers, was positively
related to course outcomes [11]. Our work adds to this area
of research by using statistical change detection techniques
to further understand course engagement.

More broadly, changepoint detection techniques for event
time-series is a widely studied topic and a variety of statis-
tical methodologies have been developed (e.g., [7, 9]), with
much of this work focused on single (univariate) time-series.
Web user behavior has been analyzed to detect changes in
an individual’s behavior, to report“interesting”sessions, and
to detect changes in user activity [8]. There has not been
any prior work (to our knowledge) on change detection ap-
plied to multiple clickstreams of students in an educational
setting.

Thus far, previous work in the analysis of clickstream data
in an educational setting has focused on grouping students
into similar groups, understanding possible dropout, pre-
dicting student success in a course, and defining learning



pathways. Our goal is to add to the current body of research
in a meaningful way by using changepoint detection tech-
niques as a proxy for understanding student engagement.
By detecting whether student behavior changes in a signif-
icant manner over the time-period of a particular term, we
hope to identify students who increase, decrease, or show
no change in their clickstream activities, and whether these
changes relate to course performance.

3. METHODOLOGY
We discuss below our approach for modeling and change

detection of student activity. We begin by defining some
general notation and then introduce two different models:
a Bernoulli model for binary data and a Poisson model for
count data. The section concludes with a description of
changepoint detection for both of these models.

3.1 Notation
Let N be the number of individual students in a course

where i is an index that refers to an individual student in
the class, i = 1, . . . , N . We will assume below that time is
discrete1 with T discrete time-points and t = 1, . . . , T being
an index running from the first to the last time-period of
clickstream logging for the course. Below we will refer to
t on a daily time-scale for convenience but in general other
time-periods–such as days or weeks–could be used.

Let X be the observed data for a course, represented as an
N×T array whose entries are counts xit ? {0, 1, 2, ....}. Note
that xit represents the number of click events for student i on
day t, where 1 ? i ? N and 1 ? t ? T . We will also consider
a binarized version of the data x?it = I(xit > 0), where I()
is an indicator function (as in Figure 1 for example). The
number of clicks xit (counts) by student i on a given day
t in principle contains more information than the binarized
version x?it, but could also be quite noisy in the sense that
more clicks might not necessarily correlate well with relevant
student activity. We explore both options since the choice
of looking at a count versus the binarized version in practice
will depend on the context of a particular analysis.

3.2 Bernoulli Models for Binary Data
For the binary data, x?it, let ?it be the probability that

each student i is active on day t (i.e., the probability that
student i generates one or more clicks on day t). The log-
odds of ?it is modeled as:

log
?it

1? ?it
= µt + ?i (1)

where µt, t = 1, . . . , T can be viewed as a time-varying popu-
lation mean for the log-odds and ?i, 1 ? i ? N is a student-
dependent offset to account for individual-level variation in
student behavior.

The role of ?i in this model is to modulate the time-
varying population mean µt in a student-specific manner.
A positive value of ?i for student i will increase the log-
odds above the population mean µt, which in turn means
that student i tends to click more than the mean student as
represented by µt. A negative value of ?i has the opposite
effect; student i has a lower probability of clicking compared

1A changepoint methodology using a continuous-time model
could in principle also be developed in a manner similar to
the discrete-time methodology we describe in this paper.

0 10 20 30 40 50 60 70 80

DAYS

0.0

0.2

0.4

0.6

0.8

1.0

F
R

A
C

T
IO

N
 O

F
 S

T
U

D
E
N

T
S

Figure 2: Proportion of students who click each day
during a 10-week course.

0 10 20 30 40 50 60 70 80

DAYS

0

5

10

15

20

25

30

35

40

A
V

E
R

A
G

E
 N

U
M

B
E
R

 O
F
 C

LI
C

K
S

EXAM

Figure 3: Average number of click events per stu-
dent each day during a 10-week course.

to the average student. µt represents time-varying popula-
tion behavior on a log-odds scale.

Our approach to change detection relies on modeling each
student’s activity relative to that of the overall student pop-
ulation in the class. This population (or background) rate µt
will typically vary significantly as a function of time t since
student behavior is strongly affected by temporal effects such
as days of lectures, weekday versus weekend effects, assign-
ment deadlines, exams, and so on. As an example, Figure 2
shows the proportion of students who clicked on a file each
day, summarizing the data shown earlier in Figure 1.

Modeling the log-odds as a linear function (Equation 1)
is a standard technique in generalized linear modeling and
ensures that the resulting probability ?it above lies between
0 and 1, i.e., Equation 1 above can be rewritten as

?it =
1

1 + e?(µt+?i)
. (2)

3.3 Estimation of Model Parameters
The parameters µ = {µ1, . . . , µt} and ? = {?1, . . . , ?N}

are estimated from the N × T data array X ? with entries
x?it ? {0, 1}, 1 ? i ? N, 1 ? t ? T . Since the x?it’s are
binary the likelihood for each individual data point x?it can
be written as:

L(µ, ?|x?it) = ?
x?it
it (1? ?it)

(1?x?it), (3)

where ?it is defined in Equation 2. The likelihood of the full
data set X ? is then defined as:

L(µ, ?|X ?) = P (X ?|µ, ?)

=

N?
i=1

T?
t=1

?
x?it
it (1? ?it)

(1?x?it). (4)

Here we make the assumption that the observed data for



each student on each day is conditionally independent of all
other observations (for students and for days) given the pa-
rameters µ and ?. This is a simplification since it ignores
(for example) possible time-varying trends in student be-
havior. Nonetheless, as we will see later in the experimental
results it provides a useful basis for change detection.

We use a two-stage procedure for parameter estimation as
follows2. We first generate an estimate µ?t for the population
mean as follows:

µ?t = log
q?t

1? q?t
, 1 ? t ? T (5)

where q?t =
1
N

?N
i=1 x

?
it, which is the proportion of students

(across all students) that generated a click on day t.
In the second step, we fit a regression model for each stu-

dent i in Equation 1 with the population mean µ?t set as an
offset. ?i can be thought of as a student-specific intercept
term for each student i.

3.4 Poisson Models for Count Data
We can also model the counts xit directly, where xit can

have values {0, 1, 2, ...}. A natural model in this context is
the Poisson model.

We develop the count model in a manner similar to that
for binary case earlier. In particular, we model the loga-
rithm of the mean of the Poisson distribution, log ?it as a
linear function of a time-varying population rate µt and an
individual student effect ?i:

log ?it = µt + ?i. (6)

Note that although for convenience we use the same nota-
tion, µ and ?, for our two sets of parameters, and they play
an analogous role as their “namesake” parameters in the bi-
nary model, these parameters are different from those in the
binary model described earlier.

Figure 3 shows the average number of click events for each
student per day, reflecting the type of time-varying popula-
tion behavior that µt is intended to capture. The red dashed
lines are the dates for the three midterms and the final, and
we can see much more click activities right before the exam
dates.

We can write the likelihood function for a single count xit
as

P (xit|µt, ?i) =
?xitit e

??it

xit!
, (7)

where ?it is defined in Equation 6. As with the binary
case, assuming that the observations xit are conditionally
independent given the parameters, the full likelihood can be
written as:

L(µ, ?|X) = P (X|µ, ?)

=

N?
i=1

T?
t=1

?xitit e
??it

xit!
. (8)

We again make a conditional independence assumption
for the Poisson model. A two-stage parameter estimation
process is carried out as before. In the first step we estimate
µ?t as follows:

µ?t = log m?t, 1 ? t ? T (9)
2The estimation could be done in a single-step; we would
expect similar results to what we obtain in the two-step
approach.

where m?t =
1
N

?N
i=1 xit, representing the average number

of click events across the population that were generated on
day t. In the second step we fit a Poisson regression model
for each student i as in Equation 6 with an offset µ?t to get
an estimate for each ?i.

3.5 Detecting Changes in Activity
To detect changes in activity we allow for the possibility

that each student’s activity rate changes at some unknown
time point during the course. The proposed approach that
we describe below works in the same manner for both the
Bernoulli binary model and the Poisson count model, the
only difference being in how the likelihood is defined and
the parameters are estimated for each (as described earlier).
For simplicity, the reader can assume below that we are
using either the Bernoulli or Poisson model, and the issue is
whether to fit a model with a change or with no change.

We fit two different models for each student i. The first
model is the one where we assume that the student’s rate
of activity ?i, defined relative to the background activity
µt, does not change over time. In the second model, the
changepoint model, we assume that a student’s activity rate
switches at some unknown changepoint. We fit both models
to the data for each student and use a data-driven model
selection technique to select which model is justified given
the observed data.

In the changepoint model we assume that there is one ac-
tivity rate ?i1 for student i before changepoint ?i and a dif-
ferent activity rate ?i2 after the changepoint ?i. The change-
point model for binary data (for example) can be written as
follows, where I is an indicator function:

log
?it

1? ?it
= µt + ?i1I(t < ?i) + ?i2I(t > ?i) (10)

with a similar definition for the Poisson model. We can
interpret this model as fitting two regression models with
different means on either side of the changepoint.

The value of the changepoint ?i for each student is un-
known. Since time t is discrete the values of ?i can take
one of T ? 1 possible values, corresponding to the T ? 1
boundaries between the T observation times.

In effect this changepoint model has 3 parameters (assum-
ing µt is known): the two activity rates and the changepoint.
We generate maximum likelihood estimates of these param-
eters by maximizing the log-likelihood defined as follows (for
each student i)

li(?i1, ?i2, ?i, µ)

=
?
t<?i

logP (x?it|?i1, µt) +
?
t>?i

logP (x?it|?i2, µt) (11)

(with a similar equation for counts xit and the Poisson model).
To fit this model, we use a similar two-stage approach as

for the model with no-change described earlier. In the first
stage we fit the background rate µt using the data across all
students, in the same manner as for the no-change model.
In the second stage we find the values ?i1, ?i2, ?i, for each
student i, that maximize the log-likelihood defined above.
Since ?i is discrete we can reduce the optimization problem
to finding the values of ?i1 and ?i2 for a fixed ?i and then
iterate over the T ? 1 possible values of ?i. For each fixed
value of ?i, the log-likelihood splits into the two parts on
the right-hand side of Equation 11 above, a log-likelihood
term containing ?i1 and a second log-likelihood term con-



STUDENT1

0 10 20 30 40 50 60 70 80 90

DAYS

STUDENT2

Figure 4: Simulated activity data for two students.

taining ?i2. Each can be optimized independently using the
same procedure described earlier for estimating ?i for the
no-change model.

For each student i, once the parameters of both the no-
change and the changepoint models have been estimated,
we select the best model from the two candidate models.
The likelihood (or log-likelihood), evaluated at the maxi-
mum likelihood values of the parameters, is not useful for
model selection since the changepoint model will always have
a likelihood value that is at least as high as the no-change
model (this is because the changepoint model contains the
no-change model as a special case).

There are a variety of model selection techniques in the
statistical literature to handle the issue of how to fairly
compare models (in the case where models have different
numbers of parameters) including techniques such as penal-
ized likelihood, Bayesian criteria, and cross-validation [4].
In the results in this paper we use the Bayesian Informa-
tion Criterion (BIC) which is a well-established and easily
interpretable method for model selection. The BIC score is
defined for each student as

BICiM = ?2liM + pM log T (12)

where M indicates a particular model (M = 1 corresponds
to the no-change model, and M = 2 corresponds to the
changepoint model), liM is the log-likelihood for model M
for student i’s data evaluated at the maximum likelihood
values of the parameters, pM is the number of parameters
in each model (p1 = 1, p2 = 3, for the no-change and change-
point models respectively)3, and T is the number of obser-
vations per student. The second term in the BIC, pM log T ,
can be interpreted as a penalty for having additional param-
eters in a model.

The BIC method selects the model with the lowest BIC
score for each student. In particular, in the context of our
changepoint application, we can use BIC to detect if there
is evidence that a student’s rate of activity changed, i.e., if
BICi2 < BICi1 then the evidence supports the changepoint
model over the no-change model for student i.

4. RESULTS FOR SIMULATED DATA
To illustrate how the change-detection methods work, we

simulated daily binary time-series of student click activity
for 400 students over 85 days (numbers that are roughly sim-
ilar to the larger of the two classes we analyze later in the
paper). The true population rate µt switched between two
different values over time, one with a high rate and one with

3Technically we should also count the background model
parameters µ here, but since this is the same for both models
we can omit it.

0 5 10 15 20 25 30

DAYS

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

¼?
it

POPULATION
STUDENT1 ®i=0.7

STUDENT2 ®i=-1.52

Figure 5: Estimated activity probabilities (??it) for
the two simulated students and for the population.

0 10 20 30 40 50 60 70 80
?3.5

?3.0

?2.5

?2.0

?1.5

?1.0

?0.5

0.0

0.5

1.0

¹?
t
+
®?
i

M1, BIC=82.43

M2, BIC=75.74

0 10 20 30 40 50 60 70 80

DAYS

RAW DATA

DETECTED CP

TRUE CP

Figure 6: Log-odds of ??it, for M1 and M2, and simu-
lated data of a student with a changepoint at t = 57.

a low rate. The variability in the simulation roughly corre-
sponds to what we observed in the real student data. The
offsets, ?ij , for each student were sampled independently
from a normal distribution; ?ij ? Normal(0, ? = 1.5). Half
of the students were simulated with one ?i1, i.e. no change
in behavior over time. The other half of the students had
two different offsets sampled, ?i1 and ?i2, on either side of
a changepoint ?i which was sampled independently from a
uniform distribution; ?i ? U(15, 70).

Figure 4 is a plot of binary data for two simulated students
who did not have changepoints. Student 1 is much more
active than Student 2, and therefore Student 1 is going to
have a larger estimated value for ?i. The estimated ?it’s for
these students over the first 30 days (time t = 1, ..., 30) is
shown in Figure 5. The plot illustrates how the estimated
activity varies relative to the population probability µt (the
red solid curve). The more active student (green dashed
line) has higher probabilities of clicking over time, while the
less active student (blue dotted line) has lower probabilities,
and both probabilities rise and fall relative to the behavior
of the population. For example, when student activity on
average rises on a particular day such as day 10 (e.g., due to
an assignment), the click probability for both students rises.

Next we show the results of two different simulated stu-
dents, one with a changepoint and the other without a change-
point. Figure 6 is a plot from a student with a changepoint,
with the raw data in the lower plot and the fitted model
(plotted on a log-odds scale) in the upper plot. There is a
clear change in student behavior around day 59, and this
is visible both in the raw data (lower plot) and the fitted



0 10 20 30 40 50 60 70 80
?2.5

?2.0

?1.5

?1.0

?0.5

0.0

0.5

¹?
t
+
®?
i

M1, BIC=86.96

M2, BIC=94.55

0 10 20 30 40 50 60 70 80

DAYS

RAW DATA

DETECTED CP

Figure 7: Log-odds of ??it, for M1 and M2, and sim-
ulated without a changepoint.

changepoint model (top plot). The BIC for the model with
the changepoint (M = 2) is significantly smaller than that of
a model without the changepoint (M = 1) for this simulated
student’s data, i.e., the BIC method was able to successfully
detect that a model with a changepoint is preferred over a
model with no changepoint for this data. In contrast, Figure
7 displays the results for a simulated student with no change-
point. Both plots show the results of fitting the changepoint
model M2. The changepoint model puts a change at day 63,
but the BIC method selects the no-changepoint model since
the BIC for no change M = 1 is much smaller than that of
the changepoint model M = 2.

The BIC method for binary simulated data reliably de-
tected changepoints when the magnitude of the change in
?i1 and ?i2 (before and after the changepoint ?i) was rel-
atively large, but as the change became smaller it became
more conservative. Out of the 400 simulated cases, BIC de-
tected a change in 100 cases, with a precision of 91% (91
out of the 100 detected were true changes) and a recall of
46% (91 out of the 200 true changes were detected). The
remaining 54% of true changes had much lower magnitude
changes (0.96 on average) compared to the detected cases
(magnitude 2.61 change on average).

5. CLICKSTREAM DATA SETS
The clickstream data that we used in our study was recorded

via the Canvas learning management system (LMS). Can-
vas is an open-source LMS that serves as a supplemental
instructional technology for students. It has been adopted
as the campus-wide LMS system by a number of US universi-
ties, including UC Irvine. Students use Canvas to download
course content, take online quizzes, watch videos, and sub-
mit assignments. The most common data available are click-
stream data; every time a student clicks on a URL within the
Canvas LMS, the click is recorded and logged with student
ID, URL, and time-stamp.

Canvas provides an application programming interface (API)
which we used to extract a log of all Canvas clickstream data
for a course in addition to other relevant course data such as
a list of lecture files, pages, and etc. Our Canvas API crawler
for non-clickstream portion of the data is publicly available
on github at https://github.com/dkloz/canvas-api-python.

Table 1: Number of students who showed increase,
decrease, or no change in their activities for each ac-
tivity data type for the 10-week face-to-face course.

Event Type NIncrease NDecrease NNoChange
Preview, binary 7 9 361
Preview, count 112 96 169
Review, binary 39 23 315
Review, count 121 159 97

Two types of student behavior are considered for the stu-
dents described in this paper. The first is a student’s pre-
viewing behavior. An event is defined as a Preview Event
when a student views or downloads a file prior to the event
start date. This could indicate how well a student is per-
forming in terms of being prepared for the course. The sec-
ond type of behavior is related to the students’ reviewing
activities. A Review Event is defined as an event when a
student views or downloads a file after the event end date,
e.g., a student downloading a lecture file after the class in
which the material was covered. We found that focusing on
these two types of events allowed us to screen out less rele-
vant information in the clickstream data and extract more
meaningful information about students’ activities. While
in this paper we will focus on events related to our defini-
tions of previewing and reviewing activity, our methodology
for change detection is applicable to arbitrary sets of click-
stream events.

Extracting each of the previewing and reviewing events
results in an activity matrix of size of N × T matrix, where
the cell i, t indicates that the number of previewing or re-
viewing events by student i on day t. The data can be
binarized to create a binary representation for the Bernoulli
model described in section 3.2.

We used data sets from two courses at UC Irvine in our
study, both offered during the 2015-2016 academic year. The
first is a face-to-face 10-week course with 377 enrolled stu-
dents. Lectures were held three times a week, and there were
3 midterms and one final exam. Figure 3 shows the average
number of click events on each day per student. There is
significant variation in students’ clicking activity over time.
For example, students tended to be much more active during
days close to the exams (shown as red dashed lines).

The second data set is somewhat different from the first
in that it was an online course offered for 5 weeks. There
were 176 enrolled students in this course. This data set
is significantly smaller than that for the first course both
in terms of the number of students N and the number of
days T . There were 25 video lectures in total and students
were supposed to watch one lecture per day from Monday
through Friday. The final exam was held on campus after
the 5 lecture weeks.

6. EXPERIMENTAL RESULTS
In this section we will discuss the application of our change

detection methodology to the two clickstream data sets de-
scribed in the previous section.

6.1 Example 1: 10-Week Face-to-Face Course
The clickstream data spanned 85 days, which included 10

weeks of instruction as well as activity before and after the
10 weeks. We applied our change-detection methodology to

https://github.com/dkloz/canvas-api-python


0 10 20 30 40 50 60 70 80
DAYS

NInc

NDec

PREVIEW, COUNTS

0 10 20 30 40 50 60 70 80
DAYS

NInc

NDec

REVIEW, COUNTS

Figure 8: Student preview and review activity data over time, for the students who increased or decreased
their behavior in the 10-week face-to-face course. The gray marker at t-th column in each row means that
there was click activity on day t for that student, with darker colors reflecting larger counts (more clicks).

4 different versions of the N × T data matrices: for preview
and review events, in binary and count form. We restricted
changepoints to be in the range from day 10 to day 75,
since changepoint detection at the beginning or end of the
sequences (i.e., outside of this range) tends to be unreliable
due to small sample sizes and not so meaningful in terms of
interpreting actual student behavior.

The students that were considered to have changed by the
BIC scores were categorized into two groups: students who
increased their click activity and students who decreased
their click activity. We will refer to these groups as “In-
creased”and“Decreased”respectively. Note that these terms
should be interpreted in a relative sense, since increase and
decrease is for the ?i coefficient for each student relative to
the background rate µt. Thus, a detected increase for stu-
dent i means in effect that the student is ranked higher in
the class in terms of activity relative to other students after
the changepoint ?i, compared to their rank before ?i (and
conversely for a decrease).

The numbers of students detected as belonging to each
group, for each event type, are shown in Table 1. The Pois-
son count model detects significantly more student changes
than the Bernoulli binary model, for both preview and re-
view event types. This is to be expected since the Poisson
model has more information to work with (and thus has bet-
ter sensitivity) compared to the Bernoulli model which only
sees a binarized version of the daily counts (and thus has
less information per day about student activity). In the dis-
cussion below we focus primarily on the Poisson results with
counts given its better sensitivity.

Figure 8 shows the click data for each of the students for
which a change was detected, with one student per row, and
one plot per type of event (Preview and Review). The stu-
dents are split into two groups within each plot depending on
whether their detected changes were increases or decreases,
and rows were then ordered within each group based on the
chronological location of the changepoint per student. The
changepoint locations are marked in red and the plots show
a clear distinction between the days with more activity and
the days with less activity.

Figure 9 provides a week-by-week summary of the infor-
mation in Figure 8, showing the number of detected student

WEEK1 WEEK3 WEEK5 WEEK7 WEEK9 FINAL

0

2

4

6

8

10

12

P
E
R

C
E
N

T
A

G
E
 O

F
 S

T
U

D
E
N

T
S

PREVIEW COUNTS

INCREASED

DECREASED

EXAM

WEEK1 WEEK3 WEEK5 WEEK7 WEEK9 FINAL

WEEKS

0

2

4

6

8

10

12

14

P
E
R

C
E
N

T
A

G
E
 O

F
 S

T
U

D
E
N

T
S

REVIEW COUNTS

INCREASED

DECREASED

EXAM

Figure 9: Percentage of 10-week face-to-face course
students who increased or decreased within each
week.

changes per week, for each type of event. The vertical lines
that are visible in the two count matrices are the exam dates.
There are some obvious temporal patterns in this data. For
example, the upper plot (preview events) shows that more
than a quarter of the students increased their previewing
activities in the third week, which is the week before the
first midterm. This agrees with the intuition that prior to
the first major exam in a class we would expect to see some
significant shifts in student activity. The lower plot shows
that the most of the changes in reviewing activity happened
towards the end of the quarter, particularly during week 10
before the final exam. Again it makes sense that there are
significant changes across students in their relative rates of
reviewing activity prior to the final exam. We can also see
in both plots that the number of detected changes per week,
for increases and for decreases, are strongly correlated. As
mentioned earlier this is to be expected with this model
since increase and decrease for this model is defined relative
to overall mean population behavior.

We also investigated how detected changes in preview and
review activities were correlated with student outcome in



Table 2: Probability of a student getting a passing
grade (A, B, C) depending on which group the stu-
dent is in.

P (Pass|Inc) P (Pass|Dec) P (Pass)
Probability 0.93 0.76 0.83
?Pass (%) 12.1 -7.4 0

p-value 0.0025 0.0458 -

0 10 20 30 40 50 60 70 80
?5

?4

?3

?2

?1

0

1

2

3

¹?
t
+
®?
i

M1, BIC=261.02

M2, BIC=228.6

0 10 20 30 40 50 60 70 80

DAYS

0

5

10

15

20

25

30

x
it

DETECTED CP

REVIEW COUNTS

Figure 10: Log of ??it from M1 and M2, and the
raw data of a student from the 10-week face-to-face
course. The BIC method selected the model with
changepoint (M2).

terms of the students’ final grades in the class. We calculated
the probability of a student getting a passing grade given
that the student is in the Increased group, P (Pass|Increase),
or in the Decreased group, P (Pass|Decrease), and com-
pared these numbers with the marginal (unconditional) prob-
ability of a student passing P (Pass). For both preview
and review count events we used a two-sided binomial test
with P (Pass) as the null hypothesis to compute p-values for
P (Pass|Increase) and P (Pass|Decrease).

Table 2 shows the results for review count data. At the
0.01 level of significance, P (Pass|Increase) is significant
and P (Pass|Decrease) is significant at the 0.05 level. Stu-
dents in the Increased group have a higher probability of
passing the course, while the students in the Decreased group
have a higher probability of failing. This means that stu-
dents who increased their reviewing behavior (relative to all
of the students in the course), at some point during the quar-
ter, ended up getting better grades on average that those
that did not. For preview counts, the probabilities were also
in the direction of increases in previewing leading to bet-
ter outcomes on average (and vice versa), but these changes
were not statistically significant. This may suggest, for this
particular course, that changes in review activities are better
predictors of student outcomes than preview activities.

Finally, for the 10-week course, we analyzed in more de-
tail the results for two specific students (using their Review
data) to illustrate how the model can be used to interpret
clickstream activity at the individual student level. Fig-
ure 11 illustrates the results for a student where the lower
plot shows the observed daily review clicks, and the upper
plot shows the Poisson models for the no-change model and

0 10 20 30 40 50 60 70 80
?8

?6

?4

?2

0

2

¹?
t
+
®?
i

M1, BIC=184.26

M2, BIC=189.3

0 10 20 30 40 50 60 70 80

DAYS

0

1

2

3

4

5

6

7

x
it

DETECTED CP

REVIEW COUNTS

Figure 11: Log of ??it from M1 and M2, and the
raw data of a student from the 10-week face-to-
face course. The BIC method selected the no-
changepoint model (M1).

Table 3: Number of students who showed increase,
decrease, or no change in their activities for each
activity data type for online course.

Data Type NIncrease NDecrease NNoChange
Preview, binary 6 8 162
Preview, binary 41 40 95
Review, binary 11 6 159
Review, counts 47 66 63

the changepoint model (with a detected change at day 70).
For this student the BIC method preferred the changepoint
model over the no-change model, with BIC2 < BIC1 by
a large margin. This is reflected in the observed data in
the lower plot where the number of counts for this student
increase significantly after the changepoint.

Figure 11 shows the same type of plot for a student where
the BIC method selected the model without the change-
point. From the raw counts (lower plot) it looks like the
student’s activity level could have changed (increased) after
day 68. However, relative to the background activity (par-
ticularly around days 76 to 78, leading up to the final exam)
this student’s activity level is not sufficiently different to the
mean population behavior to justify the additional parame-
ters in the changepoint model, as reflected in the BIC scores
(BIC1 < BIC2).

6.2 Example 2: Online 5-Week Course
The second course we analyzed was a 5-week online sum-

mer course. The event data set for this course we analyzed
was smaller than the first in terms of both the number of
students (N = 176) and number of days with clickstream
activity (T = 50). The course was offered online and the
students were expected to watch a lecture video on every
weekday over the 5 weeks, leading to more uniformity and
less variability in student clickstream activity over time. In
addition, the 10-week class had 3 midterm exams and a final
exam, while the 5-week online class only had a single final
exam at the end of the course.

The numbers of students detected for each of the Increased



0 10 20 30 40
DAYS

NInc

NDec

PREVIEW, COUNTS

0 10 20 30 40
DAYS

NInc

NDec

REVIEW, COUNTS

Figure 12: Student preview and review activity data over time, for the students who increased or decreased
their behavior in the 5-week online course. The gray marker at t-th column in each row means that there
was click activity on day t for that student, with darker colors reflecting larger counts (more clicks).

WEEK0 WEEK1 WEEK2 WEEK3 WEEK4 WEEK5 WEEK6

0

5

10

15

20

25

P
E
R

C
E
N

T
A

G
E
 O

F
 S

T
U

D
E
N

T
S

PREVIEW COUNTS

INCREASED

DECREASED

EXAM

WEEK0 WEEK1 WEEK2 WEEK3 WEEK4 WEEK5 WEEK6

WEEKS

0

5

10

15

20

25

P
E
R

C
E
N

T
A

G
E
 O

F
 S

T
U

D
E
N

T
S

REVIEW COUNTS

INCREASED

DECREASED

EXAM

Figure 13: Percentage of students with detected in-
crease or decrease in activity for each week in the
online 5-week course.

and Decreased groups, for both preview and review events,
are shown in Table 3. We see a similar overall pattern to
that for the 10-week class, namely that the Poisson model
using counts detects considerably more changes than the
Bernoulli method using binary data. The overall propor-
tions of changes detected are roughly similar across both
classes, with about 50% of students having increased or de-
creased count activity relative to the population, for each of
the two types of events. One difference we found between
the two courses was the proportion of students who exhib-
ited no change at all, for either preview or review events:
13% of students in the 10-week course and 25% in the 5-
week courses. This difference might be due to the interme-
diate exams (3 midterms) in the 10-week course, leading to
more variability in student behavior compared to the 5-week
course which only had a final exam.

The clickstreams for the students with detected changes
are shown in Figure 12. We observe very high activities at
the end of the course session for students in the Increased
group, for both Preview and Review event types. The ma-
jority of the changepoints occur just before the darker area of
the plot. Figure 13 shows that, among the students who had
an increased change that most of them had a changepoint in
the fifth week, which is the last week of the course before the

0 10 20 30 40 50
?8

?6

?4

?2

0

2

4

¹?
t
+
®?
i

M1, BIC=154.65

M2, BIC=142.02

0 10 20 30 40 50

DAYS

0

2

4

6

8

10

12

14

x
it

DETECTED CP

REVIEW COUNTS

Figure 14: Log of ??it from M1 and M2, and the raw
data of a student from the 5-week online course. The
BIC method selected the changepoint model (M2).

final. We did not analyze the relationship of click activity
and course outcomes for this course since fewer than 5% of
the students received grades of D or F in the class, resulting
in a sample size that is too small for reliable inferences.

As with the 10-week class, we examine the results for re-
view events for 2 specific students, to illustrate the method-
ology at the level of individual students. Figure 14 shows the
results for a student where the method detected a change in
activity at day 35. Figure 15 shows the results for a student
where the no-change model was preferred by BIC. Both stu-
dents exhibited increases in their review activities after day
40, but the magnitude of change for the first student is sig-
nificantly greater than that for the second student (as can
be seen in the lower panels of both plots)—relative to the
student population as a whole, the second student did not
exhibit a significant change in activity.

7. CONCLUSIONS AND FUTURE WORK
Student clickstream data is inherently difficult to work

with given its complex and noisy nature. This paper de-
scribed a statistical methodology for detecting changepoints
in such data and illustrated the potential of the approach by
applying the methodology to two large university courses.
The proposed approach is relatively simple and allows for



0 10 20 30 40 50

?15

?10

?5

0

5

¹?
t
+
®?
i

M1, BIC=54.42

M2, BIC=59.3

0 10 20 30 40 50

DAYS

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

x
it

DETECTED CP

REVIEW COUNTS

Figure 15: Log of ??it from M1 and M2, and the
raw data of a student from the 5-week online course.
The BIC method selected the no-changepoint model
(M1).

a number of possible extensions; the development of more
flexible changepoint models (such as systematic drifts in stu-
dent activity levels), allowing for more than a single change-
point, post hoc adjustments for multiple testing, and using
robust estimation techniques for parameters and their re-
spective standard errors. Bayesian methods could also be
potentially useful in this context for both parameter estima-
tion and model selection to more fully reflect uncertainty in
inferences at the individual student level. A useful extension
for educators would be to develop an online detection variant
of the offline approach proposed here, potentially allowing
for identification of at-risk students, instructor feedback, or
interventions while a course is in session.

While the results in this paper are promising and there
are interesting methodological avenues to pursue, the most
important future direction from an education research per-
spective will involve more in-depth investigation of the util-
ity of these types of methods in terms of providing actionable
insights that are relevant to the practice of education.

Acknowledgments
This paper is based upon work supported by the National
Science Foundation under Grants Number 1535300 (for all
authors) and 1320527 (for PS). The authors would like to
thank Sarah Eichhorn, Wenliang He, and Dimitris Kotzias
for their assistance in acquiring and preprocessing of the
clickstream data used in this paper.

8. REFERENCES
[1] Y. Bergner, D. Kerr, and D. E. Pritchard.

Methodological challenges in the analysis of MOOC
data for exploring the relationship between discussion
forum views and learning outcomes. In Proceedings of
the EDM Conference, pages 234–241. International
Educational Data Mining Society (IEDMS), 2015.

[2] S. Bhat, P. Chinprutthiwong, and M. Perry. Seeing
the instructor in two video styles: Preferences and
patterns. In Proceedings of the EDM Conference,
pages 305–312. International Educational Data Mining
Society (IEDMS), 2015.

[3] C. G. Brinton and M. Chiang. MOOC performance
prediction via clickstream data and social learning
networks. In Proceedings of the INFOCOM
Conference, pages 2299–2307. IEEE, 2015.

[4] G. Claeskens. Statistical model choice. Annual Review
of Statistics and its Application, 3:233–256, 2016.

[5] S. Crossley, L. Paquette, M. Dascalu, D. S.
McNamara, and R. S. Baker. Combining click-stream
data with NLP tools to better understand MOOC
completion. In Proceedings of the Sixth International
Conference on Learning Analytics & Knowledge, LAK
’16, pages 6–14. ACM, 2016.

[6] D. Davis, G. Chen, C. Hauff, and G.-J. Houben.
Gauging MOOC learners’ adherence to the designed
learning path. In Proceedings of EDM Conference,
pages 54–61. International Educational Data Mining
Society (IEDMS), 2016.

[7] I. A. Eckley, P. Fearnhead, and R. Killick. Bayesian
Time Series Models, chapter 10 Analysis of
changepoint models, pages 205–224. Cambridge
University Press, Cambridge, 2011.

[8] P. Hofgesang and J. P. Patist. Online change detection
in individual web user behaviour. In Proceedings of
WWW Conference, pages 1157–1158. ACM, 2008.

[9] C. Kirch and J. Tajduidje Kamgaing. Detection of
change points in discrete valued time series. In
R. Davis, S. Holan, R. Lund, and N. Ravishanker,
editors, Handbook of Discrete Valued Time Series,
chapter 11, pages 219–244. Chapman and Hall, 2014.

[10] G. D. Kuh, T. M. Cruce, R. Shoup, J. Kinzie, and
R. M. Gonyea. Unmasking the effects of student
engagement on first-year college grades and
persistence. The Journal of Higher Education,
79(5):540–563, 2008.

[11] C. Learning. Community insights: Emerging
benchmarks and student success trends from across
the civitas. Technical report, December 2016.

[12] Z. Liu, R. Brown, C. Lynch, T. Barnes, R. S. Baker,
Y. Bergner, and D. S. McNamara. MOOC learner
behaviors by country and culture; an exploratory
analysis. In Proceedings of the EDM Conference, pages
127–134. International Educational Data Mining
Society (IEDMS), 2016.

[13] C. Mulryan-Kyne. Teaching large classes at college
and university level: Challenges and opportunities.
Teaching in Higher Education, 15(2):175–185, 2010.

[14] K. H. R. Ng, K. Hartman, K. Liu, and A. W. H.
Khong. Modelling the way: Using action sequence
archetypes to differentiate learning pathways from
learning outcomes. In Proceedings of the EDM
Conference, pages 167–174. International Educational
Data Mining Society (IEDMS), 2016.

[15] G. Wang, X. Zhang, S. Tang, H. Zheng, and B. Y.
Zhao. Unsupervised clickstream clustering for user
behavior analysis. In CHI Proceedings, pages 225–236.
ACM, 2016.

[16] X. Wang, D. Yang, M. Wen, K. R. Koedinger, and
C. P. Rose. Investigating how student’s cognitive
behavior in MOOC discussion forum affect learning
gains. In Proceedings of the EDM Conference, pages
226–233. International Educational Data Mining
Society (IEDMS), 2015.


	Introduction
	Related Work
	Methodology
	Notation
	Bernoulli Models for Binary Data
	Estimation of Model Parameters
	Poisson Models for Count Data
	Detecting Changes in Activity

	Results for Simulated Data
	Clickstream Data Sets
	Experimental Results
	Example 1: 10-Week Face-to-Face Course
	Example 2: Online 5-Week Course

	Conclusions and Future Work
	References


