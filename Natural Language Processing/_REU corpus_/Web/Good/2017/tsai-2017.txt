
Learning Analytics in Higher Education – Challenges and 

Policies: A Review of Eight Learning Analytics Policies 
Yi-Shan Tsai 

The University of Edinburgh 

Edinburgh, United Kingdom 

+44 131 651 6243 

yi-shan.tsai@ed.ac.uk 

Dragan Gasevic 

The University of Edinburgh 

Edinburgh, United Kingdom 

+44 131 651 3837 

dragan.gasevic@ed.ac.uk 

 

ABSTRACT 
This paper presents the results of a review of eight policies for 

learning analytics of relevance for higher education, and discusses 

how these policies have tried to address prominent challenges in 

the adoption of learning analytics, as identified in the literature. 

The results show that more considerations need to be given to 

establishing communication channels among stakeholders and 

adopting pedagogy-based approaches to learning analytics. It also 

reveals the shortage of guidance for developing data literacy 

among end-users and evaluating the progress and impact of learn-

ing analytics. Moreover, the review highlights the need to estab-

lish formalised guidelines to monitor the soundness, effectiveness, 

and legitimacy of learning analytics. As interest in learning ana-

lytics among higher education institutions continues to grow, this 

review will provide insights into policy and strategic planning for 

the adoption of learning analytics. 

CSS Concepts 

?Applied computing ? Education; ?Security and privacy ? 

Human and societal aspects of security and privacy 

Keywords 
Learning analytics, policy, code of practice, challenge, strategy, 

higher education 

1. INTRODUCTION 
While interest in learning analytics remains high among higher 

education institutions, some hesitate to embrace it due to various 

challenges regarding data procurement, institutional capabilities 

and buy-in from relevant stakeholders [13]. There are many unan-

swered questions with respect to the effectiveness and usefulness 

of learning analytics to specific institutional contexts and prob-

lems even for institutions that have taken initiative to adopt learn-

ing analytics [10]. This pervading uncertainty calls for an investi-

gation into challenges faced by higher education institutions in 

their adoption of learning analytics, and into policies that are 

meant to frame the practice of learning analytics and ensure that it 

is effective and appropriate.  

In light of this, this paper first reviews literature about the state of 

learning analytics adoption and related challenges based on 23 

empirical studies of learning analytics (Section 4). This is then 

followed by a review of existing policies for learning analytics 

(Section 5) and a discussion on how the policies have addressed 

the identified challenges (Section 6), so as to provide insights into 

future policy development. In the context of this paper, ‘learning 

analytics policy’ is perceived as a set of guidelines that include 

both legislative regulations and non-legislatives principles for the 

use of learning analytics. Therefore, a code of practice for learn-

ing analytics is also considered a policy in this study.  

2. BACKGROUND 
Educational institutions are complex adaptive systems, which tend 

to be stable and resistant to change due to a range of political, 

social, cultural and technical norms [17]. Therefore, the challenge 

to bring about change in higher education institutions where com-

plex and adaptive systems exist has been described as a ‘wicked 

problem’ [17]. It is suggested that persistent, dedicated and strate-

gic efforts are required for the adoption of learning analytics 

[3][26], which highlights the imperative to analyse challenges 

faced by institutions in their adoption of learning analytics so as to 

develop a comprehensive policy that is visionary, pertinent to the 

context, concurrent to the institution’s mission, and addresses the 

challenges faced by the institution. 

There have been approaches to learning analytics policy develop-

ment, such as the cause-effect framework and the RAPID Out-

come Mapping Approach (ROMA) [17][36]. The former is used 

to identify the relationships between multiple linkages in complex 

systems where step-by-step action is needed to bring about effect 

change. The latter contains seven steps: define (and redefine) 

policy objectives, map political context, identify key stakeholders, 

identify desired behaviour changes, develop an engagement strat-

egy, analyse internal capacity to effect change, and establish mon-

itoring and learning frameworks. This framework is meant to be 

used iteratively and reflectively so as to allow refinement and 

adaptation of goals and solutions. For example, Ferguson and 

others used the ROMA framework to analyse three institution-

wide learning analytics cases, which gives a strategic view of the 

practice and suggests practical steps to consider at each phase of 

the ROMA cycle [13]. 

It is of great importance that institutions adopt learning analytics 

under clear guidelines that are grounded in cultural, social, eco-

nomic and political contexts specific for each institution and are 

based on existing best practices for learning analytics and learning 

theories. However, there has not been much analysis of existing 

learning analytics policies, especially not with respect to how they 

address challenges that have emerged in the empirical literature 

on learning analytics adoption. This paper sheds light on existing 

practices and makes a call for more methodological, comprehen-

sive and coordinated work by the community. 

3. METHODOLOGY 
In order to understand the state of learning analytics adoption and 

Permission to make digital or hard copies of all or part of this work for 

personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 

bear this notice and the full citation on the first page. Copyrights for com-

ponents of this work owned by others than ACM must be honored. Ab-
stracting with credit is permitted. To copy otherwise, or republish, to post 

on servers or to redistribute to lists, requires prior specific permission 

and/or a fee. Request permissions from permissions@acm.org 

LAK '17, March 13-17, 2017, Vancouver, BC, Canada 

Copyright 2017 ACM. ISBN 978-1-4503-4870-6/17/03…$15.00. 
DOI: http://dx.doi.org/10.1145/3027385.3027400  

http://dx.doi.org/10.1145/3027385.3027400


challenges that higher education institutions face in implementing 

learning analytics and how existing learning analytics policies 

have tried to address these challenges, we searched relevant litera-

ture using simple key words including ‘learning analytics’ and 

‘policy or policies’ in journals and databases that were well-

known for their collections of learning analytics related research 

and educational research. Table 1 gives an overview of the 

sources that we have consulted. Some of the journals were includ-

ed at a later stage when we conducted a “snowball” search by 

following up references cited in literature that we included at the 

initial stage. It was also at this stage that we were able to retrieve 

some learning analytics policies.  

Table 1. Sources of the bibliographical research 

Databases SCOPUS, Wiley Online Library, ERIC, ACM, 

IEEE 

Journals Journal of Learning Analytics, Journal of 

Computer Assisted Learning, Journal of Edu-

cational Technology & Society, American 

Behavioural Scientist, Journal of Educational 

Technology & Society, The Information Socie-

ty, Computers & Education 

Proceedings The LAK Conference 

Organisational 

databases 

LACE publications, EDUCAUSE library 

After the first stage of automatic search using key words, we fil-

tered out less relevant literature based on the titles and abstracts, 

using our selection criteria of topics and publication types (Table 

2). We were then able to conduct a ‘snowball’ search with the 

remaining literature, as mentioned earlier, and examine papers 
using the same filter criteria. 

Table 2. Filter criteria for the bibliographical research 

 Topics Types of publica-

tions 

Included Ethics and privacy, poli-

cies, institutional strategies, 

institutional readiness, 

institutional capacities, 

learning analytics, academ-

ic analytics 

Research reports, 

conference proceed-

ings, journal articles, 

book chapters, poli-

cy documents, all 

years of publication, 

English language 

Excluded Affordances of learning 

analytics models and tools, 

interventions on class or 

individual levels, ap-

proaches to analytics, stud-

ies not based in higher 

education institutions 

PowerPoint presen-

tations, blog articles, 

news articles, work-

shops 

While topics included in the filter criteria were exclusive to the 

context of learning analytics, we decided to include literature that 

looked at “academic analytics” if they met two conditions: one of 

our interested topics was covered, and the insights learnt were 

applicable to discussions about learning analytics. In order to get 

an overview of institutional adoption of learning analytics, we 

excluded literature that examined technological frameworks for 

learning analytics or small scales of interventions that were not 

centrally supported by institutions. We also excluded literature 

that explored methods for data analysis and studies that were not 

carried out in the higher education context.  

During the snowball phase of our bibliographical research, we 

identified four existing policies for learning analytics, including 

Jisc’s “Code of Practice” [24], LACE’s “DELICATE checklist” 

[11], LEA’s Box’s “Privacy and Data Protection Policy” [29], the 

UK National Union of Students’ (NUS) “Learning Analytics: A 

Guide for Students' Unions” [18], and the Open University’s “Pol-

icy on Ethical use of Student Data for Learning Analytics” [31]. 

Then we used the Google search engine to look for other learning 

analytics policies using the key words – “learning analytics AND 

policies”, and manually examined individual cases that have been 

studied in the literature that was included in the second phase. We 

found three more institutional policies with these methods: 

Charles Sturt University’s “CSU Learning Analytics Code of 

Practice” [9], Nottingham Trent University’s “Use of Learning 

Analytics to Support Student Success Policy” [21], and the Uni-

versity of Sydney’s “Principles for the Use of University-held 

Student Personal Information for Learning Analytics at the Uni-

versity of Sydney” [25]. 

The bibliographical research finished in July 2016 and rendered 

71 pieces of literature, among which 25 were empirical studies, 38 

were desk studies, and eight were policies for learning analytics. 

We conducted a systematic literature review on 23 empirical stud-

ies after we further filtered out two studies based on the degree of 

relevance to our research interest. The findings of the empirical 

studies are presented in Section 4, while the review of the analysis 

of existing policies is presented in Section 5.  

4. RESULTS – STATE AND CHALLENGES 
IN LEARNING ANALYTICS ADOPTION 

This review identified six prominent challenges among higher 

education institutions in terms of the adoption of learning analyt-

ics. We first present the findings related to the state of learning 

analytics adoption (Section 4.1) and then challenges identified in 

the empirical literature (Section 4.2).  

4.1 The State of Learning Analytics Adoption 
A review of the state of learning analytics shows that interest is 

high among higher education institutions, but adoption remains 

immature. In the context of the US higher education sector, learn-

ing analytics has been used to eliminate impediments to retention 

and student success, and to create personalised learning environ-

ments [6]. However, there is a greater interest in monitoring or 

measuring student progress than predicting learning success or 

prescribing intervention strategies [35]. Moreover, learning ana-

lytics remains an interest rather than a major priority at most insti-

tutions [5].   

A similar phenomenon was observed in the Australian higher 

education sector. A study conducted by Colvin and others re-

vealed that only 2 out of the 32 institutions under study reached 

the advanced stage – having evidence of implementation of multi-

ple interventions or initiatives informed by data [10]. The rest of 

the cases were either at the preparatory stage of learning analytics 

or early stage of implementation.    

The adoption of learning analytics in the UK higher education 

sector is also in its infancy. A survey (N=53) conducted by the 

Heads of e-Learning Forum – which includes heads of 130 UK-

based universities – among their members discovered that 25 

respondents did not implement learning analytics at all, 18 were 

working towards implementation, 9 partially implemented, and 

only 1 fully implemented learning analytics across their institution 



[19]. Moreover, another study that took a qualitative approach to 

investigate the adoption of learning analytics in 12 institutions in 

the UK discovered that few interviewees were willing to claim 

significant outcomes from their learning analytics activities to 

date due to the nascent stage of learning analytics technologies 

and practice [23].  

4.2 Challenges in the Adoption of Learning 
Analytics 
In addition to technical challenges in data and system integration 

[5, 34], several studies have identified challenges related to strate-

gic planning and policy. Six primary challenges have been identi-

fied: 

- Challenge 1: There is a shortage of leadership capabilities to 
ensure that implementation of learning analytics is strategically 

planned and monitored. 

- Challenge 2: There are infrequent institutional examples of 
equal engagement with different stakeholders at various levels. 

- Challenge 3: There is a shortage of pedagogy-based approaches 
to removing learning barriers that have been identified by ana-

lytics. 

- Challenge 4: There are insufficient training opportunities to 
equip end users with the ability to employ learning analytics. 

- Challenge 5: There are a limited number of studies empirically 
validating the impact of analytics-triggered interventions. 

- Challenge 6: There is limited availability of policies that are 
tailored for learning analytics-specific practice to address issues 

of privacy and ethics as well as challenges identified above. 

4.2.1 Challenge 1 – Shortage of Leadership 
Studies found that the maturity of learning analytics is closely 

related to the presence of leadership. Yanosky and Arroway iden-

tified the lack of advanced analytics-based projections and proac-

tive responses to analytics results among the US higher education 

institutions, and claimed that this pattern had not changed since 

2012 [35][8]. They attributed one of the causes to the shortage of 

dedicated leadership in that leadership was associated with higher-

level analytics maturity. The lack of support of key leadership in 

learning analytics activities among higher education institutions in 

the USA was again identified in a recent report published by ED-

UCAUSE [5]. 

In the Australian study, Colvin and others observed a different 

degree of leadership commitment in two clusters of institutions. 

One cluster perceived learning analytics as a vehicle or tool for 

measurement or efficiency gains. The other cluster perceived 

learning analytics as a means to aid reflections on the connection 

between retention and antecedent teaching, learning and student 

experience factors [10]. The first cluster tended to lack reflections 

upon the relationship between implemented strategies and the 

development of organisational capacity for learning analytics, 

whereas the second cluster contained a higher degree of readiness 

factors, more mature strategy development, and more involvement 

of senior leadership. Although this study does not suggest that 

there is a lack of leadership in the adoption of learning analytics 

among Australian higher education institutions, Colvin and others 

stress that a strategic vision that responds to the needs of an or-

ganisation is critical for long-term impact. Furthermore, they ar-

gue that the development of strategic institutional capability is a 

key prerequisite for the growth of implementation capability for 

learning analytics and overall institutional uptake by staff mem-

bers. The latter has been identified as one of the stoppers of learn-

ing analytics in that resistance to change is pervasive in higher 

education institutions [5, 17]. Therefore, the involvement of lead-

ership and visionary implementation with considerations of the 

interests of various stakeholders is imperative to the development 

of analytics. 

4.2.2 Challenge 2 –Shortage of Equal Engagement 
Learning analytics distinguishes itself from academic analytics by 

the learner-centred and learner-concerned nature. However, few 

studies have tried to explore opinions of students regarding the 

use of their data for learning analytics or the impact on their learn-

ing journeys. Among these studies, Slade and Prinsloo’s study 

looked at 300 posts from the University Students’ Consultative 

Forum to understand students’ views regarding the use of their 

data for learning analytics [28]. Similarly, two publications by 

EDUCAUSE present both positive and negative views of students 

at Purdue University regarding the impact of a learning analytics 

project – Course Signals – on their learning [4, 27]. Unfortunate-

ly, neither of these publications offers clear descriptions of the 

methods adopted to collect and analyse student opinions. Another 

study by Drachsler and Greller made an attempt to engage stu-

dents to explore the current level of understanding and expecta-

tions towards learning analytics among relevant stakeholders [11]. 

However, returns of the survey from students were few due to 

possibilities of uneven distribution among the stakeholders and 

exclusive dissemination channels. This study flags the top-down 

bias that exists in current research on learning analytics and the 

need to engage students strategically. 

In addition to proactive engagement with students, sound commu-

nication between other stakeholders to establish common under-

standing of learning analytics and institutional readiness still 

needs facilitation. Oster and others investigated factors that influ-

enced institutions’ evaluations of their readiness to implementing 

learning analytics [22]. Their study discovered that information 

officers tended to give higher rating to an institution’s readiness 

than other roles within the institution. Their explanation is that 

technology professionals have daily interaction with institutional 

data in various ways from collection to management and report-

ing, and hence their familiarity with and comfort working with 

data may have led to discrepancies in the view of institutional 

readiness when compared to other stakeholders in their institu-

tions. As a result, Oster and colleagues suggest that bridging per-

ceptions among stakeholders within an institution is critical to 

ensure a cohesive and collaborative implementation. Similarly, 

the Heads of E-Learning Forum (HeLF) conducted a survey on 

the adoption of learning analytics in the UK higher education, and 

41 of the 53 respondents (Heads of e-learning) indicated that there 

was limited understanding about the possible benefits and out-

comes of learning analytics across their institutions, while 4 sug-

gested ‘no understanding at all’ [19]. In addition, this question 

received 13 free text comments, among which all but one referred 

to different levels of awareness and understanding within and 

across departments. Particularly, teams within technical areas had 

the greatest understanding. The aforementioned studies reveal the 

gap of understanding between different stakeholders within one 

institution, which could potentially become an obstacle to the 

institutional embrace of learning analytics. 

4.2.3 Challenge 3 – Shortage of Pedagogy-based 
Approaches 
Although learning analytics is claimed to have a great potential in 

reforming the way people learn and the way teaching is delivered, 

pedagogical approaches are not always considered as part of the 



strategy for learning analytics. For example, Macfadyen and Daw-

son investigated the extent to which an institution has informed 

decisions based on analytics results [16]. They found that the 

institution was prone to addressing technical challenges while the 

development of pedagogical plans was ignored. Another study 

exploring what influenced the beliefs of educators concerning the 

adoption of learning analytics tools identified the weakness of 

learning analytics tools to move from spotting student weakness 

and risk levels to providing pedagogically informed suggestions 

[1]. Despite the fact that the overall perception of the usefulness 

of learning analytics was positive among the participants, the only 

variant that was found to have a significant relation with the inten-

tion to adopt LA tools was when the educator encountered learn-

ing contents that needed improvement. Similarly, Dyckhoff found 

that teachers were particularly concerned with how teaching tools 

correlate with learning behaviours and outcomes [12]. She sug-

gested that the design of a prototype for an exploratory learning 

analytics tool must differentiate learning offerings that the tool 

can afford. The abovementioned studies highlight the importance 

to consider pedagogical requirements and solutions in the design 

of learning analytics tools and projects. 

4.2.4 Challenge 4 –Shortage of Sufficient Training 
Shortage of skilled people has been identified as one of the ele-

ments in gaps between needs and solutions in the adoption of 

learning analytics [20]. For example, a pilot survey that examined 

readiness for learning analytics among nine institutions in the 

USA found that one of the respondents’1 greatest concern is the 

lack of analytics ability among the staff [3]. The skill shortage 

makes it hard to move learning analytics towards an institution-

wide scale. Goldstein and Katz explored the characteristics of 

institutions that claimed to have achieved success outcomes from 

the use of analytics systems, and discovered that the effectiveness 

of an institution’s training programme and present staff skilled at 

analytics were key to the success [14]. Wasson and Hansen advo-

cate that relevant training opportunities should be offered to all 

relevant stakeholders to improve understanding of learning analyt-

ics and equip them with skills to operate the tools and interpret 

data [32]. This can potentially bridge the gap in understanding and 

capabilities of learning analytics among different stakeholders, as 

identified in challenge 2.  

4.2.5 Challenge 5 –Shortage of Studies Empirically 
Validating the Impact 
Establishing successful cases has been identified as a requirement 

to persuade senior staff who can allocate budgets to support learn-

ing analytics [19]. However, it has been identified as a challeng-

ing task to evaluate the success of learning analytics or demon-

strate advanced employment of learning analytics. The reason for 

this is attributed to the fact that the majority of higher education 

institutions that have taken initiatives to adopt learning analytics, 

are still in a preparatory or early stage [23][10]. Moreover, the lag 

time required to measure the effects of analytics-triggered inter-

ventions has made access to learning analytics outcomes even 

harder [5]. So far, success claimed for learning analytics has 

mainly been based on data collected during a short period of time. 

For example, a study collected data from four institutions during 

the 2012 spring semester found a significant difference in mean 

course grades between groups of students that had received inter-

ventions and those that did not [7]. While the size of data is signif-

                                                                 

1 Respondents were involved in work related to learning analytics, 

data analysis, and research related to educational technologies. 

icantly large and useful to obtain preliminary insights, the study is 

not able to demonstrate long-term impacts or sustained effects 

brought about by analytics-triggered interventions. 

4.2.6 Challenge 6 –Shortage of Learning Analytics 
Specific Policies 
While institutions generally have regulations regarding the use of 

data, the ambiguous and divergent views towards ethical issues 

across countries has created much difficulty in the development of 

learning analytics frameworks, and hence impeded the advance-

ment of learning analytics [15]. In light of this challenge, 

Drachsler and Greller created an eight-point checklist named 

DELICATE, in order to facilitate a trusted implementation of 

learning analytics [11]. However, a survey conducted by the 

aforementioned Heads of e-Learning Forum in the United King-

dom revealed that principles around the ethical use of data for 

learning analytics as well as codes of practice were mainly under 

consideration with very few institutions having addressed these 

issues to date (13 out of 53 UK-based institutions – that responded 

to the survey – claimed to have considered principles and best 

practices around the ethical use of data, and 5 out of 53 institu-

tions have adopted a code of practice) [19]. The results of this UK 

report show that current practice of learning analytics at higher 

education institutions lacks clear guidance that is designed for 

learning analytics-specific practice.  

Another key element in learning analytics policies is strategy, 

which allows leaders to purposefully, tactically, and continuously 

move projects towards their goals [2, 20]. Macfadyen and Dawson 

advocate ‘visionary data analysis’; that is, to present data in a 

logical way which highlights progress and room for growth 

against a backdrop of institutional targets [16]. They believe that 

learning analytics data needs to be presented with consideration of 

the socio-technical sphere in order to motivate organisational 

adoption and cultural change. The idea that learning analytics 

need to be implemented under a strategic vision that responds to 

the needs of an organisation concurs with arguments made by 

Colvin and others, as mentioned earlier [10]. Further, Ferguson 

and others promote the RAPID Outcome Mapping Approach 

(ROMA) as a framework for learning analytics to achieve learn-

ing and teaching goals [13]. It is recommended that a policy that 

ensures the practice of learning analytics to be legal, ethical and 

strategic should be installed in every higher education institution. 

The six challenges identified in the literature highlight the fact 

that learning analytics need to be implemented with considera-

tions of multiple dimensions that include institutional contexts, 

stakeholders at various levels, pedagogical applications, institu-

tional capacities, success evaluation, legal and ethical considera-

tions, and a strategy that aligns with the institutional missions. 

Thus, it is imperative that higher education institutions develop 

learning analytics specific policies or update existing policies to 

meet the requirements of learning analytics, and make them rele-

vant to the institutional contexts and all stakeholders therein. 

5. RESULTS – EXISTING LEARNING AN-
ALYTICS POLICIES 

The owners of the eight policies fall in two groups:  

1. Support organisations and research consortiums: 

a. Jisc (a non-profit organisation that supports digital ser-
vices and solutions in the UK higher, further education and 

skills sectors).  

b. LACE (Learning Analytics Community Exchange, an EU 
funded project which aims to integrate communities work-



ing on learning analytics and educational data mining from 

schools, workplace and universities).  

c. LEA’s Box (an EU funded project which aims to create a 
learning analytics toolbox to enable a goal-oriented and 

proactive educational assessment that will provide forma-

tive support to learners).  

d. National Union of Students (NUS), UK (a confederation 
of 600 students’ unions from more than 95 per cent of all 

higher and further education unions in the UK, committed 

to promote, defend and extend student rights).  

2. Higher education institutions: 

a. Nottingham Trent University (NTU), UK 

b. The Open University (OU), UK 

c. Charles Sturt University (CSU), Australia 

d. The University of Sydney (USyd), Australia 

In the following sections, we present these organisations’ expecta-

tions of learning analytics and features of their policies. Then, we 

summarise key topics that have been covered by one or more 

policies including strategy, legal/ organisational obligations, pri-

vacy protection, data management and governance (3).  

Table 3. Aspects of review 

Dimensions Aspects 

Strategy Goal setting, methods, evaluation of im-

pact, assurance of validity, communica-

tion and support, and user roles 

Obligations Legal and organisational obligations 

Privacy protection Data anonymity, informed consent, and 

opt-out options 

Data management 

and governance 

Data handling process and access to data 

Whenever applicable, the results are summarised based on points 

that are in common, in addition to tables that present additional 

points distinct to individual policies. 

5.1 Expectations 
While all of the eight owners of the policies acknowledge that the 

use of learning analytics must serve the purpose of enhancing 

learning, six of them explicitly state their expectations of learning 

analytics. They are summarised into four goals: 

- Providing timely intervention 

- Providing personalised learning 

- Strengthening student-teacher relationships 

- Developing a data-informed culture 

5.2 Features of the Policies 
While privacy protection and ethical use of data are of primary 

concerns to all, each of the eight policies is distinct in some fea-

tures, which have shaped the emphases of these policies (Table 4).  

Table 4. Features of the eight policies 

Policies Features 

Jisc 
The code of practice covers issues around data, re-

sponsibilities, interventions and adverse impacts of 

learning analytics. There is a particular emphasis on 

the effectiveness of learning analytics, which is re-

flected in the detailed suggestions for methods. 

LACE  

The policy is presented succinctly with eight principles 

that focus on dealing issues of privacy and ethics. It is 

known as the DELICATE checklist. 

LEA’s 

Box  

The document is meant to balance individual privacy 

and beneficial uses of data when developing techno-

logical projects. It is part of a 44-page-long report, in 

which an overview of literature on ethical issues, pri-

vacy and data protection is included in addition to 

privacy and data protection regulations. 

NUS  

The policy is meant to state NUS’ beliefs about proper 

practices of learning analytics and where they stand to 

defend students’ right when issues arise. NUS adopts 

Jisc’s Code of Practice for legal and ethical regula-

tions. 

NTU  

The policy is developed specifically for the use of the 

student dashboard at NTU, and focused on methodolo-

gies that are adopted. 

OU  

The policy details the scope for and oversight on the 

ethical use of data in addition to the policy statement 

(eight principles). The policy statement is further de-

veloped into a page-long version for easy communica-

tion2 with end users. 

CSU  

The policy contains seven principles grounded in rele-

vant literature and a table of commitments that ex-

plains how these principles should look like in prac-

tice.  

USyd  

The policy contains eight principles that succinctly 

summarise the purpose, process, obligations, responsi-

bilities, rights of students, and implications of learning 

analytics to students. 

5.3 Strategy 
Strategy identified in the eight policies comprises six components: 

goal setting, methods, evaluation of impact, validity assurance, 

communication and support, and user roles.  

5.3.1 Goal Setting 
With the exception of Lea’s Box, all of the policies investigated 

explicitly define goals for learning analytics. JISC and LACE 

have stressed the need to set up goals but have not provided speci-

fications for the goals, perhaps due to the diversity of the institu-

tions in their partnership. While the rest of the policies all suggest 

that enhancing learning and teaching should be the ultimate goals 

for learning analytics, some provide additional information as to 

what learning analytics should and should not do (Table 5). 

Table 5. The 'should' and 'should not' of learning analytics 

goals 

Policies Learning analytics should/ should not aim to… 

NUS 
Learning analytics must support the student-teacher 

partnership, which is at the heart of education. 

NTU Student dashboards should enable rather than replace 

                                                                 

2 The eight principles comprise a four-page-long publicity, called 

“Using Information to Support Student Learning”. 



dialogues with students. It is not to be used for the 

purposes of assessment. 

CSU 

Learning analytics should enable “personalised man-

agement” of the relationship between the university 

and its students and employees. Moreover, it should 

provide input into decision-making for all the universi-

ty staff. 

OU 

Learning analytics should inform institutional strate-

gies to improve student retention and progression 

(macro level), and drive interventions and develop 

personalised learning paths (micro level).  

Both NUS and NTU show concerns about student-teacher rela-

tionship, while CSU and OU highlight the goal of using data to 

inform decisions. 

It is worth mentioning that CSU makes it clear that data generated 

by learning and teaching systems “will not be used as an official 

record of the University, and do not, in themselves, create an obli-

gation to act” [9]. That is to say, the university holds the right to 

make the final decision in terms of the extent to which they will 

take action in response to analytics results. By contrast, OU states, 

“Where data indicates that there is potential for action to be taken 

which might better support students in achieving their study goals 

or in reaching their potential, the University has a responsibility to 

act on this” [31]. 

5.3.2 Methods 
All the eight policies make suggestions on the methods that 

should be used to approach data, while some consider interven-

tions, resources and the engagement with students. Table 6 sum-

marises the methods. 

Table 6. A summary of suggested methods 

Suggested methods Policies 

The types of data that will be collected for learn-

ing analytics need to be stated. 

All but 

NTU 

The ways student data will be collected and used 

should be clearly explained. 
All 

The approaches to analysis and interventions 

should be explained. 
Jisc 

The circumstances for interventions should be 

specified.  

Jisc, NTU, 

OU and 

CSU 

Resources that will be allocated for learning ana-

lytics should be specified with consideration of 

students of different requirements 

Jisc 

The use of learning analytics should respect the 

differences between individual students. 

OU and 

CSU 

Bias or other adverse impacts may occur as a re-

sult of analytics, and such unintended results 

should be prevented by all means. 

Jisc, OU 

and CSU 

Students should be engaged actively in the adop-

tion of learning analytics. 

NTU, OU 

and CSU 

 

The NTU policy encourages students to use student dashboards as 

a tool to reflect engagement with their studies. OU states that 

students should be engaged as active agents in the implementation 

so that students take responsibility for learning and the university 

can provide a more accurate interpretation of data and tailored 

interventions for individual students. Similarly, CSU suggests that 

students should be encouraged to be active “managers” of their 

own learning through the use of analytics, and interventions 

should be made under professional, sensitive and fair judgements 

while promoting student-centred practices. 

5.3.3 Evaluation of Impact 
There is generally a lack of plans for evaluating the impact of 

learning analytics in most policies. The USyd policy is the only 

policy that mentions plans to review their practice of learning 

analytics regularly to examine its relevance to the goal – enhanc-

ing learning experiences and outcomes. NUS states that they will 

continue to review the effects of learning analytics on policy is-

sues and the public accountabilities of higher education institu-

tions. Specifically, NUS object to any possibility of “datafication” 

of student behaviours and “dictatorship of data” in education. 

5.3.4 Assurance of Validity 
Many of the policies have made suggestions to enhance the validi-

ty of learning analytics in terms of data quality and comprehen-

siveness (Table 7). 

Table 7. Considerations of data validity 

Considerations of data validity Policies 

Learning analytics cannot capture or present a com-

plete picture of a learning process. 

All but 

LACE 

and USyd 

Limitations of analytics must be revealed and inac-

curacies should also be disclosed and minimised. 

Jisc and 

Lea’s 

Box 

Learning analytics tools should allow the record of 

date stamps for new inferences or information so as 

to keep data updated and accurate. 

LEA’s 

Box 

5.3.5 Communication and Support 
Communication and support are essential elements to smooth 

implementation of any project within an organisation. While Jisc, 

LACE, and LEA’s Box have not dealt with these aspects, NUS 

and the four universities have attended to them to different de-

grees (Table 8). 

Table 8. Elements of communication and support 

Communication and support Policies 

Students will receive support to resolve disputes en-

countered in their learning environments, and NUS 

members (officers and staff at higher education insti-

tutions) will receive assistance to engage with their 

institutions on learning analytics issues and defend 

students’ rights. 

NUS 

The purpose, boundaries and methods used for the 

student dashboard, and the expectations of students to 

reflect on their own learning process using the dash-

board should be communicated when students start 

the university (e.g., at induction or an early tutorial). 

NTU 

Learning analytics users need to be informed about 

how the results of learning analytics may affect them, 

and what their rights and obligations are. 

OU and 

CSU 

Staff and students will be provided with a set of guid- OU 



ance notes to engage them with learning analytics, 

and the university will provide training to develop the 

required skills across the institution. 

Students would be notified of any privacy breaches 

related to their information and would be informed of 

their rights as to how to make a formal complaint. 

USyd 

Although each of the four universities states that communication 

about the adoption of learning analytics needs to be in place, none 

of them mentions any two-way communication channels for dif-

ferent stakeholders to share ideas and experience to work together 

on learning analytics. 

5.3.6 User Roles 
In terms of the roles that users play in the implementation of 

learning analytics, Jisc, NTU, OU and CSU all emphasise that 

students should be treated as active agents with certain degrees of 

responsibility to manage their data, make decisions related to their 

learning, and engage with their studies. In addition, Jisc suggests 

that higher education institutions should specify the obligations 

for students and staff to act on analytics, and that staff should 

have sound working knowledge of the legal and ethical practice of 

learning analytics. 

5.4 Legal/ Organisational Obligations 
All of the policies were developed under the framework of nation-

al or international policies for data protection (Table 9).  

Table 9. National and international data protection policies 

Policies Place 
National/ International data protection 

laws 

Jisc, 

NTU, 

OU, NUS 

UK 
- Data Protection Act 1998 

(NUS adopts Jisc’s Code of Practice) 

LACE Europe - EU Directive 95/46/EC 

LEA’s 

Box 
Europe 

- Act No. 101/2000 Coll. (Czech Repub-
lic) 

- DSG 2000 (Austria) 

- Data Protection Act 1998 (UK) 

- 1995 Data Protection Directive (Di-
rective 95/46/EC) (EU) 

CSU, 

USyd 
Australia 

- NSW Privacy and Personal Infor-
mation Protection Act 1998 (PPIPA) 

- National Statement on Ethical Conduct 
in Human Research (NSECHR) 

- NSW Health Records and Information 
Privacy Act 2002 (Only USyd) 

In addition to the aforementioned laws, all universities consulted 

relevant policies existing in their institutions. Table 10 summaris-

es those that were listed in the four policies. 

Table 10. Institutional data protection policies 

Institutions Institutional data protection policies 

NTU - The University’s data protection policy 

OU - The University’s Teaching and Learning Policy 

CSU - CSU Privacy Management Plan and 19 other 
policies held in the CSU Policy Library 

USyd - The University’s Privacy Policy 2013 

- Privacy Management Plan 

- University of Sydney Act 1989 

It is also worth noting that, with the upcoming application of gen-

eral data protection regulations (GDPR) in 2018 [30], higher edu-

cation institutions in Europe and UK are likely to have to update 

existing policies soon. 

5.5 Privacy Protection 
Considerations of privacy issues deal with identification of data, 

informed consent, and options to opt out of data collection. 

5.5.1 Data Anonymity 
Table 11 summarises the considerations of data anonymity in the 

eight policies. 

Table 11. Considerations of data anonymity 

Considerations of data anonymity Policies 

Any data collected for or generated by learning 

analytics must remain anonymous. 

All but 

USyd 

Data must remain anonymous when it is trans-

ferred between multiple sources and aggregated. 

JISC, 

LACE, and 

LEA’s Box 

Whenever third parties are involved, special at-

tention needs to be paid to data anonymity, and 

any data sharing needs to comply with privacy 

requirements in the current policy as well as insti-

tutional and national/international policies. 

LACE, 

LEA’s 

Box, OU, 

and CSU 

Although the USyd policy does not explicitly mention data ano-

nymity in its short list of principles, they point out that students 

will be appropriately notified about how their data will be dis-

closed, and the university will adopt appropriate safeguard to 

protect the security and integrity of university-held student infor-

mation. 

5.5.2 Informed Consent 
All of the policies state that consent must be obtained before data 

is collected from students, and three provide additional infor-

mation about the contexts of consent seeking (Table 12). 

Table 12. Context of consent seeking 

Contexts of consent seeking Policies 

Informed consent will only be sought at student en-

rolment. 

NTU 

Personal interventions also require consent from stu-

dents. 

Jisc 

Whenever HTTP cookies are used in a learning ana-

lytics system, user consent should be obtained. 

LEA’s 

Box 

5.5.3 Opt-out Options 
Table 13 summarises the considerations of opportunities to opt 

out of data collection for learning analytics in the eight policies. 

Table 13. Opt-out options 

Opt-out options Policies 

Users should be given the option to opt out of the 

data pool or collection process. 

All but 

NTU and 

USyd 



Students can amend consent agreements on a peri-

odical basis. 

OU and 

CSU 

Users should be able to opt out without any conse-

quences. 
LACE 

Any potential adverse consequences as a result of 

opting-out of data collection must be clearly ex-

plained. 

Jisc 

Learning analytics mechanisms must allow a specif-

ic user’s data to be withdrawn at any time. 

LEA’s 

Box 

While the USyd policy has no mention about opt-out options, 

NTU clearly states that such an option is not available, and their 

justification says, 

As part of their enrolment conditions, students give 

permission to the University to use and process data. It 

would not be possible to deliver courses or manage 

support for students without this data. It is therefore 

not possible for students to opt out from having their 

data in the Dashboard or other University core infor-

mation systems [21]. 

By contrast, OU and CSU offer the option to amend consent 

agreements on a periodical basis. However, it is not clear whether 

students are given the opportunity to amend their consent to exist-

ing data collection or to give consent to the collection of a new set 

of data. Unlike universities that are more concerned about data 

completeness, research consortiums offer more generic statements 

regarding data opt-out.  

5.6 Data Management and Governance 
A parallel issue to privacy concerns is data management and gov-

ernance. The current review deals with the following aspects: the 
data-handling process and access to data.  

5.6.1 Data Handling Process 
Each of the eight policies deals with data handling to various de-

grees of detail, but all policies indicate that the process for han-

dling student (and staff) data must be transparent, with clear ex-

planation on ways such data will be used. Additional information 

about data handling is provided in Table 14. 

Table 14. Considerations of data transparency 

Considerations of data transparency Policies 

The methods used to collect data have to be 

disclosed to the subjects of the data collection. 

LACE, LEA’s 

Box, OU, CSU 

and USyd 

The information about how data will be stored 

needs to be provided. 

LACE and 

CSU 

Users need to be notified about where their 

data has travelled in any integration process 

between multiple entities and informed about 

any changes made to the analytics process. 

LEA’s Box 

5.6.2 Access to Data 
All of the policies state that users should be given the right to 

access their own data that is held on institutional systems, and 

some (LACE, LEA’s Box, OU, and USyd) explicitly indicate that 

users should be able to manage and update such data. Table 15 

summarises considerations of data access in various contexts. The 

first four items focus on students’ access, while the remaining 

four deal with the access to data by other stakeholders, including 

third parties. 

Table 15. Considerations of data access 

Considerations of data access Policies 

Learning analytics needs to be in forms accessible to 

students. 
Jisc 

Analytics facilities should be available to anyone to 

whom the service has been provided even if consent 
for the collection of their data is not given. 

LEA’s 

Box 

Any metrics and labels attached to students must be 

revealed on students’ request under the condition that 
no harmful impact results. 

Jisc 

Students should be given access to data on their learn-

ing in a way that i) enhances agency and autonomous 

learning, ii) promotes quality learning and engagement, 
and iii) recognises student diversity and individuality. 

CSU 

Different stakeholders will be granted access to data 

with different authorisation privileges as to the range 
of data that they can access. 

LEA’s 

Box 

and 
NTU 

When external parties, including educational authori-

ties, are involved, their access to student data needs to 
be clearly defined. 

Jisc 

Student data will only be shared with third parties un-

der the circumstance that useful information in helping 
students can be provided. 

NTU 

Student data must not be sold or shared with commer-

cial third parties. 
NUS 

6. DISCUSSION 
In response to the six challenges identified in the literature, this 

section will examine whether and how the eight existing policies 

address these and provide a bigger picture of these challenges in 

the wider environment of higher education. Certain discussions 

will focus on the four higher education institutions alone. 

6.1 Leadership Involvement and Learning 
Analytics Specific Policies 
The fact that the four universities have developed institutional 

policies for learning analytics suggests that there is some degree 

of senior management involvement and support from the institu-

tions. Moreover, the governing bodies of these policies will be 

responsible for ensuring that the practice of learning analytics 

complies with the guidelines for data management and with the 

goals to enhance learning and teaching. However, the fact that 

only four policies from higher education institutions were re-

trieved indicates some possible scenarios. First, most institutions 

either implement learning analytics without formalised guidelines 

or with guidelines that are not originally developed specifically to 

meet requirements of learning analytics or specific institutional 

contexts. Second, some institutions are at a nascent stage of adopt-

ing learning analytics, and they allow this pilot stage to be part of 

the process for policy development. Third, the adoption of learn-

ing analytics in some institutions is a grassroots movement, and it 

has not gained support from senior management yet. 

6.2 Communications between Stakeholders 
Thoughts that have been put into communications between rele-

vant stakeholders in the eight policies are limited to practical as-



pects as to what, how and why data is collected. It is noticeable 

that this communication strategy is predominantly top-down. 

None of the policies proposed dynamic channels to allow two-

way communication among stakeholders at different levels. Alt-

hough the USyd policy states that students will be informed of 

their right to make formal complaints, this is limited to the com-

munication of negative opinions. However, the exchange of posi-

tive and constructive ideas should also be valued, as this can lead 

to an understanding of best practices.  

6.3 Pedagogy-based Approaches 
None of the eight policies made an attempt to suggest any peda-

gogy-based approach that i) teaching staff may take in response to 

learning analytics results, ii) technology developers should con-

sider when designing and developing learning analytics tools, or 

iii) institutions should consider when acquiring external learning 

analytics tools. Although teaching staff are expected to make de-

cisions based on analytics results, it is not clear whether they have 

been involved in the selection of the best parameters to evaluate 

student progress or engagement, neither is it clear what alterna-

tives teachers have when analytics results do not provide actiona-

ble insights into teaching design. 

6.4 Skills for Learning Analytics 
While institutions normally have general instructions on the use of 

learning analytics systems, NTU states that engagement with stu-

dents in learning analytics needs to happen as early as when they 

start their studies at the university. As discussed earlier, Jisc, 

NTU, OU and CSU expect students to engage with learning ana-

lytics actively by  managing their data and taking action based on 

analytics results. The emphasis on putting learning analytics in the 

hands of students highlights the need to develop ‘data literacy’ – 

the skill to accurately interpret and critique presented analysis of 

data [33]. However, among all the universities, only OU has 

promised to develop the skills required for learning analytics 

across the organisation. It is unclear what kinds of “skills” are 

considered as required though, and further discussion with input 

from learning analytics experts and system designers is required 

to increase institutional capacity for learning analytics.  

6.5 Evidence of Effectiveness 
Of the eight policies, only the USyd policy mentions the need to 

validate learning analytics outcomes against desired goals. More-

over, it is clear that all the policies have neglected the importance 

of evaluation of interventions introduced to learning or teaching 

design. The literature suggests that not many mature cases are 

available for evaluations on long-term impact due to the nascent 

stage of learning analytics [23][10][5][7]. This is reflected in the 

policies under current review. 

Evaluation should be included as an integral part of a learning 

analytics policy regardless of the maturity stage of implementa-

tion, as it offers opportunities for institutions to learn from previ-

ous experiences. Some policy development methods have empha-

sised this element. For example, the RAPID Outcome Mapping 

Approach (ROMA) includes the development of monitoring and 

learning frameworks as one of its seven critical phases in policy 

development [36][17].  

Moreover, a solid process of evaluation allows institutions to 

build successful cases to promote learning analytics not only with-

in institutions, but also in the wider sector of higher education 

[35]. For example, two respondents in the study conducted by 

Newland and others suggested that case studies that demonstrate 

benefits of learning analytics would provide credible evidence for 

raising awareness and understanding among all stakeholders [19]. 

7. CONCLUSION 
The review shows that the eight policies have not given enough 

considerations to the establishment of two-way communication 

channels and pedagogical approaches. Most of the policies also 

lack guidance for the development of data literacy among end-

users and for evaluation of the impact and effectiveness of learn-

ing analytics. Nevertheless, the existing learning analytics policies 

have established some examples that are of value for reference, 

particularly for institutions that are planning to develop their own 

institutional policies for learning analytics. The review also con-

firms that one of the biggest challenges in the adoption of learning 

analytics is the lack of institutional policies that are developed 

specifically to guide the practice of learning analytics with con-

siderations of an individual institution’s own cultural, economic, 

political and technical context as posited in policy development 

approaches such as the ROMA Rapid Framework [17, 36].  

It should be noted that the findings and recommendations present-

ed in this paper are by no means all encompassing. A small subset 

of the retrieved policies was composed in a succinct style for effi-

cient communication to their targeted audiences. It is not clear 

whether these institutions have more detailed versions of policies 

that are only available to internal stakeholders. We would like to 

encourage all higher education institutions to publish their policies 

for learning analytics, both in succinct and fully comprehensive 

formats. The former enables rapid communication with relevant 

stakeholders, while the latter can provide opportunities for further 

engagement and keep higher education institutions accountable to 

their students and employees. We would also like to encourage 

the community to create a common (Web) space where all such 

policies will be indexed and shared3. 

We acknowledge that gaps exist between policy and practice. For 

areas that these policies have failed to address, it does not neces-

sarily mean that these organisations have neglected them in their 

practice of learning analytics, and vice versa. The intention of this 

review is to highlight areas policy makers can consider when up-

dating an existing policy or developing a new policy for learning 

analytics, so as to ensure that the implementation of learning ana-

lytics is appropriate, effective and legitimate. 

8. ACKNOWLEDGMENTS 
We would like to thank the European Commission for funding the 

SHEILA project (Ref. 562080-EPP-1-2015-1-BE-EPPKA3-PI-

FORWARD). The project involved collaborative input from all 

the partners involved and their contributions are highly appreciat-

ed. This document does not represent the opinion of the European 

Community, and the European Community is not responsible for 

any use that might be made of this content. 

9. REFERENCES 
[1] Ali, L., Asadi, M., Gaševi?, D., Jovanovi?, J. and Hatala, M. 

2013. Factors Influencing Beliefs for Adoption of a Learning 

Analytics Tool: An Empirical Study. Computers & Educa-

tion. 62, (Mar. 2013), 130–148. 

[2] Arnold, K.E., Lonn, S. and Pistilli, M.D. 2014. An Exercise 

in Institutional Reflection: The Learning Analytics Readi-

ness Instrument (LARI). Proceedings of the Fourth Interna-

tional Conference on Learning Analytics And Knowledge 

(New York, NY, USA, 2014), 163–167. 

                                                                 

3 A short collection of existing learning analytics policies is avail-

able on the project website of a European Commission funded 

project – SHEILA (http://sheilaproject.eu/la-policies/) 



[3] Arnold, K.E., Lynch, G., Huston, D., Wong, L., Jorn, L. and 

Olsen, C.W. 2014. Building Institutional Capacities and 

Competencies for Systemic Learning Analytics Initiatives. 

Proceedings of the Fourth International Conference on 

Learning Analytics And Knowledge (New York, NY, USA, 

2014), 257–260. 

[4] Arnold, K.E. and Pistilli, M.D. 2012. Course Signals at Pur-

due: Using Learning Analytics to Increase Student Success. 

Proceedings of the 2nd International Conference on Learn-

ing Analytics and Knowledge (New York, NY, USA, 2012), 

267–270. 

[5] Arroway, P., Morgan, G., O’Keefe, M. and Yanosky, R. 

2016. Learning Analytics in Higher Education. ECAR. 

[6] Baer, L.L., Duin, A.H., Norris, D. and Brodnick, R. 2013. 

Crafting Transformative Strategies for Personalized Learn-

ing/Analytics. Proceedings of the Third International Con-

ference on Learning Analytics and Knowledge (New York, 

NY, USA, 2013), 275–277. 

[7] Baron, J., Whatley, M., Schilling, J. and McGovern, M. 

2013. Scaling Learning Analytics across Institutions of 

Higher Education. EDUCAUSE. 

[8] Bichsel, J. 2012. Analytics in Higher Education: Benefits, 

Barriers, Progress, and Recommendations. ECAR. 

[9] Charles Sturt University 2015. CSU Learning Analytics 

Code of Practice. Charles Sturt University. 

[10] Colvin, C., Rogers, T., Wade, A., Dawson, S., Gasevic, D., 

Shum, S.B., Nelson, K., Alexander, S., Lockyer, L., Kenne-

dy, G., Corrin, L. and Fisher, J. 2015. Student Retention and 

Learning Analytics: A Snapshot of Australian Practices and 

a Framework for Advancement. The Australian Government 

Office for Learning and Teaching. 

[11] Drachsler, H. and Greller, W. 2016. Privacy and Analytics: 

It’s a DELICATE Issue a Checklist for Trusted Learning 

Analytics. Proceedings of the Sixth International Conference 

on Learning Analytics & Knowledge (New York, NY, USA, 

2016), 89–98. 

[12] Dyckhoff, A.L. 2011. Implications for Learning Analytics 

Tools:  A Meta-analysis of Applied Research Questions. In-

ternational Journal of Computer Information Systems and 

Industrial Management  Applications. 3, (2011), 594–601. 

[13] Ferguson, R., Macfadyen, L.P., Clow, D., Tynan, B., Alex-

ander, S. and Dawson, S. 2014. Setting Learning Analytics 

in Context: Overcoming the Barriers to Large-Scale Adop-

tion. Journal of Learning Analytics. 1, 3 (Sep. 2014), 120–

144. 

[14] Goldstein, P.J. and Katz, R.N. 2005. Academic Analytics: 

The Uses of Management Information and Technology in 

Higher Education. ECAR. 

[15] Hoel, T. and Chen, W. 2015. Data Sharing for Learning 

Analytics - Questioning the Risks and Benefits. Proceedings 

of the 23rd International Conference on Computers in Edu-

cation (ICCE (2015). 

[16] Macfadyen, L. and Dawson, S. 2012. Numbers Are Not 

Enough. Why E-learning Analytics Failed to Inform an Insti-

tutional Strategic Plan. Faculty of Education - Papers (Ar-

chive). (Jan. 2012), 149–163. 

[17] Macfadyen, L.P., Dawson, S., Pardo, A. and Gaševic, D. 

2014. Embracing Big Data in Complex Educational Sys-

tems: The Learning Analytics Imperative and the Policy 

Challenge. Research & Practice in Assessment. 9, (2014), 

17–28. 

[18] National Union of Students 2015. Learning Analytics: A 

Guide for Students’ Unions. National Union of Students. 

[19] Newland, B., Martin, L. and Ringan, N. 2015. Learning 

Analytics in UK HE 2015: A HeLF Survey Report. 

[20] Norris, D.M. and Baer, L.L. 2013. Building Organizational 

Capacity for Analytics. 

[21] Nottingham Trent University 2015. Use of Learning Analyt-

ics to Support Student Success Policy. Nottingham Trent 

University. 

[22] Oster, M., Lonn, S., Pistilli, M.D. and Brown, M.G. 2016. 

The Learning Analytics Readiness Instrument. Proceedings 

of the Sixth International Conference on Learning Analytics 

& Knowledge (New York, NY, USA, 2016), 173–182. 

[23] Sclater, N. 2014. Learning Analytics: The Current State of 

Play in UK Higher and Further Education. Jisc. 

[24] Sclater, N. and Bailey, P. 2015. Code of Practice for Learn-

ing Analytics. Jisc. 

[25] SEG Education Committee 2016. Principles for the Use of 

University-held Student Personal Information for Learning 

Analytics at The University of Sydney. The University of 

Sydney. 

[26] Siemens, G., Dawson, S. and Lynch, G. 2013. Improving the 

Quality and Productivity of the Higher Education Sector: 

Policy and Strategy for Systems-level Deployment of Learn-

ing Analytics. Society for Learning Analytics Research for 

the Australian Office for Learning and Teaching. 

[27] Signals: Applying Academic Analytics: 2010. 

http://er.educause.edu/articles/2010/3/signals-applying-

academic-analytics. Accessed: 2016-07-27. 

[28] Slade, S. and Prinsloo, P. 2014. Student Perspectives on the 

Use of Their Data: Between Intrusion, Surveillance and 

Care. Challenges for Research into Open & Distance Learn-

ing: Doing Things Better–Doing Better Things (Oxford, UK, 

Oct. 2014), 291–300. 

[29] Steiner, C.M., Masci, D., Johnson, M., Türker, A., Drnek, 

M. and Kickmeier-Rust, M. 2014. Privacy and Data Protec-

tionPolicy. Deliverable D2.3. LEA’s Box. 

[30] The European Parliament and the Council of the European 

Union 2016. Regulation (EU) 2016/679 of the European Par-

liament and of the Council of 27 April 2016 on the Protec-

tion of Natural Persons with Regard to the Processing of 

Personal Data and on the Free Movement of Such Data, and 

Repealing Directive 95/46/EC (General Data Protection 

Regulation) (Text with EEA Relevance). Official Journal of 

the European Union. OJ, L119 (May 2016), 1–88. 

[31] The Open University 2014. Policy on Ethical use of Student 

Data for Learning Analytics. The Open University. 

[32] Wasson, B. and Hansen, C. 2015. Data Literacy and Use for 

Teaching. Measuring and Visualizing Learning in the Infor-

mation-Rich Classroom. P. Reimann, S. Bull, M. Kickmeier-

Rust, R. Vatrapu, and B. Wasson, eds. Routledge. 56–73. 

[33] Wolff, A., Moore, J., Zdrahal, Z., Hlosta, M. and Kuzilek, J. 

2016. Data Literacy for Learning Analytics. Proceedings of 

the Sixth International Conference on Learning Analytics & 

Knowledge (New York, NY, USA, 2016), 500–501. 

[34] Yanosky, R. 2009. Institutional Data Management in Higher 

Education. ECAR. 

[35] Yanosky, R. and Arroway, P. 2015. The Analytics Land-

scape in Higher Education. ECAR. 

[36] Young, J. and Mendizabal, E. 2009. Helping Researchers 

Become Policy Entrepreneurs-How to Develop Engagement 

Strategies for Evidence-based Policy-making. Overseas De-

velopment Institute. 

 



