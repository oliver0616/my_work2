Attention Please! Learning Analytics for Visualization and Recommendation  Erik Duval Dept. Computer Science  Katholieke Universiteit Leuven Celestijnenlaan 200A  B3000 Leuven, Belgium erik.duval@cs.kuleuven.be  ABSTRACT This paper will present the general goal of and inspiration for our work on learning analytics, that relies on attention metadata for visualization and recommendation. Through information visualization techniques, we can provide a dash- board for learners and teachers, so that they no longer need to drive blind. Moreover, recommendation can help to deal with the paradox of choice and turn abundance from a problem into an asset for learning.  Categories and Subject Descriptors K.3 [Computers and Education]: Computer Uses in Ed- ucation  General Terms Design, Human Factors, Theory  Keywords Learning Analytics, Visualization, Recommendation  1. INTRODUCTION Attention is a core concern in learning: as learning re-  sources become available in more and more abundant ways, attention becomes the scarce factor, both on the side of learners as well as on the side of teachers. (This is a wider concern, as we evolve towards an attention economy [10].)  Learners and teachers leave many traces of their attention: some are immediately obvious to others, for instance in the form of posts and comments on blogs, or as twitter mes- sages. These explicit traces are human readable, but can be difficult to cope with in a world of abundance [29]. Although some refer to information overload, we prefer Shirkys fil- ter failure as a way to think about the problem of dealing with this abundance [30]. In any case, human attention traces are extremely valuable, but do not scale very well.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK11 February 27-March 1, 2011, Banff, AB, Canada. Copyright 2011 ACM 978-1-4503-1057-4/11/02 ...$10.00.  In this paper, we will explain how machine readable traces of attention can be used to filter and suggest, provide aware- ness and support social links.  This paper is structured as follows: section 2 provides a brief background on the field of analytics in general. The section thereafter focuses on applications from the world of jogging, as these provide a particularly rich source of inspi- ration for our work. The general concept of goal oriented visualizations is at the core of learning dashboard applica- tions: that is why it is the topic of section 4. In order to scale up the current work on learning analytics and achieve broad adoption, it is imperative to establish a global open infras- tructure, as we briefly explain in section 5. The two next sections briefly present the two approaches weve explored so far to leverage learning analytics: learning dashboard (sec- tion 6) and learning recommenders (section 7). Before con- cluding the paper, we briefly mention exciting opportunities that learning analytics provides for data based research on learning in section 8.  2. BACKGROUND ON ANALYTICS The new field of learning analytics is quite related to sim-  ilar evolutions in other domains, such as Big Data [1], e- science [14, 9, 26], web analytics [7], educational data min- ing [25]. All of these have in common that they rely on large collections of quite detailed data in order to detect patterns. This detection of patterns can be based on data mining tech- niques, so that for instance recommendations can be made for resources, activities, people, etc. that are likely to be relevant. Alternatively, the data can be processed so that they can be visualized in a way that enables the teacher or learner rather than the software to make sense of them.  In fact, the research in my team gets much of its inspira- tion from tools like wakoopa (http://social.wakoopa.com) or rescuetime (http://www.rescuetime.com), that install tracker tools on the machine of a user and then automati- cally record all activities (applications launched, documents accessed, web sites visited, music played, etc.) by that user.  A typical illustration of the visualizations that such tools provide is figure 1, where a simple overview is presented of the software applications I used last week and last month and how their usage is distributed over time. (Tuesday- Thursday were travel days...) In this way, such an applica- tion can help a user to be more aware of her activities.  Moreover, based on these tracking data, the wakoopa tool can compare the activity of a user with that of other users and recommend software or contacts - see figure 2. It doesnt  9  http://social.wakoopa.com http://www.rescuetime.com   Figure 1: My wakoopa dashboard  require much imagination to see how similar visualizations could be useful in the world of learning, for instance to chart learning activities, tools used or recommended, and peer learners or suitable teachers in ones social network.  It is important to note that the tracking occurs without any manual effort by the user - although it is of course im- portant that the user is aware that her activities are being tracked. Actually, such tools typically also make it possible to pause tracking. Some applications allow users to set goals (spend less than 1 hour per day on emailor play computer games for less than 1,5 hour per day or write more than 3 hours per day) and will notify them when they are in danger of not meeting their goal, when they get close to the self-imposed limit - or signal them that they did reach their goal. Moreover, they provide quite detailed visualizations of all the activities of a user, so that she can analyze where most of her on-line activity takes place and make better in- formed decisions on how to manage these activities.  A similar tool is tripit (http://www.tripit.com): note- worthy about this tool is that when a user forwards flight or hotel reservations to a tripit email address, all the struc- tured data is extracted and a calendar is created with all the relevant information. This is an excellent example of auto- matic metadata generation [6] or information extraction [5], an essential technology if we want to collect metadata of resources, activities and people at scale. Note also that, if other people from the users network are near, tripit will mention that - see figure 3.  Of course, more mainstream tools like google offer similar functionality, such as for instance google history that pro- vides an overview of every search that a user ever submitted (when logged in) or that indicates who from a users social circle tweeted about an item included in a search result, etc.  3. INSPIRATION FOR OUR WORK A particularly inspiring set of applications comes from  the domain of jogging, and sports in general, where appli- cations like nikeplus (http://nikerunning.nike.com/, see figure 4) or runkeeper (http://runkeeper.com/) provide de- tailed statistics on how fast, far, often, etc. one runs.  What is particularly relevant in a learning context is that many running applications also help runners to set goals (run a marathon in november), develop a plan to achieve that goal, find running routes in a foreign town, locate other runners with a similar profiles, challenge them so as to main- tain motivation, etc. Sometimes, such tools even take a more pro-active role and send messages to users to enquire why they have stopped uploading activities, whether they need to re-define goals and plans, or want to be connected to other users that can help, etc.  Although there are few studies that show whether these special purpose social networks actually change user behav- ior, [16] did find that users weight changes correlated pos- itively with the number of their friends and their friends weight-change performanceand users weight changes have rippling effects in the Online Social Networks due to the so- cial influence. The strength of such online influence and its propagation distance appear to be greater than those in the real-world social network.. An early overview of how the combination of tracking and social network services can lead to a more patient-driven approach to medicine is provided in [32].  One assumption underlying our work is that similar ap- plications can be built to track learner progress, to assist in developing and maintaining motivation, to help define re- alistic goals and develop plans to achieve them, as well as connect learners or teachers with other learning actors, etc. In that way, they can help to realize a more learner-driven  10  http://www.tripit.com http://nikerunning.nike.com/ http://runkeeper.com/   Figure 2: My Wakoopa recommendations  Figure 3: My tripit dahsboard  11    approach to education, training and learning in general.  4. GOAL ORIENTED VISUALIZATIONS Many of these inspiring applications take a visual ap-  proach. Yet, if visualizations are to have any effect beyond the initial wowfactor, it would be useful to have more clar- ity on what the intended goal is and how to assess whether that goal is achieved. Many visualizations look good - and some are actually beautiful. But how we can connect visu- alization not only with meaning or truth, but with taking actions This is very much a quantified self approach (see http://quantifiedself.com/) [31], where for instance a vi- sualization of eating habits can help to lead a healthier life, or where a visualization of mobility patterns can help to explore alternative modes of transport, etc. Such visualiza- tions are successful if they trigger the intended behaviour (change). That can be measured, as in people smoke less when they use this visualization or people discover new publications based on this visualization (we are actually evaluating such an application) or people run more using this visualization etc.  It would be really useful if we could draw up some guide- lines to design effective goal oriented visualizations. As an example, it is probably kind of useful to be able to visualise progress towards a goal - or lack thereof. If you want to run further, a visualization can help you to assess whether youre making progress. Or if you want to spend less time doing email, a simple visualization can help. Another guideline could relate to social support, that enables you to compare your progress with that of others.  5. TECHNICAL INFRASTRUCTURE FOR LEARNING ANALYTICS  If we want to apply learning analytics at a broader scale, then it is imperative that we realize an infrastructure that can support the development of tools and services. Such an infrastructure will need basic technical agreement on com- mon standards and protocols [8].  A first question is how to model the relevant data. Our early work on Contextualized Attention Metadata (CAM) [19] [36] defines a simple model to structure attention meta- data, i.e. the interactions that people have with objects. The ontology-based user interaction context model (UICO) [24] focuses more on the tasks that people carry out while interacting with resources. Either we need to better un- derstand how to map and translate automatically between different such models, or we need to find a way to achieve broad consensus on and adoption of a common schema or a small set of schemas, as in the case of learning resources where nearly everyone has now adopted Learning Object Metadata (LOM) or Dublin Core (DC) [8].  Similar to the way we manage learning objects and their metadata [33], we will need a service architecture that can power a plethora of tools and applications. One interest- ing approach is to rely on technologies like widgets that enable the dynamic embedding of small application com- ponents - an approach at the core of Personal Learning Environments (PLEs), researched in the ROLE project on Responsive Open Learning Environments (see http://www. role-project.eu/) [15] [12]. Another approach is the Learn- ing Registry architecture that makes user data trails avail- able through a network of nodes that provide services to  publish, access, distribute, broker or administer paradata (see http://www.learningregistry.org/).  6. LEARNING DASHBOARDS For learners and teachers alike, it can be extremely useful  to have a visual overview of their activities and how they relate to those of their peers or other actors in the learning experience.  In fact, such visualizations can also be quite useful for other stakeholders, like for instance system administrators. Figure 5 provides an early example of such a visualization that displays the number of events in different widgets de- ployed in the ROLE context [27]. From the visualization, it is rather obvious that users were most active in the May- July period (towards the left of the diagram), that they enter chat rooms (top of area 4 on Figure 5) much more often than they post messages (third row of area 4 on Figure 5), etc. Such information can help a teacher to re-organize the ac- tivities or even to retract or add widgets that learners can deploy in their PLE.  Similarly, [28] describes a tool that includes a zeitgeistof action types (opening a document, sending a message, etc.) and specific user actions. By selecting a time period and the relevant action types, the user can control the visualization of relevant data (see also http://www.role-showcase.eu/ role-tool/cam-zeitgeist).  Following a similar visual approach, the Student Activity Monitor (SAM) supports self-monitoring for learners and awareness for teachers [13]: In area A on figure 6, every line represents a student in a course. The horizontal axis repre- sents calendar time and the vertical axis total time spent. If the line ascends fast, then the student worked intensely during that period. If the line stays flat, the student did not work much on the course. For example, student s1 started late and worked very hard for a very short time. Student s2 started early and then worked harder in about the same period as student s1. At the bottom, a smaller version of the visualization is shown with a slider on top to select a part of the period for analysis of data dense areas. Area 2 dis- plays global course statistics on time spent and document use. The colored dots represent minimum, maximum and average time spent per student and the time spent for the currently logged in user and for a user selected in one of the visualizations. The recommendation area in Box 3 enables exploration of document recommendations (see also section 7). The parallel coordinates in area B display  1. the total time spent on the course,  2. the average time spent on a document,  3. the number of documents used and  4. the average time of the day that the students work.  For example, the green line (the logged in user) works on average in the early evening and is spending an average time in line with the majority. He does not use so many different documents and on average looks at these for a short time. He scores the worst here. The average student of the class (in yellow) is also presented. This is a somewhat complex visualization, but our evaluation studies show that students considered the visualizations clear [13]. They rated the tools as usable, useful, understandable and organized.  12  http://quantifiedself.com/ http://www.role-project.eu/ http://www.role-project.eu/ http://www.learningregistry.org/ http://www.role-showcase.eu/role-tool/cam-zeitgeist http://www.role-showcase.eu/role-tool/cam-zeitgeist   Figure 4: My Nike Plus dashboard     At the top of the dashboard (label 1), there is the option of  filtering per application. The modification of this filter affects all  visualizations. The charts are also interlinked. Table 1 presents  which actions trigger updates of other visualizations.     Table 1 Actions overview   Section Action triggered Affected  visualizations   Sent Information   1 Selecting an application 2,3,4,5 Name of the widget   2 Restricting a period of  time   3,4,5 Starting date   Ending date   3 Selecting a day of the  week   2,4,5 Day of the week   4 Selecting a type of action 5 Type of Action   5 Selecting a type of item 4       5. USE CASE: XMPP CHAT BEHAVIOR    This use case describes the behavior of a specific widget in a PLE  environment, deployed during a course at RWTH Aachen  University during the period May to July 2010. After this period,  the environment was occasionally used in an informal way. In this  PLE, four widgets were used. The widgets use Open Social [13]  for their communication in a PLE.    ABC Testing widget. This widget was only used during the  first two weeks (this information is also displayed in the  dashboard).    Cam Widget. This widget tracks the Open Social  communication and translates this communication to  CAM. Users can deactivate or activate tracking of their  data.    Role Web 2.0 Knowledge Map. This widget allows to  search for articles by entering keywords.    XMPP Multiuser Chat. This widget enables chat  functionality between different users based on the  XMPP technology.        In this use case, we will focus on the XMPP Multiuser Chat  widget because it is the most active in terms of event  communication providing us more information about its particular  characteristics. We will now explain how we can derive the  conclusions from:  1. Detect changes on usage patterns:  When we select theXMPP   Multiuser Chat in part 1 one of Figure 2 and we obtain an  overview of the overall activity (Figure 3). The annotated time  line chart (Figure 3) enables us to see that the activity was  concentrated during the period from May to July 2010. After  this period, the activity was reduced considerably. In the  events per type of action chart (Figure 3), we can see that  people enter to room chats more than sending messages (if we   Figure 3 XMPP Multiuser Chat visualization   Figure 2 CAM Dashboard overview   Figure 5: The CAM dashboard [27]  13    A much more simple such visualization is edufeedr [21], where a matrix includes a row for every student that dis- plays his progress along a series of assignments. A nice fea- ture is that such progress can take place on the individual blog of the student, outside of the institutional Learning Management System (LMS), Virtual Learning Environment (VLE) or even institution provided PLE widgets. Rather, the coherence of the course is maintained through the track- back mechanism between the teacher blog and those of the students.  What these visualizations have in common is that they enable a learner or teacher to obtain an overview of their own efforts and of those of the (other) learners. This is the essence of our dashboard approach to visualizations for learning that remedy the blind driving that often oc- curs on the side of teachers and learners alike. Similar ap- proaches have proven to be beneficial in for instance software engineering [3] and social data analysis [18].  7. LEARNING RECOMMENDERS By collecting data about user behavior, learning analytics  can also be mined for recommendations, of resources, activ- ities or people [17]. In this way, we can turn the abundance of learning resources into an asset, by addressing lembarras du choix that is at the core of the paradox of choice [29]. Of course, similar approaches have been deployed for books, music, entertainment, etc. Yet, only by basing rec- ommenders on detailed learning attention metadata can they take into account the learning specific characteristics and re- quirements of our activities.  In one particular tool, we applied this approach to fil- ter and rank search results when a learner searches for ma- terial in YouTube (http://www.youtube.com/), SlideShare (http://www.slideshare.net/) and Globe (http://www.globe-info. org/): as figure 7 illustrates, every search activity in our tool is tracked in the form of attention metadata that are stored in a repository. The user can indicate whether search results are relevant or not and that feedback is also stored in the attention metadata repository. Search results are filtered and ranked based on earlier interaction by the user and by other users in her social network, as made available through OpenSocial. Although we need to do more user evaluations, the first results are very encouraging [11, 20].  8. DATA BASED RESEARCH ON LEARN- ING  On a meta-level, learning analytics provides exciting op- portunities to ground research on learning in data and to transform it from what is currently all too often a collection of opinions and impossible-to-falsify conceptualizations and theories [23].  As a precursor to making that happen, it is important that we agree on ways to share data sets, in an open science approach [26, 9, 14]. That is why a group of interested re- searchers has started an initiative around dataTEL (http: //www.teleurope.eu/pg/groups/9405/datatel/ [34]. The main objective is to promote exchange and interoperability of educational data sets.  9. CONCLUSION Of course, one of the big problems around learning an-  alytics is the lack of clarity about what exactly should be  measured to get a deeper understanding of how learning is taking place: typical measurements include time spent, number of logins, number of mouse clicks, number of ac- cessed resources, number of artifacts produced, number of finished assignments, etc. But is this really getting to the heart of the matter  Moreover, there are serious issues about privacy when de- tailed data of learner interactions are tracked [4]. An in- teresting early approach to deal with these issues was the proposal of the no longer active not for profit Attention- Trust [35]: their guiding principles included   property: the data about a persons attention remain the property of that person;   mobility: it should be possible to move data about a person out of one system and into another system - see also googles recent data liberation initiative (see http://www.dataliberation.org/);   economy: a person should be able to sell data about his attention;   transparency: it should always be clear to a person that she is being tracked.  Especially that last principle seems key: tools like ghostery (http://www.ghostery.com/) enable a user to know when she is being tracked on a web site. As we evolve towards a world where not only learning activities, but virtually every- thing will be tracked [2], this issue is likely to become even more important.  Some people are quite concerned about the filter bubble that personalization and recommendation engines may cre- ate [22]: we agree that there is a certain danger there, but we also believe that more advanced algorithms and ethical reflection can help us to address these issues.  In any case, we believe that learning analytics can be used to put the user in control, not to take control away in an Intelligent Tutoring Systems kind of way, by using attention to filter and suggest, provide awareness and support social links.  10. ACKNOWLEDGMENTS This research has received funding from the European  Community Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 231396 (ROLE) and no. 231913 (STELLAR). Much more importantly, the support, com- ments and feedback from my team and students have thought me much more than I will ever be able to teach them.  11. REFERENCES [1] C. Anderson. The End of Theory: The Data Deluge  Makes the Scientific Method Obsolete. Wired Magazine, 16(7), 2008.  [2] G. Bell and J. Gemmel. Your life, uploaded. Plume, 2010.  [3] J. Biehl, M. Czerwinski, G. Smith, and G. Robertson. FASTDash: a visual dashboard for fostering awareness in software teams. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 13131322. ACM, 2007.  14  http://www.youtube.com/ http://www.slideshare.net/ http://www.globe-info.org/ http://www.globe-info.org/ http://www.teleurope.eu/pg/groups/9405/datatel/ http://www.teleurope.eu/pg/groups/9405/datatel/ http://www.dataliberation.org/ http://www.ghostery.com/   Visualizing Activities for Self-reflection and Awareness 5  Fig. 1. The user interface with the 3 dierent visualizations.  The parallel coordinates in visualization B are a common way to display high-dimensional data [10]. On the vertical axes, we show (i) the total time spent on the course, (ii) the average time spent on a document, (iii) the number of documents used and (iv) the average time of the day that the students work. A student is represented as a polyline connecting the vertices on the vertical axes. The position of the vertex on the i-th axis corresponds to the i-th data point coordinate. For example, the green line (the logged in user) works on average in the early evening and is spending an average time in line with the majority. He does not use so many dierent documents and on average looks at these for a short time. He scores the worst here. The average student of the class (in yellow) is also calculated. This is a much more advanced visualization but can provide a good overview of the tendencies in the behavior of the students.  Figure 6: The Student Activity Monitor (SAM) [13]  include search results from content on your personal social networks (e.g. Twitter and Google Buzz). Bing collaborates with Facebook to provide similar functionalities: searching in the profiles of your friends and in content liked by your friends. IBM also added social features based on its em- ployee directory, tags, bookmarking behavior and ratings to its intranet to show more relevant blogs, wikis, forums and news to great success [2, 8]. Social search is also a popu- lar research topic. Haystaks [9] extends mainstream search engines with new features: users can create sharable search result folders and the content of these folders is used for rec- ommendations. I-SPY [10] allows people to create search communities and based on the queries and results in these communities it will adaptively re-rank the search results.  To enable search over multiple repositories, one can apply federated or harvested search, which collects all the meta- data from all repositories in a central repository for faster re- trieval [11]. When working with vast repositories of web 2.0 sources (e.g. YouTube), it is impossible to apply harvested search. As far as federated search is concerned, Ariadne [11] focuses on the interoperability between different reposito- ries. The search engine relies on the Standard Query Lan- guage (SQI) to ensure interoperability and to offer a trans- parent search over a network of repositories. Ariadne also provides harvesting. This is applied in the GLOBE network with 13 repositories3. MetaLib and WebFeat provide feder- ated search over scientific content [12]. WebFeat sends the query to all search engines and then shows the results in all the native user interfaces. In contrast with MetaLib that uses its own UI and communicates with the repositories over the standardized Z39.50 protocol. ObjectSpot [13] is originally a federated search widget for scientific publications, but it is now extended for web 2.0 sources. It uses a cover den- sity algorithm to rank the search results, which can also be manually re-ordered. Basic recommendations based on the user selection of search results are also provided. Extending ObjectSpot with social features was not trivial due to the use of incompatible technology. None of these federated search engines provides social features or social search results. Just as mainstream search engines are exploring social networks and attention metadata (voting & sharing), we adopted this strategy in our federated search engine.  THE DESIGN OF THE WIDGET In this section, the software architecture behind the search widget is explained and the user interface (UI) design and implementation is discussed.  Software architecture To enable search over multiple data sources, we employ a client-server architecture, as shown in Figure 1. When the user enters a search term, it is sent to the federated search service (step 1), which transmits it to all the different data sources concurrently (step 2). Currently it queries YouTube, SlideShare.net, Wikipedia, GLOBE and the OpenScout repos-  3The Global Learning Objects Brokered Exchange (GLOBE) al- liance, http://www.globe-info.org  Figure 1. The client-server architecture of the federated search and recommendation service.  itory4, but extra sources can be easily added to support dif- ferent learning scenarios. When all the results are returned from the data sources (step 3), the federated search service re-ranks the results (step 4) based on the metadata with the Apache Lucene library [14]. The ranked results are returned to the widget in the ATOM format [15] (step 5), which en- ables us in the future to make the service OpenSearch com- pliant [16]. OpenSearch allows search engines to publish search results in a standard and open format to enable syn- dication and aggregation. In future, we plan to adapt the service to return results every time the repositories return them to improve search speed. Once the widget receives the search results, they are presented to the user and the search result URLs are sent to the recommendation service (step 6). This service will return recommendations, based on the at- tention metadata stored in the database. The recommenda- tions are sent back to the widget, where they will be pre- sented to the user. The user can interact with the search re- sults, e.g. preview a movie inside the widget or (dis)like it. When some of these interactions happen, they are tracked and the attention metadata (basically the user, the URL of the search result and the action) is sent to the recommen- dation service (step 7). The service then stores the attention metadata in a database (step 8) to be able to calculate recom- mendations later. The next section describes the recommen- dation algorithm in more detail. The client-server architec- ture enables us to expose repositories not openly accessible by deploying the service inside the intranet.  User Interface Design The main design goal was to provide a simple, clean search interface with visually rich search results to enable better decision making while selecting a search result. The widget provides a simple Google-like search interface over multiple web 2.0 data sources. Although advanced search settings are available (see Figure 3), they are not visible by default. Morville et al. [2] advice this as well, because advanced search is often used by expert users. Figure 3 shows the ad- vanced search settings where the wanted media types, repos- itories and social recommendations can be configured. This can be operated with the wrench icon.  The search results are presented in a uniform way (see Fig- ure 2): basic metadata and tags together with a screenshot  4The OpenScout repository, http://www.openscout.net/ demo  2  Figure 7: Storing attention metadata in federated search [11]  15    [4] D. Boyd. Facebooks Privacy Trainwreck: Exposure, Invasion, and Social Convergence. Convergence: The International Journal of Research into New Media Technologies, 14(1):1320, 2008.  [5] C.-H. Chang, M. Kayed, M. R. Girgis, and K. F. Shaalan. A Survey of Web Information Extraction Systems. IEEE Transactions on Knowledge and Data Engineering, 18:14111428, 2006.  [6] N. Corthaut, S. Lippens, S. Govaerts, E. Duval, and J.-P. Martens. The integration of a metadata generation framework in a music annotation workflow. Oct. 2009.  [7] A. Croll and S. Power. Complete Web Monitoring. OReilly Media, Inc., 2009.  [8] E. Duval and K. Verbert. On the role of technical standards for learning technologies. IEEE Transactions on Learning Technologies, 1(4):229234, Oct. 2008.  [9] J. Fry, R. Schroeder, and M. den Besten. Open science in e-science: contingency or policy JOURNAL OF DOCUMENTATION, 65(1):632, 2009.  [10] M. H. Goldhaber. The Attention Economy and the Net. First Monday, 2(4), Apr. 1997.  [11] S. Govaerts, S. E. Helou, E. Duval, and D. Gillet. A Federated Search and Social Recommendation Widget. In Proceedings of the 2nd International Workshop on Social Recommender Systems (SRS 2011) in conjunction with the 2011 ACM Conference on Computer Supported Cooperative Work (CSCW 2011), pages 18, 2011.  [12] S. Govaerts, K. Verbert, D. Dahrendorf, C. Ullrich, S. Manuel, M. Werkle, A. Chatterjee, A. Nussbaumer, D. Renzel, M. Scheffel, M. Friedrich, J. L. Santos, E. Duval, and E. L.-c. Law. Towards Responsive Open Learning Environments : the ROLE Interoperability Framework. In ECTEL11: European Conference on Technology Enhanced Learning, Lecture Notes in Computer Science, 2011.  [13] S. Govaerts, K. Verbert, J. Klerkx, and E. Duval. Visualizing Activities for Self-reflection and Awareness. In Proceedings of the 9th international conference on Web-based Learning, pages 91100. Springer, 2010.  [14] T. Hey and A. E. Trefethen. Cyberinfrastructure for e-Science. Science, 308(5723):817821, 2005.  [15] U. Kirschenmann, M. Scheffel, M. Friedrich, K. Niemann, and M. Wolpers. Demands of Modern PLEs and the ROLE Approach. In M. Wolpers, P. Kirschner, M. Scheffel, S. Lindstaedt, and V. Dimitrova, editors, Sustaining TEL: From Innovation to Learning and Practice, volume 6383 of Lecture Notes in Computer Science, pages 167182. Springer, 2010.  [16] X. Ma, G. Chen, and J. Xiao. Analysis of An Online Health Social Network. In Proceedings of the 1st ACM International Health Informatics Symposium, pages 297306. ACM, 2010.  [17] N. Manouselis, H. Drachsler, R. Vuorikari, H. Hummel, and R. Koper. Recommender Systems in Technology Enhanced Learning. In F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor, editors, Recommender Systems Handbook, pages 387415.  Springer US, 2011.  [18] M. McKeon. Harnessing the web information ecosystem with wiki-based visualization dashboards. IEEE transactions on visualization and computer graphics, 15(6):10818, 2009.  [19] J. Najjar, M. Wolpers, and E. Duval. Contextualized attention metadata. D-Lib Magazine, 13(9/10), Sept. 2007.  [20] X. Ochoa and E. Duval. Use of contextualized attention metadata for ranking and recommending learning objects. In CAMA06: Proceedings of the 1st international workshop on Contextualized attention metadata: collecting, managing and exploiting of rich usage information, pages 916, 2006.  [21] H. Poldoja. EduFeedr-following and supporting learners in open blog-based courses. In Proceedings of OpenEd2010, number 2010. Universitat Oberta de Catalunya, 2010.  [22] E. Pariser. The Filter Bubble: What the Internet Is Hiding from You. Penguin Press, 2011.  [23] K. Popper. The Logic of Scientific Discovery. Routledge, 1959.  [24] A. S. Rath, D. Devaurs, and S. Lindstaedt. UICO: an ontology-based user interaction context model for automatic task detection on the computer desktop. In Proceedings of the 1st Workshop on Context, Information and Ontologies, CIAO 09, pages 8:1-8:10, New York, NY, USA, 2009. ACM.  [25] C. Romero and S. Ventura. Educational data mining: A survey from 1995 to 2005. Expert Systems with Applications, 33(1):135146, July 2007.  [26] S. S. Sahoo, A. Sheth, and C. Henson. Semantic provenance for eScience - Managing the deluge of scientific data. IEEE INTERNET COMPUTING, 12(4):4654, 2008.  [27] J. L. Santos, K. Verbert, S. Govaerts, and E. Duval. Visualizing PLE Usage. In Proceedings of EFEPLE11: 1st Workshop on Exploring the Fitness and Evolvability of Personal Learning Environments. CEUR workshop proceedings, 2011.  [28] H. Schmitz, M. Scheffel, M. Friedrich, M. Jahn, K. Niemann, and M. Wolpers. CAMera for PLE. In U. Cress, V. Dimitrova, and M. Specht, editors, Learning in the Synergy of Multiple Disciplines, volume 5794 of Lecture Notes in Computer Science, pages 507520. Springer, 2009.  [29] B. Schwartz. The paradox of choice - Why more is less. HarperCollins, 2007.  [30] C. Shirky. Here Comes Everybody: The Power of Organizing Without Organizations. Penguin Press, 2008.  [31] E. Singer. The Measured Life. Technology Review, 2011.  [32] M. Swan. Emerging patient-driven health care models: an examination of health social networks, consumer personalized medicine and quantified self-tracking. International journal of environmental research and public health, 6(2):492525, Feb. 2009.  [33] S. Ternier, K. Verbert, G. Parra, B. Vandeputte, J. Klerkx, E. Duval, V. Ordonez, and X. Ochoa. The Ariadne Infrastructure for Managing and Storing  16    Metadata. IEEE Internet Computing, 13(4):1825, July 2009.  [34] K. Verbert, E. Duval, H. Drachsler, N. Manouselis, M. Wolpers, R. Vuorikari, and G. Beham. Dataset-driven Research for Improving TEL Recommender Systems. In 1st International Conference on Learning Analytics and Knowledge, Banff, Canada, 2011.  [35] M. Wolpers, J. Najjar, and E. Duval. Workshop report on the international {ACM} workshop on contextualized attention metadata: collecting, managing and exploiting rich usage information (cama 2006), June 2007.  [36] M. Wolpers, J. Najjar, K. Verbert, and E. Duval. Tracking actual usage: the attention metadata approach. Educational Technology and Society, 10(3):106121, 2007.  17   	Introduction 	Background on Analytics 	Inspiration for Our Work 	Goal Oriented Visualizations 	Technical Infrastructure for Learning Analytics 	Learning Dashboards 	Learning Recommenders 	Data Based Research on Learning 	Conclusion 	Acknowledgments 	References     