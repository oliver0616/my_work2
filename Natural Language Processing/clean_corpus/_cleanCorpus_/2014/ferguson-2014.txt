Setting Learning Analytics in Context:   Overcoming the Barriers to Large-Scale Adoption   Rebecca Ferguson1, Doug Clow1, Leah Macfadyen2,   Alfred Essa3, Shane Dawson4, Shirley Alexander 5   1 Institute of Educational Technology 2 Faculty of Arts, The University of British Columbia  The Open University Buchanan C110, 1866 Main Mall   Milton Keynes Vancouver, BC  MK7 6AA, UK Canada, V6T 1Z1   +44-1908-654956 +1 604 809 5013  firstname.surname@open.ac.uk leah.macfadyen@ubc.ca     3 Alfred Essa 4 Learning and Teaching Unit 5 Deputy Vice Chancellor   VP, R&D and Analytics University of South Australia University of Technology, Sydney  McGraw-Hill Education Adelaide 15 Broadway   Boston South Australia 5001 Ultimo, NSW  USA Australia Australia   Alfred.essa@mheducation.com Shane.Dawson@unisa.edu.au Shirley.Alexander@uts.edu.au   ABSTRACT  Once learning analytics have been successfully developed and  tested, the next step is to implement them at a larger scale  across  a faculty, an institution or an educational system. This introduces  a new set of challenges, because education is a stable system,  resistant to change. Implementing learning analytics at scale  involves working with the entire technological complex that exists  around technology-enhanced learning (TEL). This includes the  different groups of people involved  learners, educators,  administrators and support staff  the practices of those groups,  their understandings of how teaching and learning take place, the  technologies they use and the specific environments within which  they operate. Each element of the TEL Complex requires explicit  and careful consideration during the process of implementation, in  order to avoid failure and maximise the chances of success. In  order for learning analytics to be implemented successfully at  scale, it is crucial to provide not only the analytics and their  associated tools but also appropriate forms of support, training  and community building.   Categories and Subject Descriptors  K.6.0 [Management of Computing and Information Systems]:  Information resource management  project and people  management, management techniques, staffing, strategic  information systems planning, systems analysis and design   General Terms  Design, Implementation.   Keywords  Administration; change; change management; education; higher  education; implementation; learning; learning analytics; teaching;  technology-enhanced learning; TEL complex.   1. INTRODUCTION  Learning analytics are concerned with the measurement,  collection, analysis and reporting of data about learners and their  contexts, for purposes of understanding and optimising learning  and the environments in which it occurs [1]. The intention is to  develop models, algorithms and processes that can be widely  used. Transferability is a key factor here; the analytics that are  developed need to be reliable and valid at a scale beyond the  individual course or cohort.   There are currently few reports in the learning analytics literature  of deployment at scale. In England and Australia, standardized  testing of school children has been employed for decades, through  the English SAT tests and the Australian NAPLAN tests [2, 3].  These tests are aligned with stated government aims, make use of  agreed proxies for learning, provide clear and standardized  visualisations of analytics and drive behaviour at every level of  the education system. The data generated by these tests is  collected, analysed and reported with the intention of optimizing  learning and the environments in which it occurs. Despite the  scale of this deployment, media reports suggest that many  educators, learners and parents have not been convinced that these  programmes are optimizing learning [4, 5].  On an institutional scale, the best-known example of roll-out at  scale is Purdue University. By 2012, the university had applied its  Course Signals analytics tool to over 100 courses, providing      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses,  contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '14, Mar 24-28 2014, Indianapolis, IN, USA  ACM 978-1-4503-2664-3/14/03.  http://dx.doi.org/10.1145/2567574.2567592     251    formative grade feedback to over 23,000 students [6]. This is a  significant achievement, but it has taken time. Course Signals is  rooted in a study carried out at the university in 2005 [7]; nine  years of development have not yet resulted in deployment across  the entire university. In the UK, The Open University has been  carrying out the learning analytics process as defined above [1]  for over 40 years, but has also been identifying barriers to the  implementation of that research for more than three decades [8].   2. BARRIERS TO ANALYTICS  RESEARCH IMPLEMENTATION   2.1 Barriers identified in 1979  In 1979, McIntosh reported that those of us in the Survey  Research Department continue to be dissatisfied at our ability to  have an impact on many major problem areas [8]. This was  before the development of learning analytics. McIntosh was  engaged in the related area of educational evaluation,  delineating, obtaining and providing information that would be  useful in judging decision alternatives. She identified seven  reasons why competent research findings were never put into  practice. These included an unwillingness for academics to accept  and act on methods or findings from outside their own research  area, individual preferences for qualitative or quantitative  approaches, a tendency to base decisions on anecdote rather than  on research, the different languages used by researchers and  decision-makers, an unfamiliarity with statistical methods on the  part of decision makers, and a tendency by researchers to hedge  their conclusions.   Her recommendations focused on the need for researchers and  decision-makers to work together: Researchers should get clients  politically, emotionally and financially committed to the outcome  of the research. They are then more likely to take notice of its  results [8]. The focus of the article was on university decision- makers as clients; the clientele for learning analytics might now  be referred to as stakeholders and would include learners,  educators and administrators.   2.2 Barriers identified in 2012  In 2012, Macfadyen and Dawson reported on the non- implementation of a study relating to an institutions use of  learning analytics and its learning management system (LMS).  They found that the institutional planning process was dominated  by technical concerns and, because of this, made little use of the  intelligence revealed by the analytics process [9]. Indeed, after  the current state analysis had been completed and noted by the  institutions standing committee on learning technologies,  meeting minutes and reports showed that no references to or  discussions of the findings were made in subsequent meetings.    Macfadyen and Dawson suggest the powerful analytic  conclusions were set aside because the development and  presentation of these analytics was coupled with a lack of  attention to the institutional culture of higher education, a lack of  awareness of the degree of resistance to change, and a lack of  understanding of approaches that have been developed for  motivating change within an organisation. They suggest that  greater attention is needed to the accessibility and presentation of  analytics processes and findings so that learning analytics  discoveries also have the capacity to surprise and compel, and  thus motivate behavioural change [9].   This presents a significant challenge for learning analytics  researchers, whose primary focus is on issues such as the   development and testing of algorithms and visualisations. Few  analytics projects will have the capacity to undertake an  ethnographic study of institutional culture or a review of recent  thinking on change management, or will have team members with  experience of writing a research report that both surprises and  compels its audience. Yet the learning analytics community needs  to investigate these issues and to engage its audience, if it is to  achieve its aim of optimising learning and the environments in  which it occurs.   Initial investigative work has been carried out in this area by Lonn  and his colleagues [10]. They reflected on the issues encountered  and lessons learned when scaling up a learning analytics  intervention. The focus of their reflection was on the benefits and  challenges of institutional partnership between a research team  and a technology service group. They identified gaps between the  two teams in areas such as usability, access, performance and  calculation. In each case, they identified possible solutions,  although many of these solutions were specific to the context in  which they were working.   Overall, although they employ different language and describe  different situations, these three studies identify some common  problems [8-10]. These problems are related to different  expectations around communication between researchers and  those responsible for implementation, different levels of  engagement with the research, and different expectations about  the role and purpose of educational research. These discrepancies  are found in other areas of technology-enhanced learning (TEL)  research, and it is increasingly clear that significant innovation in  this area is not possible without taking into account the entire TEL  Technology Complex [11].   3. TEL TECHNOLOGY COMPLEX  Learning analytics, like other areas of TEL, can rarely be  considered as a set of tools that can be developed, transferred and  immediately adopted by practitioners. There are many associated  elements that must work alongside the analytics in order to realise  their full potential. These can be understood as part of a  technology complex, a series of components that all need to be  addressed together [12]. In the case of TEL and learning analytics,  key components include pedagogy, stakeholders, communities,  current practices, context, technical components and the business  model [11]. When scaling up learning analytics, all these  components need to be taken into account.   The introduction of learning analytics requires changes to the  practices of several communities at once. Educators need to be  able to evaluate them and to use them effectively. Learners need  to be convinced that they are reliable and will improve their  learning without intruding into their privacy. Support staff need to  be trained to maintain the infrastructure and add data to the  system. University administrators need to be convinced that they  are both valid and cost effective. In order to convince all these  stakeholders to put in the sustained effort necessary to make use  of learning analytics, a clear vision of the gains to be made is  required at the outset [11].   4. THE PANEL DISCUSSION  In order to consider how these changes can be carried out  successfully, the panel brings together researchers who have taken  on the task of implementing learning analytics at scale. They will  offer insight into the processes involved, outlining different  perspectives on these, identifying barriers to implementation and  presenting ways of overcoming those barriers.   252    4.1 Rebecca Ferguson and Doug Clow  Rebecca and Doug work as data wranglers at The Open  University in the UK. In this role, they have responsibility for  mediating between different university faculties and the  department responsible for collecting and analyzing analytic data.  They will talk about why the university decided to set up a team  of data wranglers, and some of the implications of their role in  bridging different communities within the university.   4.2 Leah Macfadyen  Leah is Program Director for Evaluation and Learning Analytics  in the Faculty of Arts at the University of British Columbia, and  has several years experience of efforts to garner institutional  support for learning analytics. She will discuss a range of  institutional barriers encountered, and introduce a systems  framework that may allow more careful analysis of structural and  cultural blockages in institutions, and identification of points for  intervention.   4.3 Alfred Essa  Alfred is Vice President, Analytics and R&D at McGraw-Hill  Education and was, until August 2013, Director of Analytics  Research and Strategy at Desire2Learn. There, he led product  development on the Student Success System, a set of predictive  analytic tools that was incorporated within the companys cloud- base learning systems. He will talk about his experience of scaling  analytics up for use across institutions and across national  boundaries.   4.4 Shane Dawson  Shane is Deputy Director of Academic Learning Services at the  University of South Australia and has extensive experience of  developing and implementing learning analytics. He will discuss  his research into problems encountered when working to  implement analytic findings at institutional level, and introduce  ways of overcoming the barriers to success.   4.5 Shirley Alexander  Shirley is Deputy Vice-Chancellor and Vice-President (Teaching,  Learning and Equity) at the University of Technology, Sydney.  Her responsibilities include enhancing the quality of the  universitys teaching, creating an environment of innovation and  excellence in teaching and learning. She will talk about her  experience of implementing analytics across a university.     5. REFERENCES  [1] SoLAR, Open Learning Analytics: An Integrated &   Modularized Platform. White Paper, Society for Learning  Analytics Research. 2011.   [2] Kirkup, C., Sizmur, J., Sturman, L. and Lewis, K., Schools'  Use of Data in Teaching and Learning. National Foundation  for Educational Research: Research Report RR671, UK,  2005.    [3] ACARA, National Assessment Programme (NAP). 2013.  [4] Harrison, A., Sats boycott hits tens of thousands of pupils   (BBC News, 10 May). BBC News. 2010.  [5] Bantick, C., NAPLAN: It's All about Jumping Hoops for   Number Crunchers (The Sydney Morning Herald, 27  November). The Sydney Morning Herald. 2012.   [6] Pistilli, M. and Arnold, K., Course Signals at Purdue: Using  Learning Analytics To Increase Student Success. In:  LAK12: 2nd International Conference on Learning  Analytics and Knowledge (30 April - 2 May) (Vancouver,  Canada, 2012). ACM Press.   [7] Campbell, J. P., Utilizing Student Data within the Course  Management System to Determine Undergraduate Student  Academic Success: An Exploratory Study. Purdue  University, 2007.   [8] McIntosh, N. E., Barriers to implementing research in  Higher Education. Studies in Higher Education, 4, 1,  (1979), 77-86.   [9] Dawson, S. and Macfadyen, L. P., Numbers Are Not  Enough. Why e-Learning Analytics Failed To Inform an  Institutional Strategic Plan Educational Technology &  Society, 15, 3, (2012), 149-163.   [10] Lonn, S., Aguilar, S. and Teasley, S. D., Issues, Challenges,  and Lessons Learned When Scaling up a Learning Analytics  Intervention. 2013.   [11] Scanlon, E., Sharples, M., Fenton-O'Creevy, M., Fleck, J.,  Cooban, C., Ferguson, R., Cross, S. and Waterhouse, P.,  Beyond Prototypes: Enabling Innovation in Technology- Enhanced Learning. Technology-Enhanced Learning  Research Programme, London, 2013.    [12] Fleck, J. and Howells, J., Technology, the technology  complex and the paradox of technological determinism.  Technology Analysis and Strategic Management, 13, 4,  (2001), 523-531.        253      