Explaining Predictive Models to    Learning Specialists using Personas  Christopher Brooks   ARIES Laboratory  Department of Computer Science   University of Saskatchewan  Saskatoon, SK, Canada   cab938@mail.usask.ca   Jim Greer     University Learning Centre   University of Saskatchewan   Saskatoon, SK, Canada   jim.greer@usask.ca      ABSTRACT  This paper describes a method we have developed to convert   statistical predictive models into visual narratives which explain   student classifications. Building off of the work done within the   user experience community, we apply the concept of personas to   predictive models. These personas provide familiar and   memorable descriptions of the learners identified by data mining   activities, and bridge the gap between the data scientist and the   learning specialist.    Categories and Subject Descriptors   H.5 INFORMATION INTERFACES AND PRESENTATION   (e.g., HCI)    General Terms  Design, Human Factors.   Keywords  Personas, sensemaking, learning designers, predictive modelling   1. INTRODUCTION  This work describes a process by which personas can be created   from predictive models for the use of learning specialists. Unlike   other approaches [1] that focus on predictive modelling with the   learner or instructor in mind, our activity focuses on creating and   explaining a model that describes a particular group of learners   who may be of strategic importance to the institution. Our target   audience is institutional learning specialists, academic advisors or   instructional designers: individuals within higher education   systems who focus on creating large-scale, innovative learning   interventions. These interventions may be delivered in the form of   workshops, tutorials, help centers, online resources/activities or   other forms of personal coaching.   When dealing with learning specialists it is both important to   create an accurate predictive model as well as an explainable   predictive model. Achieving one of these goals may require a   trade-off with the other: to create an explainable model, one must   often reduce the accuracy of the model, while to create an   accurate model one must often include complex explanations of   learners. It is important to note the separation of roles we   envision: as data scientists building the predictive model we see   our role as enabling learning specialists to better understand the   student population that can be modelled. While we are often   forced to make decisions based on understanding of the   institutional questions being asked, we take the view that it is the   learning specialists who are best empowered to make pedagogical   and instructional decisions based on the particulars of a predictive   (classified) target population of learners.   To this end, our goal is to communicate both the nature and   likelihood of the learners who should be receiving interventions   (true positives), as well as learners who may be incorrectly   identified as needing interventions but do not (false positives).   Depending upon the intended intervention outcome, learning   specialists may choose more or less risky pedagogical methods   based on how accurately a group of learners can be modelled.    The approach we have taken to explaining predictive models is   that of learner personas: narrative descriptions of typical learners   that can be identified through centroids of machine learning   classification processes. The modelling methods we use are based   on historic data of learner activity, and can be used to predict the   activities of new learners at the institution. Once the rules inherent   in the predictive models have been used to classify groups of   learners, learners so classified can be offered particular   instructional/learning interventions.   The contribution this paper makes to the learning analytics   community is one of sense-making for practitioners. While   automated computer-based early alert systems have been powerful   agents in increasing retention and learning in higher education [1]   [2], we aim to use predictive models to enable human experts to   build, customize, and target educational programs for at risk   learners. Our goal is to describe a process and give examples of   how data scientists can better enable the exploration of the   middle space1 where learning-supports and analytics intersect in   higher education settings.                                                                       1 The middle space which exists at the intersection of   technology and learning has been recognized as a key concept   of interest and was the theme for the 2013 conference on   Learning Analytics and Knowledge (LAK13), see [7].   Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights   for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or   republish, to post on servers or to redistribute to lists, requires prior   specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '14, March 24 - 28 2014, Indianapolis, IN, USA   Copyright is held by the owner/author(s). Publication rights licensed to   ACM.   ACM 978-1-4503-2664-3/14/03$15.00.   http://dx.doi.org/10.1145/2567574.2567612     2. DATASET OVERVIEW  The examples given in this paper2 are inspired by an institutional   student records / admissions dataset that has been augmented with   entrance survey information. The institutional dataset includes a   variety of information including;    demographic information such as gender, nationality,  and self-declared disability status;    entrance information such as secondary school grades,  rural or urban backgrounds, and postal code data;    academic information such as program/degree  information, enrollment in courses, and grades in   courses.   In addition to this, the entrance survey asked students a variety of   questions about their backgrounds and plans for university, such   as;    whether they were planning to transfer colleges or  institutions during their undergraduate degree;    how they were funding their education;    how busy they expected their schedule to be, and  whether they were planning to volunteer or work part-  time throughout their degree;    their views on the responsibilities of the institution to  provide them with an education;    items to measure factors such as academic grit, help- seeking tendencies, and approaches to studying.   While some open ended questions were asked, for the purposes of   building predictive models only those questions that could be   represented as nominal, original, or real values were considered.   3. MODELLING PROCESS  The building of personas from this institutional data is part of a   five-step process.   3.1 Step 1: Dataset Reduction  The first step in building a predictive model is to identify the   kinds of attributes to be included in the model as well as the kinds   of learners for whom an intervention is to be created. Attributes   can be partitioned into three categories: those known at the time   one wishes to make a prediction and thus can be used to clarify   questions of interest (cut-off attributes), those known at the time   one wishes to make a prediction and are believed to be useful in   helping to make the prediction (predictive attributes), and those   known only after evaluating whether the learner has met the   criteria of interest (classification attributes).   For instance, one might want to describe those students who enter   the university with high academic achievement in secondary   education but fail to do well in their first year of university. To do   so, one would limit data from the institutional dataset where   students had high school averages (e.g. a cut off of 80% or higher,   or high SAT scores) and low first year grade point averages (e.g. a                                                                      2 It is important to note that all of the example fragments,   predictive models, and personas described in this paper are   simulated. While our underlying data is based on actual   experience building predictive models and personas from   institutional datasets, we removed identifying information and   instead aim to demonstrate the process and typical result instead   of providing explicit institutional personas.   grade point average of 65% or less at the end of their first   semester). The first of these values is a cut-off attribute, data that   is known at the beginning of an academic program that can be   used to remove students not being targeted for a particular   learning intervention. The second of these variables is a   classification attribute, only known post-hoc from previous data,   and is used to train the predictive model. Finally, other variables   in the survey and institutional datasets could be candidates for   being predictive attributes, which form the rules linking learners   to predictions.    Careful attention to identifying suitable cut-off and classification   attributes is important as these can reduce the size of the dataset   significantly. Under-specifying these attributes can cause large,   hard to understand models that tackle overly-broad questions.   Over-specifying these attributes can rob the predictive model of   its power  the model becomes a summary of data described by a   collection of constraints, which represent individual biases to   high-level questions instead of data-driven results. A more full   discussion of attribute reduction in predictive modelling can be   found in the literature [2].   In this example we aim to build a simple, binary predictive model    learners will be classified either as strong performers (e.g. they   entered the university with high grades and continued to achieve   these high grades), or weak performers (e.g. they entered the   university with high grades and achieved low grades after the first   semester). While predictive modelling does not have to be limited   to two categories, explaining higher dimension models with   personas becomes difficult because of misclassifications as   described in section 3.5.   3.2 Step 2: Building a Predictive Model  There are a variety of different tools and algorithms that can be   used for building predictive models, and the Weka toolkit [3] is a   free software package frequently used for this purpose. Once a   dataset has been reduced in size and classification has been run,   attention should be paid to the size of the classification groups.   For instance, the actual dataset we used to create the personas we   describe here was consisted of 30% weak performers and 70%   strong performers. Building a predictive model on such   unbalanced data can reduce accuracy, especially of the minority   group. While a complete treatment of the issues and solutions to   unbalanced datasets is outside of the scope of this work, we   suggest balancing the dataset using methods such as the Synthetic   Minority Over-Sampling Technique (SMOTE) [4]. This technique   generates new, simulated instances in the minority dataset. In this   example, additional simulated students would be generated adding   to the individuals in the weak-performer set (the minority set).   The simulated students would share some combination of the   demographic and survey characteristics of other weak performers.    Table 1: Four candidate nodes for personas. Each node has a   misclassification rate of <=20% while classifying correctly at   least ten individuals.       Classifications    Correct Incorrect   Node 1 783 32   Node 2 57 14   Node 3 44 2   Node 4 35 0     It is important to only run SMOTE on attributes that are   independent of one another. Since SMOTE considers all of the   different values across the attributes in the minority set, it is   possible to generate instances which have combinations of   attributes that do not make sense. For instance, if there are   attributes for country of origin and state/province of origin in the   underlying data, SMOTE may generate synthetic instances   describing people from states and countries that are not related.   After applying SMOTE and achieving a balanced distribution of   learners in each category, it is possible to build a predictive model   such as a J48 decision tree. In this kind of model, leaf nodes of the   tree are classifications of learners (either strong or weak   performers), and paths to these leaf nodes form a series of rules   that describe the conditions by which a classification is formed.   Leaf nodes include a classification, the number of learners who   were correctly classified with that label (true positives), and the   number of learners with other labels who were misclassified as   belonging to this group (false positives). An example of such a   rule is shown in Figure 1.      3.3 Step 3: Uninteresting Classifications  Not all classifications are of equal value. For instance, if we are   interested in helping learning specialists build interventions for   the weak performers, then anyone who can be correctly identified   as a strong performer can be removed from our predictive model.   This is done through pruning of the decision tree, and any node   that is exclusively made up of strong performers can be removed,   along with the path to that node.   If the leaf node is made up predominantly but not completely of   strong performers, the data scientist faces the first tradeoff as to   whether they should leave the node in the model (high accuracy,   low explainability), or remove it (lower accuracy, high   explainability). If the node is removed, it reduces the complexity   of the model, but it introduces false negatives, since some of the   target learners (weak performers) are now excluded from the   predictive model. The trade-off here is between how much harm   is done by not intervening versus how much harm is done   intervening with the wrong persons. The decision to choose   between one route or the other depends on the initial question   being asked. If it is the perception of the data scientist that   missing a few interesting classifications is worth simplifying the   data model, then these nodes can be removed, and the interesting   classifications are now considered to be false negatives.   In our experience, we found that removing nodes that had very   high ratios of correct but uninteresting classifications to incorrect   but interesting classifications made the decision tree much easier   to understand. For instance, removing nodes made up   predominantly of strong performers where the number of   misclassifications is <=20% did not seem unreasonable, and   reduced the size of our decision tree considerably. In the synthetic   dataset we have uses for the purposes this this paper, where there   were 1400 classifications (after the SMOTE process), pruning   uninteresting classifications at the level of <=20% removed 524   strong performers from our dataset while only removing 33 weak   performers.   3.4 Step 4: Interesting Classifications  Our purpose in building a predictive model is to help learning   specialists create effective interventions for strategically important   groups of learners. As explained in section 3.5, we aim to describe   these groups of learners using personas. Personas can be separated   into two groups: affirmation personas, which describe a target   group of learners we are interest in having interventions designed   for (in our case, weak performers) and erroneous personas, which   describe classifications that the predictive model makes that we   are not focused on supporting (in our case, strong performers).   Nodes in the decision tree that are made up of mostly weak   performers are good candidates for affirmation personas. The best   affirmation personas are those that represent a large population   (large number of instances classified by a given node), and   contain few false positives (e.g. a high weak-performer to strong-  performer ratio). It is important to limit the number of affirmation   personas created to a small number so they remain memorable. In   our studies, we aimed for a maximum of three affirmation   personas per predictive model, though large models, which may   prove likely to have multiple interventions and involve larger   teams of learning specialists responsible for creating these   interventions, may benefit from more.   A difficult decision for the data scientist is to determine how large   the ratio of true positives to false positives can be before the node   should be rejected as a persona candidate. Beyond just the ratio of   correct to incorrect classifications, attention should be paid to the   total number of learners that have been classified at any particular   node. If there are very few learners classified at a leaf, it can   represent both an overfitting of the predictive model, as well as an   indication of poor prevalence in the dataset. Overfitting is a well-  studied problem in predictive modelling (see [5] for a discussion)   and is of strong importance for the data scientist, but determining   how many individuals a given learning design can be used for   depends on the pedagogical approach being used and the business   case. For our example we envisioned workshops being held for   those learners who were weak performers, so we required nodes   to have at least 10 individuals of the correct classification in order   to be considered, and required a correct classification rate of at   least 20%.   An important consideration when developing learning   interventions is the effect such interventions have on learners who   have been misclassified. In order to better understand the mixture   of learners that may receive interventions, we collapse the rest of   the nodes in the decision tree (which are made up of a variety of   persons in either the poor or good achievement groups) into a pair   of personas, one positive and one negative. These personas share   traits, but represent different student outcomes, and help to inform   learning designers as to the quality of the predictive model.   3.5 Step 5: Developing Personas  The outcome of the previous step is a relatively small set of   decision-tree leaf nodes, which are candidates for affirmation   personas. In our example, we identified four nodes, which covered   a cross-section of the student population (Table 1). As we were   creating personas for a small group of learning specialists who   aboriginal_status=2        distance_from_campus=3             wanting_to_transfer_college=1                  high_school_admission_avg<=91                       weak_performers (23,3)   Figure 1: An example rule produced by the data mining   process. Each attribute is defined in the oiginal dataset, and   the bottom line (a leaf node) represents the group of learners   classified when attribute conditions are met. In this case, the   classification is of weak learners, with 23 correctly classified   instances and 3 misclassified instances.     were aiming for a single intervention, we limited ourselves to two   personas. One node in particular contained a large number of   learners and had a low misclassification rate, so we chose this   node to represent our primary persona. Another node covered a   very small part of the population, but was a perfect classifier, so   we chose this to be our secondary persona. The other two nodes   were discarded.   Affirmation personas serve as examples to learning specialists of   the learners who might benefit from help. These personas serve as   examples and, while there is a danger in over-designing to a   persona, they are intended to reflect realistic proto-typical   individuals. In developing these personas, we follow the advice of   [6], and we also included the following design elements;   1. a colour positive image (e.g. smiling, happy) showing a  student who fit the demographic centroid of the group;   2. a narrative description, in bold text, of the attribute path  that described the classification rule for this group of   students;   3. a narrative description, in regular text, of the mean  attributes of this class that are not specified in the   classification rule (e.g. the prototype or centroid user in   this group)   Table 2: An example handout of the personas developed.  Stylistic choices such as colours, images, and text are intended to   make the personas easy to remember and easy to relate to. Some images iStock.com/alkir, gokychan, andresrimaging,   benstevens     Predictive models are rarely perfect and, depending upon the   intervention being designed, false positives can experience some   negative outcomes. For instance, if the institution attaches   mandatory extra course work to certain outcomes of predictive   models, students who are misclassified as false positives may end   up with an increased course load despite their lack of need for   help. As discussed in the next section, it is important to work with   learning specialists to construct interventions that take into   account the accuracy of predictive models.   It may be important to build erroneous personas as well, in order   to highlight the potential downside of interventions targeted at   false-positives. In our example, we chose one node that had both a   high number of classifications (5.8% of the total population) and a   fairly high number of misclassifications (22 learners which was   37% of the node population) for which to build an erroneous   persona. The erroneous persona was created similarly to the   affirmation personas, but two pictures were used, (one positive   image in colour for the correctly identified learners, and one   negative image in black and white for the misclassifications). The   narrative included describes how the learners are similar (the   predictive rule), and notes explicitly how the possible false-  positive classification could arise.   4.  DESIGNING INTERVENTIONS  While it may not be the job of data scientists to create learning   interventions, we think it is important to do some iterative work   with learning specialists during the design process. Personal   experience gained by learning specialists allows them to quickly   identify with the personas of learners and, in our experience, they   find familiarity with both the affirmation and erroneous personas.    Using the example presented here, the task facing a learning   specialist is to design an intervention that may be helpful for both   Bob, Sally, and potentially Ken while not being harmful for Jerry.   Typical learning interventions might be sending warning   messages to the students in the persona group, guiding the   students toward web resources, promoting specialized learning   resources, designing group workshops or short-courses for these   students, or arranging personal advising/counselling sessions.   These interventions vary in terms of their expense, invasiveness   and potential prejudicial outcomes. It is the responsibility of data   scientists to communicate the subtleties of the composition of the   persona group and likelihood of false positives. It is also   important to communicate the coverage of the persona groups  to   offer some indication of the number of false negatives.   Once persona groups have been created and interventions have   been designed, the data scientist can help with identifying the   actual learners predicted to be in the persona group and who   would be the targets of learning interventions.   5. USING PERSONAS FOR CRAFTING  EXPERIMENTAL STUDIES  A persistent problem facing learning specialists is how to know   whether or not learning interventions actually work (the program   evaluation problem). If a specialized workshop or course is   designed and delivered and outcomes are measured, it is hard to   know what would have transpired had the intervention not been   made. Our work on personas provides one potential solution to   this problem: The set of learners identified to constitute a persona   group can readily be assigned randomly into a treatment and   control sub-group. The treatment group can be offered the   designed learning intervention, while the control group can be left   with no intervention (or offered the intervention at a later time,   perhaps). The achievement difference between treatment and   control groups offers strong experimental evidence of the effect of   the given learning intervention. While there may be some ethical   issues associated with withholding learning interventions from   control group subjects, the idea of a later offering of the   intervention can satisfy some concerns. Once answered, the   question of the effectiveness of a learning intervention for a   highly specific group of learners is a very powerful result.   6. CONCLUSIONS & FUTURE WORK  In our work to date, we have attempted to bring data scientists and   learning specialists together to gather better demographic data,   build predictive models, communicate those models in terms of   personas and persona groups, design interventions for actual   groups of learners, and design program evaluation experiments.   The success of the persona work will be measured through the   improved ability of our learning specialists to create or utilize   existing learning interventions in a more strategic and cost-  effective manner, targeting resources and efforts toward more   narrowly defined groups of students. The success will also be   validated through experimental differential interventions.    Predictive models are useful only if they can be made actionable.   Our work on personas offers a way to make such models   actionable by enabling learning specialists to do their jobs better.   7. ACKNOWLEDGMENTS  Thanks to Carrie Demmans-Epp for her insights on this work, and   to the learning specialists who provided feedback on the personas.   8. REFERENCES  [1] Arnold, Kimberly E. and Pistilli, Matthew D. Course signals   at Purdue: using learning analytics to increase student success.   In Proceedings of the 2nd International Conference on   Learning Analytics and Knowledge (LAK12) (Vancouver   2012), ACM, 267-270.   [2] Laura, Eitel J. M., Moody, Erik W., Jayaprakash, Sandeep   M., Jonnalagadda, Nagamani, and Baron, Joshua D. Open   academic analytics initiative: initial research findings.   (Leuven Belgium 2013), ACM, 150-154.   [3] Fodor, Imola K. A survey of dimension reduction techniques.   Center for Applied Scientific Computing, Lawrence   Livermore National Laboratory, Livermore, CA, June 2002.   [4] Hall, Mark, Frank, Eibe, Holmes, Geoffrey, Pfahringer,   Bernhard, Reutemann, Peter, and Witten, Ian H. The WEKA   Data Mining Software: An Update. SIGKDD Explorations, 11,   1 (2009).   [5] Chawla, Nitesh V., Bowyer, Kevin W., Hall, Lawrence O.,   and Kegelmeyer, W. Philip. SMOTE: synthetic minority over-  sampling technique. Journal of Artificial Intelligence   Research , 16, 1 (JAnuary 2002), 321-357.   [6] Witten, Ian H. and Frank, Eibe. Data Mining: Practical   machine learning tools and techniques. Morgan Kaufmann,   2005.   [7] Adlin, Tamara and Pruitt, John. The Essential Persona   Lifecycle: Your Guide to Building and Using Personas.   Morgan Kaufmann Publishers, Burlington, MA, USA, 2010.   [8] Suthers, Dan and Verbert, Katrien. Learning analytics as a   "middle space". (Leuven, Belgium 2013), ACM, 1-4.        