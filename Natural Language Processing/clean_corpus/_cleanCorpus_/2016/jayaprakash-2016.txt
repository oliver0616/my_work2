Benchmarking Student Performance and Engagement in  an Early Alert Predictive System using Interactive Radar   Charts  Sandeep M. Jayaprakash, Eitel J.M. Laura, Pritesh Gandhi, Dinesh Mendhe   Marist College, Poughkeepsie, NY, USA   {Sandeep.Jayaprakash1, Eitel.Lauria, Pritesh.Gandhi1, Dinesh.Mendhe1}@marist.edu  ABSTRACT  This poster synthesizes the design features of a visualization layer  applied on the Open Academic Analytics Initiative (OAAI), an  open source academic early alert system based on predictive  analytics. The poster explores ways to convey the predictive model  outputs and benchmark student performances using visually  intuitive radar plots.    Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning, Visualization, Dashboards   General Terms  Algorithms, Measurement, Information Visualizations, Design,  Experimentation, Interventions, Predictive analytics     Keywords  Learning Analytics, Open Source, Data Mining, Visualization,  Instructional Assessment, Benchmarking, Intervention   INTRODUCTION  The Open Academic Analytics Initiative (OAAI) [1] is a project  originally supported by the EDUCAUSE Next Generation  Learning Challenges (NGLC) program and funded primarily by the  Bill and Melinda Gates Foundation. OAAI uses the Sakai  Collaboration and Learning Environment, an existing open-source  Learning Management System (LMS), and the open-source  Pentaho Business Intelligence tool suite as its core technology  platforms.  This poster briefly describes the predictive model  building process implemented by the OAAI and delivers the model  outcomes using visualization techniques (radar plots) via a  dashboard to respective stakeholders.   PREDICTIVE MODELING  Four sources of data  were considered in the model building  process: student demographic data and course enrollment data,  extracted from the student information system (SIS); and learning  management system (LMS) data, extracted from Sakai, which  included logs of interactions of the students with the LMS as well  as partial contributions to the final grade  as reported by Sakais  gradebook (Sakai includes a gradebook tool that can be set up by   instructors to automatically record grades on graded activities such  as assignments, tests and forums). The data sources were combined  and cleaned to derive an enriched dataset which constituted the unit  of analysis (students taking courses) subsequently used for  predictive model training and testing.  The data set was made up of  the following attributes: FTPT AND ONLINE are indicators of  student full-time/ part-time status, and whether the course taken by  the student is online or on the ground. ACADEMIC _STANDING  identifies probation, regular, or honors status. ENROLLMENT is  the course size. CLASS identifies the year the student is in  (freshman, sophomore, junior and senior). GPA_CUMULATIVE  is the students cumulative GPA. LMS engagement metrics  (measured as ratios to the course average):  R_SESSIONS is the  number of Sakai sessions opened by a student in a course;  R_CONTENT_READ is the number of content resources viewed  by a student; R_FORUMS is the numbers of forum contributions  of the student; R_ASSIGMENTS is the number of assignments  submitted by the student. RMN_SCORE_PARTIAL is an  aggregated metric derived from partial contributions to the final  grade as recorded in the Gradebook tool. ACADEMIC_RISK is  used as the binary classification target attribute, with a cut-off grade  (below C for undergraduates, below B for graduate students) to  identify at-risk students.    As a result of the model training and testing process,  RMN_PARTIAL_SCORE and CUMULATIVE_GPA emerged as  the strongest predictors, followed by other LMS engagement  metrics (sessions, content read, forums and assignments). Adequate  depiction and visualization of these metrics can provide a holistic  portrait on the students performance and engagement patterns  which is the main thrust of this work.   DASHBOARDS AND RADAR CHARTS  Immediate access to data through learning dashboards offers new  opportunities to instructors and students by providing prompt  feedback [2]. This opportunity to act fast is not trivial especially  when you consider the high rates of withdrawal after a single course  failure in the first year of college. Learning dashboards in  combination with early alert systems offer students feedback in  timely fashion, alerting struggling students of their academic status.   Learning Activity Radar charts  As pointed by [3], unique visuals like pictorials, graphs,  grid/matrix, trees and networks and diagrams have significantly  high memorability scores compared to common charts like lines,  bars, circles, points. Radar graphs are efficient ways of  communicating a multivariate data in an intuitive way. The graphs  are laid in a circular fashion with multiple axes representing the  different metrics under analysis. The quantitative scales that run  along the axes (identical or different for each axis) are arranged to  begin at the center corresponding to 0 and then extended outside     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883940     with increasing values.  The farther away from the center the score  is in a criteria, the better demonstration of success in the criteria.     Performance Benchmarking   At the core of the Learning Activity Radar is a data-driven  predictive model determining the key performance predictors.  These predictive metrics are then mapped to the multivariate axes  on the Learning Activity Radar: (1) Cumulative GPA (2)  Gradebook scores (3) Session Activity (4) Content Read Activity  (5) Assignment Activity (6) Forums Activity. The Cumulative  GPA and Gradebook demonstrates the students performance  assessment criteria whereas the rest of the metrics form the  engagement assessment criteria.      Figure 1: Student Learning Activity Radar (a) No Risk   student profile (b) High Risk student profile.  The example presented in this work uses data from a graduate  online program and therefore sets a threshold of good standing at a  minimum cumulative GPA of 3.0 in a 0-4 scale. The predictors  from the LMS engagement and gradebook data are derived as ratios  comparing student performance with the class average fixed at a  score of 100 in a 0-200 scale.  This allows us to map the class  averages in each of the criteria and create a Class Benchmark  comparison polygon denoted in dark gray color in Figure 1.  When  an individual student score in each criteria is mapped against the  axis we get a student performance polygon which we can compare  against the class benchmark to identify areas of excellence and  problem areas. If the students polygon is on par or beyond the class  benchmark polygon in all criteria then they demonstrate a good  balance between performance and engagement. Color coding is  used to represent the risk confidence level as predicted by the model  (No, low, medium, and high risk). See Figure 1 (a) and (b).   The individual student performance benchmark view can be used  by both instructors and students to infer learning patterns. If a  student is highly engaged but the performance is weak or vice versa  or both, suitable instructional design strategies and intervention  supports could be made available to the student to change outcomes  and mitigate the problem areas.     Additionally, instructors have access to visualize the collective  patterns of all the students in a particular Risk group by using the  filters provided in the dashboard.  All the student polygons in the  risk group are color coded and layered on the class benchmark  polygon to easily identify common problem areas. Figure 2 shows  a collective pattern benchmark for a high risk group. It is evident  that the students are falling short on multiple categories: grades are  low among the group which is a result of lack of engagement in  forums and also some of them are falling short on assignments. The   instructor can act upon this insight to try to improve engagement  and content mastery.    Figure 2:  Example - Collective patterns and benchmarking  performance in a high risk group.   ONGOING WORK  We are currently working on enhancing the interactivity of the  visual layer by adding on-click histogram capabilities to the  performance predictors. This would further inform the instructors  on the class distribution and trends.    The Learning Activity Radar chart was built using D3 and the  Angular visualization library. An enhanced version of the chart is  already embedded into an open-source Dashboard project within  the Apereo Foundation. The visualization is available as a  configurable widget with support for Learning Tools  Interoperability (LTI) standards allowing the visual to be rendered  in variety of LMSes.   Currently, the dimensions mapped on the radar are on student  activity and performance as determined by an early alert predictive  model. Going forward we would like to turn this into a Learning  Competency Radar where there could be multiple learning  analytics applications feeding dimensions (radar chart axes) of  student skills and competencies. Examples include among others  student social engagement metrics determined by social network  analysis and writing skills metrics assessed by text mining  applications.    REFERENCES  [1] Jayaprakash S., Moody E., Laura E., Regan J., Baron J.,   "Early Alert of Academically At-Risk Students: An Open  Source Analytics Initiative",  Journal of Learning Analytics.   [2] Verbert, K., Duval, E., Klerkx, J., Govaerts, S., & Santos, J.  L. (2013). Learning Analytics: Dashboard Applications.  American Behavioral Scientist. 57(10), 1500-1509.    [3] B Michelle A. Borkin, Azalea A. Vo, Zoya Bylinskii, Phillip  Isola, Shashank Sunkavalli, Aude Oliva, Hanspeter Pfister,  "What Makes a Visualization Memorable", IEEE  Transactions on Visualization & Computer Graphics, vol.19,  no. 12, pp. 2306-2315, Dec. 2013     	REFERENCES   