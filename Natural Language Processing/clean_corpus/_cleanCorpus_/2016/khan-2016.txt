Data2U: Scalable Real time Student Feedback in Active  Learning Environments  Imran Khan  imran_ak@outlook.com  Abelardo Pardo  abelardo.pardo@sydney.edu.au  Faculty of Engineering and IT  The University of Sydney, NSW 2006 Australia  ABSTRACT The majority of applications and products that use learning ana- lytics to understand and improve learning experiences assume the creation of actionable items that will affect students through an in- termediary. Much less focus is devoted to exploring how to provide insight directly to students. Furthermore, student engagement has always been a relevant aspect to increase the quality of a learning experience. Learning analytics techniques can be used to provide real-time insight tightly integrated with the learning outcomes di- rectly to the students. This paper describes a case study deployed in a first year engineering course using a flipped learning strategy to explore the behavior of students interacting with a dashboard updated in real time providing indicators of their engagement with the course activities. The results show different patterns of use and their evolution throughout the experience and shed some light on how students perceived this resource.  CCS Concepts Applied computing ! Computer-assisted instruction; E-learning;  Keywords Feedback; Learning Analytics; Dashboard; Visualizations  1. INTRODUCTION Learning analytics applications are increasing their presence in  higher education institutions after being portrayed as the most dra- matic factor shaping the future of this sector [26]. There is a grow- ing portfolio of approaches and products that rely on ubiquitous data collection applications to increase the insight on how students learn. But the target group for the majority of these initiatives are either governments, institutional management teams, curricu- lum designers, or instructors. The body of research considering students as the direct target group to receive data-driven insight is not as comprehensive. Using data to support decision making pro- cesses is complex and may be more effective when mediated by an expert. But students in higher education institutions are partici- pating in learning experiences hosted in increasingly diverse tech-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883911  nological platforms and with an equally diverse type of strategies. Student engagement is one of the aspects that has been investigated in detail specially in active or online learning scenarios.  There are numerous barriers preventing a more direct connection between learning analytics and the provision of frequent, meaning- ful feedback as it requires a tight integration between the collection, analysis and reporting stages of the process. Additionally, feed- back given to students should provide insight beyond simple click counts when visiting resources in the Learning Management Sys- tem (LMS). It should contain information about their higher level engagement in a course to facilitate reflection and the identifica- tion of remediation actions. This paper presents a naturalistic case study to explore how students use a dashboard displaying infor- mation about their engagement with preparatory activities in a first year engineering course used a flipped learning strategy.  2. RELATED WORK The increasing presence of Learning Management Systems (LMS)  as mediating platforms in learning experiences has position them as one of the most commonly used data sources for analytics [3, 1, 11]. Numerous studies have been presented in the area of learn- ing analytics in which the data recorded by LMSs is used to cre- ate reports, visualizations, recommendations, etc. [7, 20, 24]. One of the most common problems tackled with LMS data is the de- tection of students at risk of abandoning a course, the institution, or under-performing. The first systems took basic indicators from LMSs and explored the correlation between them and the course outcomes [21]. Macfadyen and Dawson presented a comprehen- sive analysis of which factors derived from LMS interactions had a strongest correlation with academic performance [18]. Goggins et al. expanded the data sources to include social interactions among students in the contexts requiring group work and formal collabo- ration [10]. The value of these systems is in identifying students at risk so they can receive support to reduce such risk. Although most of the studies were in the context of online learning, simi- lar analyses were also proposed in face-to-face or blended learning environments with similar results [22].  Other systems focus on visualizations to increase either student or instructor awareness [5, 28]. These visualizations can be graph- ical representations of discussion forum activities, social interac- tion, or the relation between scores and estimated cognitive activ- ities [25, 19, 4]. The underlying assumption is that visualization help users increase their insight about the learning process. The immediacy of these visualizations is also a relevant aspect influ- encing their effectiveness [17]. The systems in this space can be divided into two categories: those providing information directly to students and those in which the information is made available to an intermediary [16]. Several systems have been proposed to    support this mediated provision of feedback. For example, the Sig- nals project allows instructors to run a set of pre-defined procedures to process student scores throughout the semester and generate a traffic light conveying the estimated risk of failing the course to students [27]. Other systems offer instructors sophisticated mech- anisms to collect data from the students and dispatch highly per- sonalized actions [17, 30, 16]. Kay and Bull [2] proposed the use of open student models to manage student information captured in a learning experience. Fritz proposed a tool offering students a dashboard derived from LMS data [9]. However, the metrics were course-agnostic factors such as the number of visits to the platform or the number of clicks in the course pages. In a contribution closes to the one described in this paper, Gunn and Bushway [12] explored the effect of providing students a map of their competency-based assessment. They observed a slight increase in academic perfor- mance and a significant increase in registration levels in adult edu- cation. Reimers and Neovesky [23] published a review of the state of the art in student dashboards and a study about the type of infor- mation that students would find more valuable.  From a pedagogical point of view, feedback has been shown to have a positive effect on students [13, 14]. However, serious bar- riers appear when trying to scale techniques for large student co- horts. At the same time, pedagogical strategies based on active learning are gaining momentum as the body of evidence showing their effectiveness is growing [8, 29]. But active learning methods rely heavily on frequent feedback to maintain students engaged and productive [6].  This paper is positioned at the intersection of these three spaces: the increasing presence of active learning methodologies, the in- creasing requirements for feedback in these contexts, an the use of learning analytics to report pedagogically meaningful indicators to students in real time.  3. CASE STUDY  3.1 Design The case study collected data about student interactions with the  online resources available in a first-year engineering course de- ployed in a flipped and blended learning context at an Australian higher education institution. The events were captured and used to produce a set of low-level indicators, their values were aggregated and made available through the dashboard shown in Figure 1 pro- vided as an additional resource in the online platform (a link in the template used for every course page). The system was deployed in a naturalistic context during the course and the anonymized data was analyzed after the course was finished.  3.2 Data Sources The following data sources were used in the study:   Interaction with video clips embedded in the course notes.   Answers to formative assessment questions next to the video clips and embedded in the course notes.   Answers to a summative assessment sequence of exercises.   Access to the any course resource.   Access to the dashboard.  The information provided by the dashboard is produced by com- bining data from other sources. It includes four indicators about the level of engagement with videos, formative questions next to the videos, formative question in the course notes, and a sequence  Figure 1: Example of Dashboard with Real Time Feedback  of summative exercises. Each indicator is represented as a percent- age and two values are shown: the individual score and the average with respect to the cohort.  3.3 Sample The course had an enrollment of 290 students. A total of 224  students used the dashboard at some point during the course (N = 224). The course lasted for 13 weeks and was designed following a flipped learning strategy. Every week students were required to attend a 2 hour lecture, a 2 hour tutorial, and a 3 hour laboratory sessions. The total workload was estimated at 11 hours per week. Students had to prepare the lecture and tutorial sessions with online activities. The information provided in the dashboard was aggre- gated from the engagement with these preparation activities that included summative assessments accounting for 20% of the course final mark.  The dashboard was available to the students through a link in the course pages, updated every 15 minutes, and the values reset at the start of every week. Students could reviewing the data reported in the preceding weeks.  3.4 Variables The interactions with any online resource, videos, multiple choice  questions and exercise sequences were recorded as events. Every event was labeled with its type, time, date, user, and contextual in- formation describing the action recorded. For the purpose of the study the events were divided into two groups: Dashboard Events and Regular Events. Dashboard events are recorded when a student accesses the dashboard. Regular events include the rest of events occurring in the environment.  The date and time for each event was used to extract informa- tion about sessions. We assumed that a study session was defined as a continuous sequence of events with separation below 30 min- utes. Although there are several heuristics to calculate these ses- sions [15], we selected this method based on the type of activities that were part of the course. If the separation between two events from the same user was larger than 30 minutes, the first event was marked as the end of the previous session, and the second event marked as the start of the next session.  The length of a session was calculated as the difference in min- utes between the first and last event. Additionally, for each session and student, a variable was calculated with the number of dash- board views per session and the number of events before and after the first dashboard view.    3.5 Data Analysis Four data analysis were carried out. The first one used a K-means  clustering algorithm to explore if the students could be clustered based on the number dashboard accesses during the course. The elbow method was used to detect the optimum number of clusters. The second calculation also used K-means but based on when stu- dents accessed the dashboard during a study session. For this anal- ysis a new variable was introduced with the number of events in a session before and after the first access to the dashboard. The third analysis explored how the pattern of engagement with the dash- board evolved throughout the semester. The number of events be- fore and after the interaction with the dashboard for each session and week were calculated. The clustering results obtained in the previous analysis were then used to label student interactions and obtain the changes in the dashboard use. The last study explored the relationship between dashboard use and the scores obtained in the midterm examination.  4. RESULTS AND DISCUSSION The descriptive statistics for the variables in the study are shown  in Table 1.  Table 1: Descriptive Statistics of Variables  Variable name Mean (SD) Avg Session Length (min) 18.03 (33.82) Total Sessions 82.71 (37.72) Avg Events Per Session 29.45 (13.47) Total Events 2364 (1228) Avg Dashboard View Per Session 0.31 (0.44) Total Dashboard Views 26.10 (38.23) dbrd.events.bfr 23.23 (16.31) dbrd.events.aftr 25.29 (21.85)  Several variables such as the average session length (in minutes) or the average events per session have high standard deviations.  4.1 Access to the Dashboard The first study aims at characterizing students based on the num-  ber of times they access the dashboard. We used the number of dashboard events per user and the K-means algorithm to calculate the clusters. The elbow method showed an optimum of 4 clusters with a quotient between the Sum of Square in Between (SSB) and the Total Sum of Squares (SST) of SSB/SST = 92.5% denoting a clear differentiation among the clusters.  Table 2 shows the cluster centers and the percentage of students in each cluster. The clusters have been labeled Low Engagement (LE), Medium Engagement (ME), High Engagement (HE) and Ex- treme Engagement (EE). The LE cluster contains most of the co- hort and they viewed the dashboard once every 10 study sessions. The ME cluster contains students who viewed the dashboard ev- ery 2 sessions. Almost 10% of the students access the dashboard once per study session and there is a small percentage of students checking the dashboard more than once per session  Given the structure of the course, only a subset of activities affect the values shown in the dashboard. For this reason, the LE cluster seems to contain the expected behavior whereas the access rate for the HE and EE clusters is unusually high. This result provides a characterization based on the number of accesses. A more nuanced analysis is needed to distinguish the type of usage.  4.2 Dashboard Interaction Styles  Table 2: Cluster Centers and Student Proportions  Cluster Proportion Dashboard Views Per Session LE 64.57% 0.143 ME 24.21% 0.595 HE 9.41% 1.213 EE 1.79% 2.615 SSB / SST = 92.5% N = 224  The second study aimed at gaining a deeper understanding of how students interacted with the dashboard. For each student we selected the sessions that contained at least one dashboard view and then calculated the number of regular events before and after the first dashboard access.  The K-Means algorithm was also used for this analysis and, as in the previous case, an optimum number of clusters of 4 was found using the elbow method. In this case we obtained the quality metric SSB/SST = 84.7%, lower than in the previous case, but still denoting a robust differentiation among clusters.  Table 3 shows the cluster centers. Based on the number of events before and after a dashboard view, the clusters were labeled as:  1. Dashboard View in the middle of a study session (DBM)  2. Dashboard View in the beginning of a study session (DBB)  3. Dashboard View in the middle of a long study session (DBML)  4. Dashboard View near the end of a study session (DBE)  Table 3: Dashboard Styles and Proportions  Cluster Proportion events.bfr events.aftr Middle (DBM) 38.91% 12.33 10.52 End (DBE) 21.72% 44.62 14.72 Beginning (DBB) 28.99% 19.39 35.86 Mid. Long (DBML) 10.41% 30.06 73.13 SSB / SST = 84.6%, N = 221  The distribution of students in these clusters is more balanced that in the previous analysis. The largest cluster, DBM, contains those students that viewed the dashboard right in the middle of their study session. A possible interpretation is that the dashboard provided students in this cluster information consulted after cer- tain steps in a study session. The second largest cluster, DBB, may contain students that the need to know in advance where they are positioned with respect to the activities requested in a week. The DBML cluster is very similar to DBB except that the students tended to have longer study sessions. The two clusters (DBB and DBML) account for almost 40% of students pointing to the most common use of the dashboard. The smaller cluster, DBE, contains students that access the dashboard towards the end of the study ses- sion. These students may use the dashboard only to verify that it is at the expected levels. The interpretation of these results provides a preliminary description of how students interact with the dash- board. A more comprehensive data collection with qualitative data derived from interviews would be required to verify these assump- tions.  4.3 Interaction Styles over the Semester The dashboard provides feedback about the engagement for one  week in the course. Every seven days all indicators are reset to zero. This feature was decided to provide students with a more    Figure 2: Evolution of Dashboard Interaction Styles over 12 Weeks  accurate notion of their sustained effort. In this context we wanted to explore if the use of the dashboard changed over the duration of the semester. This part of the study was conducted based on the clusters obtained in the previous section. The events were divided into 12 regions, one for each week in the semester (weeks 1 and 6 were discarded as there were no preparation activities scheduled).  For every week and student, we extracted all sessions contain- ing dashboard views. We then calculated the events before and after each dashboard views as in the second analysis and averaged all sessions for each student. The K-Means model created in the previous section was used to predict the category of the dashboard interaction for every week of the semester. Figure 2 shows the evo- lution of the number of students in each cluster over the 12 week period.  The DBM cluster is the predominant type of interaction in almost every week of the course. This means that most of the students sys- tematically used the dashboard as a reference in the middle of their work sessions. The evolution of the DBB cluster shows an interest- ing pattern. After a large number of students being in this category in the first and second weeks, the size of the cluster has a very sig- nificant reduction. The evolution of the number of students in the DBE cluster is almost the opposite. The first two weeks the cluster has a reduced number of students, but then it grows until it becomes the second most populated cluster. A possible explanation for this trajectory is because the course was a first-year course students needed to monitor their engagement to adapt to the new context. As the weeks went by students only needed the dashboard towards the end of their study session to verify that they were achieving the expected levels of engagement. A more detailed study would be required to establish the causality of this relation.  The DBML cluster starts with a modest number of students and remains with a minority of members throughout the semester. A user is included in this cluster when the dashboard is in the middle of a session, but the session has a large duration. If we consider both the DBM and DBML clusters together we can conclude that as the semester advances more students use the dashboard in the middle of their study sessions.  Aside from the evolution of the size of the clusters over time, the figure also shows a decrease on the number of accesses to the dashboard as the semester advances. This seems to reinforce the hypothesis that as the semester advances students do not need to  consult the resource as often. An increase in familiarity with the required tasks may cause a less frequent access to the feedback about the level of engagement.  4.4 Dashboard Use and Midterm Scores The final study explored the relation between the use of the dash-  board and the scores obtained in the midterm examination held in Week 6 of the semester and worth 20% of the total course marks. A model was derived using linear regression between the scores and the average dashboard views (ADVS). The linear coefficients had no statistical significance and R2 = 0.002 suggesting that the number of dashboard views are not associated with the midterm score.  5. CONCLUSIONS The wide variety of data being collected through the use of tech-  nology in learning scenarios provides an unprecedented opportu- nity to improve the quality of the student experience. Numerous initiatives focus on discovering knowledge to be used by the in- structor to provide student support. Using analytics to provide real-time feedback to students is a much less explored area. Real- time feedback becomes more relevant for learning experiences that require frequent student engagement. Active learning or flipped learning are strategies that require students to interact frequently with the learning environment. This paper has presented a study case to explore student behavior in the context of a flipped learn- ing course in which students are given indicators of their weekly engagement through a dashboard integrated in the online platform. The results identified four clearly delimited cluster of users that used the dashboard differently to observe their level of engagement with respect to a reference initially defined by the instructor. Addi- tionally, students change the use of the dashboard as they progress through the learning experience suggesting a process of adaptation to the course requirements in terms of participation. No statistically significant relation has been found between the use of the dash- board and academic performance.  These initial results point to various avenues for further explo- ration. The most important is the nee to complement the quanti- tative data with qualitative information derived from student inter- views. A more detailed description of how students perceive the    data could deepen the understanding for the changes in the use of the dashboard and its connection with learning approaches. Addi- tionally, using different versions of the dashboard can help to iden- tify the most valuable format to present the information to students. Further improvements could be obtained by analyzing the data in relation with the type of activities in the course. Detecting varia- tions in the access to the dashboard depending on the type of ac- tivity could provide a more precise characterization of its use and even inform the learning design process.  6. REFERENCES [1] P. Baepler and C. J. Murdoch. Academic Analytics and Data  Mining in Higher Education. International Journal for the Scholarship of Teaching and Learning, 4(2), 2010.  [2] S. Bull and J. Kay. Student Models that Invite the Learner In : The SMILI:() Open Learner Modelling Framework. International Journal of Artificial Intelligence in Education, 17(2):89120, 2007.  [3] J. Campbell, P. DeBlois, and D. Oblinger. Academic Analytics. Technical report, EDUCAUSE White Paper, 2007.  [4] S. Dawson, A. Bakharia, and E. Heathcote. SNAPP: Realising the affordances of real-time SNA within networked learning environments. In L. Dirckinck-Holmfeld, V. Hodgson, C. Jones, M. D. Laat, D. McConnell, and T. Ryberg, editors, International Conference on Networked Learning, pages 125133, 2010.  [5] E. Duval. Attention please!: learning analytics for visualization and recommendation. In International Conference on Learning Analytics, pages 917. ACM Press, 2011.  [6] R. M. Felder and R. Brent. Active Learning: An Introduction. ASQ Higher Education, 2(August), 2009.  [7] R. Ferguson. The State of Learning Analytics in 2012: A Review and Future Challenges a review and future challenges. Technical report, The Open University UK, 2012.  [8] S. Freeman, S. L. Eddy, M. McDonough, M. K. Smith, N. Okoroafor, H. Jordt, and M. P. Wenderoth. Active learning increases student performance in science, engineering, and mathematics. Proceedings of the National Academy of Sciences of the United States of America, pages 16, May 2014.  [9] J. Fritz. Classroom walls that talk: Using online course activity data of successful students to raise self-awareness of underperforming peers. The Internet and Higher Education, 14(2):8997, Mar. 2011.  [10] S. P. Goggins, K. Galyen, and J. Laffey. Network Analysis of Trace Data for the Support of Group Work: Activity Patterns in a Completely Online Course. ACM Press, 2010.  [11] P. Goldstein and R. Katz. Academic analytics: The uses of management information and technology in higher education, volume 8. Educause Center for Applied Research, 2005.  [12] J. Grann and D. Bushway. Competency Map: Visualizing Student Learning to Promote Student Success. In Proceedings of the Fourth International Conference on Learning Analytics And Knowledge, LAK 14, pages 168172, New York, NY, USA, 2014. ACM.  [13] J. Hattie and H. Timperley. The Power of Feedback. Review of Educational Research, 77(1):81112, Mar. 2007.  [14] J. A. C. Hattie. Visible learning: A synthesis of over 800 meta-analyses related to achievement. Routledge, New York,  2008. [15] V. Kovanovi, D. Gaevic, S. Dawson, S. Joksimovic, R. S.  Baker, and M. Hatala. Penetrating the Black Box of Time-on-task Estimation Categories and Subject Descriptors. In Proceedings of the International Conference on Learning Analytics and Knowledge. ACM Press, 2015.  [16] A. E. Krumm, R. J. Waddington, S. D. Teasley, and S. Lonn. A Learning Management Sytem-Based Early Warning Sytem for Academic Advising in Undergraduate Engineering. In Learning Analytics. From Research to Practice, pages 103119. Springer New York, 2014.  [17] S. Lonn, A. E. Krumm, R. J. Waddington, and S. D. Teasley. Bridging the Gap from Knowledge to Action : Putting Analytics in the Hands of Academic Advisors. In International Conference on Learning Analytics and Knowledge, pages 184187. ACM Press, 2012.  [18] L. P. Macfadyen and S. Dawson. Mining LMS data to develop an early warning system for educators: A proof of concept. Computers & Education, 54(2):588599, Feb. 2010.  [19] R. Mazza and V. Dimitrova. CourseVis: A graphical student monitoring tool for supporting instructors in web-based distance courses. International Journal of Human-Computer Studies, 65(2):125139, Feb. 2007.  [20] R. Mazza and C. Milani. Gismo: a graphical interactive student monitoring tool for course management systems. TEL04 Technology Enhanced Learning04 . . . , 2004.  [21] L. V. Morris, C. Finnegan, and S.-S. Wu. Tracking student behavior, persistence, and achievenet in online courses. The Internet and Higher Education, 8:221231, 2005.  [22] A. Pardo and C. Delgado Kloos. Stepping out of the box. Towards analytics outside the Learning Management System. In International Conference on Learning Analytics, pages 163167. ACM New York, USA, 2011.  [23] G. Reimers and A. Neovesky. Student Focused Dashboards. An Analysis of Current Student Dashboards and What Students Really Want. In International Conference on Computer Supported Education, 2015.  [24] C. Romero and S. Ventura. Educational data mining: a review of the state of the art. IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 40(6):601618, 2010.  [25] J. Saltz, R. Hiltz, and M. Turoff. Student social graphs: Visualizing a students online social network. Nov. 2004.  [26] G. Siemens and P. Long. Penetrating the Fog: Analytics in Learning and Education. Educause Review, 48(5):3140, 2011.  [27] Z. Tanes, K. E. Arnold, A. S. King, and M. A. Remnet. Using Signals for appropriate feedback: Perceptions and practices. Computers & Education, 57(4):24142422, Dec. 2011.  [28] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. L. Santos. Learning Analytics Dashboard Applications. American Behavioral Scientist, (February), Feb. 2013.  [29] C. E. Wieman. Large-scale comparison of science teaching methods sends clear message. Proceedings of the National Academy of Sciences of the United States of America, 111(23):831920, June 2014.  [30] M. Wright, T. McKay, C. Hershock, K. Miller, and J. Tritz. Better Than Expected: Using Learnign Analytics to Promote Student Success in Gateway Science. Change: The Magazine  of Higher Learning, 46(1):2834, 2014.    