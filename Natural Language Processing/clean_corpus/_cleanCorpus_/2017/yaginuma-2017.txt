Video Annotation Tool for Learning Job Interview     Yoshitomo Yaginuma  The Open University of Japan,    Japan   yaginuma@ouj.ac.jp        Masako Furukawa   National Institute of Informatics,   Japan   furukawa@nii.ac.jp        Tsuneo Yamada   The Open University of Japan,   Japan    tsyamada@ouj.ac.jp     ABSTRACT  In this paper, video annotation tool for learning job interview is  proposed. To visualize the difference of obtained descriptions, the  proposed tool uses correspondence analysis. The results of  correspondence analysis are used to give feedback to learners. By  the results, the learner can understand the characteristics of his/her  descriptions among the others.   CCS Concepts   Applied computingEducationE-learning.   Keywords  Video Annotation Tool, Job Interview, Visualization.   1. INTRODUCTION  In the job interview scene, not only verbal communication but also  non-verbal communication is important [1]. To learn such job  interview, Furukawa et al. developed a multi-angle video  annotation tool with layout editing function [2]. Pardo et al.  utilized video annotation tool to identify learning strategies [3].   On the other hand, analysis of students' learning behaviors plays  an important role to realize adaptive and effective feedback. For  example, Bouchet et al. employed a differential sequence mining  technique to identify activity patterns between student groups [4].  Peckham et al. described how multidimensional k-means  clustering combined with Bloom's Taxonomy can be used to  determine positive and negative cognitive skill sets [5]. Sharma et  al. developed a tool that provides a visual-aid superimposed on the  video [6].   In this paper, we describe video annotation tool for learning job  interview and its feedback mechanism that visualizes difference  between learners.    2. VIDEO ANNOTATION TOOL  For the better understanding of job interview, it is useful to  evaluate job interview scenes of others. For this purpose, we  developed a video annotation tool. This tool was developed as a  standalone application that runs on the Mac OS. Using the  proposed tool, the learners write down what they have noticed in  the video.    Figure 1 shows the interface of the video annotation tool. This tool  can play multi-angle video as shown on the left side of the figure.  In the upper right part of the tool, the learners can set the start time   and the end time of the annotation.  In the center right part of the  tool, the learners can mark the attention area in the video by  drawing a rectangle. In the lower right part of the tool, the learners  can describe what they have noticed. By clicking the bottom  button, the descriptions are saved in CSV format, which is  readable by statistical analysis tools such as R.       Figure 1. Video annotation tool.   3. EXPERIMENT  3.1 Data Acquisition   To evaluate the proposed tool, evaluation experiment was carried  out. An interview scene of Chinese student was used in the  experiment. The interview scene was taken by 3 cameras, which  were placed to take interviewer, interviewee, and both of them.  The questions asked by the interviewer were shown in Table 1.    21 participants annotated the video. 7 of them are Japanese  students, and 7 of them are Chinese students. The other 7 have  experience of evaluating interviewees. These data are used to see  the difference of viewpoints between students and interviewers.   Table 1. Questions in the job interview.   Please introduce yourself.   What is the thing you have been devoted to   What did you major in at university   What is the theme of the graduation thesis   Do you have any job experience   What are you interested in lately   What is the reason for you to apply for us   What do you want to do with us   Please tell me your vision after 5 years.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '17, March 13-17, 2017, Vancouver, BC, Canada  ACM 978-1-4503-4870-6/17/03.  http://dx.doi.org/10.1145/3027385.3029444     3.2 Analysis  To visualize the difference of descriptions, the proposed tool uses  correspondence analysis. The correspondence analysis is a method  that is widely used in the natural language processing. At first, by  morphological analysis of descriptions, only the nouns are  extracted. Then, 60 keywords that are effective to distinguish user  groups are extracted. The frequency of these keywords is  calculated for each learner, and reduced to two-dimensional  vectors using correspondence analysis. The locations of the 60  keywords in the same two-dimensional space are also calculated.    The results of correspondence analysis are shown in Figure 2. In  the figure, the learners () are located based on the similarity of  descriptions. J1 - J7 indicate Japanese students. C1 - C7 indicate  Chinese students. I1 - I7 have experience as interviewers. The  related keywords () are also shown in the figure.   In the lower left part of the tool, there are I4, I5, I6 and I7, who  have experience as interviewers. There are keywords such as   "Graduation thesis", "Answer" and "Reason" in this area. This  implies that they are paying attention to the content of the answers.  In the center left part of the tool, there are C1, C3, C6 and C7, who  are Chinese students. There are keywords such as "Voice", "Eye  direction" and "Greeting" in this area. This implies that they are  paying attention to the appearance of the interviewee. In the upper  left part of the tool, there are J3, J4 and J6, who are Japanese  students. There are keywords such as "Door", "Room" and  "Impoliteness" in this area. This implies that they are paying  attention to the behavior when the interviewee enters the room.  These results are useful to see the characteristics of learners.     Figure 2. Results of correspondence analysis.   4. FEEDBACK TO LEARNERS  Figure 2 is used to give feedback to learners. After a learner  annotates the video, the results of correspondence analysis are  shown including the learner's data. By the results, the learner can  understand the characteristics of his/her descriptions among the  others.    When the user ID in Figure 2 is clicked, related descriptions are  shown as Figure 3. The descriptions of selected user are shown in  the lower right part of the window. Since the overlay of the   descriptions on the video is realized by SMIL (Synchronized  Multimedia Integration Language), the descriptions can be  displayed on a Web browser.      Figure 3. Playback of descriptions.   5. CONCLUSION  In this paper, video annotation tool for learning job interview is  proposed, and preliminary results of evaluation experiments were  shown. Using the proposed tool, the learner can understand the  characteristics of his/her descriptions among the others, and can  understand the viewpoints that the learner has not noticed but the  others have noticed. The detailed evaluation of the proposed tool  will be the future work.   6. REFERENCES  [1] von Raffer-Engel, W. (ed.). 1980. Aspects of Nonverbal   Communication. Swets & Zeitlinger.    [2] Furukawa, M., Yaginuma, Y., Yamada, T. 2004. Developing  a multi-angle video annotation package with layout editing.  In Proceedings of the World Conference on Educational  Multimedia, Hypermedia & Telecommunications, 1773-1778.   [3] Pardo, A., Mirriahi, N., Dawson, S., Zhao, Y., Zhao, A. and  Gasevic, D. 2015. Identifying Learning Strategies Associated  with Active use of Video Annotation Software. In  Proceedings of the 5th International Conference on Learning  Analytics & Knowledge, 255- 259   [4] Bouchet, F., Kinnebrew, J., Biswas, G. and Azevedo, R. 2012.  Identifying Students' Characteristic Learning Behaviors in an  Intelligent Tutoring System Fostering Self-Regulated  Learning. In Proceedings of the 5th International Conference  on Educational Data Mining, 65-72.   [5] Peckham, T. and McCalla, G. 2012. Mining Student Behavior  Patterns in Reading Comprehension Tasks. In Proceedings of  the 5th International Conference on Educational Data Mining,  87-94.   [6] Sharma, K., Alavi, H., Jermann, P., and Dillenbourg, P.  2016.  A Gaze-based Learning Analytics Model: In-Video Visual  Feedback to Improve Learners Attention in MOOCs. In  Proceedings of the 6th International Learning Analytics and  Knowledge Conference, 417-421.     