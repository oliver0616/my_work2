Understanding Student Learning Trajectories Using  Multimodal Learning Analytics within an Embodied-  Interaction Learning Environment  Alejandro Andrade   Indiana University  laandrad@indiana.edu     ABSTRACT  The aim of this paper is to show how multimodal learning analytics  (MMLA) can help understand how elementary students explore the  concept of feedback loops while controlling an embodied  simulation of a predator-prey ecosystem using hand movements as  an interface with the computer simulation. We represent student  motion patterns from fine-grained logs of hands and gaze data, and  then map these observed motion patterns against levels of student  performance to make inferences about how embodiment plays a  role in the learning process. Results show five distinct motion  sequences in students embodied interactions, and these motion  patterns are statistically associated with initial and post-tutorial  levels of students understanding of feedback loops. Analysis of  student gaze also shows distinctive patterns as to how low- and  high-performing students attended to information presented in the  simulation. Using MMLA, we show how students explanations of  feedback loops look differently according to cluster membership,  which provides evidence that embodiment interacts with  conceptual understanding.    CCS Concepts   Applied computing ~ Computer-assisted instruction      Applied computing ~ Interactive learning environments   Keywords  Embodied Cognition; Embodiment; Learning Environments;  Multimodal Learning Analytics; Sensing Technologies; Science  Education.   1. INTRODUCTION  Learning Analytics aim to understand and improve learning by  means of analyzing large quantities of student data that has been  automatically captured within technologically-rich learning  environments [21; 22]. To better understand learning and to provide  insights into particular learning processes, researchers use Learning  Analytics to correlate traces of student behaviors with measures of  student academic performance or conceptual, cognitive, or  emotional processes [13]. Furthermore, with the use of emerging  sensing technologies (e.g., Kinect, Leap Motion, Computer Vision)  researchers have been able to develop new learning environments   to support learning via embodied interactions [2; 19] as well as to  capture large amounts of learning-related data through multiple  modalities such as bodily movements, eye gaze, and speech  prosodics [8; 20]. The aim of this paper is to show how we used  multimodal learning analytics (MMLA) to help answer the question  of how elementary students explore the concept of feedback loops  while controlling an embodied simulation of a predator-prey  ecosystem through bodily movementsusing hand movements as  an interface with the computer simulation.  Our goal was twofold: (a) represent student motion patterns from  fine-grained logs of hands and gaze data, and then (b) map these  observed motion patterns against levels of student performance to  make inferences about how embodiment plays a role in the learning  process. This endeavor was technically demanding because motion  tracking is captured by fine-grained temporal data. These time- dependent data had to be analyzed with statistical models that can  account for the lack of independency inherent within body  movement observations. Our methodological approach can be  summarized in the following steps: (1) find motion vectors; (2)  translate the combination of observed motion vectors into latent  motion states using a Hidden Markov Model; (3) compute distance  measures from the sequences of latent motion states using an  Optimal Matching algorithm that accounts for the transition  probabilities; (4) find natural groupings, or clusters, across students  using the distance matrix in a Hierarchical Cluster analysis; (5) find  the association between these clusters and student performance;  and (6) qualitatively show how these clusters represent different  patterns of student reasoning.   Results show that students can benefit from short embodied  tutorials to learn about ecosystem dynamics. Five distinct motion  sequences can represent the variability in the students embodied  interactions with the simulation, and these motion patterns are  statistically associated with initial and post-tutorial levels of  students understanding of feedback loops. Analysis of student  gaze shows distinctive patterns as to how low- and high-performing  students attended to information presented in the simulation. We  show how students explanations of feedback loops look differently  according to cluster membership, which provides evidence that  embodiment interacts with conceptual understanding. Because of  this interaction, and through the use of MMLA techniques, in this  paper we show that embodiment can be used as an indicator of  student current conceptual level, and also as a promising  instructional strategy to foster conceptual developmentas various  theoretical approaches of embodiment have suggested [3; 17].   2. EMBODIED LEARNING AND SENSING  TECHNOLOGIES  Recent theories of learning and cognition, called embodied  cognition and embodied learning, build upon the hypothesis that  body movement and interaction with the physical world has deep   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '17, March 13-17, 2017, Vancouver, BC, Canada   2017 ACM. ISBN 978-1-4503-4870-6/17/03$15.00  DOI: http://dx.doi.org/10.1145/3027385.3027429     consequences on peoples cognitive processes [3; 12; 17; 19; 24].  It follows that, if cognition is grounded in body movement, then  increasing ones body movement repertoire might spark the  learning of a new concept (or at least the learning of the same  concept in a new light). To elicit movement in such specific ways,  the use of sensing technologies (e.g., Kinect, Leap Motion) in the  design of new learning environments is gaining traction. This is  because these emerging technologies can shape the ways in which  students interact with the computer and help students explore new  ways in which they can move. Therefore, studying the role  technology plays in cueing the body to move in particular ways,  and how this movement facilitates learning and conceptual  development, is of great relevance to furthering our understanding  of how people learn through embodied interactions [9; 18].  However, while most studies have focused on spontaneous  movements (where students freely explore the interaction space),  few studies have looked into eliciting certain kinds of bodily  movements for embodied learning. Because our focus on body  movement is with the students hands in particular, we try to elicit  certain kinds of gestures. In elicited gesturing, the student is cued  to gesture in a particular way [18], congruent with the to-be-learned  concept [6; 16], as illustrated in Figure 1.       Figure 1. Student use bimanual gestures to represent an  inverse relationship between two variables.   In order to study a movement repertoire, new research tools can  shed light on students embodied learning. Specifically, learning  analytics can provide powerful insights into understanding  particular aspects of the learning process, such as embodied  learning. When using sensing technologies to capture students  observable behaviors, these tools are referred to as Multimodal  Learning Analytics [MMLA, 7]. MMLA represents observable  behaviors via multiple modalities such as body movement,  gestures, gaze tracking, speech prosodics, and other discourse  features. Using MMLA techniques, we explore the analysis of  embodied learning in the context of short tutorials with a group of  3rd and 4th graders who interacted with an embodied simulation. We  designed this embodied simulation with the intention to help  students explore complex systems concepts by gesturing with both  hands to represent dynamic relationships within a predator-prey  ecosystem.   3. AN EMBODIED SIMULATION OF  PREDATOR-PREY DYNAMICS    We designed a predator-prey simulation using computer vision to  track color in a video stream using a Python algorithm to track  students hand movements while they interacted with a graphing  simulation, as follows: The student holds two colored balls (see   Figure 2.a) to interact with both the bar chart and line graph  displayed on the computer screen (see Figure 2.b). The red bar (left)  represents the hypothesized fox population size, whereas the blue  bar (right) represents the rabbit population size. The tutorial walks  the student through some tasks of increasing difficulty, from pattern  matching only one bar height to more difficult tasks where the  student replicates the full predator-prey dynamics on her own. We  designed this interaction for congruency between the system (two  population variables) and the interaction afforded by the computer  interface (bimanual gestures).      Figure 2. Our Predator-Prey Dynamics Embodied Simulation.  Our focus is to explore whether and how short tutorials with our  embodied learning environment help elementary students  recognize ecosystem dynamics (e.g., feedback loops and delays) in  the context of predator-prey relationships. Our preliminary results  show that students tend to include more gestures in their  explanations of system dynamics after the tutorial intervention [4;  5]. We argue that students post-tutorial gestures are learned during  the tutorial time, while they interact with the embodied simulation.  Thus, we hypothesize that an analysis of the log data can reveal  patterns in students movements, which then would correlate with  various learning levels. Therefore, our research questions are: Did  students learn about feedback loops after interacting with the  embodied simulation Are there any distinguishable patterns in  students embodied interactions And, do these patterns correlate  with students learning of predator-prey dynamics  In order to answer these questions, we use MMLA to explore fine- grained representations of student bodily movements during their  embodied interaction with the simulation. The goal was to perform  an exploratory analysis in which students hand movements and  gaze direction are regarded as performance indicators. These  performance indicators represent particular ways in which students  interact with the simulation, and are mapped onto various student     conceptual understanding levels. In particular, MMLA analysis is  used to explore how students reveal different patterns to move the  colored balls. We hypothesize that some patterns are more  productive than others. The productive patterns would capture  student understanding of system dynamics, and therefore the  gestures within this pattern would embody a fluid representation of  feedback loops. Less productive patterns, on the other hand, would  embody these feedback loops only partially. Because we gathered  a large number of data points per student, these patterns can be  expressed numerically and, therefore, differences can be compared  statistically.  Additionally, we compare student gaze data to explore whether  there were also differences in information encoding. Because the  students are presented with two types of visual representations, a  bar chart and a line graph, they were actively integrating between  these two sources of visual information. Gaze analysis can help  answering the question of whether students engaged with these  graphs differently, and, hopefully, some possible reasons as to why.  These differences can be indicative of different student readiness  for interacting with these kinds of representations, and even shed  light on possible reasons as to the observed variation in reasoning  levels during the post-tutorial explanations.  In the following sections, we present a detailed description of these  analyses, but first we provide a brief description of the context  where the data was collected.   4. METHODS  Fifteen 3rd and 4th graders (F = 8, M= 7, Avg. Age = 9.13, SD Age  = 0.8) from a mixed class at a school in the Midwest of the US took  part in the study. Students were individually interviewed and they:  (a) answered a pre-tutorial questionnaire, then (b) interacted with  the simulation, and then (c) answered a post-tutorial questionnaire.  Interviews were videotaped and the three interview phases took  thirty minutes in average. The embodied tutorial took about  eighteen minutes and the protocol included nine tasks (each task  lasting for about two minutes). The pre and post-tutorial  questionnaires were adapted from Hokayem, Ma, and Jins [14]  study. We scored students answers with a Feedback Loop  Reasoning coding scheme, also adapted from Hokayem et al. [14]  which consists of seven levels (the higher the level the more in- depth the student understanding is), detailing the reasoning  progression about feedback loops within a predator-prey  ecosystem.   In order to analyze the students hands movements, we focused on  a 2-min task which required students to match both population bars.  The prompt for this task read: As in the previous tasks, the  computer is moving the bars up and down. Now, try and match the  two populations by moving both balls with your hands. In the  following sections, we explain how we built a representation of the  bodily movements with a set of motion sequence typologies, and  how we studied whether these types of motion sequences were  correlated with student understanding of feedback loops.   4.1 Motion vectors  In order to spot distinct patterns in students actions, we used the  simulation log data to create movement vectors of students hands  (see Figure 3). The computer tracks students hand movements and  predicts the direction of the movement of each hand, whether it is  going up, down, or remains static in each time interval. Each time  interval was composed of two consecutive video frames (video  frame rate was seven frames per second). The motion vectors were,  thus, the difference between frame at time t+1 and frame at time t.  In order to account for the idiosyncratic amplitude of each student   movements, the motion vectors were normalized by dividing each  value over the maximum motion vector value per student. Then, to  better distinguish whether the hand was static or actually moving  up or down, a threshold was used to reduce the noise in the values  captured by the tracking system. The threshold was defined as the  semi-interquartile range of the motion vector values per student.     Figure 3. In (a) both hands are moving in opposite directions   simultaneously, whereas in (b) the student moves only one  hand at a time.   4.2 Latent motion states  Direction vectors only indicate the direction of movement between  time t and t+1. If the interest is in understanding the students  underlying intentions with the bimanual movement, a statistical  model can be used to represent the students mental states at each  time point for the duration of the activity. For instance, the student  might be trying to coordinate a motor schema of a simultaneous  movement of one hand going up and the other hand going down.  Or perhaps the student might be trying to coordinate the movement  of one hand after she starts the movement of the other hand. When  using a model-based statistical approach, latent states can be  inferred from the patterns of fine-grained hands movements. This  data reduction would go from two data streams of (categorical)  direction vectors, to a sequence of (cognitive) motion states. A  Hidden Markov Model was fit to the time-series of motion vectors.  An HMM model assumes that the observed combinations of  direction vectors at every point in time are produced by the previous  time point and by a finite set of latent cognitive states. The model  inputs a sequence of observations and predicts a sequence of latent  states of length N, where N is the number of time points in the data  frame. An example of a hypothetical predicted 9-State sequence, fit  to a 150-frame window of a students movement data, is shown in  Figure 4. The top of Figure 4 shows the plotted hands position over  time as blue and red lines. The vertical lines show where there is a  state change in the hands coordination. The periods between  vertical lines correspond to latent states. Figure 4, below, plots  when each state is occurring throughout the time series. The state  sequence is plotted as a sequence of colors where each color  represents a state. This colored graphical representation is preferred  because it does not convey the idea that the latent states have any  meaningful order.  The meaning of the latent states is evaluated by examining the  composition of the mixture of the observed variables. In order to  find the number of latent states responsible for the observed  combinations of direction vectors, several models are fit to the data  and the best model is selected using a fit index, in this case the  negative log likelihood. The HMM also produces a transition  probability matrix between latent states. This matrix contains  information as to which states are more likely to follow after which  states. To run the HMM models, we used the depmixs6 R package  [23]. In summary, our analysis of the students hands movement     requires us to understand the presence of sensorimotor schemas.  Therefore, the HMM helps us translate the hands movement raw  data into a vector of latent states (i.e., a bimanual movement  arrangement) per student.           Figure 4. An example of a sequence of latent states. Top: the  trajectory of hands movement over time (right hand in red   and left hand in blue). The vertical lines show the division of  the hands trajectory by latent state. Bottom: a categorical plot   with the inferred/predicted latent states.   4.3 Dissimilarity within latent motion state  sequences  To characterize the students latent motion states sequences and to  account for time dependencies, a dissimilarity measure was  produced using an Optimal Matching (OM) algorithm. This  quantitative measure is important in that it appraises the degree of  similarity between students level of sensorimotor coordination.  The OM algorithm is a dissimilarity measure, part of the family of  measures known as edit distances, based on the minimal cost of  transforming one sequence into the other [10]. Temporal  information is accounted for by insertion/deletion (indel) costs and  transition values. The OM algorithm inputs a matrix of sequences,  an indel cost, and a matrix of substitution costs based on the matrix  of latent states transition rates. The output is a matrix of pairwise  distances quantifying the number of transformations (insertions,  deletions, or substitutions) required to transform one sequence into  another. To run the OM algorithm, we used the TramineR R  package [10].   4.4 Hierarchical clustering  After the distance matrix is calculated, an agglomerative  hierarchical clustering is used to obtain distinctive sequence  typologies based on the empirical relationships observed in the  data. To run the cluster analysis, we used the cluster R package.   4.5 Categorical statistical analysis  After the sequence typologies were identified and each student  assigned to a particular cluster, various categorical analyses were  conducted to measure the association between this set of clusters  and student performance during the pre- and post-tutorial tests.  Specifically, we used the Cochran-Mantel-Haenszel statistic to  capture the association between ordinal and nominal level  variables, such as is the case for levels of feedback loop reasoning  versus motion sequence clusters. To compute the Cochran-Mantel- Haenszel statistic, we used the vcdExtra R package.   4.6 Eye gaze  To analyze eye gaze, we first extracted gaze coordinates by post- processing each video using the OpenFace software [25]. We chose  to post-process the video data because we did not have in-situ eye                                                                     1 Where r = Z / N   tracking data. The OpenFace software, however, allow one to  extract an approximation of eye gaze and retrieves three  dimensional features to project the location of the gaze in every  frame in the video, provided that the algorithm is able to detect a  face in the video frame (see Figure 5). We explored patterns in  student gaze using heat plots created with the contour function in  the MASS R package. Heat plots show the regions on the screen  that were most gazed at by the student during this task. We made  use of the identified clusters of student motion sequences to create  groups of heat plots. We proceeded to interpret the variability in the  gaze to guide our interpretation of the different way students paid  attention to the graphs displayed by the simulation. We hypothesize  that successful students would focus their attention more on the bar  graphs because it is these bar graphs which elicit their hands  movements.      Figure 5. Postprocessing student gaze using the OpenFace   software.   5. RESULTS  Did students feedback loop reasoning increase after interacting  with our predator-prey embodied simulation The level of feedback  loop reasoning significantly increased 2 points in the feedback loop  reasoning scale from pre to post-tutorial scores (Mdn = 4 and 6,  respectively), Z(15) = 2.779, p-value = .008, with a large effect size,  r = .718.1 Although without a control group our causal claims are  not granted, these results suggest that students might have, at least  partially, benefited from our tutorial with the simulation. However,  were there any distinguishable patterns in students embodied  interactions To find these patterns, we conducted a cluster analysis  in two stages. First, we created latent motion states to reduce the  dimensionality of the log data, and then ran a hierarchical cluster  analysis on the sequences of latent motion states across students.    5.1 Five latent motion states  Several HMM models were fit to the motion vector data, and a 5- state model fit the data better (with a -Log Likelihood = -25,527.7,  compared to -30,030.3 from a 4-state model, and -26,176 from a 6- state model). Latent motion state A represents left hand leveled  while right hand is going up; latent motion state B represents left  hand going up while right hand is leveled; latent motion states C  and D represent left hand going up (or down) while right hand goes  down (or up)although one hand may partially stay still; and latent  motion state E represents both left and right hands staying still (see  Table 1).   5.2 Five clusters of latent motion sequences   Using an Optimal Matching algorithm, we created a matrix of  pairwise distances between motion sequences. This distance matrix  was fed into an agglomerative hierarchical cluster analysis using  Wards method. The dendrogram from the cluster analysis informs     the number of distinct types of motion sequences. The dendrogram  suggests a partition of five (although six is possible too) distinct  sequences (see Figure 6).      Table 1. HMM State Representation   Left Hand Right Hand   State Down Static Up Down Static Up  A 0.00 0.74 0.26 0.00 0.00 1.00  B 0.00 0.00 1.00 0.00 1.00 0.00  C 0.00 0.45 0.55 1.00 0.00 0.00  D 1.00 0.00 0.00 0.15 0.35 0.50  E 0.00 1.00 0.00 0.00 1.00 0.00        Figure 6. Agglomerative dendrogram using Wards method.   The dendrogram suggests a four-cluster partition.  In order to find distinctive cluster characteristics, we explored these  differences visually and numerically. Figure 6 shows the  characteristic progression for each cluster. Clusters 3 and 4 seem to  have longer distinct states, compared to the more rapid state  changes of Clusters 1, 2, and 5. In fact, it is apparent that Clusters  3 and 4 have less successive distinctive states (82 and 84,  respectively), compared to the more successive states of Clusters 1,  2, and 5 (116, 117, and 111, respectively). In addition, Table 2  shows the normalized mean state durations per cluster. Results  show that Cluster 1 is dominated by states A and D; in State A, the  right hand is going up while the left hand remains static; and in  State D, the left hand goes down while the right hand goes up. This  combination makes the movement look unsynchronized, as the  simultaneous movement of both hands is followed by the  movement of only one hand (see Figure 8). Clusters 2 and 4 are  dominated by simultaneous, opposite movements of both hands  (States C and D). The difference between Cluster 2 and 4 lays in  the number of distinct states, as mentioned above, and also in the  transition probabilities among states (not shown here). Cluster 3  has, besides simultaneous, opposite movements of both hands  (States C and D), high frequency of State E, which is when both  hands are still. Cluster 5, is dominated by State E, that is, frequent  pauses between movements. How, then, do we make sense of these  different kinds of movement types In order to find how these  distinct typologies are associated with variation in student  conceptual understanding, we correlated these typologies with  students scores in the pre- and post-tutorial questionnaires.    5.3 Association with student conceptual  understanding  Albeit a small sample size, we were able to spot regularities in the  way that student motion sequences correlate with student  conceptual understanding. In particular, we noted that Clusters 1  through 3 are mostly associated with low understanding, whereas  Clusters 4 and 5 are associated with higher understanding (see  Table 3). Specifically, evidence from the pre-tutorial questionnaire   shows that students in Cluster 1 had the lowest initial performance,  and students in Clusters 2 and 3 performed at a low to moderate  level. On the other hand, students in Clusters 4 and 5 had a high  initial understanding of feedback loops, and 2 students from cluster  4 were already at the ceiling level. This association is statistically  significant at alpha = .05, X2 (4) = 9.69, p = .045, with a large effect  size, Cramers V = 0.688. Furthermore, evidence from the post- tutorial questionnaire shows that not only is this trend very similar  to the pre-tutorial questionnaire, but also that some clusters made  better learning gains than others. Neither did Cluster 1 nor 4 make  good learning gains. On the contrary, Clusters 2, 3, and 5 students  understanding improved greatly. This association is significant at  alpha = .10, X2 (4) = 9.16, p = .057, with a large effect size,  Cramers V = 0.644.        Figure 7. Five Characteristic Motion Sequences.     Table 2. Normalized State Frequency per Cluster    State    A B C D E   Cluster 1 0.48 -1.05 0.19 1.32 -0.94   Cluster 2 -0.68 -1.16 0.98 1.09 -0.23   Cluster 3 -0.81 -1.32 0.47 0.73 0.92   Cluster 4 -0.1 -1.11 1.2 0.81 -0.81   Cluster 5 -0.49 -0.83 -0.27 -0.14 1.73       Figure 8. Cluster 1 characteristic movements   Table 3. Median Feedback Loops Understanding Scores per  Cluster    1 2 3 4 5  Pre-Test 3.0 3 4.0 6.5 6  Post-Test 3.5 5 6.0 6.5 7   Gain 0.5 2 2.5 0.0 1     These associations can be interpreted in the following way. If a  student shifts through latent motion states very frequently (clusters  1 and 2), she would seem to have a hard time understanding how  she is supposed to move to physically represent the ecosystem  feedback loops, unless, as in the case of Cluster 5 where there is  high frequency of State E, she would stop constantly to reflect on  what this movement should look like. In fact, frequent pauses (State  E) seem to be correlated with better initial understanding and better  learning gains, as evidenced by Clusters 3 and 5, in which there is  a high frequency of State E and also high scores in the post-tutorial  questionnaire. On the contrary, not pausing before shifting to other  motion states may represent that the student is having a hard time  improving on her understanding of these feedback loops (Cluster  1). On the other hand, if a student does not shift frequently through  states, but presents long distinct motion states (Clusters 3 and 4),  she would seem to have a good idea about these feedback loops. If  this motion does not show pausesindicated by low incidence of  State E, then she is likely to have a good initial understanding of  feedback loops and perhaps be already at the ceiling of her  understanding for this particular task (Cluster 4). We complement  our interpretation of these motion sequence typologies by referring  to the patterns in student gaze.  Gaze patterns. In this exploratory analysis, we focus on how  student gaze shifted (or not) throughout the task, and leave for  future analyses the study of what students paid attention to. In  particular, we aim to understand differences in how gaze shifted  over time. Specifically, there seems to be regularities in the  amplitude of students gaze corresponding to different motion  clusters (see Figure 9).      Figure 9. Representative gaze patterns by motion sequence  clusters. The other students gaze plots seem very similar to   these within their respective clusters.    Low performing students (Clusters 1 and 2) tend to have a more  limited gaze band on the screen. High performing students  (Clusters 4 and 5), on the contrary, seem to have broader foci on  the screen. Those students who pause frequently (Clusters 3 and 5),  have also broader gaze patterns than other students. These eye gaze  patterns help explain the differences on apparent similarities on  mean frequency motion states between Clusters 2 and 4. Although  both clusters tend to have high frequency for States C and D,  students in Cluster 2 shifts states more frequently and also fix their  gaze on a particular spot on the screen, whereas students in Cluster  4 shift their gaze to various points on the screen. This ampler gaze  pattern for Cluster 4 might indicate that students are able to better   recognize the information coming from the simulation in order to  map it onto their movement efficiently. The even ampler gaze  pattern in Cluster 5 explains the very frequent pauses in students  movement, and perhaps indicate intense efforts to integrating visual  and motion information together, which seems to have paid off at  the end with good learning gains (as reflected by pre- to post- questionnaire scores increases).   6. DISCUSSION  We found that students can learn about feedback loops from a short  tutorial intervention with our predator-prey simulation. Also, we  found five distinct motion sequences in the way students interacted  with the embodied simulation. Cluster 1 presents an asymmetric  movement; Cluster 2 rapid shifts in the type of movement; Cluster  3 slow shifts and frequent pauses; Cluster 4 slow shifts and  symmetric movement; and Cluster 5 symmetric movement but very  frequent pauses. We also found an association between these  distinct motion sequences and student understanding levels. In fact,  motion sequences are associated with initial levels of understanding  as well as with learning gains, as evidenced by changes between  pre- and post-tutorial scores. Cluster 1 has the lowest performance  and low learning gains. Although it is hard for us to disentangle  with the current experimental design whether or not asking students  to move is preventing them from learning (for that we will use a  control group in future replications of the experiment), the apparent  asymmetry in the students movements might at least suggest a  relationship with the low achievements in this group. Also, this  positive association is illustrated in Cluster 4. Students in Cluster 4  display symmetrical and fluid movements (showing distinct states)  and also a good initial understanding of feedback loops. The  association shows that good conceptual understanding correlates  with good understanding on how to physically represent the  concept via movements. In these two cases (i.e., Clusters 1 and 4),  the student is successful (or not) in embodying the concept through  her physical movements and, thus, we argue that initial conceptual  understanding might have some consequence on the students  performance with the simulator. However, the association between  concepts and embodiment can also start and end the other way  around. The great learning gains of students in Clusters 3 and 5,  evidences that pausing to reflect on how to align ones own  movement with that of the simulation might have helped, albeit  indirectly, students conceptual understanding. We illustrate how  this can happen with the following example (see Figure 10).   The student and the interviewer have an interesting discussion  about what is happening with the motion of predator and prey  populations. In order to reconcile the two opposite forces in the  ecosystem (i.e., predators enable a negative feedback loop because  as their population increases prey population decreases; and prey a  positive one because as their population increases predator  population also increases; which creates a dynamic equilibrium  over time), students have to develop, even if tacit, an understanding  of a lag in the way that population sizes affect each other over  time. In this excerpt, Roberto (pseudonym)who stands in cluster  5considers the full sequence of the ecosystem dynamic. We  suggest that he finds there exists a delay in the way these feedback  loops affect the system, and although he does not articulate the idea  of a lag explicitly, we argue that this delay emerges first as a  sensorimotor scheme, and then as a conceptual gain. It is apparent  in his gestures that this delay helps Roberto resolve his conceptual  conflict about positive and negative feedback loops in the system.  In Lines 14-16, Roberto starts his explanation by articulating the  negative feedback loop as foxes eat the rabbitsi.e., more foxes  less rabbits. He accompanies this with a simultaneous movement of     his right hand down (representing less rabbits) and his left hand up  (representing more foxes). Then, in Lines 17-18, Roberto  articulates a positive feedback loop in which less rabbits would  cause there to be less foxes because the foxes would not have  enough food to eat. But note that he accompanies his verbal  articulation with a gesture where he simultaneously moves his right  hand up (representing more rabbits) and his left hand down  (representing less foxes), and then suddenly stops his movement.  Something really interesting has just happened! In Line 19, as  Roberto notices this speech-gesture mismatch [11], he lowers down  his right hand, correcting his mistake. It is a mismatch because he  has not articulated that rabbits would go up because the foxes are  going down, and we believe that he has discovered that he should  move in a different way. This movement would help him represent  positive feedback loops using a sequential movement with a  delayi.e., between the movement of one hand and the other. The  gesture he probably wanted to perform was to just lower his left  hand (representing less foxes) after his right hand is down   (representing less rabbits); not to move both hands simultaneously  in opposite directions, as he did. Then, in Line 20, Roberto  articulates the next negative feedback in which less foxes would  cause an increase in the number of rabbits, and thus he moves his  right hand up (representing more rabbits). In Line 21, Roberto  engages with the next positive feedback loop, where more rabbits  would cause an increase in the number of foxes, but now he has  learned he has to move one hand only after the other has moved.  Then, Roberto offers a view into his understanding of how there is  a lag between these feedback loops (see Line 22). Our interpretation  of this sequence is as follows. When Roberto initiates the  movement, he leaves a lag between the starting point of one hand  before he starts moving the other. Because Roberto is able to create  this delay, he is able to include both positive and negative feedback  loops in an oscillatory equilibrium, as he has seen in the sinusoidal  graphs on the computer screen after interacting with the embodied  simulation.      13 Interviewer: So both [foxes and rabbits] would go up and down at the same time  14 Student: Okay, so [2 sec pause] so the foxes would be eating all the rabbits          [places both hands in the air in front of him]  [foxes represented by the red ball in his left  hand; rabbits represented by the yellow ball  in his right hand]   15  16   Student: and the rabbits would be just going down and the foxes would be going up         [simultaneously moves his right hand  (rabbits/yellow) down and his left hand  (foxes/red) up]   17  18   Student: But then there arent enough rabbits for the foxes to eat so they would go down uhm         [simultaneously moves his right hand  (rabbits/yellow) up and his left hand  (foxes/red) down, but suddenly stops the  movement]   19 Student: Uhm it would go down           [moves his right hand (rabbits/yellow)  down, as in retracting or correcting his  previous movement]   20 Student: Then the rabbits would go up        [moves his right hand (rabbits/yellow) up  while his left hand (foxes/red) remains still]   21 Student: so they [the foxes] would go up        [moves his left hand (foxes/red) up while  his right hand (rabbits/yellow) remains  still]   22 Student: Kind of like they would go like this...           a. Raises right hand  before moving his  left hand   b. Lowers right hand  while still raising  left hand   c. Pauses right hand  at the bottom while  left hand at the top   d. Lowers left hand  before moving right  hand up   23 Interviewer: Uh-huh, kind of like following each other.  24  25   Student: Yeah, like the rabbit is the first for everything. Its like the first to go down and the first to go up.  The foxes would only go up and down according to where the rabbits go.   Figure 10. Qualitative excerpt of bimanual gestures capture the delay as a new motor-scheme    This is one out of many examples we found in our data where we  observed students making use of congruent gestures after  interacting with our simulation. These congruent gestures help  students map meaning via action information in a holistic way not  readily available in speech alone [16]. However, these gestures  appeared after the tutorial, when we had students explain how they  understand the material. We think that this sort of explaining to  others, which is in and of itself a useful pedagogical set up,   encourages students to build their explanations upon both mental  and physical representations constructed and developed throughout  the tutorial experience. If this tutorial experience would have not  had any effect upon the students understanding of feedback loops  at all, students would not have used this particular kind of  conceptually congruent gestures in their explanations. Thus, there  seems to be an interaction in the way that embodiment and  understanding are related to each other. In our study, it seems that     learning to move in a particular way while reflecting upon what this  movement represents (in terms of predator-prey feedback loops)  seems to be beneficial for students to learn about ecosystem  dynamics. These findings are in line with previous findings from  embodied learning research in mathematics [see for instance, 1; 3;  15].  Nonetheless, further replications of this experiment are required to  rule out other possible explanations for the associations found in  this study. In particular, the lack of a control group makes it  difficult to disentangle whether the increase on feedback loop  understanding scores are due to the embodied experience and not  due to other, conflated, sources of variation. With a control group  where, for instance, a video of the graphical representations of the  dynamic equilibrium between predators and preys was shown, it  would be possible to tease whether these are indeed learning gains  from the embodiment. If the video group has lower scores than the  embodiment group, then one can confidently infer that embodiment  does not prevent students to learn. This conclusion, that  embodiment might be preventing students to learn, is possible  because asking students to follow the movement of the graph bars,  instead of simply watching them move, increase the cognitive load  of the task. The experimental-control comparison will help rule out  that this additional cognitive load is not germane to the learning of  feedback loops.   In addition, to better understand how generalizable our results are,  future replications of the study should look into whether other kinds  of physical movements can support the learning of other concepts.  This type of up-down bimanual movements might be applicable to  only so many conceptual relationships. Other kinds of movements,  however, can be tracked by the computer to expand the range of  congruent mappings that could be done. Also, movement speed can  add a new dimension to the kinds of physical representations that  can be simulated. All in all, the up-down bimanual movement and  the relative position of the hands seem like a good mapping space  for conveying numbers of interrelated populations.   7. CONCLUSION  Using MMLA techniques, we were able to spot differences in  students motion sequences while students interacted with our  embodied simulation. Our methodological approach required us to  represent student movement patterns from the low-level, fine- grained logs of hand movements, and then model latent states and  motion sequences statistically. In doing so, we accounted for the  time dependency between observations and were able to create a  meaningful representation of how students engaged with the  simulation. Because these high-level representations of student  movement were found to be correlated with student learning gains,  we are able to make inferences about how embodiment interacts  with the development of students understanding of feedback loops.  By capturing these patterns, we further our own understanding of  how embodied learning happens, and we believe this knowledge  can help orient new, and improve upon old, learning environment  designs. In particular, the use of bimanual gestures and having  students reflect upon what these movements mean in the context of  predator-prey dynamics, contributed to productively reorient their  attention to the interplay between positive and negative feedback  loops in the system. There are other topics on system dynamics that  we believe can be implemented in the embodied simulationfor  instance, representing patterns of accumulation and rate of change  with body movements. Also, we envision a diagnostic system to  help detect student preparedness level for the embodied tasks. This  is because certain clusters of motion sequences indicated low  learning gains, either because the task seemed outside the student   preparedness levelas was the case for Cluster 1 students-, or  because the task was probably too easyas was the case for Cluster  4 students. This diagnostic system can guide the selection of  subsequent tasks to tailor optimal student learning experiences.  There are a couple of limitations we find worth mentioning. The  sample size in this study was small, and our claims should be taken  tentatively. Because of this, we supplemented significance tests  with effect size measures to provide a practical interpretation of the  statistical results. In future studies, not only will we try and increase  our sample sizes, but also, we envision a more authentic interaction  as we plan to move this embodied learning environment into  elementary school classrooms. These more authentic scenarios will  allow for longer interventions than the short 20-min tutorial  experience from this study. In addition, we have plans to integrating  the Kinect and the Leap Motion sensors to track students  movements in a more unobtrusive way and as precise as possible.  Finally, although this study is of an exploratory nature, our results  show promise and value in continuing to design embodied ways to  support student learning and to develop automated, multimodal  ways to capture how students learn while they explore why it is  important to move in different ways.   8. ACKNOWLEDGMENTS  I am grateful to Joshua A. Danish and Adam J. Maltese for their  support in this project. I am also grateful to Dor Abrahamson and  other three anonymous reviewers for their valuable comments  which have greatly helped improving this paper.   9. REFERENCES  [1] Abrahamson, D., Gutirrez, J., Charoenying, T., Negrete, A.,   and Bumbacher, E., 2012. Fostering hooks and shifts:  Tutorial tactics for guided mathematical discovery.  Technology, Knowledge and Learning, 17(2), 61-86. DOI=   https://doi.org/10.1007/s10758-012-9192-7    [2] Abrahamson, D. and Lindgren, R., 2014. Embodiment and  embodied design. In The Cambridge Handbook of the  Learning Sciences, K. Sawyer (Ed.). Cambridge University  Press, Cambridge, UK, 358-376. DOI=  https://doi.org/10.1017/cbo9781139519526.022    [3] Abrahamson, D. and Snchez-Garca, R., 2016. Learning is  moving in new ways: The ecological dynamics of  mathematics education. Journal of the Learning Sciences  25(2), 203-239. DOI=  https://doi.org/10.1080/10508406.2016.1143370    [4] Andrade , A., Danish, J.A., and Maltese, A., 2017. Why are  you gesturing Elicited gestures and learning gains in an  embodied learning environment. In Proceedings of the Anual  Meeting of the American Educational Research Association  AERA 2017, San Antonio, TX, 1-20.   [5] Andrade , A., Maltese, A., and Danish, J.A., 2017. Foxes and  rabbits, fish and dolphins: Learning and representing  ecosystem dynamics with embodied-interaction sensing  technologies. In Proceedings of the National Association of  Research in Science Teaching NARST 2017, San Antonio,  TX, 1-4.   [6] Barsalou, L.W., 2008. Grounded cognition. Annual Review  of Psychology, 59(1), 617-645. DOI=  https://doi.org/10.1146/annurev.psych.59.103006.093639    [7] Blikstein, P., 2013. Multimodal learning analytics. In  Proceedings of the Third International Conference on  Learning Analytics and Knowledge ACM, Leuven, Belgium,  102-106. DOI= https://doi.org/10.1145/2460296.2460316    [8] Blikstein, P. and Worsley, M., 2016. Multimodal learning  analytics and education data mining: using computational   https://doi.org/10.1007/s10758-012-9192-7 https://doi.org/10.1017/cbo9781139519526.022 https://doi.org/10.1080/10508406.2016.1143370 https://doi.org/10.1146/annurev.psych.59.103006.093639 https://doi.org/10.1145/2460296.2460316   technologies to measure complex learning tasks. Journal of  Learning Analytics, 3(2), 220-238. DOI=  https://doi.org/10.18608/jla.2016.32.11    [9] Dourish, P., 2004. Where the action is: The foundations of  embodied interaction. MIT press, Cambridge, Massachusetts.  DOI= https://doi.org/10.1162/leon.2004.37.1.81    [10] Gabadinho, A., Ritschard, G., Mueller, N.S., and Studer, M.,  2011. Analyzing and visualizing state sequences in R with  TraMineR. Journal of Statistical Software, 40(4), 1-37.  DOI= https://doi.org/10.18637/jss.v040.i04    [11] Goldin-Meadow, S., 2004. Gesture's role in the learning  process. Theory into Practice, 43(4), 314-321. DOI=  https://doi.org/10.1353/tip.2004.0045    [12] Goldin-Meadow, S. and Alibali, M.W., 2013. Gestures role  in speaking, learning, and creating language. Annual Review  of Psychology, 64(1), 257-283. DOI=  https://doi.org/10.1146/annurev-psych-113011-143802    [13] Hershkovitz, A., Knight, S., Dawson, S., Jovanovi, J., and  Gaevi, D., 2016. About learning and analytics. Journal  of Learning Analytics, 3(2), 1-5. DOI=  https://doi.org/10.18608/jla.2016.32.1    [14] Hokayem, H., Ma, J., and Jin, H., 2015. A learning  progression for feedback loop reasoning at lower elementary  level. Journal of Biological Education, 49(3), 246-260.  DOI= https://doi.org/10.1080/00219266.2014.943789    [15] Hutto, D.D., Kirchhoff, M.D., and Abrahamson, D., 2015.  The enactive roots of STEM: Gethinking educational design  in mathematics. Educational Psychology Review, 27(3), 371- 389. DOI= https://doi.org/10.1007/s10648-015-9326-2    [16] Kang, S. and Tversky, B., 2016. From hands to minds:  Gestures promote understanding. Cognitive Research:  Principles and Implications, 1(1), 4. DOI=  https://doi.org/10.1186/s41235-016-0004-9    [17] Lee, V., 2014. Learning Technologies and the Body:  Integration and Implementation in Formal and Informal  Learning Environments. Routledge, New York, NY. DOI=  https://doi.org/10.4324/9781315772639       [18] Lindgren, R., 2015. Getting into the cue: Embracing  technology-facilitated body movements as a starting point for  learning. In Learning Technologies and the Body:  Integration and Implementation in Formal and Informal  Learning Environments, V. Lee Ed. Routledge, New York,  39-54.   [19] Lindgren, R. and Johnson-Glenberg, M., 2013. Emboldened  by embodiment six precepts for research on embodied  learning and mixed reality. Educational Researcher, 42(8),  445-452. DOI= https://doi.org/10.3102/0013189x13511661    [20] Ochoa, X. and Worsley, M., 2016. Editorial: Augmenting  Learning Analytics with Multimodal Sensory Data. Journal  of Learning Analytics, 3(2), 213-219. DOI=  https://doi.org/10.18608/jla.2016.32.10    [21] Siemens, G. and Baker, R., 2012. Learning analytics and  educational data mining: towards communication and  collaboration. In Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge ACM,  Vancouver, British Columbia, Canada, 252-254. DOI=  https://doi.org/10.1145/2330601.2330661    [22] Siemens, G. and Long, P., 2011. Penetrating the fog:  Analytics in learning and education. Educause Review, 46(5),  30-32.   [23] Visser, I. and Speekenbrink, M., 2010. depmixS4: An R- package for hidden Markov models. Journal of Statistical  Software, 36(7), 1-21. DOI=  https://doi.org/10.18637/jss.v036.i07    [24] Wilson, M., 2002. Six views of embodied cognition.  Psychonomic Bulletin & Review, 9(4), 625-636. DOI=  https://doi.org/10.3758/bf03196322    [25] Wood, E., Baltruaitis, T., Zhang, X., Sugano, Y., Robinson,  P., and Bulling, A., 2015. Rendering of eyes for eye-shape  registration and gaze estimation. In 2015 IEEE International  Conference on Computer Vision (ICCV) IEEE, Santiago,  Chile, 3756-3764. DOI=  https://doi.org/10.1109/iccv.2015.428                 https://doi.org/10.18608/jla.2016.32.11 https://doi.org/10.1162/leon.2004.37.1.81 https://doi.org/10.18637/jss.v040.i04 https://doi.org/10.1353/tip.2004.0045 https://doi.org/10.1146/annurev-psych-113011-143802 https://doi.org/10.18608/jla.2016.32.1 https://doi.org/10.1080/00219266.2014.943789 https://doi.org/10.1007/s10648-015-9326-2 https://doi.org/10.1186/s41235-016-0004-9 https://doi.org/10.4324/9781315772639 https://doi.org/10.3102/0013189x13511661 https://doi.org/10.18608/jla.2016.32.10 https://doi.org/10.1145/2330601.2330661 https://doi.org/10.18637/jss.v036.i07 https://doi.org/10.3758/bf03196322 https://doi.org/10.1109/iccv.2015.428  	1. INTRODUCTION 	2. EMBODIED LEARNING AND SENSING TECHNOLOGIES 	3. AN EMBODIED SIMULATION OF PREDATOR-PREY DYNAMICS 	4. METHODS 	4.1 Motion vectors 	4.2 Latent motion states 	4.3 Dissimilarity within latent motion state sequences 	4.4 Hierarchical clustering 	4.5 Categorical statistical analysis 	4.6 Eye gaze  	5. RESULTS 	5.1 Five latent motion states 	5.2 Five clusters of latent motion sequences 	5.3 Association with student conceptual understanding  	6. DISCUSSION 	7. CONCLUSION 	8. ACKNOWLEDGMENTS 	9. REFERENCES   