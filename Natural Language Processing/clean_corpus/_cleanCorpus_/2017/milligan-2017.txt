DesignLAK17: ality metrics and indicators for analytics of assessment design at scale  DesignLAK17 workshop: hps://sites.google.com/site/designlak17  Ulla Ringtved Aalborg University and University  College Northern Denmark Soendalsvej 60  Aalborg, Denmark 9100 ulr@hum.aau.dk  Sandra Milligan University of Melbourne  234eensberry St Parkville, VIC 3010, Australia s.milligan@unimelb.edu.au  Linda Corrin University of Melbourne  Elisabeth Murdoch Building Parkville, VIC 3010, Australia  lcorrin@unimelb.edu.au  Allison Lilejohn e Open University  Walton Hall Milton Keynes MK7 6AA, UK allison.lilejohn@open.ac.uk  Nancy Law University of Hong Kong  Pokfulam Road Hong Kong  nlaw@hku.hk  ABSTRACT Notions of what constitutes quality in design in traditional on- campus or online teaching and learning may not always translate into scaled digital environments. e DesignLAK17 workshop builds on the DesignLAK16 workshop to explore one aspect of this theme, namely the opportunities arising from the use of analytics in scaled assessment design. New paradigms for learning design are exploiting the distinctive characteristics and potentials of ana- lytics, trace data and newer kinds of sensory data usable on digital platforms to transform assessment. But, characteristics of quality assessment design need to be reconsidered, and newmetrics for cap- turing quality are required. is symposium and workshop focuses on what might be appropriate quality metrics and indicators for assessment design in scaled learning. It aims to build a community of interest round the topic, to share perspectives, and to generate design and research ideas.  CCS CONCEPTS Applied computing E-learning;  KEYWORDS Learning analytics, learning design, assessment, scaled courses, feedback, learning at scale.  ACM Reference format: Ulla Ringtved, Sandra Milligan, Linda Corrin, Allison Lilejohn, and Nancy Law. 2016. DesignLAK17: ality metrics and indicators for analytics of assessment design at scale. In Proceedings of LAK 17, Vancouver, BC, Canada, March 13-17, 2017, 2 pages. DOI: hp://dx.doi.org/10.1145/3027385.3029431  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 17, Vancouver, BC, Canada  2017 Copyright held by the owner/author(s). 978-1-4503-4870-6/17/03. . .$15.00 DOI: hp://dx.doi.org/10.1145/3027385.3029431  1 WORKSHOP BACKGROUND e workshop DesignLAK17 builds on the DesignLAK16, work- shop run in Edinburgh [7]. A clear theme that emerged from the workshop was that a new paradigm for learning design is emerging, exploiting the distinctive characteristics and potentials of analytics, trace data and newer kinds of sensory data available on digital plat- forms to transform both teaching and learning. Notions of what is and is not good quality teaching and design are changing, and new metrics for capturing quality in this context are required [3]. is workshop explores further one particular thread of this theme: how analytics can be used to inform the quality of assessment design at scale.  Good assessment is not just a maer of developing faster, more scalable, more intelligent assessment instruments. High quality assessment is ultimately about generating or warranting reliable learning outcomes. e desirable characteristics of assessment for higher education courses are well documented [1] [5]. Close alignment of assessment practice is required with good curriculum design, and with good pedagogical practice. Learning outcomes need to be carefully designed, preferable expressed in terms of desired competencies, where competency is not used in a nar- row sense, but encompasses the full range of cognitive, aective, physical, generic and metacognitive skills and abilities required to develop expertise in the domain of a course. For learning outcomes to be convincingly assessed and moderated, a clarity is required of behaviours that dierentiate lesser from greater levels of aain- ment. Learners should be provided with assessments and feedback about their own performance as learners, as well as their mastery of domain competence, so they can become skilled, self-regulated, collaborative learners, able to learn in unstructured environments.  Assessment should be regarded as the collection and presenta- tion of a range of authentic, compelling evidence that supports the on-balance judgment about where on the continuum of expertise the learner has reached, at that point in time. Formerly impor- tant distinctions such as those between formative and summative assessment, or between objective and subjective assessment, fall away. Award of credentials, an important function of assessment,    LAK 17, March 13-17, 2017, Vancouver, BC, Canada U. Ringtved et al.  should be based on an overall judgment that a learner knows and can do what is required, to the standards required. ese under- standings set the context for judging the appropriateness of any analytics-based assessment approach.  A new generation of digital, analytics-based tools are available to support assessment and learner feedback [9] [8]. Scaled assessment practices include use of tools like e-portfolios, features to manage judgments about student learning based on artifacts and products, and the use of peer- and self-assessment. Automated assessment tools are proliferating, including simulations of virtual patients or legal clients, haptic devices, and intelligent essay marking us- ing machine learning, cognitive computing and natural language processing. New kinds of data and information are being used for assessment, validation or security purposes, including log stream traces le by student learning activity, and data generated by de- vices such as eye trackers, facial recognition devices, and so on. ese provide new opportunities to assess generic skills such as collaboration, creativity, and metacognitive skills [6]. ere is huge promise to enable more and beer feedback to support learners, and to make beer judgments about learning, what students know and can do, and what they still have to learn.  Despite the promise of such tools and techniques in scaled learn- ing, there is some skepticism about automated or digitally enhanced assessment, perhaps fueled by thewidespread and inappropriate use of multiple-choice exams, and direct tutoring systems [4]. Many of the new assessment are unsupported by evidence of validity and reliability, yet are oered condently in scaled environments. estions rise about how dierent indicators should be ultilised to inform learning design and implementation at scale [2]. ere are concerns that the wider availability of analytics-based assessment will intensify inappropriate metrication of learning, focusing on learning outcomes that are easily measured, in the short term, at the cost of learning that is hard to measure and that develops over time.  2 WORKSHOP OBJECTIVES e workshop provides an opportunity to further develop Design- LAK themes, with a particular focus on quality of assessment design in scaled digital environments. emain objectives of the workshop will be:   to build a community of interest around learning design topics;   to share perspectives; and  to generate design and research ideas.  e DesignLAK17 workshop will focus on what quality metrics and indicators for assessment design can be used in scaled learning. is is not limited to a particular form of assessment design, and discussion throughout the day will consider existing as well as new and innovate approaches to delivering quality assessment in scaled learning environments. e workshop incorporates symposium pa- pers, showcases of innovative approaches to analytics-based assess- ment, and lively collaborative working sessions to build knowledge and understanding.  e outcomes from the workshop will include:  Publication of the reviewed symposium papers in work-  shop proceedings;   Identication and development of a community of interest; and   Knowledge building on the topic, and identication of a range of metrics and indicators that could assist in improv- ing the quality of assessment design in scaled learning  As part of the DesignLAK series of workshops, DesignLAK17 allows for the exploration of an important element of learning design and the role analytics can play in supporting quality learn- ing. emes emerging from the 2017 workshop will form the basis of continued conversations with the resulting community of in- terest and will inform the focus and design of future DesignLAK workshops.  REFERENCES [1] David Boud and Elizabeth Molloy. 2013. Rethinking models of feedback for  learning: the challenge of design. Assessment & Evaluation in Higher Education 38, 6 (2013), 698712.  [2] Nancy Law, Dale S Niederhauser, Rhonda Christensen, and Linda Shear. 2016. A multilevel system of quality technology-enhanced learning and teaching indica- tors. Journal of Educational Technology & Society 19, 3 (2016), 7283.  [3] Allison Lilejohn and Chris Pegler. 2007. Preparing for blended e-learning. Rout- ledge.  [4] Wenting Ma, Olusola O Adesope, John C Nesbit, and Qing Liu. 2014. Intelligent tutoring systems and learning outcomes: A meta-analysis. Journal of Educational Psychology 106, 4 (2014), 901.  [5] Geo N Masters. 2013. Reforming educational assessment: Imperatives, princi- ples and challenges. (2013).  [6] Sandra Kaye Milligan and Patrick Grin. 2016. Understanding Learning and Learning Design in MOOCs: A Measurement-Based Interpretation. Journal of Learning Analytics 3, 2 (2016), 88115.  [7] Ulla Ringtved, Sandra Milligan, and Linda Corrin. 2016. Learning design and feedback processes at scale: stocktaking emergent theory and practice. In Pro- ceedings of the Sixth International Conference on Learning Analytics & Knowledge. ACM, 479480.  [8] Marlene Scardamalia, John Bransford, Bob Kozma, and Edys ellmalz. 2012. New assessments and environments for knowledge building. In Assessment and teaching of 21st century skills. Springer, 231300.  [9] Valerie Shute and Mahew Ventura. 2013. Stealth assessment: Measuring and supporting learning in video games. MIT Press.   	Abstract 	1 Workshop Background 	2 Workshop Objectives 	References   