Learning Analytics and Policy (LAP)  international  aspirations, achievements and constraints     Megan Bowe  DISC   239 Hampden Av. Narberth, PA 19072   megan@datainteroperability.org     Tore Hoel   Oslo and Akershus University  PO box 4, NO-0130 Oslo, Norway   Tore.Hoel@hioa.no      Griff Richards  Athabasca University   Athabasca, AB T9S 3A3, Canada   griff@sfu.ca   Weiqin Chen   University of Bergen,   PO box 7807, BERGEN, Norway   Weiqin.Chen@uib.no     Jaeho Lee  University of Seoul   163 Seoulsiripdae-ro, Seoul. S. Korea   jaeho@uos.ac.kr     Li Yuan  Cetis LLP,    Lancaster LA2 6ND, UK   Li@cetis.ac.uk    Dai Griffiths  University of Bolton  Bolton BL3 5AB, UK   D.E.Griffiths@bolton.ac.uk     Hiroaki Ogata, Japan  Kyushu University  Fukuoka, Japan   hiroaki.ogata@gmail.com     Jingjing Zhang  Beijing Normal University   19 Xin-Jie-Kou Wai, Beijing, China   jingjing.zhang@bnu.edu.cn   ABSTRACT  The Learning Analytics and Policy (LAP) workshop explores and   documents the ways in which policies at national and regional level   are shaping the development of learning analytics. It brings   together representatives from around the world who report on the   circumstances in their own country. The workshop is preceded by   an information gathering phase, and followed by the authoring of a   report. The aspirations, achievements and constraints in the   different countries are contrasted and documented, providing a   valuable resource for the future development of learning analytics.   CCS Concepts   Social and professional topics~Privacy policies    Social and   professional topics~Government technology policy   Keywords  Learning analytics, policy, privacy, data protection, open data.   1. BACKGROUND TO THE WORKSHOP  Institutional readiness for analytics, has been studied by, for   example the DELTA model from Davenport et al. [1], the work of   Jisc, including Sclater et al. [2], Educause, including Oster et al.   [3], and the OLT in Australia [4]. Such work provides valuable   insight into the scaling up learning analytics. However, the   capability of institutions to implement and leverage learning   analytics systems is determined not only by their internal   conditions and dynamics, but also by the policies and degree of   development of the education system and policy environment of the   state in which they find themselves. Moreover, the state does not   only create constraints on the institution, but also actively   intervenes in establishing the terms of reference and internal   dynamics of the institution. The importance of these factors was   recognized by Pea and Jacks, who proposed as one of their   milestones for measuring progress in building the field of analytics   Changes in policy related to data privacy and data sharing for   education, corporations [5, p.64]. To address these policy issues,   we must delve into the socio-technical sphere, as recommended   by Macfadyen and Dawson [6, p.161].   Several US federal government laws determine the way that   learning analytics can develop [7, p.20], and State policies play a   significant role in the move toward learning analytics [7, p.23].   The LACE project (www.laceproject.eu) has shown that while   there is consensus on the importance of policy, there is no   agreement on what the ideal policies are, particularly with regard   to fair and ethical use of data [8, p.12], [9, p.21-23]. The LAP   workshop builds on LACE in surveying the national and regional   policies which impinge on learning analytics in some key countries   where learning analytics has become established, and undertakes a   comparative analysis of the effects of these varying environments   on the development of applications of learning analytics.    This topic is of relevant to the meta-issues raised by LAK17,   including Ethics and Law, Adoption and Scalability. Developments   in these areas cannot be understood without knowledge of the   various policies that enable and constrain progress. The Workshop   contributes to the research field by documenting relevant policies   across the globe, and theorizing their consequences. This provides   input for research which seeks to understand the successes and   failures of LA, and provide valuable input for countries which are   currently starting to implement LA.   2. OBJECTIVES OF THE WORKSHOP  The objective of the workshop is to explore and document the ways   in which policies at national and regional level are shaping the   development of learning analytics. The scope includes policies   which are explicitly focused on learning analytics, policies which   regulate the use of data, and policies for the use of data in education.   The enquiry is focused on three aspects:   Permission to make digital or hard copies of part or all of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for third-  party components of this work must be honored. For all other uses, contact   the Owner/Author. Copyright is held by the owner/author(s).   LAK '17, March 13-17, 2017, Vancouver, BC, Canada   ACM 978-1-4503-4870-6/17/03.   http://dx.doi.org/10.1145/3027385.3029436           a) Aspirations. What do the policy creating bodies in each country   seek to achieve through the use of learning analytics This includes   related policies, such as data protection and privacy, which may not   be labelled with the term learning analytics.   b) Achievements. To what extent can it be claimed that the policy   environment has led to increased or more effective use or   preparation to use of learning analytics   c) Constraints. What are the factors which are preventing the   fulfillment of policies for learning analytics   3. OUTCOMES   3.1 Knowledge Sharing  The policy environment for learning analytics is an immediate   reality for researchers and practitioners in an individual country,   but they are often unaware of the conditions in other countries.   Sharing this knowledge in LAK improves participants ability to   engage in the policy debates in their own country. Moreover, the   examples of achievements and constraints provides a detailed view   of responses to policy for the wider community, and material for   reflection on relationship between policy and practice in   participants own operating environment.    3.2 Synthesis Report  Following the workshop, a report will be prepared synthesizing the   relationship between policy, infrastructure development and   application of learning analytics around the world. It will identify   key variables between countries, and examine how they affect the   aspirations, achievements and constraints which are observed.   Examples include technical and organizational infrastructure, legal   frameworks, educational institutions and culture, acceptability to   practitioners, state investment, the role of commercial players, etc.   The report will develop understanding within the LAK community   of the relationship between policy and learning analytics.   4. PROCESS  To achieve these outcomes, it is not sufficient to gather participants   from around the world for a discussion. Rather, the workshop itself   is the centerpiece of an extended set of activities, and is preceded   by a data collection phase. This involves establishment of common   ground on the scope of the enquiry, the specification of the material   to be gathered by participants, and the pooling and pre-processing   of data in preparation for the workshop. The outcomes of this phase   are statements about the policy environment in each participating   country in a standard format, which are shared before the workshop   and are required reading for participants.    At the workshop itself, the participants consider the implications of   the data that has been gathered, and the emerging open questions.   In addition to setting out the prescribed procedures, timetables and   restrictions, the social mechanisms at work are analyzed. This   discussion is documented, summarizing the data gathered, and the   conclusions of the workshop presented in a synthesis report for   publication. In the light of the workshop report, a journal paper will   be authored by the workshop participants. This will address the   open questions established at the workshop, through an online   discussion and collaborative authoring process. A complex picture   is expected to emerge, with the interaction of different policies,   implementations, and professional and economic interest groups.   To provide a framework for discussion and analysis, this landscape   can be characterized as a policy network, i.e. a set of formal   institutional and informal linkages between governmental and other   actors structured around shared if endlessly negotiated beliefs and   interests in public policymaking and implementation. [10, p.424].   This approach has been widely applied in policy analysis, including   in areas with conflicting social and technological interests, such as   software patents (see [11]).   5. PARTICIPATION  In order to maintain a strong focus and commitment, the workshop   is by invitation and application, and all participants are required to   participate in preparatory activities. The scope includes both   government departments and other agencies and organizations,   such as DISC (with an international remit), Jisc (UK), Educause   (USA), KERIS (Korea), NCET (China), etc. The authors cover   North America, Europe, and Asia, and will invite participants from   their own areas. Four authors are members of LACE, which worked   extensively on standards (see for example [12]) and ethical aspects   of learning analytics. This extensive community of researchers and   practitioners is the principal source of participants.     6. REFERENCES  [1] Davenport, T. H., Harris, J. G., and Morison, R. 2010.   Analytics at Work: Smarter Decisions, Better Results.   Harvard Business School Press Books, Cambridge MA.   [2] Sclater, N., Peasgood, A., and Mullan, J. 2016. Learning   analytics in higher education. Jisc, UK. Retrieved January   9th, 2017, from Jisc: https://www.jisc.ac.uk/reports/learning-  analytics-in-higher-education   [3] Oster, M., Lonn, S., Pistilli, M.D., and Brown, M.G. 2016.   The Learning Analytics Readiness Instrument. Proceedings   of the Sixth International Conference on Learning Analytics   & Knowledge. ACM, NY, 173-182.    [4] Australian Government Office for Learning and Teaching.   2013. Student retention and learning analytics.    [5] Pea, R., Jacks, D. 2014. Building the Field of Learning   Analytics for Personalized Learning at Scale. The Learning   Analytics Workgroup. Stanford University, CA.   [6] Macfadyen, L. P., Dawson, S. 2012. Numbers Are Not   Enough. Why e-Learning Analytics Failed to Inform an   Institutional Strategic Plan. Educational Technology &   Society, 15(3), 149-163.   [7] Wolf, M. A. 2014. Capacity Enablers and Barriers for   Learning Analytics: Implications for Policy and Practice.   Alliance for Excellent Education.   [8] Cardinali, F., Ferguson, R., Griffiths, D., Hoel, T. Karlberg,   P., Paini, M., Reynolds, S., Rienties, B., van der Schaaf, M.,   Scheffel, M., Wastiau, P. 2015. Policy recommendations for   learning analytics from three stakeholder workshops. LACE   Learning Analytics Review, no. 3, July 2015. Retrieved from   LACE, January 9th, 2017, from LACE:   http://www.laceproject.eu/publications/policy-  recommendations-lace-workshops.pdf   [9] Cooper, A., Hoel, T. 2015. Data Sharing Requirements and   Roadmap. LACE project deliverable D7.2.   [10] Rhodes, R. A. W. 2006. Policy Network Analysis. In The   Oxford Handbook of Public Policy, M. Moran, M. Rein and   R. E. Goodin, Eds. Oxford University Press, 423-45.   [11] Leifeld, P., Haunss, S. 2010. A Comparison between   Political Claims Analysis and Discourse Network Analysis:   The Case of Software Patents in the European Union. Report   2010/21. Max Planck Society.   [12] Griffiths, D., Hoel, T. Cooper, A. 2016. Learning Analytics   Interoperability: Requirements, Specifications and Adoption.   Lace Deliverable D7.4.     