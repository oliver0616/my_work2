

@inproceedings{Agirre:1996:WSD:992628.992635,
 author = {Agirre, Eneko and Rigau, German},
 title = {Word Sense Disambiguation Using Conceptual Density},
 booktitle = {Proceedings of the 16th Conference on Computational Linguistics - Volume 1},
 series = {COLING '96},
 year = {1996},
 location = {Copenhagen, Denmark},
 pages = {16--22},
 numpages = {7},
 url = {http://dx.doi.org/10.3115/992628.992635},
 doi = {10.3115/992628.992635},
 acmid = {992635},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {This paper present a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.}
} 



@INPROCEEDINGS{Agirre04smoothingand,
    author = {Eneko Agirre and David Martinez},
    title = { Smoothing and Word Sense Disambiguation},
    booktitle = {IN PROCEEDINGS OF ESTAL - ESPAÑA FOR NATURAL LANGUAGE PROCESSING},
    year = {2004},
    publisher = {},
	annotation = {This paper presents an algorithm to apply the smoothing techniques described in [1] to three different Machine Learning (ML) methods for Word Sense Disambiguation (WSD). The method to obtain better estimations for the features is explained step by step, and applied to n-way ambiguities. The results obtained in the Senseval-2 framework show that the method can help improve the precision of some weak learners, and in combination attain the best results so far in this setting.}
}


@incollection{Banerjee2002
year={2002},
isbn={978-3-540-43219-7},
booktitle={Computational Linguistics and Intelligent Text Processing},
volume={2276},
series={Lecture Notes in Computer Science},
editor={Gelbukh, Alexander},
doi={10.1007/3-540-45715-1_11},
title={An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet},
url={http://dx.doi.org/10.1007/3-540-45715-1_11},
publisher={Springer Berlin Heidelberg},
author={Banerjee, Satanjeev and Pedersen, Ted},
pages={136-145},
language={English}
annotation = {This paper presents an adaptation of Lesk’s dictionary-based word sense disambiguation algorithm. Rather than using a standard dictionary as the source of glosses for our approach, the lexical database WordNet is employed. This provides a rich hierarchy of semantic relations that our algorithm can exploit. This method is evaluated using the English lexical sample data from the Senseval-2 word sense disambiguation exercise, and attains an overall accuracy of 32%. This represents a significant improvement over the 16% and 23% accuracy attained by variations of the Lesk algorithm used as benchmarks during the SENSEVAL-2 comparative exercise among word sense disambiguation systems.}
}


@inproceedings{Brown:1991:WDU:981344.981378,
 author = {Brown, Peter F. and Pietra, Stephen A. Della and Pietra, Vincent J. Della and Mercer, Robert L.},
 title = {Word-sense Disambiguation Using Statistical Methods},
 booktitle = {Proceedings of the 29th Annual Meeting on Association for Computational Linguistics},
 series = {ACL '91},
 year = {1991},
 pages = {264--270},
 numpages = {7},
 url = {http://dx.doi.org/10.3115/981344.981378},
 doi = {10.3115/981344.981378},
 acmid = {981378},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.}
} 


@inproceedings{Chen:2009:FUW:1620754.1620759,
 author = {Chen, Ping and Ding, Wei and Bowes, Chris and Brown, David},
 title = {A Fully Unsupervised Word Sense Disambiguation Method Using Dependency Knowledge},
 booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
 series = {NAACL '09},
 year = {2009},
 isbn = {978-1-932432-41-1},
 pages = {28--36},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=1620754.1620759},
 acmid = {1620759},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {Word sense disambiguation is the process of determining which sense of a word is used in a given context. Due to its importance in understanding semantics of natural languages, word sense disambiguation has been extensively studied in Computational Linguistics. However, existing methods either are brittle and narrowly focus on specific topics or words, or provide only mediocre performance in real-world settings. Broad coverage and disambiguation quality are critical for a word sense disambiguation system. In this paper we present a fully unsupervised word sense disambiguation method that requires only a dictionary and unannotated text as input. Such an automatic approach overcomes the problem of brittleness suffered in many existing methods and makes broad-coverage word sense disambiguation feasible in practice. We evaluated our approach using SemEval 2007 Task 7 (Coarse-grained English All-words Task), and our system significantly outperformed the best unsupervised system participating in SemEval 2007 and achieved the performance approaching top-performing supervised systems. Although our method was only tested with coarse-grained sense disambiguation, it can be directly applied to fine-grained sense disambiguation.}
} 




@inproceedings{Chali:2007:UWS:1621474.1621580,
 author = {Chali, Yllias and Joty, Shafiq R.},
 title = {UofL: Word Sense Disambiguation Using Lexical Cohesion},
 booktitle = {Proceedings of the 4th International Workshop on Semantic Evaluations},
 series = {SemEval '07},
 year = {2007},
 pages = {476--479},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=1621474.1621580},
 acmid = {1621580},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {One of the main challenges in the applications (i.e.: text summarization, question answering, information retrieval, etc.) of Natural Language Processing is to determine which of the several senses of a word is used in a given context. The problem is phrased as "Word Sense Disambiguation (WSD)" in the NLP community. This paper presents the dictionary based disambiguation technique that adopts the assumption of one sense per discourse in the context of SemEval-2007 Task 7: "Coarse-grained English all-words".}
} 



@inproceedings{Chao:2001:PNM:2387364.2387379,
 author = {Chao, Gerald and Dyer, Michael G.},
 title = {Probabilistic Network Models for Word Sense Disambiguation},
 booktitle = {The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems},
 series = {SENSEVAL '01},
 year = {2001},
 location = {Toulouse, France},
 pages = {63--66},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=2387364.2387379},
 acmid = {2387379},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {We present the techniques used in the word sense disambiguation (WSD) system that was submitted to the Senseval-2 workshop. The system builds a probabilistic network per sentence to model the dependencies between the words within the sentence, and the sense tagging for the entire sentence is computed by performing a query over the network. The salient context used for disambiguation is based on sentential structure and not positional information. The parameters are established automatically and smoothed via training data, which was compiled from the SemCor corpus and the WordNet glosses. Lastly, the One-sense-per-discourse (OSPD) hypothesis is incorporated to test its effectiveness. The results from two parameterization techniques and the effects of the OSPD hypothesis are presented.}
} 



@inproceedings{Faralli:2012:NMF:2390948.2391109,
 author = {Faralli, Stefano and Navigli, Roberto},
 title = {A New Minimally-supervised Framework for Domain Word Sense Disambiguation},
 booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
 series = {EMNLP-CoNLL '12},
 year = {2012},
 pages = {1411--1422},
 numpages = {12},
 url = {http://dl.acm.org/citation.cfm?id=2390948.2391109},
 acmid = {2391109},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fully-unsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables high-performance results on dozens of domains at a coarse and fine-grained level.}
} 





@MISC{Fujii98corpus-basedword,
    author = {Atsushi Fujii},
    title = {Corpus-Based Word Sense Disambiguation},
    year = {1998},
	annotation = {Resolution of lexical ambiguity, commonly termed "word sense disambiguation", is expected to improve the analytical accuracy for tasks which are sensitive to lexical semantics. Such tasks include machine translation, information retrieval, parsing, natural language understanding and lexicography. Reflecting the growth in utilization of machine readable texts, word sense disambiguation techniques have been explored variously in the context of corpus-based approaches. Within one corpus-based framework, that is the similarity-based method, systems use a database, in which example sentences are manually annotated with correct word senses. Given an input, systems search thedatabase for the most similar example to the input. The lexical ambiguity of a word contained in the input is resolved by selecting the sense annotation of the retrieved example. In this research, we apply this method of resolution of verbal polysemy, in which the similarity between two examples is computed as the weighted average of the similarity between complements governed by a target polysemous verb. We explore similarity-based verb sense disambiguation focusing on the following three methods. First, we propose a weighting schema for each verb complement in the similarity computation. Second, in similarity-based techniques, the overhead for manual supervision and searching the large-sized database can be prohibitive. To resolvethis problem, we propose a method to select a small number of effective examples, for system usage. Finally, the efficiency of our system is highly dependent on the similarity computation used. To maximize efficiency, we propose a method which integrates the advantages of previous methods for similarity computation.}
}





@MISC{Jimeno-yepes_wordsense,
    author = {Antonio Jimeno-yepes and Bridget T Mclnnes and Alan R Aronson},
    title = {word sense disambiguation},
    year = {}
	annotation = {Background: The effectiveness of knowledge-based word sense disambiguation (WSD) approaches depends in part on the information available in the reference knowledge resource. Off the shelf, these resources are not optimized for WSD and might lack terms to model the context properly. In addition, they might include noisy terms which contribute to false positives in the disambiguation results. Methods: We analysed some collocation types which could improve the performance of knowledge-based disambiguation methods. Collocations are obtained by extracting candidate collocations from MEDLINE and then assigning them to one of the senses of an ambiguous word. We performed this assignment either using semantic group profiles or a knowledge-based disambiguation method. In addition to collocations, we used second-order features from a previously implemented approach. Specifically, we measured the effect of these collocations in two knowledge-based WSD methods. The first method, AEC, uses the knowledge from the UMLS to collect examples from MEDLINE which are used to train a Naïve Bayes approach. The second method, MRD, builds a profile for each candidate sense based on the UMLS and compares the profile to the context of the ambiguous word. We have used two WSD test sets which contain disambiguation cases which are mapped to UMLS concepts.}
}



@inproceedings{Le:2006:IPS:2081346.2081404,
 author = {Le, Anh-Cuong and Shimazu, Akira and Nguyen, Le-Minh},
 title = {Investigating Problems of Semi-supervised Learning for Word Sense Disambiguation},
 booktitle = {Proceedings of the 21st International Conference on Computer Processing of Oriental Languages: Beyond the Orient: The Research Challenges Ahead},
 series = {ICCPOL'06},
 year = {2006},
 isbn = {3-540-49667-X, 978-3-540-49667-0},
 pages = {482--489},
 numpages = {8},
 url = {http://dx.doi.org/10.1007/11940098_51},
 doi = {10.1007/11940098_51},
 acmid = {2081404},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
 annotation = {Word Sense Disambiguation (WSD) is the problem of determining the right sense of a polysemous word in a given context. In this paper, we will investigate the use of unlabeled data for WSD within the framework of semi supervised learning, in which the original labelled dataset is iteratively extended by exploiting unlabelled data. This paper addresses two problems occurring in this approach: determining a subset of new labeled data at each extension and generating the final classifier. By giving solutions for these problems, we generate some variants of bootstrapping algorithms and apply to word sense disambiguation. The experiments were done on the datasets of four words: interest, line, hard, and serve; and on English lexical sample of Senseval-3.}
} 



@article{Le:2008:SLI:1367140.1367220,
 author = {Le, Anh-Cuong and Shimazu, Akira and Huynh, Van-Nam and Nguyen, Le-Minh},
 title = {Semi-supervised Learning Integrated with Classifier Combination for Word Sense Disambiguation},
 journal = {Comput. Speech Lang.},
 issue_date = {October, 2008},
 volume = {22},
 number = {4},
 month = oct,
 year = {2008},
 issn = {0885-2308},
 pages = {330--345},
 numpages = {16},
 url = {http://dx.doi.org/10.1016/j.csl.2007.11.001},
 doi = {10.1016/j.csl.2007.11.001},
 acmid = {1367220},
 publisher = {Academic Press Ltd.},
 address = {London, UK, UK},
 keywords = {Computational linguistics, Semi-supervised learning, Word sense disambiguation},
 annotation = {Word sense disambiguation (WSD) is the problem of determining the right sense of a polysemous word in a certain context. This paper investigates the use of unlabelled data for WSD within a framework of semi-supervised learning, in which labeled data is iteratively extended from unlabelled data. Focusing on this approach, we first explicitly identify and analyse three problems inherently occurred piecemeal in the general bootstrapping algorithm; namely the imbalance of training data, the confidence of new labelled examples, and the final classifier generation; all of which will be considered integratedly within a common framework of bootstrapping. We then propose solutions for these problems with the help of classifier combination strategies. This results in several new variants of the general bootstrapping algorithm. Experiments conducted on the English lexical samples of Senseval-2 and Senseval-3 show that the proposed solutions are effective in comparison with previous studies, and significantly improve supervised WSD.}
} 




@inproceedings{Lee:2002:EEK:1118693.1118699,
 author = {Lee, Yoong Keok and Ng, Hwee Tou},
 title = {An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation},
 booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
 series = {EMNLP '02},
 year = {2002},
 pages = {41--48},
 numpages = {8},
 url = {http://dx.doi.org/10.3115/1118693.1118699},
 doi = {10.3115/1118693.1118699},
 acmid = {1118699},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.}
} 



@inproceedings{Liu:2000:WSD:974456.974463,
 author = {Liu, Mary Xiaoyong and Diamond, Ted and Diekema, Anne R.},
 title = {Word Sense Disambiguation for Cross-language Information Retrieval},
 booktitle = {Proceedings of the Workshop on Student Research},
 year = {2000},
  pages = {35--40},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=974456.974463},
 acmid = {974463},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
 annotation = {We have developed a word sense disambiguation algorithm, following Cheng and Wilensky (1997), to disambiguate among WordNet synsets. This algorithm is to be used in a cross-language information retrieval system, CINDOR, which indexes queries and documents in a language-neutral concept representation based on WordNet synsets. Our goal is to improve retrieval precision through word sense disambiguation. An evaluation against human disambiguation judgements suggests promise for our approach.}
} 



@inproceedings{Liu:2005:WSD:1099554.1099696,
 author = {Liu, Shuang and Yu, Clement and Meng, Weiyi},
 title = {Word Sense Disambiguation in Queries},
 booktitle = {Proceedings of the 14th ACM International Conference on Information and Knowledge Management},
 series = {CIKM '05},
 year = {2005},
 isbn = {1-59593-140-6},
 pages = {525--532},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1099554.1099696},
 doi = {10.1145/1099554.1099696},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {WordNet, information retrieval, word sense disambiguation},
 annotation = {This paper presents a new approach to determine the senses of words in queries by using WordNet. In our approach, noun phrases in a query are determined first. For each word in the query, information associated with it, including its synonyms, hyponyms, hypernyms, definitions of its synonyms and hyponyms, and its domains, can be used for word sense disambiguation. By comparing these pieces of information associated with the words which form a phrase, it may be possible to assign senses to these words. If the above disambiguation fails, then other query words, if exist, are used, by going through exactly the same process. If the sense of a query word cannot be determined in this manner, then a guess of the sense of the word is made, if the guess has at least 50% chance of being correct. If no sense of the word has 50% or higher chance of being used, then we apply a Web search to assist in the word sense disambiguation process. Experimental results show that our approach has 100% applicability and 90% accuracy on the most recent robust track of TREC collection of 250 queries. We combine this disambiguation algorithm to our retrieval system to examine the effect of word sense disambiguation in text retrieval. Experimental results show that the disambiguation algorithm together with other components of our retrieval system yield a result which is 13.7% above that produced by the same system but without the disambiguation, and 9.2% above that produced by using Lesk's algorithm. Our retrieval effectiveness is 7% better than the best reported result in the literature.}
} 



@inproceedings{McCarthy:2004:FPW:1218955.1218991,
 author = {McCarthy, Diana and Koeling, Rob and Weeds, Julie and Carroll, John},
 title = {Finding Predominant Word Senses in Untagged Text},
 booktitle = {Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics},
 series = {ACL '04},
 year = {2004},
 articleno = {279},
 url = {http://dx.doi.org/10.3115/1218955.1218991},
 doi = {10.3115/1218955.1218991},
 acmid = {1218991},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand-tagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL-2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domain-specific corpora.}
} 




@article{Navigli:2009:WSD:1459352.1459355,
 author = {Navigli, Roberto},
 title = {Word Sense Disambiguation: A Survey},
 journal = {ACM Comput. Surv.},
 issue_date = {February 2009},
 volume = {41},
 number = {2},
 month = feb,
 year = {2009},
 issn = {0360-0300},
 pages = {10:1--10:69},
 articleno = {10},
 numpages = {69},
 url = {http://doi.acm.org/10.1145/1459352.1459355},
 doi = {10.1145/1459352.1459355},
 acmid = {1459355},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {WSD, Word sense disambiguation, lexical ambiguity, lexical semantics, semantic annotation, sense annotation, word sense discrimination},
 annotation = {Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.}
} 



@article{NLE:138359,
author = {EDMONDS,PHILIP and KILGARRIFF,ADAM},
title = {Introduction to the special issue on evaluating word sense disambiguation systems},
journal = {Natural Language Engineering},
volume = {8},
issue = {04},
month = {12},
year = {2002},
issn = {1469-8110},
pages = {279--291},
numpages = {13},
doi = {10.1017/S1351324902002966},
URL = {http://journals.cambridge.org/article_S1351324902002966},
annotation = { ABSTRACT Has system performance on Word Sense Disambiguation (WSD) reached a limit? Automatic systems don&apos;t perform nearly as well as humans on the task, and from the results of the SENSEVAL exercises, recent improvements in system performance appear negligible or even negative. Still, systems do perform much better than the baselines, so something is being done right. System evaluation is crucial to explain these results and to show the way forward. Indeed, the success of any project in WSD is tied to the evaluation methodology used, and especially to the formalization of the task that the systems perform. The evaluation of WSD has turned out to be as difficult as designing the systems in the first place. }
}



@article{Preiss2004,
author = {Preiss, Judita},
journal = {Computer Speech \& Language},
keywords = {Disambiguate\'{o}n,Probabilist},
mendeley-groups = {nlp},
number = {3},
pages = {319--337},
title = {{Probabilistic word sense disambiguation: Word Sense Disambiguation}},
url = {http://www.sciencedirect.com/science/article/B6WCW-4CKBRY2-4/2/7081a9ebec356371b2e9c5366c348031},
volume = {18},
year = {2004}
annotation = {We present a theoretically motivated method for creating probabilistic word sense disambiguation (WSD) systems. The method works by composing multiple probabilistic components: such modularity is made possible by an application of Bayesian statistics and Lidstone's smoothing method. We show that a probabilistic WSD system created along these lines is a strong competitor to state-of-the-art WSD systems.}
}




@INPROCEEDINGS{Prithviraj04softword,
    author = {Ganesh Ramakrishnan Prithviraj and Ganesh Ramakrishnan and B. P. Prithviraj and A. Deepa and Pushpak Bhattacharyya},
    title = {Soft Word Sense Disambiguation},
    booktitle = {Masaryk University Brno, Brno, Czech Republic},
    year = {2004},
    pages = {33--64},
	annotation = {Word sense disambiguation is a core problem in many tasks related to language processing. In this paper, we introduce the notion of soft word sense disambiguation which states that given a word, the sense disambiguation system should not commit to a particular sense, but rather, to a set of senses which are not necessarily orthogonal or mutually exclusive. The senses of a word are expressed by its WordNet synsets, arranged according to their relevance. The relevance of these senses are probabilistically determined through a Bayesian Belief Network. The main contribution of the work is a completely probabilistic framework for word sense disambiguation with a semi-supervised learning technique utilising WordNet.}
}


@article{reyes2009word,
  title={Word Sense Disambiguation in Information Retrieval},
  author={Reyes, Francis de la C Fern{\'a}ndez and Leyva, Exiquio C P{\'e}rez and FERN{\'a}NDEZ, Rogelio Lau and others},
  journal={Intelligent Information Management},
  volume={1},
  number={02},
  pages={122},
  year={2009},
  publisher={Scientific Research Publishing},
  annotation = {The natural language processing has a set of phases that evolves from lexical text analysis to the pragmatic one in which the author’s intentions are shown. The ambiguity problem appears in all of these tasks. Previous works tries to do word sense disambiguation, the process of assign a sense to a word inside a specific context, creating algorithms under a supervised or unsupervised approach, which means that those algorithms use or not an external lexical resource. This paper presents an approximated approach that combines not supervised algorithms by the use of a classifiers set, the result will be a learning algorithm based on unsupervised methods for word sense disambiguation process. It begins with an introduction to word sense disambiguation concepts and then analyzes some unsupervised algorithms in order to extract the best of them, and combines them under a supervised approach making use of some classifiers.}
}


@article{Sanchez-de-Madariaga:2009:BYA:1460927.1461015,
 author = {S\'{a}nchez-de-Madariaga, Ricardo and Fern\'{a}ndez-del-Castillo, Jos{\'e} R.},
 title = {The Bootstrapping of the Yarowsky Algorithm in Real Corpora},
 journal = {Inf. Process. Manage.},
 issue_date = {January, 2009},
 volume = {45},
 number = {1},
 month = jan,
 year = {2009},
 issn = {0306-4573},
 pages = {55--69},
 numpages = {15},
 url = {http://dx.doi.org/10.1016/j.ipm.2008.07.002},
 doi = {10.1016/j.ipm.2008.07.002},
 acmid = {1461015},
 publisher = {Pergamon Press, Inc.},
 address = {Tarrytown, NY, USA},
 keywords = {Bootstrapping, Domain fluctuating corpora, Homograph, Knowledge acquisition bottleneck, Polysemy, Semi-supervised learning, Word sense disambiguation},
 annotation = {The Yarowsky bootstrapping algorithm resolves the homograph-level word sense disambiguation (WSD) problem, which is the sense granularity level required for real natural language processing (NLP) applications. At the same time it resolves the knowledge acquisition bottleneck problem affecting most WSD algorithms and can be easily applied to foreign language corpora. However, this paper shows that the Yarowsky algorithm is significantly less accurate when applied to domain fluctuating, real corpora. This paper also introduces a new bootstrapping methodology that performs much better when applied to these corpora. The accuracy achieved in non-domain fluctuating corpora is not reached due to inherent domain fluctuation ambiguities.}
} 



@inproceedings{Schumacher:2007:FMS:2394705.2394742,
 author = {Schumacher, Kinga},
 title = {Four Methods for Supervised Word Sense Disambiguation},
 booktitle = {Proceedings of the 12th International Conference on Applications of Natural Language to Information Systems},
 series = {NLDB'07},
 year = {2007},
 isbn = {3-540-73350-7, 978-3-540-73350-8},
 pages = {317--328},
 numpages = {12},
 url = {http://dl.acm.org/citation.cfm?id=2394705.2394742},
 acmid = {2394742},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
 keywords = {machine learning, term weighting, word sense disambiguation},
 annotation = {Word sense disambiguation is the task to identify the intended meaning of an ambiguous word in a certain context, one of the central problems in natural language processing. This paper describes four novel supervised disambiguation methods which adapt some familiar algorithms. They built on the Vector Space Model using an automatically generated stop list and two different statistical methods of finding index terms. These proceedings allow a fully automated and language independent disambiguation. The first method is based upon Latent Semantic Analysis, an automatic indexing method employed for text retrieval. The second one disambiguates via co-occurrence vectors of the target word. Disambiguation relying on Naive Bayes uses the Naive Bayes Classifier and disambiguation relying on SenseClusters1 uses an unsupervised word sense discrimination technique. These methods were implemented and evaluated to experience their performance, to compare the different approaches and to draw conclusions about the main characteristic of supervised disambiguation. The results show that the classification approach using Naive Bayes is the most efficient, scalable and successful method.}
} 




@inproceedings{Seemakurty:2010:WSD:1837885.1837905,
 author = {Seemakurty, Nitin and Chu, Jonathan and von Ahn, Luis and Tomasic, Anthony},
 title = {Word Sense Disambiguation via Human Computation},
 booktitle = {Proceedings of the ACM SIGKDD Workshop on Human Computation},
 series = {HCOMP '10},
 year = {2010},
 isbn = {978-1-4503-0222-7},
 pages = {60--63},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1837885.1837905},
 doi = {10.1145/1837885.1837905},
 acmid = {1837905},
 publisher = {ACM},
 address = {New York, NY, USA},
 annotation = {One formidable problem in language technology is the word sense disambiguation (WSD) problem: disambiguating the true sense of a word as it occurs in a sentence (e.g., recognizing whether the word "bank" refers to a river bank or to a financial institution). This paper explores a strategy for harnessing the linguistic abilities of human beings to develop datasets that can be used to train machine learning algorithms for WSD. To create such datasets, we introduce a new interactive system: a fun game designed to produce valuable output by engaging human players in what they perceive to be a cooperative task of guessing the same word as another player. Our system makes a valuable contribution by tackling the knowledge acquisition bottleneck in the WSD problem domain. Rather than using conventional and costly techniques of paying lexicographers to generate training data for machine learning algorithms, we delegate the work to people who are looking to be entertained.}
} 



@article{Seo2004253,
title = "Unsupervised word sense disambiguation using WordNet relatives ",
journal = "Computer Speech & Language ",
volume = "18",
number = "3",
pages = "253 - 273",
year = "2004",
note = "Word Sense Disambiguation ",
issn = "0885-2308",
doi = "http://dx.doi.org/10.1016/j.csl.2004.05.004",
url = "http://www.sciencedirect.com/science/article/pii/S0885230804000166",
author = "Hee-Cheol Seo and Hoojung Chung and Hae-Chang Rim and Sung Hyon Myaeng and Soo-Hong Kim",
annotation = {This paper describes a sense disambiguation method for a polysemous target noun using the context words surrounding the target noun and its WordNet relatives, such as synonyms, hypernyms and hyponyms. The result of sense disambiguation is a relative that can substitute for that target noun in a context. The selection is made based on co-occurrence frequency between candidate relatives and each word in the context. Since the co-occurrence frequency is obtainable from a raw corpus, the method is considered to be an unsupervised learning algorithm and therefore does not require a sense-tagged corpus. In a series of experiments using SemCor and the corpus of SENSEVAL-2 lexical sample task, all in English, and using some Korean data, the proposed method was shown to be very promising. In particular, its performance was superior to that of the other approaches evaluated on the same test corpora.}
}




@incollection{singh2002open,
  title={Open Mind Common Sense: Knowledge acquisition from the general public},
  author={Singh, Push and Lin, Thomas and Mueller, Erik T and Lim, Grace and Perkins, Travell and Zhu, Wan Li},
  booktitle={On the Move to Meaningful Internet Systems 2002: CoopIS, DOA, and ODBASE},
  pages={1223--1237},
  year={2002},
  publisher={Springer},
  annotation = {Open Mind Common Sense is a knowledge acquisition system designed to acquire commonsense knowledge from the general public over the web. We describe and evaluate our first fielded system, which enabled the construction of a 450,000 assertion commonsense knowledge base. We then discuss how our second-generation system addresses weaknesses discovered in the first. The new system acquires facts, descriptions, and stories by allowing participants to construct and fill in natural language templates. It employs word-sense disambiguation and methods of clarifying entered knowledge, analogical inference to provide feedback, and allows participants to validate knowledge and in turn each other.}
}



@article{stevenson2003word,
  title={Word-sense disambiguation},
  author={Stevenson, Mark and Wilks, Yorick},
  journal={The Oxford Handbook of Comp. Linguistics},
  pages={249--265},
  year={2003}
  annotation = {Coming soon.}
}




@inproceedings{Tonelli:2009:WFI:1699510.1699547,
 author = {Tonelli, Sara and Giuliano, Claudio},
 title = {Wikipedia As Frame Information Repository},
 booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1},
 series = {EMNLP '09},
 year = {2009},
 isbn = {978-1-932432-59-6},
 pages = {276--285},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=1699510.1699547},
 acmid = {1699547},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {In this paper, we address the issue of automatic extending lexical resources by exploiting existing knowledge repositories. In particular, we deal with the new task of linking FrameNet and Wikipedia using a word sense disambiguation system that, for a given pair frame -- lexical unit (F, l), finds the Wikipage that best expresses the the meaning of l. The mapping can be exploited to straightforwardly acquire new example sentences and new lexical units, both for English and for all languages available in Wikipedia. In this way, it is possible to easily acquire good-quality data as a starting point for the creation of FrameNet in new languages. The evaluation reported both for the monolingual and the multilingual expansion of FrameNet shows that the approach is promising.}
} 



@inproceedings{Vickrey2005,
author = {Vickrey, David and Biewald, Luke and Teyssier, Marc and Koller, Daphne},
booktitle = {Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT '05)},
mendeley-groups = {big data,nlp},
pages = {771--778},
publisher = {Association for Computational Linguistics},
title = {{Word-Sense Disambiguation for Machine Translation}},
url = {http://ai.stanford.edu/~dvickrey/wordtrans\_final.pdf$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1220575.1220672},
year = {2005},
annotation = {In word sense disambiguation, a system attempts to determine the sense of a word from contextual features. Major barriers to building a high-performing word sense disambiguation system include the difficulty of labelling data for this task and of predicting fine-grained sense distinctions. These issues stem partly from the fact that the task is being treated in isolation from possible uses of automatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a significant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machine-translation task and can effectively and accurately prune the set of candidate translations for a word.}
}




@article{Wang:2014:SWS:2562348.2562704,
 author = {Wang, Tinghua and Rao, Junyang and Hu, Qi},
 title = {Supervised Word Sense Disambiguation Using Semantic Diffusion Kernel},
 journal = {Eng. Appl. Artif. Intell.},
 issue_date = {January, 2014},
 volume = {27},
 month = jan,
 year = {2014},
 issn = {0952-1976},
 pages = {167--174},
 numpages = {8},
 url = {http://dx.doi.org/10.1016/j.engappai.2013.08.007},
 doi = {10.1016/j.engappai.2013.08.007},
 acmid = {2562704},
 publisher = {Pergamon Press, Inc.},
 address = {Tarrytown, NY, USA},
 keywords = {Kernel method, Natural language processing, Semantic diffusion kernel, Support vector machine (SVM), Word sense disambiguation (WSD)},
 annotation = {The success of machine learning approaches to word sense disambiguation (WSD) is largely dependent on the representation of the context in which an ambiguous word occurs. Typically, the contexts are represented as the vector space using ''Bag of Words (BoW)'' technique. Despite its ease of use, BoW representation suffers from well-known limitations, mostly due to its inability to exploit semantic similarity between terms. In this paper, we apply the semantic diffusion kernel, which models semantic similarity by means of a diffusion process on a graph defined by lexicon and co-occurrence information, to smooth the BoW representation for WSD systems. Semantic diffusion kernel can be obtained through a matrix exponentiation transformation on the given kernel matrix, and virtually exploits higher order co-occurrences to infer semantic similarity between terms. The superiority of the proposed method is demonstrated experimentally with several SensEval disambiguation tasks.}
} 


@inproceedings{Yarowsky:1995:UWS:981658.981684,
 author = {Yarowsky, David},
 title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
 booktitle = {Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics},
 series = {ACL '95},
 year = {1995},
 pages = {189--196},
 numpages = {8},
 url = {http://dx.doi.org/10.3115/981658.981684},
 doi = {10.3115/981658.981684},
 acmid = {981684},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
 annotation = {This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.}
} 




@article{Zouaghi:2012:CIR:2400491.2400493,
 author = {Zouaghi, Anis and Merhbene, Laroussi and Zrigui, Mounir},
 title = {Combination of Information Retrieval Methods with LESK Algorithm for Arabic Word Sense Disambiguation},
 journal = {Artif. Intell. Rev.},
 issue_date = {December  2012},
 volume = {38},
 number = {4},
 month = dec,
 year = {2012},
 issn = {0269-2821},
 pages = {257--269},
 numpages = {13},
 url = {http://dx.doi.org/10.1007/s10462-011-9249-3},
 doi = {10.1007/s10462-011-9249-3},
 acmid = {2400493},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {Arabic word sense disambiguation (AWSD), Information retrieval methods, Knowledge based approach, Lesk algorithm, Unsupervised and incremental approach},
 annotation = {In this paper, we propose to use Harman, Croft and Okapi measures with Lesk algorithm to develop a system for Arabic word sense disambiguation, that combines unsupervised and knowledge based methods. This system must solve the lexical semantic ambiguity in Arabic language. The information retrieval measures are used to estimate the most relevant sense of the ambiguous word, by returning a semantic coherence score corresponding to the context that is semantically closest to the original sentence containing the ambiguous word. The Lesk algorithm is used to assign and select the adequate sense from those proposed by the information retrieval measures mentioned above. This selection is based on a comparison between the glosses of the word to be disambiguated, and its different contexts of use extracted from a corpus. Our experimental study proves that using of Lesk algorithm with Harman, Croft, and Okapi measures allows us to obtain an accuracy rate of 73%.}
} 

