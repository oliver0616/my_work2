{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus: <br>\n",
    "Tagged Sentences: pre-tagged brown corpus in sentence structure <br>\n",
    "non-Tagged Sentences: non-tagged brown corpus in sentence structure <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged_sentences:\n",
      "Total number of sentences:57340\n",
      "First two sentences:[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]]...\n",
      "\n",
      "non_tagged_sentences:\n",
      "Total number of sentences:57340\n",
      "First two sentences:[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']]...\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences=brown.tagged_sents()\n",
    "print(\"tagged_sentences:\")\n",
    "print(\"Total number of sentences:\"+ str(len(tagged_sentences)))\n",
    "print(\"First two sentences:\"+str(tagged_sentences[:2])+\"...\")\n",
    "print()\n",
    "non_tagged_sentences=brown.sents()\n",
    "print(\"non_tagged_sentences:\")\n",
    "print(\"Total number of sentences:\"+ str(len(non_tagged_sentences)))\n",
    "print(\"First two sentences:\"+str(non_tagged_sentences[:2])+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tagging Technique: <br>\n",
    "### Representing Tagged Tokens\n",
    "Tag by hand, The tagged token could be create using str2tuple() library in nltk package or create tuple object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Token:('test', 'NN')\n"
     ]
    }
   ],
   "source": [
    "tag_token = nltk.tag.str2tuple('test/NN')\n",
    "print(\"Tagged Token:\"+str(tag_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Tagged Token:[('Computing', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('new', 'JJ'), ('mathematics', 'NNS'), ('and', 'CC'), ('new', 'JJ'), ('stethoscope', 'NN'), ('of', 'IN'), ('the', 'DT'), ('21st', 'JJ'), ('century', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentences='''\n",
    "Computing/NN is/VBZ the/DT new/JJ mathematics/NNS and/CC new/JJ stethoscope/NN of/IN the/DT 21st/JJ century/NN\n",
    "'''\n",
    "tag_tokens=[nltk.tag.str2tuple(t) for t in sentences.split()]\n",
    "print(\"List of Tagged Token:\"+str(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defualt Tagger: <br>\n",
    "Using nltk package create the default tagger object, default tagger would tag all the tokens by given tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Default', 'NN'), ('Tagger', 'NN'), ('tagged', 'NN'), ('everything', 'NN'), ('as', 'NN'), ('you', 'NN'), ('given', 'NN')]\n",
      "0.13130472824476916\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "example_text = \"Default Tagger tagged everything as you given\"\n",
    "#tokenize the text\n",
    "text_token = word_tokenize(example_text)\n",
    "#tag the tokens\n",
    "result= default_tagger.tag(text_token)\n",
    "print(result)\n",
    "#evaluate the tagger, by using the pre-tagged brown corpus\n",
    "evaluation=default_tagger.evaluate(tagged_sentences)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Tagging\n",
    "### NLTK POS-tagger\n",
    "POS-tagger is a build in tagger in nltk package, using prebuilt tagger to tag. The tagger has trained and tested on the Wall Street Journal Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Computing', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('new', 'JJ'), ('mathematics', 'NNS'), ('and', 'CC'), ('new', 'JJ'), ('stethoscope', 'NN'), ('of', 'IN'), ('the', 'DT'), ('21st', 'JJ'), ('century', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(\"Computing is the new mathematics and new stethoscope of the 21st century\")\n",
    "tagTokens=nltk.pos_tag(tokens)\n",
    "print(tagTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Algorithm\n",
    "pos_tagger use perceptron tagger to rain the data and store the tagger for user to use, the perceptron tagger use Averaged Perceptron Algorithm to form the tagger <br>\n",
    "\n",
    "$$w(C,T)=\\sum_{i=1}^{n} \\alpha_{i} \\phi_{i}(C,T)$$ <br>\n",
    "$w(C,T)$:  transition weight for tag T incontext C <br>\n",
    "$n$: number of feature <br>\n",
    "$\\alpha_{i}$: the weight coefficient of the $i^{th}$ feature\n",
    "$\\phi_{i}(C,T)$:  the evaluation of the $i^{th}$ feature for context C and tag T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression <br>\n",
    "use regular expression to create the patterns to tag for <br>\n",
    "\n",
    "Regular expression: <br>\n",
    "\n",
    "| Operation|Behavior|\n",
    "|:----------|:---------------------------------------|\n",
    "|.| Wildcard, matches any character |\n",
    "|^abc| Matches some pattern abc at the start of a string|\n",
    "|abc$|Matches some pattern abc at the end of a string|\n",
    "|[abc]|Matches one of a set of characters|\n",
    "|[A-Z0-9]|Matches one of a range of characters|\n",
    "|ed &#x7c; ing &#x7c;s|Matches one of the specified strings (disjunction)|\n",
    "|*|Zero or more of previous item, e.g. a*, [a-z]*|\n",
    "|+|One or more of previous item, e.g. a+, [a-z]+|\n",
    "|?|Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?|\n",
    "|{n}|Exactly n repeats where n is a non-negative integer|\n",
    "|{n,}|At least n repeats|\n",
    "|{,n}|No more than n repeats|\n",
    "|{m,n}|At least m and no more than n repeats|\n",
    "|a(b &#x7c; c)+|Parentheses that indicate the scope of the operators|\n",
    "\n",
    "'r' tell the python that backslash is not a special character in the string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20163762754135406\n"
     ]
    }
   ],
   "source": [
    "#define patterns\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),\n",
    "    (r'.*ed$', 'VBD'),                \n",
    "    (r'.*es$', 'VBZ'),                \n",
    "    (r'.*ould$', 'MD'),               \n",
    "    (r'.*\\'s$', 'NN$'),               \n",
    "    (r'.*s$', 'NNS'),                 \n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  \n",
    "    (r'.*ness$', 'NN'),\n",
    "    (r'.*ment$', 'NN'),\n",
    "    (r'.*ful$', 'JJ'),\n",
    "    (r'.*ious$', 'JJ'),\n",
    "    (r'.*ble$', 'JJ'),\n",
    "    (r'.*ic$', 'JJ'),\n",
    "    (r'.*ive$', 'JJ'),\n",
    "    (r'.*ic$', 'JJ'),\n",
    "    (r'.*est$', 'JJ'),\n",
    "    (r'.*', 'NN')                    \n",
    "]\n",
    "#create regular expession tagger using the paterns\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "print(regexp_tagger.evaluate(tagged_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Tagger <br>\n",
    "tag words with most common use POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4804614568477909\n"
     ]
    }
   ],
   "source": [
    "#create freqDist of words and CondFreqDist of words with tags\n",
    "fd = nltk.FreqDist(brown.words())\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words())\n",
    "#find the first 100 most common words\n",
    "most_freq_words = fd.most_common(100)\n",
    "#create a dictionary that has frist 100 most common words as the key and the most used tag for that word as value\n",
    "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\n",
    "lookup_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "print(lookup_tagger.evaluate(tagged_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3390490564374869\n"
     ]
    }
   ],
   "source": [
    "#bigram tagger\n",
    "brownTagSent = brown.tagged_sents()\n",
    "#train set 80%, test set 20%\n",
    "size = int(len(brownTagSent)*0.8)\n",
    "train = brownTagSent[:size]\n",
    "test = brownTagSent[size:]\n",
    "bigram = nltk.BigramTagger(train)\n",
    "print(bigram.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram perform poorly with unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine n-gram Tagger\n",
    "combine the n-gram Tagger using backoff-tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9068555627774779\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "uni_tagger = nltk.UnigramTagger(train, backoff=default_tagger)\n",
    "bi_tagger = nltk.BigramTagger(train, backoff=uni_tagger)\n",
    "print(bi_tagger.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reslut improve significantly after combine the n-gram taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction and DecisionTreeClassifier Part of Speech Tagger\n",
    "Extracing features to create decision tree classifier to train the tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "0.6747886623570363\n"
     ]
    }
   ],
   "source": [
    "suffix_fdist = nltk.FreqDist()\n",
    "#extract suffix\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "#list of common suffiexes\n",
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(200)]\n",
    "\n",
    "#part of speech features extract function\n",
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "#create list of features of the words that match with tags\n",
    "featuresets = [(pos_features(word),tag) for (word,tag) in tagged_words]\n",
    "# train set 80%, test set 20%\n",
    "size = int(len(featuresets)*0.2)\n",
    "train_set=featuresets[size:]\n",
    "test_set=featuresets[:size]\n",
    "print('training')\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier,test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK\n",
    "\n",
    "@inproceedings{Loper:2002:NNL:1118108.1118117,\n",
    " author = {Loper, Edward and Bird, Steven},\n",
    " title = {NLTK: The Natural Language Toolkit},\n",
    " booktitle = {Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing    and Computational Linguistics - Volume 1},\n",
    " series = {ETMTNLP '02},\n",
    " year = {2002},\n",
    " location = {Philadelphia, Pennsylvania},\n",
    " pages = {63--70},\n",
    " numpages = {8},\n",
    " url = {https://doi.org/10.3115/1118108.1118117},\n",
    " doi = {10.3115/1118108.1118117},\n",
    " acmid = {1118117},\n",
    " publisher = {Association for Computational Linguistics},\n",
    " address = {Stroudsburg, PA, USA},\n",
    "}\n",
    "\n",
    "Perceptron Algorithm\n",
    "\n",
    "@inproceedings{hajivc2009semi, <br>\n",
    "  title={Semi-supervised training for the averaged perceptron POS tagger},\n",
    "  author={Haji{\\v{c}}, Jan and Raab, Jan and Spousta, Miroslav and others},\n",
    "  booktitle={Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics},\n",
    "  pages={763--771},\n",
    "  year={2009},\n",
    "  organization={Association for Computational Linguistics}\n",
    "  abstract= {This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, ﬁrst introduced for this task by (Collins, 2002). Experiments withaniterativetrainingonstandard-sized supervised (manually annotated) dataset (106 tokens) combined with a relatively modest (in the order of 108 tokens) unsupervised (plain) data in a bagging-like fashion showed signiﬁcant improvement of the POS classiﬁcation task on typologicallydifferentlanguages,yieldingbetter than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %).\n",
    "}\n",
    "}\n",
    "\n",
    "n-gram\n",
    "@article{Brown:1992:CNG:176313.176316,\n",
    " author = {Brown, Peter F. and deSouza, Peter V. and Mercer, Robert L. and Pietra, Vincent J. Della and Lai, Jenifer C.},\n",
    " title = {Class-based N-gram Models of Natural Language},\n",
    " journal = {Comput. Linguist.},\n",
    " issue_date = {December 1992},\n",
    " volume = {18},\n",
    " number = {4},\n",
    " month = dec,\n",
    " year = {1992},\n",
    " issn = {0891-2017},\n",
    " pages = {467--479},\n",
    " numpages = {13},\n",
    " url = {http://dl.acm.org/citation.cfm?id=176313.176316},\n",
    " acmid = {176316},\n",
    " publisher = {MIT Press},\n",
    " address = {Cambridge, MA, USA},\n",
    " abstract={We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.}\n",
    "}\n",
    "\n",
    "@inproceedings{schmid2013probabilistic,\n",
    "  title={Probabilistic part-ofispeech tagging using decision trees},\n",
    "  author={Schmid, Helmut},\n",
    "  booktitle={New methods in language processing},\n",
    "  pages={154},\n",
    "  year={2013}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
