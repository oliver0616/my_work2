
Assessing Students’ Performance Using the Learning 

Analytics Enriched Rubrics  
Ioannis Dimopoulos 
University of Piraeus 

Department of Digital Systems 

80 Karaoli & Dimitriou, 185 34 

Piraeus, Greece 
+302104142746 

 johndimopoulos@sch.gr   

Ourania Petropoulou 
University of Piraeus 

Department of Digital Systems 

80 Karaoli & Dimitriou, 185 34 

Piraeus, Greece 
+302104142746 

rpetro@biomed.ntua.gr  

Symeon Retalis 
University of Piraeus 

Department of Digital Systems 

80 Karaoli & Dimitriou, 185 34 

Piraeus, Greece 
+302104142746 

retal@unipi.gr  

 

 

ABSTRACT 
The assessment of students’ performance in e-learning 

environments is a challenging and demanding task for the 

teachers. Focusing on this challenge, a new assessment tool, 

called Learning Analytics Enriched Rubric (LAe-R) is presented 

in this paper. LAe-R is based on the concept of assessment rubrics 

which is a very popular assessment technique in education. LAe-R 

contains “enriched” criteria and grading levels that are associated 

to data extracted from the analysis of learners’ interaction and 

learning behavior in an e-learning environment. LAe-R has been 

developed as a plug-in for the Moodle learning management 

system. Via an example, we will show how LAe-R can be used by 

teachers and students.  

Categories and Subject Descriptors 
J.1 [Administrative Data Processing]: Education; K.3.1 

[Computer Uses in Education]: Collaborative Learning; 

Distance Learning;   

General Terms 
Measurement, Performance, Design 

Keywords 
Learning Analytics, Enriched Assessment Rubrics, Students’ 

Assessment Performance, Interaction Analysis Indicators 

1. INTRODUCTION 
During the last decades, teachers use learning management 

systems (LMSs) at all levels of education for enriching the 

traditional learning paradigm with more flexible and multimedia 

rich learning experiences. Students are often engaged into 

orchestrated learning activities which require the finding of 

appropriate solutions to problems via inquiry and 

experimentation, brainstorming, collaboration (synchronous or 

asynchronous), social networking as well as the use of various

 types of resources for self-paced learning [1]. According to 

modern learning practices, learning should not be considered as 

an isolated self-paced process. It should contain of a wide range 

of interactions between students, between students and teachers 

and between students and learning resources, that are described in 

learning scripts [2, 3, 4].  

One of the major challenges in these technologically and 

pedagogically enhanced learning environments is that teachers 

face serious difficulties to thoroughly capture, track, and analyze 

the massive amounts of data gathered during an interactive 

learning process in order to assess students’ performance [5, 6, 7].  

To support teachers  in this complex and demanding task, new 

assessment methods and tools based on educational data mining 

or learning analytics have been proposed [8, 9, 10]. Nevertheless, 

“most of the data mining tools are too complex for educators to 

use and their features go well beyond the scope of what an 

educator might require” [11]. So, current research studies in the 

field of students’ assessment in e-learning environments indicate 

the need for new assessment techniques and tools, integrated to 

LMSs, capable of effectively supporting the teachers to 

holistically assess both products of learning and the very complex 

process of learning [12, 13].  

This paper proposes an innovative assessment tool, called 

Learning Analytics Enriched Rubric (LAe-R), which has been 

developed as a Moodle plug-in. LAe-R allows teachers to easily 

create “enriched rubrics” containing criteria and related grading 

levels that are associated to data extracted from the analysis of 

learners’ interaction and learning behavior in a Moodle course, 

(i.e. number of post messages, times of accessing learning 

material, assignments grades, etc.). LAe-R, using learning 

analytics related to collaborative interactions, past grading 

performance and inquiries of learning resources in Moodle, can 

automatically calculate the score of the various levels per 

criterion. The total rubric score is calculated as a sum of the 

scores per each criterion (also weighted sum if required by the 

teacher). 

The rest of the paper is structured as follows: In the next section, 

we briefly present the educational concept of the enriched rubrics 

and the need to integrate them with learning analytics method. In 

section 3 the Learning Analytics Enriched Rubric (LAe-R) is 

presented in detail. Finally the conclusions and further research 

are outlined. 

(c) 2013 Association for Computing Machinery. ACM acknowledges that 

this contribution was authored or co-authored by an employee, contractor 

or affiliate of the national government of Greece. As such, the government 

of Greece retains a nonexclusive, royalty-free right to publish or reproduce 

this article, or to allow others to do so, for Government purposes only. 

LAK '13, April 08 - 12 2013, Leuven, Belgium  

Copyright 2013 ACM 978-1-4503-1785-6/13/04…$15.00 

195

mailto:johndimopoulos@sch.gr
mailto:rpetro@biomed.ntua.gr
mailto:retal@unipi.gr


2. ADDING LEARNING ANALYTICS 

INDICATORS TO TRADITIONAL 

ASSESSMENT RUBRICS 
 

Assessment is a very important part of the learning process. 

Teacher in a traditional classroom use paper-pencil assessment 

which is method of grading students’ subject knowledge. 

Moreover, thanks to their face-to-face interaction with the 

students, they can have a good understanding of individual 

student’s skills (e.g. real life problem solving skills, ability to 

integrate knowledge across disciplines, communication and 

collaboration skills) and talents [14]. Teachers often prefer to 

make use of the assessment rubrics in order to reflect on and 

evaluate student output, performance, and products. Rubrics are 

“scoring guides that formalize the evaluation process and provide 

fair and clear results to students” [15].  

However, in eLearning environments where face-to-face is 

lacking, there is a well justified need to equip teachers with new 

techniques and tools for assessing student’s performance that will 

not only take into consideration the students’ performance in 

online tests and assignments but also several learning interaction 

indicators [16]. Several sets of such learning interaction indicators 

have been proposed to be considered as well, which are related to 

students’ studying habits and active participation in group 

discussions and group deliverables, e.g. total number of activities-

messages per student/group, proportion of writing-reading of 

messages per student/group, density of social network, rate of 

learning resources that are read per student/group, and so on [17, 

18, 19, 11].  

This is why a new assessment technique, called "Enriched 

Rubrics”, has been proposed.  Like a “classic” rubric, it consists 

of a set of criteria and for each criterion, several descriptive levels 

are provided. An enriched rubric contains some criteria and 

related grading levels that are associated to data from the analysis 

of learners’ interaction and learning behavior in an online course, 

such as number of post messages, times of accessing learning 

material, assignments grades and so on [20, 21].  

However, for applying this technique, teachers need to use 

specialized educational data retrieving tools that automatically 

collect, extract and filter all the necessary data about students’ 

learning interactions, which can be retrieved from the logfiles of 

an e-learning platform. Such tools are GISMO [22], iPET [23], 

Moodog [24], MOCLog [25] and others [26]. They help teachers 

analyze data according to a set of indicators (e.g. number of 

activities-messages per user/team, number of visited resources per 

student, etc.) and produce reports in the form of simple statistical 

tables, or visualizations such as bar charts, pie charts, etc. 

According to these reports, a teacher can add scores to the various 

rubric criteria.  

Needless to say, that when teachers have to use several tools, 

which had not been developed for their exact needs, they quickly 

abandon the application of a more sophisticated assessment 

process. So there is an imperative need for developing new 

assessment tools capable of providing the teachers and the 

students, the ability to effectively use the enriched rubrics 

smoothly and quickly throughout the educational process via a 

unified environment [27, 28].  

Consequently, we developed an innovative Moodle plug-in, called 

the Learning Analytics Enriched Rubric (LAe-R), which comes to 

help teachers in applying the enriched rubrics assessment 

technique. The tool allows teachers to easily create an “enriched 

rubric” containing criteria and related grading levels that are 

associated with indicators about learners’ interaction and learning 

behavior in a Moodle course, (i.e. number of post messages, times 

of accessing learning material, assignments grades, etc.). LAe-R, 

is more analytically presented in the following sections. 

3. THE LAe-R MOODLE PLUG-IN 

3.1 LAe-R functionality  
The LAe-R plug-in was created in order to be included in Moodle 

(version 2.2+) as an advanced grading method. It is an enhanced 

version of the existing rubric plug-in. A teacher can create an 

enriched rubric (ER) from scratch, or create a new one from a 

template or edit an existing one. When creating an ER, the teacher 

adds the assessment criteria with their corresponding descriptions 

and the grading levels (see Figure 1). At its current version, LAe-

R allows a teacher to add types of criteria that are associated to 

learning analysis indicators: collaboration, grades to assignments, 

study of learning resources. 

In the case of “collaboration”, the teacher can use various log 

file data for assessing students’ performance such as forum posts 

(new or reply messages), chat messages, number of files attached 

to forum post messages. For measuring students’ study behavior, 

the number of students’ views upon selected learning recourses 

can be considered. For taken into account students’ performance 

in various assignments, the grading scores on selected 

assignments can be used.  LAe-R also allows teachers not only to 

calculate each student’s performance but also to measure a 

student’s performance at each criterion in comparison to the 

average score of the performance of all students (assessment 

according to class scope).    

 

Figure 1. Screenshot of how a teacher can specify an 

assessment criterion in LAe-R 

3.2 Assessing students’ performance with 

LAe-R 
 

LAe-R can automatically give a score at each criterion by 

collecting the related data from the Moodle log files or data 

entries from the selected course modules. The way that is done is 

simple (see Figure 2). For each “enriched” criterion, a value 

(benchmark) is calculated from the corresponding data. Then, the 

appropriate rubric level gets picked. For example, if a student has 

posted 4 reply messages to a forum (4 is the value/benchmark) 

he/she will be at level 2 of that criterion which has been described 

as the collaboration level where students post between 3 and 6 

reply messages. As already mentioned, the score that a student 

gets at a criterion might have to do with his/her performance in 

comparison to other fellow students. This is called “global scope 

196



evaluation” and uses percentages for all logical comparisons. For 

example, a teacher might want to check if the number of one 

student’s file submissions in specific forums is more than 50% of 

students’ average, or student’s grade in a particular assignment is 

30% over the fellow students’ average grade. 

 

Figure 2. LAe-R automatic evaluation work flow 

 

Of course, the teacher can review the assessment results and make 

some extra (optional) comments before publishing the student’s 

rubric. As shown in Figure 3, an enriched rubric presents the 

assessment results in a very clear way, i.e. with explanations per 

criterion.  

3.3 Data retrieval to produce Learning 

Analytics in LAe-R 
 

At the current version of LAe-R, collection and process of data is 

performed in various Moodle database tables according to 

enrichment criteria in order to produce learning analytic (see 

Table 1). 

Table 1. Basic database querying characteristics 

Enrichment 

type 

Collaboration 

type 

Database 

Table 

Condition / 

Calculation 

Collaboration 
Simple 

occurrences 
log 

action =  

‘add post’, 

action = 

‘talked’ 

Collaboration 
File 

submissions 

forum_ 

posts 
attachment(s) 

Collaboration Forum replies 
forum_ 

posts 
forum_id(s) 

Collaboration 
People 

interacted 

forum_ 

posts / 
userid(s) 

chat 

messages 

Study -  log 
action = 

‘view’ 

Grade -  
grade_ 

grades 
finalgrade(s) 

  

To produce learning analytics some specific principles have been 

adopted: 

? Time stamps are embedded in database queries 

providing that the corresponding options are enabled for 

the LAe-R and respective dates are set in the particular 

assignment. 

? For average scores estimation, the arithmetic mean of 

values is calculated. 

? When calculating the students’ level of performance in 

assignments/tests, all grades from the assignment course 

modules are converted to 100 point scale for preserving 

the consistency.  

 

 

 

Figure 3. Evaluation results 

 

4. EVALUATION & CONCLUDING 

REMARKS 
Before publishing LAe-R on the Moodle developers’ community, 

extended tests were made in order to ensure that there are not 

functionality problems. Also we performed an initial evaluation 

study which concerned: 

1. Code compatibility and compliance according to 

Moodle coding styles and standards 

2. Usability testing by experienced teachers 

Regarding the coding style used, LAe-R was written according to 

all coding guidelines specified by Moodle standards. All defined 

classes, procedures and functions included in the code, are 

197



compatible with the Moodle platform architecture. All the 

significant changes which had been made from Moodle version 

2.2 to 2.3 affecting crucial parts of the plugin’s functionality were 

taken into account and minor adjustments were made so that LAe-

R is fully compatible to both versions. The plugin was 

successfully approved by Moodle evaluators and was published. 

Also, twelve (12) experienced primary and secondary education 

teachers, who are now attending an MSc program in e-learning 

and have excellent knowledge in Moodle, participated to a lab-

based usability inspection study. The goal of this project was to 

evaluate the usability and acceptance of LAe-R by common users. 

They were all given detail instructions for creating a course with 

their own subject in Moodle, using an Inquiry Based Learning 

technique, associating rubrics with LAe-R for evaluating students 

and filling up a questionnaire at the end of the course regarding 

the use of LAe-R. 

Qualitative and quantitative data gathered from this project 

consisted of a) the quality, competence and efficiency of the 

learning scenarios created and used by the group of teachers, b) 

the completeness and clarity of the created enriched rubrics that 

derived from the LAe-R instances and c) the filled questionnaires 

from all participating teachers. By processing these data, we come 

to the following results. 

The learning scenarios were well formed and proper for 

implementing Inquiry Based Learning in an e-learning 

environment according to educational standards. The majority of 

enriched rubrics (11 out of 12) contained sufficient criteria to 

evaluate the learning outcomes as well as the educational 

interactions during the learning process. Regarding LAe-R, 91,7% 

of the teachers stated that they felt comfortable using this tool 

witch was easy for them to work with and 75% were satisfied by 

its’ interface. According to LAe-R’s use from student evaluation, 

91,7% declared that they found LAe-R very useful, 83,3% quick 

to process and 75% stated that performing student evaluation 

seemed effortless. Very good reviews were also noted for LAe-R’s 

online instruction guide with detailed videos that helped a lot 

during rubric creation and student evaluation, giving more 

confidence during implementation. 

Making use of the teachers’ feedback comments as well as the 

notes of the research team, various enhancements of the tools can 

be made, such as:  

? Add more criteria in an enriched rubric thus allowing a 
teacher to assess more issues. An option is to embed 

into LAe-R one of the existing typology of learning 

interaction indicators and monitor teachers’ usage.   

? In order to calculate the value of a criterion, LAe-R 
could use not only first level descriptive statistics by 

inquiring data from the log files, but also more 

sophisticated indicators like the centrality in a social 

network analysis. 

? Make use of indicators that are not evaluation outcomes 
to various formats. 

? Use more sophisticated and appealing ways to visualize 
Learning Analytics indicators and distinct these 

visualizations for students and teachers accordingly.  

LAe-R seems a stable and very promising assessment tool that 

could fill-in the gap in assessing students’ performance in 

elearning environments using the wealth of interaction data. 

 

5. ACKNOWLEDGEMENTS 
This work has been partially supported by the SAILS project that 

has received funding from the European Union’s Seventh 

Framework Programme (http://www.sails-project.eu/). Source of 

funding: EU. FP7 Capacities Programme Science in Society. FP7 

Grant Agreement N° 289085 SAILS (CSA-SA_FP7-SCIENCE-

IN-SOCIETY-2011-1). 

 

6. REFERENCES 
[1] Lazakidou, G., & Retalis, S.  (2010). Using computer 

supported collaborative learning strategies for helping 

students acquire self-regulated problem-solving skills in 

mathematics. Computers & Education 54(1), 3-13.  

[2] Moore, G. (1989). Three types of interaction. The American 

Journal of Distance Education, 3(2), 1-6. 

[3] Dillenbourg, P., Järvelä, S., & Fischer, F. (2009). The 

evolution of research on computer-supported collaborative 

learning. Technology-Enhanced Learning, 3-19. Springer. 

[4] Bliuc, A.-M., Ellis, R.A., & Goodyear, P. (2011). The role of 

social identification as university student in learning: 

relationships between students’ social identity, approaches to 

learning, and academic achievement. Educational 

Psychology, 31(5), 559-574. 

[5] Quellmalz, E., & Kozma, R. (2003). Designing assessments 

of learning with technology. Assessment in Education, 10(3), 

389-406. 

[6] Petropoulou, O., Lazakidou, G., Retalis, S., & Vrasidas, C. 

(2007). Analysing interaction behaviour in network 

supported collaborative learning environments: a holistic 

approach. International Journal of Knowledge and Learning, 

3(4/5), 450 - 464. 

[7] Johnson, R., Penny, J., & Gordon, B. (2009). Assessing 

performance: designing, scoring, and validating performance 

tasks. Guilford Press. 

[8] Mazza, R., & Dimitrova, V. (2007). CourseVis: A Graphical 

Student Monitoring Tool for Facilitating Instructors in Web-

Based Distance Courses. International Journal in Human-

Computer Studies, 65(2), 125-139. 

[9] Yang, D., Richardson, J., French, B., & Lehman, J. (2011). 

The development of a content analysis model for assessing 

students’ cognitive learning in asynchronous online 

discussions. Educational Technology Research & 

Development, 59(1), 43-70. 

[10] García-Saiz, D., & Zorilla Pantaleón, M.E. (2011). E-

learning web miner: A data mining application to help 

instructors Involved in virtual courses. In M. Pechenizkiy et 

al. (Eds.), Proceedings of the 3rd Conference on Educational 

Data Mining 2011 (323-324). Eindhoven, The Netherlands. 

[11] Romero, C. (2010). Educational Data Mining: A Review of 

the State of the Art.  IEEE Transactions on Systems. Man, 

and Cybernetics, 40(6), 601-618. 

198

http://www.sails-project.eu/


[12] Strijbos, J. W. (2011). Assessment of (computer-supported) 

collaborative learning. IEEE Transactions on Learning 

Technologies, 4(1), 59-73. 

[13] Dyckhoff, A. L., Zielke, D., Bültmann, M., Chatti, M. A., & 

Schroeder, U. (2012). Design and Implementation of a 

Learning Analytics Toolkit for Teachers. Educational 

Technology & Society, 15(3), 58–76. 

[14] Carr, J. F., &. Harris, D. E. (2001). Succeeding with 

standards linking curriculum, assessment, and action 

planning. Alexandria, Virginia: Association for Supervision 

and Curriculum Development. 

[15] Oberg, C. (2009). Guiding classroom instruction through 

performance assessment. Online Journal of Case Studies in 

Accreditation and Assessment, 1, 1–11, ISSN: 1941–3386. 

Retrieved from: 

http://www.aabri.com/manuscripts/09257.pdf 

[16] Daradoumis T., Martínez-Monés A., & Xhafa F. (2006). A 

layered framework for evaluatingon-line collaborative 

learning interactions. International Journal of Human-

Computer Studies, 64(7), 622-635. 

[17] Barros, B., & Verdejo, M. F. (1999). An approach to analyse 

collaboration when shared structured workspaces are used 

for carrying out group learning processes. In S.P. Lajoie & 

M. Vivet (Eds.), Artificial intelligence in education: Open 

learning environments, 449-456. Amsterdam: IOS Press. 

[18] Mart?nez, A., Dimitriadis, Y., De La Fuente, P. (2003). 

Contributions to analysis of interactions for formative 

evaluation in CSCL. In: Llamas M, Fernandez M J, Anido L 

E (eds.) Computers and education: towards of lifelong 

learning society, 227-238. Kluwer, The Netherlands. 

[19] Ho, C., & Swan, K. (2007). Evaluating online conversation 

in an asynchronous learning environment: An application of 

Grice’s cooperative principle. Internet and Higher 

Education, 10, 3-14. 

[20] Petropoulou, O.,Vasilikopoulou, M., & Retalis, S. (2009). 

Enriched Assessment Rubrics: A new medium for enabling 

teachers easily assess students’ performance when 

participating to complex interactive learning scenarios. 

Operational Research International Journal, 11(2), 171-186. 

[21] Petropoulou, O., Retalis, S., & Lazakidou, G. (2012). 

Measuring Students’ Performance in e-Learning 

Environments via Enriched Assessment Rubrics. In 

Psaromiligkos, Spyridakos, Retalis(eds): Evaluation in e-

Learning. Nova Science Publishers, ISBN: 978-1-61942-

942-0. 

[22] Mazza, R., & Botturi, L. (2007) Monitoring an Online 

Course with the GISMO Tool: A Case Study. Journal of 

Interactive Learning Research, 18(2), 251-265. Chesapeake, 

VA: AACE. 

[23] Saltz J., Hiltz S., Turoff M., & Passerini K. (2007). 

Increasing participation in distance learning courses. IEEE 

Internet Computing, 11(3), 36-44. 

[24] Zhang, H., Almeroth, K., Knight, A., Bulger, M., & Mayer, 

R. (2007). Moodog: Tracking students' Online Learning 

Activities. In C. Montgomerie & J. Seale (Eds.), Proceedings 

of World Conference on Educational Multimedia, 

Hypermedia and Telecommunications, 4415-4422. 

Chesapeake, VA: AACE. 

[25] Mazza R., Bettoni M., Fare M., Mazzola L. (2012). 

MOCLog – Monitoring Online Courses with log data. In 

Retalis, S., and Dougiamas, M. (2012). 1st Moodle Research 

Conference Proceedings. Online: 

http://research.moodle.net/MoodleCon_Proceedings_progra

m 

[26] Sharkey, M. (2011) Academic analytics landscape at the 

University of Phoenix. In Proceedings of the 1st 

International Conference on Learning Analytics and 

Knowledge (LAK '11). ACM, New York, NY, USA, 122-

126. 

[27] Petropoulou, O., Altanis, I., Retalis, S., Nicolaou, C. A., 

Kannas, C., Vasiliadou, M., & Pattis, I. (2010). Building a 

tool to help teachers analyse learners' interactions in a 

networked learning environment. Educational Media 

International, 47(3), 231- 246. 

[28] Gobert, J., Sao Pedro, M., Baker, R., Toto, E., & Montalvo, 

O.  (2012). Leveraging Educational Data Mining for Real-

time Performance Assessment of Scientific Inquiry Skills 

within Microworlds. Journal of Educational Data Mining, 

Article 5, 4 (1), 153-185. 

 

 

 

 

 

 

 

 

199

http://www.aabri.com/manuscripts/09257.pdf
http://research.moodle.net/MoodleCon_Proceedings_program
http://research.moodle.net/MoodleCon_Proceedings_program




