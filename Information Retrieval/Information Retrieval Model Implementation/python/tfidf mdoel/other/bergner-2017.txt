
Workshop on Methodology in Learning Analytics (MLA)  
Yoav Bergner 

New York University 
New York, NY, USA 
yoav.bergner@nyu.edu 

 

Charles Lang 
Teachers College, Columbia 

University 
New York, NY, USA 

charles.lang@tc.columbia.edu 

Geraldine Gray 
Institute of Technology, 

Blanchardstown 
Dublin, Ireland 

geraldine.gray@itb.ie 
 
 
 

ABSTRACT 
Learning analytics is an interdisciplinary and inclusive field, a fact 
which makes the establishment of methodological norms both 
challenging and important. This community-building workshop 
intends to convene methodology-focused researchers to discuss 
new and established approaches, comment on the state of current 
practice, author pedagogical manuscripts, and co-develop 
guidelines to help move the field forward with quality and rigor. 

 

CCS Concepts 
• Computing methodologies ? Modeling and simulation ? 
Model development and analysis 

 

Keywords 
Models; Methodology; Measurement; Statistics; Evaluation. 

 

1. WORKSHOP BACKGROUND 
Learning analytics is an interdisciplinary and inclusive field that 
brings together educational technologists, psychologists, data 
scientists, learning scientists, substantive experts in various 
domains, and measurement specialists [7]. For all of the strength 
that comes from such diversity, there are also potential pitfalls 
when it comes to establishing norms for methodological work. For 
example, Clow [3] described learning analytics as, “a ‘jackdaw’ 
field of enquiry, picking up ‘shiny’ techniques, tools and 
methodologies… This eclectic approach is both a strength and a 
weakness: it facilitates rapid development and the ability to build 
on established practice and findings, but it—to date—lacks a 
coherent, articulated epistemology of its own.” (p. 686).  

In the years since this observation, the learning analytics 
community has grown rapidly, and the number of shiny 
techniques has grown as well. Looking just at the last two 

proceedings of the International Conference on Learning 
Analytics and Knowledge (LAK) in 2015 and 2016, the variety is 
staggering.  Methods range from descriptive statistics to 
correlation analyses, classification, clustering, regression, 
(M)AN(C)OVA, structural equation modeling, item response 
theory, hidden Markov models, time-series analysis, latent 
semantic analysis, social network analysis, and the list goes on. It 
is understandable and even expected that reviewers and readers of 
learning analytics manuscripts are unlikely to be expert evaluators 
of the methodological rigor in all of these cases.  

There is a naturally occurring process of specialization in any 
academic field. However, if growth of adoption outpaces 
systematic specialization then there is a risk that methodological 
errors will proliferate and that quality of community products will 
suffer.    

To make matters even more complex, a number of recent papers 
have emphasized the sensitivity of quantitative analyses to data 
collection and variable operationalization choices, for example 
with regard to effects of selection bias [2], results of time-on-task 
analyses [4], studies of discussion forum usage [1], and evaluation 
of student models [6]. In addition, learning analytics models often 
incorporate a selection of proxy variables as indicators of latent 
constructs, such as learning and engagement. What proxy 
variables actually measure is less clear. For example, measures of 
engagement may be influenced by instructional conditions [5], 
adding ambiguity, and a lack of consistency,  to our interpretation 
of  models of learning.  

In short, methodological concerns can arise from a range of 
practices including but not limited to selecting inappropriate 
methods, misusing methods, inadequate model evaluation or 
model comparison, sensitivity to operationalization, and over-
reliance on proxy variables. As the learning analytics community 
matures, it is particularly important to establish standards for good 
practice and to educate new students in accordance with these 
standards. Clear methodological guidelines increase the quality of 
work and facilitate communication not only within the community 
but also with practitioners in other research communities, where 
norms may be clearer. This is a challenging problem in large part 
because of the aforementioned diversity of approaches. The 
present workshop seeks to build a community of researchers with 
an interest in methodology and its rigorous application and 
development to the field of learning analytics.  
There have been several previous LAK workshops and tutorials 
that have focused on specific methodologies—a limited set of 
examples includes the tutorials for classification and clustering 
using Weka (2014, 2016), special topics in discourse analysis 
(2013-2014) and writing analysis (2016), and a recurring 
workshop on temporal analysis (2012-2016)—but not on cross-
cutting methodological issues such as developing methodological 

Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of this work must be honored. For all other 
uses, contact the Owner/Author.  
Copyright is held by the owner/author(s). 
LAK '17, March 13-17, 2017, Vancouver, BC, Canada 
ACM 978-1-4503-4870-6/17/03. 
http://dx.doi.org/10.1145/3027385.3029427 



frameworks within learning analytics, framing and prioritizing 
methodological issues for the community, and providing resources 
to move the field forward. 

 

2. WORKSHOP OBJECTIVES 
2.1 Solicit Papers about Learning Analytics 
Methodology 
The first objective of the present workshop is to solicit new 
substantive papers specializing on methodology. We imagine 
these papers to fall roughly into the following categories: papers 
presenting new methods or adaptation/modification of methods; 
position papers which take a critical look at methodological 
practice in the community; and pedagogical/instructional papers 
oriented at students or researchers who are new to the field or 
developing an interest in a particular methodology.  

We encourage the exploration of new metrics that are indigenous 
to learning analytics, papers that link metrics to the latent 
variable(s) they measure, with a view to establishing definitions of 
latent variables of relevance to learning analytics, and guidelines 
on associating observable metrics with latent variables. Papers on 
appropriate metrics for model evaluation and metrics for 
comparison of model results where multiple models are reported 
on would also exemplify an effort to add rigor to applicable 
studies. 

2.2 Develop Community Guidelines 
Related to the position and instructional papers that may be 
presented during the workshop, a second objective of convening 
will be to cooperatively develop community guidelines regarding 
the uses of various methods including data acquisition, data 
analysis and evaluation of results in conference and journal 
publications. Up for discussion will be content, process, and 
dissemination protocols. 

2.3 Provide Expertise for Review Panels 
A third objective of the workshop is to take responsibility for 
maintaining a database of methodology experts who are active in 
the learning analytics community. The expert listing is by no 
means intended to be exclusionary or to promote certain 
researchers over others but rather to help community members 

and editorial committees find methodology experts who are 
willing to consult and/or review relevant work.  

2.4 Community Building 
Last but not least,  an objective of the workshop is to provide a 
meeting place for researchers who take a special interest in 
methodological issues. We anticipate that a concentrated meeting 
will promote continuing collaboration on this important topic.  

 

3. REFERENCES  
[1] Bergner, Y., Kerr, D., and Pritchard, D. E. 2015. 

Methodological Challenges in the Analysis of MOOC Data 
for Exploring the Relationship between Discussion Forum 
Views and Learning Outcomes. Proceedings of 8th 
International Conference on Educational Data Mining, 234-
241. 

[2] Brooks, C., Chavez, O., Tritz, J., and Teasley, S. 2015. 
Reducing selection bias in quasi-experimental educational 
studies. Proceedings of the Fifth International Conference on 
Learning Analytics And Knowledge, - LAK ’15, ACM Press, 
295–299. 

[3] Clow, D. 2013. An overview of learning analytics. Teaching 
in Higher Education 18 (6), 683–695. 

[4] Kovanovi?, V., Gaševi?, D., Dawson, S., Joksimovi?, S., 
Baker, R.S.J.D., and Hatala, M. 2015. Penetrating the black 
box of time-on-task estimation. Proceedings of the Fifth 
International Conference on Learning Analytics And 
Knowledge - LAK ’15, ACM Press, 184–193. 

[5] Gaševi?, D., Dawson, S., and Siemens, G. 2015. Let’s not 
forget: Learning analytics are about learning. TechTrends 59 
(1), 64-71. 

[6] Pelánek, R., Rihák, J., and Papoušek, J. 2016. Impact of data 
collection on interpretation and evaluation of student models. 
Proceedings of the Sixth International Conference on 
Learning Analytics & Knowledge, - LAK ’16, ACM Press, 
40–47. 

[7] Siemens, G. and Gasevic, D. 2012. Guest Editorial-Learning 
and Knowledge Analytics. Educational Technology & 
Society 15 (3), 1–2. 

 



