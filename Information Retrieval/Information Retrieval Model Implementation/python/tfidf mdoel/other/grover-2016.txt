
Multimodal Analytics to Study Collaborative Problem 
Solving in Pair Programming 

Shuchi Grover 
SRI International 
Menlo Park, CA 

shuchi.grover@sri.com 
Behjat Siddiquie 

SRI International 
Princeton, NJ 

behjat.siddiquie@sri.com 

Marie Bienkowski 
SRI International 
Menlo Park, CA 

marie.bienkowski@sri.com 
David Salter  

SRI International 
Princeton, NJ 

david.salter@sri.com 
 

Amir Tamrakar 
SRI International 

Princeton, NJ 
amir.tamrakar@sri.com 

Ajay Divakaran 
SRI International 

Princeton, NJ 
ajay.divakaran@sri.com 

 
ABSTRACT 
Collaborative problem solving (CPS) is seen as a key skill in K-12 
education—in computer science as well as other subjects. Efforts 
to introduce children to computing rely on pair programming as a 
way of having young learners engage in CPS. Characteristics of 
quality collaboration are joint exploring or understanding, joint 
representation, and joint execution. We present a data driven 
approach to assessing and elucidating collaboration through 
modeling of multimodal student behavior and performance data. 

CCS Concepts 
• Human-centered computing ~ Empirical studies in 
collaborative and social computing   

Keywords 
Collaboration, Collaborative Problem Solving, Pair programming, 
Kinect, Multimodal analytics, K-12 Computer Science Education.    

1. INTRODUCTION 
Collaborative problem solving (CPS) is increasingly 
acknowledged as an important 21st century skill across all 
disciplines and especially in STEM subjects [7]. Recent efforts to 
identify the core skills that constitute computational thinking in 
K-12 computer science (CS) [2] have highlighted both the 
collaborative and the problem-solving nature of CS.   
Programming is an essential part of the CS problem solving and 
design process, and a central feature of most introductory CS 
curricula is learning to program. Pair programming has been 
shown to increase programming confidence and satisfaction in 
college settings [3, 5], and is now being advocated in pre-college 
settings by US advocacy organizations such as NCWIT and 
Code.org. A collaborative approach to computer programming 
where two people work sharing the same computer and mouse, 
pair programming provides a unique context to better understand 
the process of CPS in introductory CS classrooms in K-12.  

Past research shows that the quality of student interactions has a 
direct influence on their learning during collaborative activities 
and that successful collaboration does not occur by itself and 

needs to be supported [4]. Measuring and assessing interaction 
and collaboration is challenging but necessary if it needs to be 
supported in learning environments. The quality of pair 
programming “in process” remains relatively understudied. Self-
reports and reflections are helpful but imprecise and incomplete. 
Traditional ethnographic and qualitative research techniques are 
painstaking and do not scale, in addition to providing information 
only post-hoc. Multimodal analytics can play a significant role in 
this area, since they aim at deriving high-quality information from 
audiovisual and sensorial input (Figure 1), which can help identify 
interactions and behaviors that afford the drawing of inferences 
based on the temporal patterns in the multimodal signals. 

 
Figure 1: Range of information that can be captured by 

unobtrusive instrumentation. 

This paper presents our preliminary work in studying the quality 
of collaboration using multimodal data gleaned from unobtrusive 
sensors as pairs solve programming puzzles. We define quality 
collaboration as the process of sharing understanding, and pooling 
skills, effort, and knowledge to solve problems [7]. This work 
represents a first step in combining data from the physical 
environment with process data from the programming 
environment to study how collaboration impacts the programming 
process, with the view to eventually providing feedback to the 
teachers and learners in addition to scaffolds for supporting the 
complex process of CPS during programming.  

2. DYADIC COLLABORATION IN PAIR 
PROGRAMMING  
The goal of this ongoing research is to develop and test 
computational models of social interaction, in CPS environments. 

Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of this work must be honored. For all other 
uses, contact the Owner/Author.  
Copyright is held by the owner/author(s). 
LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  
ACM 978-1-4503-4190-5/16/04. 
DOI: http://dx.doi.org/10.1145/2883851.2883877 

mailto:shuchi.grover@sri.com
mailto:behjat.siddiquie@sri.com
mailto:marie.bienkowski@sri.com
mailto:david.salter@sri.com
mailto:amir.tamrakar@sri.com
mailto:ajay.divakaran@sri.com


We begin with approaches to classify the quality of collaboration 
from Kinect® data on the body movements and gestures of the 
pair programmers working together at a computer.  

2.1 Pilot Study: Procedures & Data Capture   
Our pilot study involves unobtrusively capturing multimodal data 
(video, audio, clickstream, and screen capture) from pairs of 
children as they work together on a pair-programming task. The 
tasks are those from the open source Blockly Games (blockly-
games.appspot.com, on which the popular Hour of Code activities 
are based as well as other applications). We developed an 
instrumented version of Blockly (developers.google.com/blockly/) 
to record clickstream data on programming actions for a related 
learning analytics research effort. We record audio and actions on 
the screen using the Camtasia® screen-capture application. We 
record video of the participants seated side-by-side, covering the 
area from the keyboard up to include their whole face, using the 
Kinect® (Figure 2). The Kinect captures a video recording of the 
scene as well as sensing and recording body position: position of 
the head, arms, and torso; in addition to eye gaze and audio.  
We have currently captured data from 4 pairs of children aged 
between 11 and 14 as they worked on the 10 levels of the Maze 
and Pond Blockly games. For each pair, we introduced pair 
programming as the way they should approach the problems, 
which uses driver and navigator roles. The driver controls the 
mouse and keyboard, and the navigator provides support by 
discussing and checking the driver's work. Both partners are 
responsible for design and development of the solution. The pairs 
were instructed to switch driver and navigator roles after they 
solved each level of the Blockly Games (10 levels in each). We 
collected 8 videos varying in length between 30 and 60 minutes. 
.

 
Figure 2: Data capture with Kinect® mounted on display. 

2.2 Data Analyses 
The first step in creating the computational models was to 
manually code some of the recorded videos to use as training 
examples for the machine learning algorithms. Based on an initial 
viewing of the videos captured from different pairs, we decided 
that using a 1-minute time interval would help us make reliable 
judgments about the pair interaction. We qualitatively coded 
videos for evidence of collaboration, roughly averaged across a 1-
minute interval, as High, Medium, or Low. The coding scheme 
was based on observables used in past research [3, 8]. For pair 
programming, our observables include screen pointing, leaning 
forward, joint attention (looking at screen), taking the mouse 
(with or without consent) and synchrony in body position. These 
were used to train machine learning algorithms that classify the 
ongoing collaboration into the three aforementioned categories.  

We leverage past work modeling multimodal predicates such as 
engagement by capturing their spatiotemporal constituents [1]. 

We use the raw features derived from the skeleton tracking of the 
two subjects with the goal of predicting the level/strength of their 
collaboration. Since the level of collaboration can be low, 
medium, or high, random classification accuracy is 33%. Using 
the 13 joints from the upper body, we extracted a set of first order 
static and dynamic handcrafted skeleton features. Given these 
features we trained a Support Vector Machine based classifier to 
perform classification. Preliminary evaluations on four sessions 
consisting of two Pond and two Maze games containing a total of 
about 117 annotated one minute segments, show that we can 
detect the level of collaboration with 48% accuracy which is 
significantly better than random. The accuracy was determined 
using ten-fold randomized cross-validation. While further work is 
required to conclusively establish the effectiveness of our 
classifiers and features, our preliminary results are encouraging.  

3. CONCLUSION AND FUTURE WORK 
Our ongoing work on this pilot will further our understanding of 
computational models that can automatically detect collaboration 
from gesture, posture, and body movement data, and assess its 
quality from pairs of students working on programming. Future 
work includes additional data capture and adding analyses of eye 
gaze and speech. Furthermore, we plan to apply Discriminative 
Conditional Restricted Boltzmann Machines [1] that will allow us 
to identify constituents of high collaboration interactions e.g., 
synchronized arm motions of the pair. Eventually, we aim to 
combine these with clickstream data from the software 
environment that we are capturing in order to get a holistic picture 
of how the collaboration impacts the problem solving involved in 
the programming task. Although this work is exploratory and 
broader adoption depends on data sharing issues that the field is 
working to resolve, such analyses have the potential to allow us to 
assess and scaffold the collaboration in complex problem solving 
settings such as programming. 

REFERENCES 
[1] Amer, Mohamed R., et al. 2015. Human Social Interaction 

Modeling Using Temporal Deep Networks. arXiv preprint 
arXiv:1505.02137 (2015). 

[2] Astrachan, O., & Briggs, A. (2012). The CS principles 
project. ACM Inroads, 3(2), 38-42. 

[3] Hannay, J. E., Dybå, T., Arisholm, E., & Sjøberg, D. I. 2009. 
The effectiveness of pair programming: A meta-analysis. 
Information and Software Technology, 51(7), 1110-1122. 

[4] Hesse, F., Care, E., Buder, J., Sassenberg, K., & Griffin, P. 
2015. A framework for teachable collaborative problem 
solving skills. In Assessment and teaching of 21st century 
skills (pp. 37-56). Springer Netherlands. 

[5] McDowell, C. et al. 2006. Pair programming improves 
student retention, confidence, and program quality. 
Communications of the ACM, 49(8), 90-95. 

[6] Meier, A., Spada, H., & Rummel, N. 2007. A rating scheme 
for assessing the quality of computer-supported collaboration 
processes. International Journal of Computer-Supported 
Collaborative Learning, 2(1), 63-86. 

[7] Organization for Economic Co-operation and Development 
(OECD). 2013. PISA 2015 draft collaborative problem 
solving framework. 

[8] Rummel, N., & Spada, H. 2005. Learning to collaborate: An 
instructional approach to promoting collaborative problem 
solving in computer-mediated settings. The Journal of the 
Learning Sciences, 14(2), 201-241. 


	1. INTRODUCTION
	2. Dyadic Collaboration in Pair Programming
	2.1 Pilot Study: Procedures & Data Capture
	2.2 Data Analyses

	3. Conclusion and Future Work
	Our ongoing work on this pilot will further our understanding of computational models that can automatically detect collaboration from gesture, posture, and body movement data, and assess its quality from pairs of students working on programming. Futu...
	References


