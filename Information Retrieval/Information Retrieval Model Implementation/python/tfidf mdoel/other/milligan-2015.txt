
Crowd-Sourced Learning in MOOCs:  
Learning Analytics meets Measurement Theory 

 Sandra Milligan 
University of Melbourne 

Parkville, VIC, Australia 3052 
s.milligan@unimelb.edu.au 

 

ABSTRACT 
This paper illustrated the promise of the combination of 
measurement theory and learning analytics for understanding 
effective MOOC learning. It reports findings from a study of 
whether and how MOOC log file data can assist in understanding 
how MOOC participants use (often) messy, chaotic forums to 
support complex, unpredictable, contingent learning processes. It 
is argued that descriptions of posting, voting and viewing 
behaviours do not in and of themselves provide insights about 
how learning is generated in MOOC forums. Rather, it is 
hypothesised that there is a skill  involved in using forums to 
learn; that theory-informed descriptions of this skill illustrate how 
MOOC participants use forums differently as they progress from 
novice to expert; that the skill progression can be validated 
through the use of forum log file data; and that log file data can 
also be used to assess an individual MOOC participant’s position 
in relation to this progression – that is, to measure an individual’s 
skill in learning through forums and similar educational settings. 
These hypotheses were examined using data drawn from forums 
in a large MOOC run at the University of Melbourne in 2013.  

Categories and Subject Descriptors 
K.3.1 Computer Uses in Education 

General Terms 
Measurement, Performance, Reliability, Experimentation, Theory. 

Keywords 
MOOC, On-line Forums, Analytics Tools, Measurement Theory, 
Learning Analytics, Rasch Analysis, Crowd-sourced learning; 
21st Century skills, Collaborative Learning, Learning Progression, 
Learner Performance 
 

1. INTRODUCTION 
In a forum in a recent University of Melbourne MOOC, one 
participant started a thread entitled Ah ha! She posted about the 
light-bulb moment when she connected a key theme of the MOOC 
with her own professional practice. The thread was heavily 
viewed and frequently posted in, commented on, and voted for. It 
was also messy and undisciplined, but replete with dialogue 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be 
honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. Request permissions 
from Permissions@acm.org.  
LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA  
Copyright 2015 ACM 978-1-4503-4/15/03 $15.00  
http://dx.doi.org/10.1145/2723576.2723596  

focused on the core learning of the MOOC, participants eager to 
generate their own ‘ah ha!’ moments, or to share their sense of 
satisfaction (sometimes euphoria) arising from mastery of valued 
knowledge or skills. The thread made manifest what every teacher 
knows: ‘learning’ is a complex, unpredictable, contingent, 
sometimes exciting process that defies simple explanation and is 
rarely linear; and that good teaching is about generating 
opportunities for learners to experience this process. This paper 
explores whether measurement theory can be used with log file 
data to understand how MOOC participants use apparently chaotic 
forums to support their own complex learning process. 

2. FORUM PARTICPATION IN MOOCS 
A rapidly-growing number of studies attempt to understand 
MOOC forum participation via network, semantic, cluster and 
other analyses of activity log and demographic data. Findings vary 
widely. Many studies find participation levels disappointingly 
low, and the content and patterns of interaction confused and 
complex [1,3,5]. But some studies, drawing on participant self-
reports, evidence from experimental situations, and correlations 
between activity levels and outcomes, find that forums appear to 
be useful to some participants in some MOOCs on some 
occasions for some purposes, including for learning [6,7,9,11]. 

These disparities in findings are not surprising. Learning is a 
complex phenomenon, difficult to understand in classrooms of 30 
students let alone in large-scale, digitally-mediated, semi-
synchronous, open-access environments in which direct 
interaction between teacher and taught is virtually non-existent. 
Research into learning in MOOCs, grappling with a new and 
complex form of learning, is as emergent as MOOCs themselves.  

Shea et al. [21] suggested that cognitive benefits of online 
interaction arise from sophisticated discourse which provides the 
active learner with opportunities to garner additional information, 
new perspectives, corrective feedback or reminders. Participants 
can check understandings, find and explore new lines of 
reasoning, scope out problems or areas of misunderstanding, 
monitor their own and others’ judgments about their own and 
others’ performance, provide or seek direction or advice, 
recognise gaps in knowledge, or chew over opinions. This is to 
posit the existence of the ‘self-regulated learner’ [20] who makes 
very selective use of learning resources such as videos, digital 
texts recommended by the teacher or by peers, forums and other 
online events, social media, assessment opportunities, or local 
meet-ups. They choose which parts of the course they are 
interested in and ignore the rest. Other researchers have developed 
related ideas about what distinguishes the process of learning in 
MOOCs [9-13, 26, 27].  

A thread common to many of these studies is the idea that the skill 
of the individual MOOC learner is paramount. Some have sought 
to define a class of complex, learning-related ‘21st century 
learning skills’. Stewart defined ‘digital media literacies’ [29] a 
multi-dimensional, diverse set of actions, skills and practices 

151



including print and visual literacy, information literacy, critical 
thinking and ability to use hypertext as well as mastery of 
complex etiquette. These are not social media literacies. Learning 
is not socialising. Nor is learning merely a technical skill. 
Learning involves participatory and knowledge-building skills. 
Kop [30] draws on work by the (American) National Councils of 
Teachers of English to set out a framework of ‘meta-literacy’ 
required by MOOC participants.  

These analyses suggest that forums are quite different from 
learning environments in which widely understood learning 
behaviours are deployed by experienced learners to generate 
higher learning. They suggest that a complex and novel 
combination of skills is needed if participants are to generate 
learning in and through MOOC forums  - an emergent set of ‘21st 
century skills’ developed in and demanded by an emergent 21st 
century environment. 

3. CORE STUDY PROPOSITIONS 
Several propositions drawn from the discussion above provide the 
conceptual foundation for this study: 
• there is a complex, latent ‘21st century’ skill required by MOOC 

participants to generate learning in MOOC forums, entitled in 
this study the Crowd-Sourced Learning Skill (C-SLS);  

• individuals possess this C-SLS to differing degrees and these 
differences explain in part differences in learning outcomes; 

• forum activities such as posting, voting, and viewing do not in 
and of themselves generate learning, but skilled learners are 
adept at using them in particular ways to generate learning;  

• measurement theory and its associated methodologies make 
possible a mapping of patterns of forum activity onto a learning 
progression describing the hypothesised latent skill, and this 
mapping can be used to infer individuals’ level of skill. 

These propositions, if capable of verification, can provide a 
valuable insight into how learning occurs. The goal is to 
understand whether and how otherwise confusing patterns of 
posting, viewing and voting might be construed as evidence as to 
the possession (or otherwise) of a deeper latent learning skill, and 
not just explanatory variables in their own right. This would make  
educational and statistical sense of complex forum behaviour 
which has to date eluded convincing analysis. It might point to 
another way of using learning analytics can be used to improve 
learning in MOOCs, and to assist developers to optimise the 
power of MOOCs.  

METHOD  
The methodology of this study derives from the measurement 
science used to develop robust, scalable assessments of complex 
skills in high-stakes testing, with an adaptation of those 
approaches for use with log file data in digital environments [31-
33]. Measurement science provides a set of techniques, mindsets 
and tools used to build trusted measures – measures for which 
meaning, relevance and utility have been tested by experimental, 
statistical and philosophical means. Central to the approach is a 
requirement  to answer the ‘validity question’ which in the case of 
this study is: to what degree, if at all, is there evidence to support 
trust in the utility, value and meaning of a construct – C-SLS- that 
can used to understand forum behavior?  

Four initial steps in this process are outlined below, each a part of 
a larger process aimed at building and validating the C-SLS 
construct. 

3.1 Step 1 Defining the C-SLS 
First, if the C-SLS exists it must be capable of being represented 
as a theoretical progression which describes the behaviour of 
participants as they move from the level of novice to expert. This 
approach has been widely used by education researchers to build 
‘developmental progressions’ that express in behavioural terms 
the acquisition of skills in areas as diverse as reading, writing, and 
mathematics, the capacity to be a good principal, or to teach well 
[34] [35].  

For this study, a hierarchical progression [37] was developed from 
a detailed review of the literature. The progression was 
structurally modeled on the Dreyfus Taxonomy of Skills 
Acquisition [39]. The progression describes the C-SLS as a 
complex set of knowledge, skills, attitudes and values of learners 
that underpin their expertise in learning. It charts how the latent 
skill develops in the novice learner to take them to beginner, to 
proficient, to competent, and thence to expert learner. Five stages 
of development in the C-SLS in a forum are outlined (Table 1), 
each detailing the knowledge, skills, attitudes and values 
characteristic of a learner at that level which they bring to bear to 
generate learning for themselves and others. 

One part of the appeal of the Dreyfus approach is that it sees skill 
development in any domain as a difficult, challenging and 
complex process. Skilled performance requires the smooth, 
practised integration of cognitive, physical, emotional, attitudinal, 
cultural and stylistic elements to achieve high levels of 
performance, with know-how becoming increasingly unconscious 
or intuitive. To perform expertly at learning requires integration of 
many diverse elements; practice is required; and it takes time and 
effort to improve performance. The Dreyfus taxonomy therefore 
provides an appropriate scaffold for the development of the 
capability of learning through MOOC forums. 

Broadly this C-SLS progression embodies a developmental view 
of learning, reflected also in the interpretive names given to each 
level in the progression (learning leader, collaborative learner, 
collegial learner, independent learner and dependent learner). The 
progression emphasises that actions like posting and viewing are 
not of themselves predictive of learning, but are merely the 
observable elements of a complex combination of attitudes, 
values, skills that comprise a latent learning skill.  

3.2 Step 2 Coding & Scoring Log File Data 
Once the hypothesised progression for the C-SLS was described, 
the second step mapped the theorised connection between the 
observable activity of participants and the levels of the 
hypothesised C-SLS. The ‘raw’ data used for this purpose 
included log stream data from each view, post, comment or vote 
by each of the 8280 forum viewers in the Introduction to 
Macroeconomics MOOC (MacroMOOC) offered by the 
University of Melbourne (UM) in 2013 on the Coursera platform. 
Over 1300 participants voted, 1534 posted or commented and 470 
created threads. Patterns of participation were logged in each of 
the nine active weeks of the course, in each of the 942 threads, in 
18 forums. The content of threads, posts and comments were not 
analysed for this part of the study.  

Data elements were then selected for mapping behavior to the 
progression. This is an interpretive exercise. Judgments have to be 
made about which behaviours are likely to provide evidence of 
learning skill, and which are not. For example, will the best 
learners always be the most prolific viewers and posters, or the 
fastest in response, or attract the highest number of peer votes? 

152



Previous research suggests that it is not necessarily the case. But 
they have been found to be more persistent, and write lengthier 
posts [15] [16], and posting behavior depends on both individual 
characteristics of the poster and the relationship between posts. 
Selection of data therefore needs to reflect judgments about 
learning behaviour in forums, rather than just loading in all data. 

After selection, data were manipulated and coded, creating 71 
simple thresholds. Examples of thresholds are: ‘posted at least 
once’, ‘contributed a post attracting votes from others’, 
‘contributed multiple posts in two or more weeks’, ‘made repeated 
thread visits in more than half the weeks’, ‘contributed posts to 
two or more thread in two or more weeks’. 

 
Table 1: Progression for the hypothesised latent skill ‘learning through a MOOC forum’ 

Level 1:Novice, dependent learner 
Learning is about consuming stable knowledge in a domain, comprised mainly of cognitive understanding or skill; seeks efficient transfer from 
authoritative or reputable sources; follows procedural guidance from teachers, relinquishing responsibility for learning process; calibrates 
performance on formal assessments and accepts standards inherent in them Forums are essentially social adjuncts to courses; the information is 
possibly unreliable and misleading. Never visits forums 
Level 2:Beginning, independent learner 
Learning is about consuming generalizable, stable knowledge, from authoritative sources but requiring adaptation to local context; understands how 
s/he absorbs knowledge; sees learning as a series of procedures which will generate required output; success arises from individual effort, with 
expert guidance or coaching from time to time; completes nominated tasks without guidance; selective about the forms of learning engaged in, 
seeking the most efficient; seeks feedback from trusted, formal sources; calibrates performance against grades performance or recognition by 
experts; Forums are interesting but unreliable and inefficient sources of information, daunting in their ambiguity and uncertainty. Canvasses forums 
for reliable content in the early part of the course; rarely return to a particular thread but might sample forums, from time to time, out of curiosity; 
usually impatient with them; unlikely to vote, or post. 
Level 3: Proficient, collegial learner 
Knowledge, to be valuable, needs to be interpreted or applied differently in different contexts; it changes; expertise in a field develops over time 
with practice and experience; knowledge comes from a variety of sources; and people with different experience and background provide different 
inputs; understands how s/he learn; monitors and regulates own learning; is able to plan work making considered use of various resources available; 
is considered and careful about feedback that is trusted; canvassing but not using necessarily using feedback from a range of formal and informal 
sources; values a sense of having learned; is able to separate learning from the activity that produces it; calibrates performance against formal and 
informal measures; Forums are a rich but daunting source of input to their learning; often overwhelming, confusing, and chaotic; tentative and wary 
about posting, concentrating on questions or seeking technical guidance, or posting general information about relevant background; reluctant to 
open self to judgment of others; gets little rewarding response; tends to scan but not focus; responds to peer activities in limited or self-protective or 
self-promoting manner. contributions might appear to be independent of a discourse 
Level 4: Competent, collaborative learner 
Knowledge is seen as: ever changing; including attitudes, values, conventions and practical wisdom and cognitive skill; usually context dependent, 
sometimes universal. believes that expertise develops over time, with practice, experience and effort; the process of learning involves gradual 
construction of knowledge, through formal learning, experience, and through questioning, seeking feedback, exploring misunderstandings and gaps 
in own knowledge, and sharing with teachers and peers; Learning derives from both giving and receiving feedback, and sharing; there is value in 
diversity of expertise and experience, but tends to focus attention on own or familiar contexts; eclectic in using feedback sources, with growing 
confidence in own judgment about standards; rewarded by the developing a sense of mastery of the domain. Understands that participants in forums 
can support knowledge building , and how they tap it; joins in to productive peer dialogues in selected learning areas using posts, votes, comments, 
searching; asks questions, expresses opinions, shares own experiences, resources, ideas, understandings, assumptions, and values about the domain 
of learning; starts to generate meaningful and rewarding response from others; is adept at interpreting the value of other’s inputs to own learning; 
tests own understandings and values against the standards of the group, develops intuitive judgments of standards of performance of self and peers; 
collaborates in learning with a small group of like- minded peers, helping to work through counter-intuitive, incoherent or chaotic input, helping 
distinguish input that is useful from input that is not 
Level 5: Expert, learning leader 
As above, but also values and seeks out new ideas, approaches and opinions; willing to press the boundaries and challenge misconceptions or 
received knowledge; innovative in recognising and supporting discussion of new understandings, concepts, values; is open to learning about 
contexts different to own, and values diversity in expertise and experience as a sources of learning, even that different for own; learns through 
questioning, advising and supporting others’ learning Understands how forums work and intuitively generates and shapes dialogues by creating 
threads, voting and posting, taking responsibility for regulating, polite, efficient operation of the forums; likely to be active in a number of 
dialogues simultaneously; systematically scans wide array of forums; may initiate activities to extend the learning beyond the forums; recalibrates 
and reshapes own and other’s views of appropriate standard of attainment in the domain; is able to recognise and interpret standards in contexts 
different to own; is recognised by peers, input is widely notices and appreciated 
 
Scoring was straightforward. If a participant ‘crossed’ a threshold, 
their score for the threshold was coded as 1. For instance, the 
simplest threshold was ‘viewed a forum at least once’ and over 
23,000 participants scored ‘0’ on this threshold because they 
failed to view any forum. More than 8000 participants scored ‘1’. 
A more difficult threshold was ‘created a thread’, on which only 
470 participants scored ‘1’. Thresholds were thus designed to be 
arranged from ‘very difficult’ to ‘very easy’. Each person was 
scored on each of the 71 thresholds and their total score calculated 
from a count of the number of threshold they crossed. 

3.3 Step 3: Calibration  
The third step involved using a measurement model to calibrate 
the thresholds (items), and the scale as a whole. If data fit a strong 
measurement model, there is support for the idea that the 
progression can generate reliable measures of individual ability 
and item difficulty.  

The measurement model selected for this study is a particular case 
of item response theory, the one-parameter simple logistic Rasch 
model as developed by Andrich [40], Masters and Wright [42, 43] 
and as expressed in the program Conquest [44]. This 

153



parsimonious, highly restrictive model was selected because the 
purpose is to test the fit of the data to the model. That is, the 
objective is to test whether the data are suitable for inferring 
positions on the progression, to standards of scientific 
measurement. The model will fit only if an underlying progression 
shows development from less to more; if threshold difficulty and 
an individual’s ability can be estimated using one parameter 
which maps their position onto this progression; if the order of 
thresholds arranged on difficulty remains invariant over the range 
of the scale and for each individual assessed; and if the order of 
individuals arranged on ability remains invariant over the range of 
the scale and for each threshold. If these conditions apply, it can 
be assumed that the scale provides an estimate of individual 
ability and item difficulty on a common scale, mapped to the 
progression, using a unit, called a ‘logit’, which is standardised to 
a mean of zero, and a standard deviation of one. 

A range of evidence suggests that fit of the coded data to the 
model was high. The item separation, EAP/PV and person 
separation reliabilities were 0.99, 0.89 and 0.82 respectively, all of 
which are in the acceptable range. Item fit was tested by 
examining discrepancy between threshold difficulty values 
estimated by the model and those observed, returning a desired 
weighted mean squared residual (Infit MSQ) of 1. In this study, an 
Infit MSQ between 0.7 and 1.3 was considered acceptable [45]. 
Most irregularities in item fit were eliminated by revising or 
rejecting items. Some retained items showed marginal misfit. Of 
the 39 items, 7 had MSQs out of the range .7 to 1.3, with values of 
1.59, 1.34, 1.37, 1.36, 1.41, 1.33, and 0.61. Mis-fitting items were 
retained if they were considered to add explanatory power to the 
progression, thus protecting progression validity, at the expense of 
some reliability. Inspection of 5 items with disordered item 
thresholds but with otherwise good fit suggested that they were 
working satisfactorily, the disorder arising from unequal numbers 
in the categories.  Item-total correlations were all positive, a good 
indicator that the items each contribute. The distribution of item 
difficulties indicated that, undesirably, the ability profile of 
participants was more heavily weighted to the easy end of the 
scale than to the harder end. The mean item difficulty was 0 
logits, variance 1.38; and the mean of ability was -3.58 logits, 
variance 4.29. This is mainly an artifact of the empirical definition 
of the first category of expertise, into which most MOOC 
participants fell, namely representing ‘no activity’. It does not 
represent unacceptable loss of precision. 

The latent skill score of participants correlates only mildly with 
the number of threads they viewed, the number of votes and 
number of posts they made: 0.3, 0.2 and 0.2 respectively. This 
suggests that the approach to measurement has, as intended, 
transformed the data in accordance with the theory. This 
transformation in turn delivers assessments (estimates) of 
participant skill that conform to a strong measurement model, as 
proposed.  

3.4 Step 4: Building a Validity Argument 
The high level of fit of the data to the measurement model 
provides provisional support for the existence of the theorised C-
SLS progression, and for the utility of using the data for reliable 
assessment of individual ability on this skill. Participant behavior 
maps simply onto five levels of the C-SLS progression. Expert 
learners, for instance, are empirically confirmed as being 
persistent forum visitors in all or even most weeks of the course. 
They scan a range of threads, do not post in all even most threads, 
but are thread creators on popular topics. They tend to return to a 
small number of threads, in which they post and vote more 

heavily and in which they are recognised by their peers. This 
pattern makes intuitive sense, and accords with expectation, 
providing further support for the propositions and the 
hypothesised 5-level structure of the C-SLS. 

The next step in the study is to continue investigations of the 
validity of the measure through compilation of further evidence as 
to the utility, meaning and relevance of the measure. For instance 
evidence will be collect to establish if the findings from one 
MOOC can be replicated in another. Investigations will be 
established to see if it is possible to improve the skill level of 
participants through good design of a MOOC. 

The extraordinary skew in the distribution of the latent skill for 
MacroMOOC participants will be examined. Only 171 of the 
participants attained Level 5 on the progression, 336 attained level 
4, 3246 attained level 3, 4540 attained level 2. Is forum 
participation low because all but a few participants don’t believe 
that forums can generate learning? Or, do they not know how to 
operate them? Or do they not wish to learn from the MOOC 
experience? What other interpretations are open? 

The apparently strong relationship between achievement levels in 
the course and score on the C-SLS will be tested. For instance at 
the expert level 5, 78% received at least a pass score and 67% a 
distinction score. These percentages steadily reduce as the level of 
C-SLS reduces, until at Level 1, less than 1 per cent received a 
pass score. This lends weight to the theorised view that C-SLS 
reflects capacity of participants to generate learning through the 
forums. But what alternate interpretations need investigation? 

The possibility of differential scale functioning with different 
demographic groups will be examined. Data collected via a post-
course survey, competed by 924 participants shows no 
relationship between C-SLS scores and a range of demographic 
characteristics including gender, age and stated entry-level of 
expertise in the domain of economics. This is interesting because 
it does not accord with at least one other study [15] which showed 
age and gender effects in forum usage patterns which were also 
related to differences in outcome. This requires further 
investigation. So too does the finding that learners with previous 
experience of university education do not show enhanced levels of 
C-SLS, providing provisional support for the idea the C-SLS 
might be a ‘21st century skill’ distinguishable from those 
developed in traditional higher education. 

These and other validity matters are being investigated to test if 
the measure can be used provide trustworthy inferences about an 
individual’s level of C-SLS, and what that might mean. 

4. CONCLUSION 
This  study provides provisional support for the four propositions 
that framed it: first, that there exists a latent progression, entitled 
C-SLS, describing a skill required to crowd-source learning in a 
MOOC, which is a composite of required knowledge, skills, 
abilities and attitudes about learning; second, that individuals have 
different amounts of this skill, which in part explains difference in 
learning outcomes; third, that descriptions of observable 
behaviours in forums (such as posting, voting, viewing) do not in 
and of themselves provide insights about how learning is 
generated in MOOC forums, but can be transformed to infer the 
level of learner skill required to improve learning; and fourth, that 
it is possible to use measurement methodologies to map forum 
activity as captured in log files onto the latent skill progression, 
and to use this mapping to infer the skill level of individuals.  

154



A major purpose of this paper has been to illustrate the distinctive 
way in which modern measurement theory can be used to better 
understand learning in MOOCs, and the mutually reinforcing 
relationship between the goals of measurement theory and 
learning analytics. It provides an example of how a theorised 
learning progression can be used to chart progress learners make 
as they develop from novice to expert in use of forums to generate 
learning. It has illustrated a measurement methodology for 
organising large quantities of digital data, at scale, for use as 
evidence to infer individuals’ positions on any progression. It 
holds open the prospect that learning analytics allied with 
measurement theory can provide each learner with a learning-
map, which accurately locates them within a progression of 
learning, showing not only what they have achieved but also what 
they can do next to learn more, thus providing what a substantial 
body of research has found to be a powerful learning generator – 
accurate, timely, helpful feedback.  

5. ACKNOWLEDGEMENTS 
This project has been conducted under the auspices of the 
Assessment Research Centre, the Science of Learning Research 
Centre (project number SR120300015), and the Learning 
Analytics Group at the University of Melbourne.  

6. REFERENCES 
[1] Gillani, N. Learner communications in massively open online 

courses. University of Oxford, Oxford, 2013. 
[3] Gillani, n., Yasseri, T., Eynon, R. and Hjorth, I. Structural 

limitations of learning in a crowd: communication 
vulnerability and information diffusion in MOOCs. Scientifc 
reports, nature.com, 4, 6447, (2014). 

[5] Creelman, A. The silent majority-why are MOOC forums so 
counterproductive. Retrieved from 
http://acreelman.blogspot.co.au/2013/09/the-silent-majority-
why-are-mooc-forums.html, 2013. 

 [7] Milligan, C., Littlejohn, A. and Margaryan, A. Patterns of 
Engagement in Connectivist MOOCs. Journal of Online 
Learning & Teaching, 9, 2 2013, 149-159. 

[9] Fournier, H., Kop, R. and Sitlia, H. The Value of Learning 
Analytics to Networked Learning on a Personal Learning 
Environment. ACM, in Proceedings of the 1st International 
Conference on Learning Analytics and Knowledge, Banff, 
Canada, 2011. 

 [11] Waite, M., Mackness, J., Roberts, G. and Lovegrove, E. 
Liminal Participants & Skilled Orienteers: Learner 
Participation in a MOOC for New Lecturers. MERLOT 
Journal of Online Learning and Teaching, 9, 2 2013. 

[13] Ahn, J., Butler, B. S., Alam, A. and Webster, S. A. Learner 
Participation and Engagement in Open Online Courses: 
Insights from the Peer 2 Peer University. Journal of Online 
Learning & Teaching, 9, 2 2013), 160-171. 

 [16] Chiu, M. M. and Fujita, N. Statistical Discourse Analysis of 
Online Discussions: Informal Cognition, Social 
Metacognition and Knowledge Creation. In Proceedings of 
the Learning Analytics and Knowledge Conference, ACM, 
Indianapolis, USA, 2014.  

[17] Brinton, C., Chiang, M., Jain, S., Lam, H., LiuZ and F, W. 
Learning about Social learning in MOOCs: from statistical 

analysis to generative model. Learning Technologies, IEEE 
Transactions on Learning Technologies, 99 2014. 

[19] Wen, M., Yang, D. and Rose, C. Sentiment Analysis in a 
MOOC Discussion Forums: What Does it tell us? Carnegie 
Mellon University, 2014. 

[20] Kop, R., Fournier, H. and Mak, J. S. F. A Pedagogy of 
Abundance or a Pedagogy to Support Human Beings? 
Participant Support on Massive Open Online Courses. 
International Review of Research in Open and Distance 
Learning, 12, 7 2011, 74-93. 

[21] Shea, P., Hayes, S., Smith, S., Vickers, J., Bidjerano, T., 
Gozza_Cohen, M., Jian, S. B., Pickett, A., Wilde, J., Tseng 
and Chi-Hua Online Learner Self-Regulation: Learning 
Presence Viewed through Quantitative Content-and Social 
Network Analysis. Athabasca University, 2013. 

[26] Siemens, G. Connectivism: A Learning Theory for the Digital 
Age. e-learn space, 2004. 

[27] Stewart, B. Massiveness + Openness = new literacies of 
Participation? MERLOT Journal of Online Learning and 
Teaching, 9, 2 2013, 10. 

[29] Stewart, B. Social Media Literacies and Perceptions of Value 
in Open Online Courses. Retrieved from 
http://portfolio.cribchronicles.com/social-media-literacies-
and-perceptions-of-value-in-open-online-courses/, 2010. 

[30] Kop, R. The Challenges to Connectivist Learning on Open 
Online Networks: Learning Experiences during a Massive 
Open Online Course. International Review of Research in 
Open and Distance Learning, 12, 3 (03/01/ 2011), 19-38. 

 [32] Griffin, P. and Care, E. Assessment and Teaching of 21st 
Century Skills: Methods and Approaches. Springer, 2015. 

[33] Wilson, M. On Choosing a Model for Assessemnt. Methods 
of Psychological Research Online, 8, 3 2003, 1-22. 

[ [35] Hambleton, R. K. Item Response Theory: a Broad 
Psychometric Framework for Measurement Advances. 
Psicothema, 6, 3 1994, 535-556. 

[37] Milligan, S. Learning Skills for the Digital Era. In 
Proceedings of the Science of Learning Centre Big Day Out 
(Adelaide, Australia, 2014). Assessment Research Centre, 
University of Melbourne, Adelaide, 2014. . 

[39] Dreyfus, S. E. & Dreyfus, H. L. A Five stage Model of the 
Mental Activites Involved in Directed Skill Acquisition. 
Operations Research Centre, University of California, 1980. 

[40] Anderson, E. B. Sufficient statistics and latent trait models. 
Psychometrika, 42 1977 , 69-81. 

[41] Andrich, D. A Rating Formulation for Ordered Response 
Categories. Psychometrica, 431978), 561-573. 

[42] Masters, G. N. A Rasch Model for Partial Credit Scoring. 
Psychometrica, 47 1982, 149-174. 

[43] Andrich, D. Rasch Model for Measurement. Sage 
Publicaions, Beverly Hills and London, 1988. 

[44] Wu, M., Adams, R. and Wilson, M. ACER conquest: 
generalised item response modelling software. ACER Press, 
Melbourne, 1998. 

[45] Wright, B. D. and Linacre, J. M. Reasonable mean-square fit 
values. Retrieved December 2013, from 
http://www.rasch.org/rmt/rmt83b.htm, 1994 

 

155





