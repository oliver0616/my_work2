
Addressing learner issues with StepUp!: an Evaluation  
Jose Luis Santos, Katrien Verbert, Sten Govaerts, Erik Duval 

 Dept. of Computer Science, KU Leuven,  
Celestijnenlaan 200A 

B-3001 Leuven, Belgium 
{JoseLuis.Santos,Katrien.Verbert, Sten.Govaerts, Erik.Duval}@cs.kuleuven.be

  
ABSTRACT 
This paper reports on our research on the use of learning analytics 
dashboards to support awareness, self-reflection, sensemaking and 
impact for learners. So far, little research has been done to 
evaluate such dashboards with students and to assess their impact 
on learning. In this paper, we present the results of an evaluation 
study of our dashboard, called StepUp!, and the extent to which it 
addresses issues and needs of our students. Through 
brainstorming sessions with our students, we identified and 
prioritized learning issues and needs. In a second step, we 
deployed StepUp! during one month and we evaluated to which 
extent our dashboard addresses the issues and needs identified 
earlier in different courses. The results show that our tool has 
potentially higher impact for students working in groups and 
sharing a topic than students working individually on different 
topics.  

Categories and Subject Descriptors 
H.5.2 [Information interfaces and presentation]: User interfaces; 
K.3.2 [Computers and Education]: Computer Science Education 

General Terms 
Design, Experimentation, Human Factors. 

Keywords 
Learning analytics, Visualization, Reflection, Evaluation, Design 
based research. 

1. INTRODUCTION 
We consider the essence of learning analytics to be the collection 
of traces that learners leave behind and the use of those traces to 
improve learning [11]. Educational Data Mining can process the 
traces algorithmically and point out patterns or compute indicators 
[40][37]. Our interest is more in visualizing traces in order to help 
learners and teachers to reflect on their activity. We focus on 
building dashboards that visualize the traces in ways that help 
learners or teachers to steer the learning process [12]. 

We focus on deploying real tools in real courses and finding out 
how these tools address the learner needs. Students can have 
different intrinsic and extrinsic motivations to follow a course and 
this will affect the use of the tools. The perception on how the 
tools address student needs and the analysis of tool use can help 
us understand how we can use these tools to improve learning. 

Our courses, in which we apply learning analytics visualizations, 
follow an ‘open learning’ approach where engineering students 
work individually or in groups of three or four on realistic project 
assignments in an open way. Students use twitter (with course 
hash tags), wikis, blogs and other web 2.0 tools such as Toggl1 
and TiNYARM[6] to report and communicate about their work 
with each other and the outside world in a ’community of 
practice’ kind of way[15][39]. 

In earlier work, we presented StepUp![31], a tool that empowers 
students to reflect on their own activity, and that of their peers, in 
open learning environments. In our courses, we encourage 
students to be responsible of their own learning activities, much in 
the same way as we expect them to be responsible of their 
professional activities later on. To support this process, StepUp! 
visualizes different learning traces, such as: time spent on the 
course, resource use (e.g. wiki and blog use) and social media use 
(e.g. Twitter) (see Section 3.2). Our earlier work focused on the 
evaluation of usability and usefulness of StepUp!. In these 
evaluations, we asked students to rate the usefulness of StepUp! to 
support awareness and reflection and to assess its usability using a 
standard SUS [4] questionnaire. This research showed: a) 
visualization of time spent on activities related to the course is a 
powerful trace to understand peer behavior, and b) StepUp! 
provides transparency about how other learners communicate. 
Although learners indicate that they should increase or decrease 
their activity, most of them did not change their behavior. 

Whereas these evaluation studies provided some insight in 
potential usefulness and usability issues of StepUp!, we can derive 
little evidence from these studies about the impact of StepUp! on 
learning. This paper therefore focuses on precisely that topic: the 
potential impact with respect to issues and needs that our students 
have. To this end, we set up three brainstorming sessions at the 
start of the semester with a total of fifty-six students in multiple 
courses. In these sessions, students discussed their learning issues 
and needs. They prioritized the issues and needs derived from the 
brainstorming sessions by rating them. In a second step, we 
deployed the tool during one month. Afterwards, we evaluated to 
which extent StepUp! addresses the issues identified by the 
students at the beginning of the semester.  

The remainder of this text is structured as follows: the next 
section presents how we performed the brainstorming sessions 
and the result of these sessions. How StepUp! addresses the issues 
identified in the brainstorming sessions is presented in Section 3. 
Section 4 presents the evaluation results. Related work is 
discussed in section 5. Conclusions and future work are presented 
in Section 6. 

                                                                 
1 https://www.toggl.com/ 

 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
LAK '13, April 08 - 12 2013, Leuven, Belgium 
Copyright 2013 ACM 978-1-4503-1785-6/13/04…$15.00. 
 

14



2. LEARNING ISSUES 
To identify issues that students face, we carried out three 
brainstorming sessions with fifty-six participants in different 
courses during the first session of the academic year. We present 
details of the courses and results of the brainstorming sessions in 
this section. These courses were selected from the engineering 
study program and they share the same ‘open learning’ approach 
methodology. This allows us to relate the results to other factors - 
such as working individually or in groups and the topic of the 
course, rather than to different course contexts. 

2.1 Multimedia course 
Multimedia (abbreviation MUME) is a course that focuses on the 
design and implementation of mobile applications. In this course, 
students develop an application in HTML5, Android OS and iOS. 
The topic this year is mood tracking in the context of Quantified 
Self2. Twenty master students are enrolled in this course. They 
work in groups of two or three.  

Before introducing the topic of the course to the students, we set 
up a brainstorming session in which we asked the students to 
identify issues they experience in their studies. The result of the 
session was a set of thirteen problems.  

In a second step, each group of students rated the problems they 
had, by assigning a budget of 10 points across the problems. The 
list of issues and their relative importance is presented in the first 
block of Table 1.  

The following issues were rated highest: 1) detection of group 
members who do not work, 2) communication within the group, 
3) how I distribute my time, 4) how to take decisions in the group 
and 5) group composition. The remaining problems got 6 points 
or less whereas the ones mentioned above got between 7 and 15. 

2.2 Problem solving and design course 
In the problem solving and design course (Dutch abbreviation 
PENO), students deal with the different phases of software 
engineering, from brainstorming, creation of scenarios and use 
cases to programming and evaluation of an application. The topic 
of this year is learning analytics. The problem solving and design 
course is part of the second year bachelor’s program. Two groups 
of eighteen students are subdivided in teams of six. 

Before introducing the topic of the course to the students, we 
again set up a brainstorming session per group. After ten minutes, 
students presented the problems they had identified. Compared 
with the outcome of the brainstorming session in the multimedia 
course, nine new issues came up in the session with the first group 
of students in this course. In the second group, eight additional 
issues were identified. These issues are presented in the second 
block and the third block of Table 1, respectively. In addition, we 
included four issues that we identified in earlier research[18] 
(block 4 in Table 1). 

The relative importance of each entry in the combined list of all 
issues was rated by the students in a second step. Results are 
presented in the PENO column of Table 1. 

2.3 Master thesis students 
We conducted a third case study with students working on their 
master thesis in our group, which focuses on human-computer 
interaction. The students typically do a literature study, design and  

                                                                 
2 http://quantifiedself.com 

Table 1: learning issues and needs (*final selected issues, 
+selected in the use case, -non selected) 

Issues MUME PENO THESIS 

Usefulness of the assignment - + + 

* Group member that does not work  
(I1) 

+ - + 

* Communication within the group (I2) + + + 

* How I distribute my time (I3) + + + 

Agreements within the group + - + 

Tracking (progress/effort) - - - 

Size of the group - - + 

Unclear assignments - + + 

Taking decisions - - + 

Difference in knowledge between team 
members 

- - - 

Group composition + - + 

* Alert if something goes wrong (I5) - - + 

Too many projects/exams - - + 

Sharing (summaries) among students  - + 

* Motivation (I4)  + + 

Too much, useless info (thick books)  - + 

Concentration (learning), distractions 
(facebook) 

 - + 

Health and personal life quality   - + 

Time lost due to transport, shopping, 
sports,… 

 - - 

Problems operating Toledo (schedule)  - - 

No pressure during the academic year  - - 

Packed days compared with other years  - - 

Lack of an effective one stop shop for 
info (cf. Blackboard) 

 + - 

Lack of a good learning environment 
(light, air, sound,…) 

 + + 

Availability of learning material when it 
is needed (e.g. for commuting students, 
or in the week-end) 

 + + 

Just in time mentoring and help  - - 

Not knowing what is important  + + 

Not knowing how much time is 
required to study a course 

 - - 

Fear of failing  - - 

* Lack of balance between social 
activities and studying (I6) 

 + + 

Understanding how others members of 
my group spend their efforts 

 - + 

Understanding how others peers in the 
course spend their efforts 

 - - 

Transparency within the community of 
practice. How the members of the 
community interact with each other. 

 - - 

* To be aware which resource and tools 
I and others students use (I7) 

 - + 

15



evaluate scenarios, use cases, paper and digital prototypes and 
release and evaluate a working version of their software. Our 
group counts thirteen thesis students who mostly work 
individually on their thesis topics. 

With this group, we did not perform a brainstorming session, but 
we asked them to rate the issues identified by the other students. 
Rating results are presented in the last column of Table 1. 

2.4 Results 
The result of the sessions is a list of thirty-four issues, prioritized 
by students in three different courses.  

After students rated the issues, we selected the issues that were 
rated high in more than one case study and that could potentially 
be addressed by StepUp! For instance, motivation is an issue that 
we think StepUp! can address, but unclear assignments and lack 

of a good learning environment (light, air, sound) are not. 
Selected issues and needs are preceded by a star in Table 1.  

In Section 5, we present results of an evaluation with students that 
assesses to which extent StepUp! addresses these issues. 

3. HOW DO WE ADDRESS THE ISSUES 
In this section, we describe which traces of learner activities we 
track and how such traces can help us to address the learning 
issues presented in Section 2. Then, we present our tool and how 
these traces are visualized. Finally, we present how we think that 
our tool addresses the learning issues listed in table1.  

3.1 Tracked data 
One of the main challenges with learning analytics is to collect 
data that reflect relevant learner and teacher activities [12]. 

Figure 1 StepUp! interface 

16



Some activities are tracked automatically: this is obviously a more 
secure and scalable way to collect traces of learning activities. 
Much of our work in this area is inspired by “quantified self'” 
applications3, where users often carry sensors, either as apps on 
mobile devices, or as specific devices, such as for instance Fitbit4 
or Nike Fuel5. 
We rely on software trackers that collect relevant traces from ‘the 
Web’: learners post reports on group blogs, comment on the blogs 
of other groups and tweet about activities with a course hash tag. 
Those activities are all tracked automatically: we basically process 
RSS feeds of the blogs and the blog comments every hour and 
collect the relevant information (the identity of the person who 
posted the blog post or comment and the timestamp) into a 
database with activity traces. Similarly, we use the twitter 
Application Programming Interface (API) to retrieve the identity 
and timestamp of every tweet with the hash tag of the course. 
Moreover, we track learner activities that may or may not produce 
a digital outcome with a tool called Toggl: this is basically a time 
tracking application that can be configured with a specific set of 
activities. Those activities can be classified based on an existing 
taxonomy [8] in assimilative (blogging and writing reports), 
communicative (twitter and comments) and productive activities 
(programming). We expect that this clear division between 
different kinds of activities will help our students to gain insight 
in how they and other students spend their time.  
When students use Toggl, they can do so in semi-automatic mode 
or manually. Semi-automatic mode means that, when they start an 
activity, they can select it and click on a start button. When they 
finish the activity, they click on a stop button. Manually means 
that the students specify activity, time, and duration to Toggl. In 
this way, students can add activities that they forgot to report or 
edit them manually. Of course, on the one hand, this kind of 
tracking is tedious and error prone. On the other hand, requiring 
students to log time may make them more aware of their time 
investment and may trigger more conscious decisions about what 
to focus on or how much time to spend on a specific activity. 
Moreover, we are also tracking other traces such as software 
development and participation in the wiki and it is part of our 
future work to include such traces in StepUp!  

3.2 Visualizing the data with StepUp! 
Figure 1 illustrates how the data are made available in their 
complete detail in our StepUp! tool: this is a “Big Table” 
overview where each row corresponds with a student. The 
students are clustered in the groups that they belong to. For 
instance: rows 1-3 contain the details of the students ‘stijnadams’, 
‘robindecroon’ and ‘nielsbillen’ (see marker 1 at Figure 1). These 
three students work together in a group called ‘findinge’, the 
second column in the table (marker 2). The green cells in that 
second column indicate that these students made 4, 4 and 3 posts 
in their group blog respectively (marker 3). Rows 4-6 contain the 
details of the second group, called ‘followap’: they made 0, 1 and 
1 comments on the blog of the first group (column 2) and 6, 3 and 
6 posts in their own blog (column 3) respectively (marker 4). The 
rightmost columns (marker 5) in the table indicate the total 
number of comments, the total number of hours spent on the 
course (Toggl) and the total number of tweets. 

                                                                 
3 http://quantifiedself.com/ 
4 http://www.fitbit.com/ 
5 http://nikeplus.nike.com/plus/products/fuelband 

The two rightmost columns are sparklines [20] that provide a 
quick glance of the overall evolution of the activity for a 
particular student (marker 6). They can be activated to reveal 
more details of student activity (marker 7 and 8 at the bottom of 
Figure 1). These bar charts show the distribution of the activity 
along the weeks. The bar chart (marker 7) visualizes the time 
spent on the different kinds of activities, such as reading 
(documentation, other blogs, etc.), programming and face-to-face 
meetings. In this way, the students get an overview of how they 
have spent their time in the past week. The other bar chart (marker 
8) shows the distribution of the participation (posts, tweets and 
comments) along the weeks. This visualization intends to trigger 
reflection about what the students did and why.   

4. EVALUATION 
We carried out a detailed evaluation six weeks into the course, 
based on online surveys. In the evaluation, we used four 
instruments, in order to obtain a broad view: 

1) The importance of the most important learning issues 
(see Table 1) was rated again by students. 

2) We asked whether students over-report or under-report 
time spent, enquired about their motivation and whether 
they thought they were doing well in the course. 

3) Fifteen questions assessed to which extent students 
believed that StepUp! addressed the issues .  

4) a SUS questionnaire  assessed usability of the 
application. 

All the questions presented in this survey use a 5-likert scale to 
grade importance or agreement (‘1 – not important at all’ to ‘5 – 
very important’ and ‘1 – strongly disagree’ to ‘5 – strongly 
agree’).  
The survey was completed by all the students of the three courses 
(see Section 2). The survey results are discussed in the remainder 
of this section.  

4.1 Analysis of the results  
First, students rated the importance of the learning issues that 
were selected with the methodology described in Section 2. This 
was done to get a full picture, since the set of issues grew during 
the collection phase and not all students had rated all issues. The 
results of these questions can be seen in the grey highlighted rows 
of Figure 2. Afterwards, students rated statements to assess their 
perception on how they were doing, their motivation and how 
others were behaving in the course and finally, to assess whether 
StepUp! addressed the selected learning issues. These statements 
and their results are presented in the white rows of Figure 2. 
In this section we discuss the results per evaluation section for the 
three case studies (MUME, THESIS & PENO, see Section 2). In 
addition, the last subsection includes an analysis of the results 
drawn by Google Analytics6. 

4.1.1 Importance of the issues 
The results in the boxplots for issue I1 (‘To be aware which 
resources and tools I and others use’) in Figure 2 are indecisive: 
whereas this does seem to be a somewhat important issue for the 
MUME students, it is much less important for the THESIS and 
PENO students. It is not immediately clear to us why this 
difference in appreciation exists. However, as MUME students 
usually face new challenges, learning how to develop software for 
iPhone and Android devices, being aware of what kind of 

                                                                 
6 http:// www.google.com/analytics/ 

17



resources are used by others can be more relevant than for PENO 
and THESIS students. PENO students have less freedom to 
choose tools to use. In addition, PENO students are bachelor 
students and they may feel less confident to discover and to try 
new resources. THESIS students share a methodology and a field 
(Human Computer Interaction), but their thesis topic often differs, 
which can be the reason that they feel less motivated to pay 
attention to what resources and tools others are using. 
The students consider issues related to how they distribute their 

time more important (median between 3.5 and 4) than I1.  
 Being aware that team members do not work (I3) is important for 
the MUME and PENO students (medians ? 4). This issue has not 
been evaluated with THESIS students because they do work much 
more individually. 
Motivation (I4) and Being aware when something goes wrong (I5) 
are important for all students (all medians ? 4). 

Communication issues are important for the students in the three 

Figure 2 Results of the evaluation 

18



courses (all medians ? 4). The results are a bit more spread for 
THESIS students. This is probably related to the fact that most of 
these students work more individually on their master thesis and 
thus care less about interacting with other thesis students. 
Problems about finding a balance between social activities and 
studying (I7) are somewhat important to the students (all medians 
? 3 or 3.5). 

4.1.2 Perceptions in the course 
Among students enrolled in MUME and THESIS, there is a high 
perception that other students may over-report. However, PENO 
students are indecisive. PENO students used less our application 
than others. In the PENO course, most of the work is done during 
the lab sessions and all the groups have one member who holds 
the role of project manager and this fact can decrease the feeling 
of over-reporting. 
However, all the students are indecisive when it relates to under-
reporting time spent (all medians ? 3). This issue was included 
because some students pointed out that others may under-report 
trying to avoid that others call them grinds.  Based on these 
results, it seems that this fear is not grounded in reality. 
All of them are motivated (all medians = 4) and they think that 
they are doing well in the course (medians = 4) except for the 
thesis students. Although they are motivated, they are indecisive 
whether they are doing well. The fact that thesis work has to be 
done during a full academic year (as opposed to a semester in the 
PENO and MUME courses) and the non-similar topics can 
influence an uncertain feeling of how they are doing. However, it 
brings an important niche that learning analytics can try to 
address.  

4.1.3 Issues 
The students do not believe that StepUp! addresses statement A1 
‘StepUp! helps me to be aware which resources and tools students 
use’ (all medians ? 3). This is probably related to the fact that 
StepUp! only visualizes blog activity and twitter use, and thus 
only covers a minor part of resources use in the three courses.  
When we assessed if StepUp! enables students to analyze their 
time spent (A2), the results differ over the three courses. MUME 
is most positive, while PENO is indecisive and THESIS is more 
negative and spread. Further analysis is required to understand 
why this is the case: it may be that StepUp! misses more of the 
relevant activity in the case of the master thesis work. 
StepUp! is not perceived to convincingly help students to analyze 
how other students spend their time (A3) (medians are 3 and 3.5).  
If we compare A2 and A3 with a previous experiment [31], 
StepUp! has decreased its effectiveness. In this experiment, the 

activities were defined with higher granularity such as 
programming whereas in the previous experiment were more 
specific activities such as first prototype. This may have affected 
the perception on how StepUp! helps them to understand efforts. 
 When asked if StepUp! allows to identify a group member that 
does not work well, the MUME students are not sure (median = 
3), while StepUp! helps the PENO students better (median = 4). 
The high perception that other students may over-report time 
spent (P1) can influence negatively student perception on this 
issue. 
According to statement A5, students are not convinced that using 
StepUp! increases their motivation (all medians ? 3). In this 
context, it is important to note that the motivation of the students 
for the course was high (P3 – all medians ? 4). 

They are more positive about StepUp! helping them to assess how 
they are doing in the course (A6), especially the THESIS and 
PENO students (median ? 3.5). Moreover, StepUp! allows the 
students to better compare themselves with their peers (A7) (all 
medians = 4).  
The students are not convinced that StepUp! helps them with 
being aware of course problems (A8) (all medians ? 3). When 
asked if StepUp! makes them work harder (A9) or slower (A10) if 
they are working not enough or too hard, THESIS and PENO 
students are not convinced StepUp! makes them work harder 
when appropriate, but MUME students are more positive (median 
= 4). Students are not convinced that StepUp! makes them work 
less (median of THESIS & PENO = 2 and MUME = 3). These 
results can be influenced by statement A2 that StepUp! fails 
helping them to understand how they spend their efforts. 
Students are not convinced (median of THESIS = 2, median of 
MUME & PENO = 3) that StepUp! enables them to gain insights 
in how others communicate (A11). Regarding this issue, other 
visualizations such as network visualizations could improve the 
results. When we asked whether StepUp! makes them comment 
more on blogs (A12), then we see that the MUME students are not 
convinced (median = 3) and the THESIS and PENO students are 
motivated by StepUp! to comment more on the blogs of fellow 
students (median = 4). When assessing whether StepUp! promotes 
the reading of blogs (A13), we see that it helps MUME and PENO 
students (median = 4), but the THESIS students are not convinced 
(median = 3). The reason for the behavior of the THESIS students 
might be that they are more focused on research specific to their 
thesis topic, while PENO and MUME students are working on a 
shared topic, so the blogs of other students are more directly 
related to their own work. When looking whether StepUp! makes 
them use Twitter more (A14), we also learn that StepUp! 
promotes the use of Twitter more for MUME and PENO students 

Figure 3 Google Analytics view 
19



(median = 4), than for THESIS students (median = 3). This might 
be again caused by the common topic of the PENO and MUME 
students and thus closer social interaction. 
From the rating of statement A15 ‘StepUp! enables me to plan my 
time better’, we learn that StepUp! does not help students to plan 
more efficiently (all medians ? 2.5). This might be because 
StepUp! only presents the time spent on past events and not the 
work that still has to be done. Nor does StepUp! provide real 
functionalities for planning. Our hope was that StepUp! would 
assist students in becoming better planners through self-reflection 
and sensemaking provided by the visualizations, but, at least in 
the perception of the students, this goal is not reached. 

4.1.4 SUS questionnaire 
All the students rated our tool between acceptable and good. 
MUME students rated the tool with 67.9, PENO students with  
with 67.5 points and thesis with 72.1. In an earlier 
experiment[31], SUS scores were 77 and 82 points. Our tool has 
thus decreased in usability. This decrease may be due to the fact 
that StepUp! has been deployed for a larger group of students – 
which in terms of scalability also caused some problems that 
caused some delay updating the data on the table.  

4.1.5 Use of the tool 
The students have used the tool regularly as indicated by the 
Google Analytics view (see Figure 3). In one month, StepUp has 
been visited more than 850 times with an average time spent on 
the tool of 4 minutes.  
The visits view indicates that StepUp! is more visited during the 
week. These visits decrease considerably during the weekend, 
when also students reported lower time spent on the activities. 
Comparing with the results in an earlier experiment, the average 
time has decreased around 50%. This can either be caused by or 
influence the usefulness perception for our tool. If students spend 
less time, they cannot find out the benefits of our tool. In Section 
6, we present some future research plans, including the 
deployment of a mobile version, to better engage the students with 
StepUp!  

5. RELATED WORK 
Learning analytics consider the analysis of communication logs 
[14][33], learning resource use [26], learning management system 
logs, learning designs [24][29], as well as the activity outside of 
learning management systems [27][7]. The result of this analysis 
can be used to improve the creation of predictive models[30][5], 
recommendations [38] and reflection [32][23]. Since we focus on 
reflection, we mainly build dashboards to enable self-reflection 
and to enable the learner to steer the learning process or teachers 
to plan interventions if they are required.   

In recent years, several dashboard applications have been 
developed to support learning and teaching. These dashboards are 
used in traditional face-to-face teaching, online learning and 
blended learning settings. Some examples are Classroom View 
[16] that shows current activity in a classroom, the dashboard 
implemented in the learning management system Moodle [28] 
which tracks online activities to support teachers and the 
educational monitoring tool based on faceted browsing and data 
portraits showing the current status of each student in distance 
education [17]. Khan Academy7 dashboard enables tutors to check 
progress of students where a table provides a goal status overview 
per student. For every student, a timeline shows the distribution of 
                                                                 
7 http:// www.khanacademy.org/ 

achieved goals and a bar chart visualizes the time spent with 
different kinds of resources. 

Some dashboards were developed specifically to support learners. 
CALMSystem [19] is an example of a dashboard that was 
developed on top of an intelligent tutoring system to give a learner 
insight into their learner model as a basis to support awareness, 
reflection and sensemaking. Performance indicators on different 
topics are visualized and can be adjusted by the learner as well.  
Tell Me More [22] is a commercial language learning application 
that tracks results of exercises as a basis to visualize progress of 
learners. S3 [13] is a dashboard that enables practitioners to plan 
interventions with students at risk. Narcissus [36] was developed 
to support small group work. GLASS [23], SAM [18] and Student 
Inspector [41] were developed to support both teachers and 
learners. The work presented in this paper is intended to support 
students and teacher reflection. 

If we analyze dashboards, we find that time spent is a commonly 
captured trace. In addition, social interaction can help to gain 
insight in collaboration [9][1] and to detect isolated students [9]. 
Document and tool use can give effort indicators of students 
[18][25].  

GISMO[21] also offers the possibility to detect students who do 
not work well. They highlight the students who have not reached a 
minimum number of post messages in the forum. They conclude 
that different learning behavior does not mean different reached 
goals. It can also influence to the results on the statement 
‘StepUp! helps me to analyze how others spend their time’. 

LOCO-Analyst[34] and SAM[18] address the issue ‘to be aware 
which resource I and others use’. However, SAM also struggles 
with analyzing time spent, because it uses logs generated by the 
LMS, and students claim that some activity happens outside of the 
LMS, so that the visualized information does not represent all 
their work. This fact inspires GLASS [23] that tracks all 
interaction within a Virtual Machine and visualizes also 
programming code logs. 

Looking at how other researchers evaluate their dashboards, we 
find one longitudinal study [2] where a tool was evaluated over 
three years, and was found to increase retention rates. Other more 
limited studies focus on effectiveness [9][20][24] and perceived 
usefulness [1][9][17][30][33]. Most studies on effectiveness and 
perceived usefulness assess problems that the lecturer or the 
literature describes. On the other hand, other studies [3][20] 
reinforce the idea to ask the students what problems they have and 
to assess whether these problems are addressed. The work 
presented in this paper focuses on potential impact of StepUp! 
may have on issues and needs identified by our students. 

6. FUTURE WORK 
Now that we analyzed the reported issues by the students, we 
would like to focus on one important factor of learning: 
motivation. Although our students are motivated in our courses 
(see 4.1.2), we see from the results of the evaluation that we are 
failing addressing some issues that may affect motivation.   
Thesis students do not know whether they are doing well in the 
course (section 4.1.2.) and this is an important factor for the 
motivation of the student [42]. We expected that StepUp! could 
address this issue by enabling comparison between peers, but 
StepUp! does not help understand how peers as well as 
themselves spend their time (section 4.1.3). Furthermore, StepUp! 
does not motivate them to increase their social interaction 
blogging, commenting and tweeting.  

20



In order to address the reported issues, we will focus more on 
generated artifacts and student motivation . To this end, we are 
currently enriching StepUp! with gamification aspects: StepUp! 
will again track blogs, comments, tweets, and, in addition, 
bookmarked resources. We will define a series of rules in order to 
reward students with badges for positive behavior. Badges will be 
strongly linked to the activities of the course in order to increase 
awareness of student progress in the course.  
However, gamification approaches can have negative effects [42]. 
By making available an overview of achievable badges, we hope 
to increase student awareness about what we expect from them. In 
this way, we expect to deploy a more goal-oriented approach 
improving perception on how students are doing in the course .  
Visualizations such as progress bar chart can also reinforce the 
idea of goal-oriented approach and activity streams increase the 
awareness of and engagement to the course. Thus, these 
approaches will also be integrated in the next version of our tools.     

7. CONCLUSIONS 
We have set up a series of brainstorming sessions to gather 
requirements and to identify the most important issues for 
students. As explained in Section 3, we considered which 
functionalities of StepUp! could potentially help to address the 
issues. 
The general conclusion is that our tool has potentially higher 
impact for students working in groups and sharing a topic such as 
PENO and MUME than students working individually on 
different topics. However, overall, students are not that convinced 
of the added value of StepUp!.  
Thus, we believe that this study helps to point out that 
demonstrating the relevance of learning analytics dashboards like 
StepUp! is a complex undertaking. We strongly believe that more 
critical evaluations of the actual use of such tools are required. 
Student perceptions of added value are not the only criterion, but 
certainly a most important one! 

8. ACKNOWLEDGMENTS 
The research leading to these results has received funding from 
the European Community Seventh Framework Programme 
(FP7/2007-2013) under grant agreement no 231396 (ROLE). 
Katrien Verbert is a Postdoctoral Fellow of the Research 
Foundation - Flanders (FWO). 

 

9. REFERENCES 
[1] Ali, L., Hatala, M., Gaševi?, D., & Jovanovi?, J. A 

qualitative evaluation of evolution of a learning analytics 
tool. Computers and Education, 58(1), 470-489. 2012 

[2] Arnold, K. E. & Pistilli, M.D. (2012). Course signals at 
Purdue: using learning analytics to increase student success. 
In S.B. Shum, D. Gasevic, and R. Ferguson 
(Eds.)Proceedings of the 2nd International Conference on 
Learning Analytics and Knowledge (LAK '12),(pp. 267-270). 
NY: ACM 

[3] Baillie, C. Motivation and attrition in engineering students, 
Geraldine Fitzgerald European Journal of Engineering 
Education Vol. 25, Iss. 2, 2000 

[4] Bangor, A., Kortum, P.T., Miller, J.T.: An empirical 
evaluation of the system usability scale. Int. J. Hum. Comput. 
Interaction (2008) 574-594 

[5] Barber,R., Sharkey,M. Course correction: using analytics to 
predict course success. In Proceedings of the 2nd 

International Conference on Learning Analytics and 
Knowledge (LAK '12), 2012 

[6] Parra, G., Klerkx, J., Duval, E. What Should I Read Next? 
Awareness of Relevant Publications Through a Community 
of Practice. In SIGCHI '13 Case Studies. Accepted. 

[7] Blikstein, P. Using learning analytics to assess students' 
behavior in open-ended programming tasks. In Proceedings 
of the Learning Analytics and Knowledge conferencd 
(LAK11), 2011Brooke, J. SUS: A quick and dirty usability 
scale. In: Usability Evaluation in Industry. Taylor & Francis, 
London, 1996. 

[8] Conole, G., Fill, K. (2005). A learning design toolkit to 
create pedagogically effective learning activities. Journal of 
Interactive Media in Education 2005(08). 

[9] Dawson, S., Bakharia, A., & Heathcote, E. SNAPP: 
Realising the affordances of real-time SNA within networked 
learning environments. In L. Dirckinck- Holmfeld, V. 
Hodgson, C. Jones, M. de Laat, D. McConnell, & T. Ryberg 
(Eds.),  Proceedings of the 7th International Conference on 
Networked Learning (pp. 125-133) 2010 

[10] Dollár, A. ,Steif, P. S. Web-based Statics Course with 
Learning Dashboard for Instructors. In Uskov, V. (Ed.), 
Proceedings of Computers and Advanced Technology in 
Education (CATE 2012), June 25 – 27, 2012, Napoli, Italy. 
2012  

[11] Duval, E.: Attention please! learning analytics for 
visualization and recommendation. In: Proceedings of 
LAK11: 1st International Conference on Learning Analytics 
and Knowledge,, ACM (2011) 9-17 

[12] Duval, E., Klerkx, J., Verbert, K., Nagel, T., Govaerts, S., 
Parra Chico, G.A., Santos, J.L., Vandeputte, B.: Learning 
dashboards and learnscapes. In: Educational Interfaces, 
Software, and Technology,. (May 2012) 1-5 

[13] Essa, A., Ayad, H.. Student success system: risk analytics 
and data visualization using ensembles of predictive models. 
In Proceedings of the 2nd International Conference on 
Learning Analytics and Knowledge (LAK '12), 2012 

[14] Ferguson, R., Buckingham, S. Shum. Social learning 
analytics: five approaches. In Proceedings of the 2nd 
International Conference on Learning Analytics and 
Knowledge (LAK '12) 2012. 

[15] Fischer, G.: Understanding, fostering, and supporting 
cultures of participation. interactions 18(3) (May 2011) 42-
53 

[16] France, L., Heraud, J.-M., Marty, J.-C., Carron, T., & Heili, 
J. Monitoring Virtual Classroom: Visualization Techniques 
to Observe Student Activities in an e-Learning System. 
Proceedings of the Sixth International Conference on 
Advanced Learning Technologies, (pp.716-720). 2006 

[17] García-Solórzano,D., Cobo, G.,  Santamaría, E., Morán, J.A.,  
Monzo, C., Melenchón,J. Educational monitoring tool based 
on faceted browsing and data portraits. In Proceedings of the 
2nd International Conference on Learning Analytics and 
Knowledge (LAK '12). 2012 

[18] Govaerts, S., Verbert, K., Duval, E., & Pardo, A. The 
Student Activity Meter for Awareness and Self-reflection. 
Proceedings of the 2012 ACM annual conference on Human 
Factors in Computing Systems Extended Abstracts (pp. 869–
884). ACM. 2012 

21



[19] Kerly, A., Ellis, R. & Bull, S. CALMsystem: A 
Conversational Agent for Learner Modelling, in R. Ellis, T. 
Allen & M. Petridis (eds), Applications and Innovations in 
Intelligent Systems XV – Proceedings of AI-2007, 27th 
SGAI International Conference on Innovative Techniques 
and Applications of Artificial Intelligence (pp. 89-102). 
Springer Verlag. 2007 

[20] Killen, R., Differences between students’ and lecturers’ 
perceptions of factors in?uencing  students’ academic  success  
at  university. Higher  Education  Research  and 
Development, 13, 199. 1994 

[21] Kobsa, E., Dimitrova, V., & Boyle, R. Using student and 
group models to support teachers in web-based distance 
education. Proc. of the 10th international conference on user 
modeling (pp. 124–133). Edinburgh, UK. 2005 

[22] Lafford, B. A. Review of Tell Me More Spanish, Journal on 
Language Learning & Technology, 8(3), 21-34. 2004 

[23] Leony,D., Pardo,A.,  Fuente,L , Sánchez,D., Delgado,C.. 
GLASS: a learning analytics visualization tool. In 
Proceedings of the 2nd International Conference on Learning 
Analytics and Knowledge (LAK '12), 2012 

[24] Lockyer,L.,Dawson, S. Where learning analytics meets 
learning design. In Proceedings of the 2nd International 
Conference on Learning Analytics and Knowledge (LAK 
'12). 2012 

[25] Mazza, R., & Milani, C. (2004). GISMO : a Graphical 
Interactive Student Monitoring Tool for Course Management 
Systems. TEL’04 Technology Enhanced Learning ’04 
International Conference (pp. 1-8). Citeseer 

[26] Niemann, K., Schmitz, H-C, Kirschenmann, U., Wolpers,M. 
Anna Schmidt, and Tim Krones. Clustering by usage: higher 
order co-occurrences of learning objects. In Proceedings of 
the 2nd International Conference on Learning Analytics and 
Knowledge (LAK '12) 2012 

[27] Pardo,A., Kloos, C. Stepping out of the box towards 
analytics outside the learning management system. In 
Proceedings of the Learning Analytics and Knowledge 
conference (LAK11), 2011 

[28] Podgorelec, V. & Kuhar, S. Taking Advantage of Education 
Data: Advanced Data Analysis and Reporting in Virtual 
Learning Environments. Electronics and Electrical 
Engineering, 114(8), 111-116. 

[29] Richards, G.,DeVries, I. Revisiting formative evaluation: 
Dynamic monitoring for the improvement of learning 
activity design and delivery. In Proceedings of the Learning 
Analytics and Knowledge conference (LAK11), 2011 

[30] Roijers, D.M., Jeuring, J., Feelders, A. Probability estimation 
and a competence model for rule based e-tutoring systems. In 
Proceedings of the 2nd International Conference on Learning 
Analytics and Knowledge (LAK '12), 2012 

[31] Santos, J.L., Verbert, K., Govaerts, S., Duval, E. 
Empowering students to reflect on their activity with 

StepUp!: Two case studies with engineering students, In 
proceedings of EFEPLE11 2nd Workshop on Awareness and 
Reflection in Technology-Enhanced Learning, CEUR WS 
(2012) (accepted) 

[32] Santos, J.L., Govaerts, S., Verbert, K., Duval, E.: Goal-
oriented visualizations of activity tracking: a case study with 
engineering students. In: LAK12: International Conference 
on Learning Analytics and Knowledge, Vancouver, Canada, 
29 April - 2 May 2012, ACM (May 2012) 

[33] Schreurs,B. , De Laat, M. Network awareness tool - learning 
analytics in the workplace: detecting and analyzing informal 
workplace learning. In Proceedings of the 2nd International 
Conference on Learning Analytics and Knowledge (LAK 
'12). 2012 

[34] Silius, K., Miilumaki, T., Huhtamaki, J., Tebest, T., 
Merilainen, J., & Pohjolainen, S. Students' Motivations for 
Social Media Enhanced Studying and Learning. Knowledge 
Management & E-Learning: An International Journal 
(KM&EL), 2(1). 51-67. 2010 

[35] Tufte, E.R.: Beautiful Evidence. Graphics Press (2006) 
[36] Upton, K., Kay, J.. Narcissus: Group and individual models 

to support small group work. In Proceedings of the 17th 
International Conference on User Modeling, Adaptation, and 
Personalization: formerly UM and AH, UMAP '09, pages 54-
65,Berlin, Heidelberg, 2009. Springer-Verlag,. 2009 

[37] Verbert, K., Manouselis, N., Drachsler, H., Duval, E.: 
Dataset-driven research to support learning and knowledge 
analytics. Educational Technology and Society (2012) 1-21 

[38] Verbert,K., Drachsler, H., Manouselis, N., Wolpers, M., 
Vuorikari, R., Duval,E. Dataset-driven research for 
improving recommender systems for learning. In 
Proceedings of the Learning Analytics and Knowledge 
conference (LAK11), 2011 

[39] Wenger, E.: Communities of Practice: Learning, Meaning, 
and Identity (Learning in Doing: Social, Cognitive and 
Computational Perspectives). 1 edn. Cambridge University 
Press (September 1999) 

[40] Yacef, K., Zaïane, O., Hershkovitz, H., Yudelson, M., and 
Stamper, J. (eds.) Proceedings of the 5th International 
Conference on Educational Data Mining. 2012 

[41] Zinn, C., Scheuer, O. How did the e-learning session go? The 
Student Inspector. In In Luckin, R., Koedinger, K.R., & 
Greer, J. (Eds.) Proceeding of the 2007 conference on 
Artificial Intelligence in Education: Building Technology 
Rich Learning Contexts That Work (pp. 487-494). IOS Press: 
Amsterdam, The Netherlands (2007) 

[42] Williams, K., Williams, C. Five key ingredients for 
improving student motivation (2011) Vol. 12. (pp. 1-23) 
Research in Higher Education Journal 

 

 
 

22





