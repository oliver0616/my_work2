
Unravelling the dynamics of instructional practice:  

A longitudinal study on learning design and VLE activities 
 

Quan Nguyen  

Open University the UK 
Institute of Educational 

Technology 

+44 7732599001 

quan.nguyen@open.ac.uk 

 

Bart Rienties 

Open University the UK 
Institute of Educational 

Technology 

+44 1908332671 

bart.rienties@open.ac.uk 

 

Lisette Toetenel 

Open University the UK 
Institute of Educational 

Technology 

+44 19083322809 

lisette.toetenel@gmail.com 

 

ABSTRACT 
Substantial progress has been made in understanding how teachers 

design for learning. However, there remains a paucity of evidence 

of the actual students’ response towards leaning designs. Learning 

analytics has the power to provide just-in-time support, especially 

when predictive analytics is married with the way teachers have 

designed their course, or so-called a learning design. This study 

investigates how learning designs are configured over time and 

their impact on student activities by analyzing longitudinal data of 

38 modules with a total of 43,099 registered students over 30 weeks 

at the Open University UK, using social network analysis and panel 

data analysis. Our analysis unpacked dynamic configurations of 

learning designs between modules over time, which allows teachers 

to reflect on their practice in order to anticipate problems and make 

informed interventions. Furthermore, by controlling for the 

heterogeneity between modules, our results indicated that learning 

designs were able to explain up to 60% of the variability in student 

online activities, which reinforced the importance of pedagogical 

context in learning analytics.  

CSS Concepts 

•Applied computing ? E-learning; Distance learning 

 

Keywords      
Learning analytics, learning design, social network analysis, 

longitudinal, panel data analysis  

 

 

 

 

 

1. INTRODUCTION  
In the last decade, there is a growing body of literature [11, 15, 32] 

that seeks to develop a descriptive framework to capture teaching, 

and  learning activities so that teaching ideas can be shared and 

reused from one educator to another, so called Learning Design 

(LD) [16]. A common metaphor of a learning design was a music 

notation which contains enough information to convey musical 

ideas from one to another over time and space [16]. Extensive 

research has been conducted focusing on technological 

implementations of LD such as the Educational Modelling 

Language (EML) [29], the SoURCE project [31], the Australian 

Universities Teaching Council (AUTC) LD project [1], and the 

Learning Activity Management System (LAMS) [14]. While the 

early work in LD have focused on transferring the design for 

learning from implicit to explicit, the relationship between LD and 

the actual learners’ response has been not fully understood. As the 

majority of feedback takes forms of assessments, and course’s 

evaluations, which typically takes place after the learning process 

has finished, it prevents teachers from making in-time 

interventions. Recently, the advancement in technology has 

allowed us to capture the digital footprints of learning activities 

from Virtual Learning Environment (VLE). This rich and fine-

grained data about the actual learners’ behaviors offer educators 

potentially valuable insights on how students react to different LDs.  

Learning analytics (LA) has the potential to empower teachers and 

students by identifying patterns and trends from a wide variety of 

learners’ data. Within the LAK community, substantial progress 

has been made both in conceptual development [10, 17] as well as 

how to design appropriate predictive learning analytics to support 

students [19, 26]. Nonetheless, in line with [26, 44] findings from 

LA research in the past have been rather limited to delivering 

actionable feedback, while ignoring the context of which the 

learning data is situated. Thus, within the LAK community there is 

an increasing interest to align LA with LD, as the former facilitates 

the transfer of tacit educational practice to an explicit rendition, 

while the latter provides educators with pedagogical context for 

interpreting and translating LA findings to direct interventions [3, 

33, 34, 37, 40]. While there are abundant discussions on the value 

and impact of integrating LD into LA to improve teacher inquiry 

[3, 37], only a few studies have empirically examined how teachers 

actually design their courses [4, 20] and whether LD influences 

satisfaction, VLE behavior, and retention [42, 44, 45, 48]. 

However, most previous studies are limited to interviews and 

experimental settings, while others have only explored LD from a 

static perspective, without accounting for the differences within 

Permission to make digital or hard copies of all or part of this work for 

personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 

bear this notice and the full citation on the first page. Copyrights for 

components of this work owned by others than ACM must be honored. 
Abstracting with credit is permitted. To copy otherwise, or republish, to 

post on servers or to redistribute to lists, requires prior specific permission 

and/or a fee. Request permissions from Permissions@acm.org. 

LAK '17, March 13-17, 2017, Vancouver, BC, Canada 
© 2017 ACM. ISBN 978-1-4503-4870-6/17/03…$15.00 

DOI: http://dx.doi.org/10.1145/3027385.3027409 



and between modules and the possible interaction between different 

types of learning activities over time.  

This study builds on previous work by Rienties and Toetenel [45], 

[48] by dynamically investigating the use of learning design in 38 

modules over 30 weeks at one of the largest distance higher 

education institutions in Europe using social network analysis and 

panel data analysis. Our work contributes to the existing LA 

literature by capturing: (1) how teachers configure their course over 

time, (2) how learning activities interact with each other across 

modules, and (3) how learning designs help to explain VLE 

behavior over time.  

2. ALIGNING LA WITH LD 
In the last five years, LA has attracted a lot of attention from 

practitioners, management, and researchers in education by 

shedding light on a massive amount of (potentially) valuable data 

in education, as well as providing means to explicitly test existing 

pedagogical theories. Scholars in the field of LA have exploited 

various sources of data, such as activity logs of students [35], 

learning dispositions [7, 39], or discussion forum [2, 51]. By taking 

advantage of advanced analytical techniques such as predictive 

modeling [46], discourse analytics [49], machine learning [30], and 

so on, LA has succeeded in uncovering meaningful patterns and 

trends occurred during the learning process. While these studies 

provide important markers on the potential of LA in education, 

critics have indicated a gap between pedagogy and LA [18, 28, 50]. 

Interesting patterns can be identified from student activities, such 

as number of clicks, discussion posts, or essays. However, these 

patterns alone are not sufficient to offer feedback that teachers can 

put into actions [19, 43]. Without a pedagogically sound approach 

to data, LA researchers may struggle with deciding which variables 

to attend to, how to generalize the results to other contexts, and how 

to translate their findings to actions [28]. Hence, LD can equip 

researchers with a narrative behind their numbers, and convert 

trends of data into meaningful understandings and opportunities to 

make sensible interventions.  

The core concepts of LD are best summarized in the Learning 

Design Conceptual Map (LD-CM) (Figure 1). It starts with the 

main objective of “creating learning experiences aligned to 

particular pedagogical approaches and learning objectives”. How 

educators make decision about designing for learning is determined 

by Characteristics & Values of the learning environment, the 

educational philosophy, and theories and methodologies. In a 

interview based study of 30 participants, Bennett, Agostinho and 

Lockyer [4] identified three main factors that influenced how 

teachers engage in the designing process: student-related factors 

(cohort profile, learning objectives, feedback from past sessions), 

teachers-related factors (beliefs about teaching, prior experiences), 

and context-related factors (colleagues, institutional policies and 

culture, resources such as workload, time, and infrastructure).  

In the teaching cycle, the reflection phase is limited to insights 

generated from assessments, course evaluations, and self-reports. 

These channels may suffer from selection bias, response bias, and 

hinder educators to make in-time interventions. A potential 

contribution of LA in LD is to include real-time learner response to 

a LD, such as how much time was spent on a particular activity, or 

how often a student visits a concept/topic. These behavioral traces 

allow educators to both make personalized interventions to each 

student as well as adjust the course according to the overall trends 

of a group of students. As illustrated below, LA allows educators 

to reflect and compare their practice in a wide range of granularity: 

from learning activities to modules, and disciplines. Overall, using 

LA in combination with other feedback channels, such as 

assessment and evaluation, could empower and speed up the 

teaching cycle by generating more feedback, allow educators to 

make in-time interventions, to reflect, and to compare their practice 

on multiple levels of granularity.  

Figure 1: A Learning Design Conceptual Map. Retrieved from  
Dalziel, Conole, Wills, Walker, Bennett, Dobozy, Cameron, 

Badilescu-Buga and Bower [16]. 

2.1 Connecting LD and LA 
Since the beginning of the 21st century, the term learning design has 

emerged as a “methodology for enabling teachers/designers to 

make more informed decisions in how they go about designing 

learning activities and interventions, which is pedagogically 

informed and makes effective use of appropriate resources and 

technologies” [11]. For more discussion on the origins of ‘learning 

design’ and ‘instructional design’, we refer readers to Persico and 

Pozzi [40]. Several approaches for designing learning have been 

proposed, yet, one common stage in almost every approach was the 

evaluation of the LD [36, 40]. Persico and Pozzi [40] argued that 

the learning process should not only depend on experience, or best 

practice of colleagues but also pre-existing aggregated data on 

students’ engagement, progression, and achievement. In a similar  

manner, Mor, Ferguson and Wasson [37] suggested that LA could 

facilitate teacher inquiry by transforming knowledge from tacit to 

explicit, and perceive students and teachers as participants of a 

reflective practice. For instance, in a study of 148 learning designs 

by Toetenel and Rienties [47], the introduction of a systematic LD 

initiative consisting of visualization of initial LDs and workshops 

helped educators to focus on the development of a range of skills 

and more balanced LDs. Feeding information on how students are 

engaged in a certain LD during or post-implementation can provide 

a more holistic perspective of the impact of learning activities [34].  

Several conceptual frameworks aiming at connecting LA with LD 

have been proposed. For example, Persico and Pozzi [40] discussed 

three dimensions of LD that can be informed by LA: 

representations, tools, and approaches. Lockyer, Heathcote and 

Dawson [34] introduced two categories of analytics applications: 

checkpoint analytics to determine whether students have met the 

prerequisites for learning by assessing relevant learning resources, 

and process analytics to capture how learners are carrying out their 

tasks. In the recent LAK conference 2016, Bakharia, Corrin, de 

Barba, Kennedy, Gaševi?, Mulder, Williams, Dawson and Lockyer 

[3]  proposed four types of analytics (temporal, tool specific, 

cohort, and comparative), and contingency and intervention 

support tools with the teacher playing a central role.  



In this paper, we will use the conceptual framework developed by 

Conole [11] and further developed by Rienties & Toetenel (REF). 

Both conceptual and empirical research has found that the Open 

University Learning Design Initiative (OULDI) can accurately and 

reliably determine how teachers design courses, and how students 

are subsequently using these LDs [45, 48]. Seven types of learning 

activities can be found in Table 1. Assimilative activities refer to 

tasks which require learner’s attention to information. These 

include watching lecture video, reading the text, listening to an 

audio file, etc. Finding and handling information activities implies, 

for example, searching and filtering for relevant literature in a 

particular topic on the internet. Communication activities refer to a 

range of practices to communicate such as posting in a discussion 

forum and replying to peer comments. Productive activities 

represent the construction of an artifact, such as writing a summary 

or resolving a problem. Experiential activities provide learners with 

opportunities to apply theories in a real-world setting such as case 

study, or field trip. Interactive/adaptive activities encourage 

learners to apply what they learned in an experietial environment, 

or interactng with a simulation. Finally, assessment activities 

evaluate the learner’s understanding such as writing through the 

construction of an essay, exam or making a presentation.  

Table 1: Learning design taxonomy 

 Type of activity Example 

Assimilative Attending to 

information 

Read, Watch, Listen, 

Think about, Access. 

Finding and 

handling 

information 

Searching for and 

processing information 

List, Analyse, Collate, 

Plot, Find, Discover, 

Access, Use, Gather.  

Communicat

ion 

Discussing module 

related content with at 

least one other person 

(student or tutor) 

Communicate, Debate, 

Discuss, Argue, Share, 

Report, Collaborate, 

Present, Describe. 

Productive Actively constructing 

an artefact 

Create, Build, Make, 

Design, Construct, 

Contribute, Complete,.  

Experiential Applying learning in a 

real-world setting  

Practice, Apply, Mimic, 

Experience, Explore, 

Investigate,. 

Interactive 

/adaptive 

Applying learning in a 

simulated setting  

Explore, Experiment, 

Trial, Improve, Model, 

Simulate.  

Assessment All forms of 

assessment 

(summarive, formative 

and self assessment)  

Write, Present, Report, 

Demonstrate, Critique. 

Source: Retrieved from Rienties and Toetenel [45] 

While there were numerous discussions in aligning LA with LD, 

the amount of empirical studies on the subject has been rather 

limited. For example, Gaševi?, Dawson, Rogers and Gasevic [19] 

examined the extent to which instructional conditions influence the 

prediction of academic success in nine undergraduate courses 

offered in a blended learning model. The results suggested that it is 

imperative for LA to taking into account instructional conditions 

across disciplines and course to avoid over-estimation or 

underestimation of the effect of LMS behavior on academic 

success. From our observation, most of the empirical studies 

attempting to connect LA and LD are derived from students 

activities [34], or differences in discipline [19],  rather than how 

teachers actually design their course [24].  

In our previous work, we have highlighted explicitly the role of LD 

in explaining LMS behavior, student satisfaction, retention, and 

differences in prediction of academic success [19, 42, 44, 45, 48].  

For example, in our first study linking 40 LDs with VLE behavior 

and retention, we found that strongly assimilative designs (i.e., lots 

of passive reading and watching of materials) were negatively 

correlated with retention [42]. In a large-scale follow-up study 

using a larger sample of 151 modules and multiple regression 

analyses of 111,256 students at the Open University, UK, Rienties 

and Toetenel [45] revealed relations between LD activities and 

VLE behavior, student satisfaction, and retention. The findings 

showed that taking the context of LD into account could increase 

the predictive power by 10-20%. Furthermore, from a practitioner’s 

perspective, the combination of a collaborative, networked 

approach at the initial design stage, augmented with visualizations, 

changed the way educators design their courses [47]. While these 

three studies at the Open University UK (OU) highlighted the 

potential affordances of marrying LD with LA on a large scale, two 

obvious limitations of these studies were the aggregation of 

learning activities in predicting behavior and performance (i.e., 

rather than their interaction), as well as the static rather than 

longitudinal perspective of LD. In these studies [42, 44], aggregate 

learning design data across the 40 weeks of each module were used, 

while in many instances teachers use different combinations of 

learning activities throughout the module [24]. While fine-grained 

longitudinal data of LD per week were not available during the 

initial implementation phase of LD at the OU, in the last year fine-

grained weekly LD data has been added, which would allow us to 

potentially identify the optimum mix of LD activities per discipline, 

level, and type of students per week and over time. 

2.2 Research Questions 
Building on previous conceptual and empirical research, we are 

particularly interested in how teachers design their learning 

activities over time since learning is a dynamic and time-variant 

process. Hence, our first research question is: 

1) How are learning designs configured across modules 
over time? 

Prior studies of Social Network Analysis (SNA) in e-learning, 

particularly in the improvement of LD have concentrated on 

examining patterns of learner communication and collaboration in 

various situations, such as when discussing, blogging and e-mailing 

[8]. Within the last three years in LA, SNA has been shown to be 

an effective tool to explore the relationships of learners in online 

discussion forum [9, 23, 25, 27, 41], or in eye tracking movements 

[52]. However, none has looked at the LD from a social network 

perspective on a large scale study. Hora and Ferrare [24] suggested 

that teaching practice should be best viewed as situated in and 

distributed among features of particular settings. According to the 

systems-of-practice theory by Halverson [21], local practices are 

informed, constrained, and constituted by the dynamic interplay of 

artifact and tasks. Thus, in order to understand how teachers design 

their course, it is necessary to consider the inter-relationships 

among different learning activities. Thus, our next research 

question aims at examining:  

2) How do different learning activities interact with each 
other across modules? 

Finally, our previous work has indicated that learning designs are 

strong predictors of VLE behaviors [45, 48]. However, we did not 

take into account the differences between modules which might 

affect the robustness of the analysis [19]. Hence using panel data 

analysis, we investigate:  



3) How do learning designs affect VLE behavior over time? 

3. METHOD 

3.1 Setting 
This study took place at the OU, which is the largest distance 

education provider in Europe. Data in this study was generated 

from OULDI, which helps teams in defining their pedagogic 

approach, choosing and integrating an effective range of media and 

technologies, and enable sharing of good practice across the 

university [13]. When using data to compare module design across 

disciplines and modules, according to our previous work [45, 48] it 

is important to classify learning activities in an objective and 

consistent manner. In particular, each module goes through a 

mapping process by a module team which consists of a LD 

specialist, a LD manager, and faculty members. This process 

typically takes between 1 and 3 days for a single module, depending 

on the number of credits, structure, and quantity of learning 

resources. First, the learning outcomes specified by the module 

team were captured by a LD specialist. Each learning activity 

within the module’s weeks, topics, or blocks was categorized under 

the LD taxonomy and stored in an ‘activity planner’ – a planning 

and design tool supporting the development, analysis, and sharing 

of learning designs. Next, the LD team manager reviews the 

resulting module map before the findings are forwarded to the 

faculty. This provides academics with an opportunity to comment 

on the data before the status of the design was finalized. To sum up, 

the mapping process is reviewed by at least three people to ensure 

the reliability and robustness of the data relating to a learning 

design.  

In this study, of 56 modules were selected with all contained LD 

data that have been documented on a weekly basis for the academic 

years 2014 and 2015, we ended up with 42 modules after excluding 

14 modules that were short, intensive training modules. The final 

selection of modules were equally distributed across a range of 

disciplines with 21% in Art & Social Sciences, 21% in Business & 

Law, 12% in Education, Languages, and Health studies, 22% in 

Science and Technology, and 24% in other disciplines. Over 90% 

of the modules were undergraduate courses. There were 20 

modules with 60 credits, 19 modules with 30 credits and 3 modules 

with missing information.  

In preparation for the panel data analysis, we linked 42 modules 

with weekly LD data in 2014 and 2015 with weekly VLE data, 

whereby 38 modules were successfully merged. The average 

number of students registered in each module was 1134 with the 

minimum of 75 and the maximum of 3707.  On average, 91% of 

the students who followed the course until the end passed (SD = 

0.058) while 63.4% of all the registered students passed the course 

(SD=0.086). The retention rate of all the modules was 69% on 

average, with a range from 56% to 85%.     

3.2 Instruments 

3.2.1 Measurement of learning designs 
Seven LD variables were measured in terms of workload, which is 

the number of hours that students are expected to study. Time spent 

on learning activities was restricted based on the size of the module, 

such as 30 credits equated to 300 hours of learning, and 60 credits 

equated to 600 hours of learning. Of the 38 modules, students were 

expected to study on average 8.10 hours per week, of which 3.92 

hours were spent on assimilative activities, 0.26 hours on finding 

information, 0.29 hours on communication, 1.32 hours on 

productive activities, 0.14 hours on experiential activities, 0.17 

hours on interactive activities, and 1.99 hours on assessment.  

3.2.2 Measurement of VLE 
In line with Tempelaar, Rienties and Giesbers [46] and our previous 

works [45], two different types of VLE data were gathered per 

module in a static and dynamic manner: average time spent (in 

minutes) on VLE per week (M=115.4, SD=88.4), and average time 

spent per visit (in minutes) on VLE (M=22.5, SD=8.7). Even 

though learner activities on VLE were recorded in 40 weeks, we 

only used the first 30 weeks data in parallel with 30 weeks data of 

learning designs. It should be noted that these crude measurements 

of VLE only represented the average time a student spent on VLE 

platform, not the actual studying time, as this can be affected by 

unobservable factors, such as when students study offline, or using 

non-OU systems such as Facebook (which the OU does not 

monitor).        

3.3 Data analysis 

3.3.1 Visualization of learning designs over time 
We used Tableau to visualize the LD of 42 modules over 30 weeks 

of study time. We displayed both static and dynamic 

representations of LD of which the former aggregated all modules, 

while the latter was per module basis.  

3.3.2 Social network analysis 
In line with [24], we used SNA to study the relationships among 

learning activities as this technique enables us to quantify and 

visualize the interactions and connections between the seven 

learning activities. The LD dataset was a weighted two-mode 

network as it consisted of different learning activities across several 

weeks as illustrated in Figure 2 below. Since we are primarily 

interested in the relationships among learning activities, the dataset 

was transformed to a one-mode network in line with Hora and 

Ferrare [24].  

 

Figure 2: Weighted two-mode network of module X across the 

first five weeks 

Firstly, two learning activities (blue nodes) become connected if 

they were present in the same week (red nodes). Since we captured 

how much time students were expected to spent on each LD each 

week, the weights of the two learning activities had directed 

towards identical weeks could also be measured. In this type of 

projected network, the weight of a tie from one LD to another was 

not necessarily equal to the weight of the reverse. For example, in 

Figure 2, if 2.8 hours were spent on assimilative activities and 1.8 

were spent on assessment activities in the same week, then the 

weight from assimilative to assessment is recorded as 2.8 and the 

weight of the reverse is recorded as 1.8.  

Second, the weight of each tie was discounted for the number of 

learning activities in the same week [38]. It can be argued that the 

tie between two learning activities is weaker when there are more 

learning activities that are present in the same week. This can be 

generalized as follows: 



??? =?
???

?? ? 1?
 

where wij is the weight between LD i and LD j, and Np is the number 

of learning activities in week p. 

After transforming the dataset, we used the Netdraw function of 

UCINET [5] to visualize the co-occurrences between each pair of 

learning activities across all weeks. The nodes represent the 

different learning activities. The tie represents the co-occurrence of 

two learning activities in the same week. The thickness of the line 

reflects the strength of the ties. Thus, the thicker the line, the higher 

the weights of the tie between two learning activities, which was 

also represented by the numbers attached along the line.  

Finally, in line with Hora and Ferrare [24] configurations of co-

occurring learning activities within each module were used to 

determine the repertoires of practice. These were computed as the 

combinations of learning activities that occurred most frequently.  

3.3.3 Panel data analysis 
In preparation for the analysis, the two  datasets on LD and VLE 

were transformed from wide to long format. Additional identifiers 

were generated as the combination of course code, presentation, 

and week. The next step was to merge this dataset according to 

these new identifiers. Next, a Hausman test was used to 

differentiate between fixed effects and random effects model. It 

tests whether the coefficients estimated by the efficient random 

effects estimator are the same as the ones estimated by the 

consistent fixed effects estimator [22]. Our result supported the 

assumption of correlation between observation’s errors and 

predictors, hence, fixed effects model was used as it removes the 

effect of time-invariant characteristics to assess the net effect of the 

predictors on the outcome. Our analysis was done in Stata.  

4. RESULTS 

4.1 How are learning designs configured 
across modules over time? 
Figure 3 illustrates the average time students were expected to 

spend per module (in hours) on different learning activities over 30 

weeks. 

At a glance, we can see that there were a lot of fluctuations in 

learning activities over time, which indicated a dynamic usage of 

LD from teachers (Figure 3). Aligned with previous findings [42, 

44, 45, 47, 48], assimilative activities accounted for the majority of 

learning time (M=3.9, SD=3.4), which were followed by 

assessment activities (M=2.0, SD=3.5). In other words, students 

were expected to spend around 6 hours per week on “traditional” 

learning activities of reading and watching materials, and 

completing formative and summative assessments. Productive 

activities were also adopted constantly over time (M=1.3, SD=1.7). 

Communication, experiential, interactive, and finding information 

activities were underused most of the time. Interestingly, 

assessment and assimilative activities followed opposite paths in 

which more assimilative activities were used at the beginning of a 

module whereas more assessments were used toward the end. There 

seems to be no correlation of any LD with the total time spent 

indicating that there is no systematic bias in favoring a particular 

learning design.  

After capturing the dynamic picture of LD over time, we took a 

further step to examine how different learning activities are 

configured across different modules. Due to the limited space, we 

only reported three exemplar modules across three disciplines with 

a variety of configurations and patterns of learning activities 

(Figure 4).  

 

 

Figure 3: Learning designs of 42 modules over 30 weeks in 2014 & 2015 

 



 

Figure 4: Feature modules 

A closer look at each module revealed a diversity of combinations 

of LD over time. Module 1 in Arts & Social Science confirmed the 

dominance of assimilative (M=4.9, SD= 4.3) and assessment 

activities (M=1.4, SD=2.8). Remarkably, there was a surge in the 

assimilative activities up to 19.6 hours in week 10 and 14.4 hours 

in week 11. On the other hand, module 2 in Business and Law 

represented a more balanced learning design. The total workload of 

15 hours each week remained constant throughout 30 weeks with 

the exception of week 30 where students were expected to spend 

more time on assessment activities. There was an assessment 

almost every 3 weeks of study. Students in this module engaged in 

multiple learning activities: assimilative (M=4.3, SD=1.5), 

communication (M=1.6, SD=0.7), finding information (M=1.6, 

SD=0.7), productive (M=2.4, SD=1.04), experiential (M=2.1, 

SD=1.4), and assessment (M=3.7, SD=5.9). Finally, module 3 used 

only three types of LD over time: assimilative, assessment, and 

productive. The workload of module 3 in Languages and Education 

stayed relatively constant over time, with the majority of studying 

time are dedicated to productive activities.  

4.2 How do different learning activities 
interact with each other across modules over 

time? 

  
Figure 5: Social network analysis of learning designs 



Our social network analysis reveals a variety of combinations of 

LD across modules. Again, due to the limited space, we only 

reported the three aforementioned exemplar modules (Figure 5).  

In Module 1 in Art & Social Sciences, assimilative activities 

displayed strong connections with productive and finding 

information while there were weak links among other learning 

activities. Furthermore, communication acted as a gatekeeper 

between experiential and other learning activities which implied 

communication was the necessary condition for the existence of 

experiential activities. This module confirmed the dominance of 

assimilative design as previously illustrated in Figure 3. The 

density of the network was 64% with 13 ties in total. The average 

distance among reachable pairs was 1.306. The most frequently 

used repertoire of practice was assimilative, information, and 

productive (38.7% of the time). The relationships among learning 

activities in module 2 were more equally distributed in the network, 

with the exception of interactive.  

Module 2 in Business and Law demonstrated a repertoire of 

practice that frequently used assimilative, information, 

communication, The network density of this module was 67% with 

14 ties in total. The average distance among pairs was 1.2. 

Assimilative and assessment shared the strongest connection. 

Again, communication played a gatekeeping role in this module in 

which it facilitated the use of interactive activities, experiential, and 

productive activities (70% of the time).  

Module 3 in Languages and Education exhibited a unique setting 

which consisted of only three learning activities: assimilative, 

assessment, and productive. The network density was 14.3% with 

3 ties, and the average distance was 1. Evidently, the most 

frequently used repertoire of practice in this module was 

assimilative, assessment, and productive (90% of the time).   

4.3 How do learning designs affect VLE 
behavior over time? 
In this section, we examined how different learning activities 

influence average time spent on VLE per visit (Table 2), and on 

VLE per week (Table 3).  

For each predictors, four models were applied. First, we ran normal 

OLS regression model. Second, we used fixed effect model to the 

control of the unobserved heterogeneity of time. Third, we 

controlled for the fixed effect across modules. Finally, we 

controlled for the fixed effects of both time and modules. The 

baseline for LD is the assimilative type. Thus, all the following 

results should be interpreted relatively to the module with the 

assimilative design. Variance inflation factor (VIF) was computed 

after each model to check for multicollinearity. The result indicated 

there were no significant correlations among the independent 

variables, in other words, there was no overlap of measurements 

among seven learning activities. Unstandardized coefficients were 

reported because all the explanatory variables were measured in the 

same unit (hours). Thus, it is more informative to report the original 

metrics.  In the first and second model (Table 2), the effects of each 

independent variable remained relatively the same . It implied that 

there were no heterogeneity overtime. Assessment, 

communication, and productive were positive and significantly 

associated with VLE per visit. However, the predictive power of 

these models was relatively weak, which only explained 7%-8% of 

the variability. In contrast, the predictive power of LD on VLE 

increased noticeably when taking into account the differences 

across module (model 3 & 4). The effect of assessment became 

smaller and insignificant.  

Table 2: Panel data analysis of the effect of learning design on 

the average time spent on VLE per visit 

 (1) (2) (3) (4) 

VLE_per_visit OLS FE_ 

week 

FE_ 

module 

FE_module

_week 

     

Assessment .51*** .51*** .03 .04 

 (.08) (.08) (.06) (.06) 

Information .25 .32 -.05 .007 

 (.35) (.35) (.24) (.24) 

Communication 2.16*** 2.16*** .69*** .68*** 

 (.35) (.35) (.26) (.26) 

Productive .49*** .52*** -.34*** -.32** 

 (.16) (.16) (.13) (.13) 

Experiential -.13 -.13 -.55 -.53 

 (.53) (.53) (.37) (.36) 

Interactive .50 .48 .17 .14 

 (.34) (.34) (.24) (.24) 

Constant 20.19*** 20.11*** 22.74*** 19.29*** 

 (.40) (0.40) (0.31) (1.28) 

     

Observations 1,114 1,114 1,114 1,114 

Adjusted 

R-squared 

0.07 0.08 0.60 0.63 

Unstandardized betas, Standard errors in parentheses  

*** p<0.01, ** p<0.05, * p<0.1 

The effect of communication also decreased to 0.69, which implied 

that on average an extra hour spent on communication activities is 

associated with 0.69 minutes increase in the time spent per visit on 

VLE. In contrast to model 1 & 2, productive activities negatively 

impacted VLE per visit. On average, an additional hour spent on 

productive activities was associated with 0.34 minutes less in time 

spent on VLE per visit. By controlling of the unobservable 

heterogeneity across modules, LD can explain up to 60% of the 

variability in time spent on VLE per visit. Our results validated the 

importance of taking into account the learning context of each 

module. 

A similar trend was observed in predicting the average time spent 

on VLE per week in Table 3. In model 1 & 2, assessment, 

communication, and interactive were positive and significantly 

related with VLE per week. In model 3 & 4, the effect of assessment 

and communication became smaller and insignificant. Productive 

activities were negatively associated with VLE per week. Students 

who spent one extra hour spent on productive activities on average 

spent 4.42 minutes less in VLE. The positive effect of interactive 

activities weakened. An additional hour spent in interactive 

activities was associated with 6.17 minutes increase in VLE. 

Moreover, more time spent on experiential was associated with less 

time on VLE per week. An extra hour spent on experiential 

activities was associated with 8.43 minutes decrease in VLE. The 

predictive power of LD on VLE per week increased largely when 

taking into account the differences between modules (Adj-R2 = 

40%). Similar models were run again with assessment as the 

reference level, however, there was no significant effect of 

assimilative activities on both VLE per week and VLE per visit.  



Table 3: Panel data analysis of the effect of learning design on 

the average time spent on VLE per week 

 (1) (2) (3) (4) 

VLE_per_week OLS FE_ 

week 

FE_ 

module 

FE_module

_week 

     

Assessment 2.96*** 2.35*** -.49 -.98 

 (.79) (.83) (.74) (.75) 

Information 4.442 5.192 .30 .72 

 (3.60) (3.60) (3.10) (3.04) 

Communication 16.53*** 16.40*** 4.32 3.79 

 (3.60) (3.57) (3.39) (3.31) 

Productive .74 1.73 -5.63*** -4.42*** 

 (1.61) (1.60) (1.66) (1.64) 

Experiential -4.14 -3.92 -8.81* -8.43* 

 (5.44) (5.40) (4.77) (4.67) 

Interactive 12.02*** 12.44*** 6.03* 6.17** 

 (3.50) (3.47) (3.13) (3.06) 

Constant 102.2*** 101.8*** 122.7*** 99.40*** 

 (4.12) (4.06) (3.98) (16.40) 

     

Observations 1,114 1,114 1,114 1,114 

Adjusted  

R-squared 

0.04 0.08 0.36 0.40 

Unstandardized betas, Standard errors in parentheses 

*** p<0.01, ** p<0.05, * p<0.1 

 

To sum up, by taking the differences between modules, LD 

activities were strong predictors of the average time spent on VLE 

platform. In particular, students spent less time on VLE if they were 

required to do more productive and experiential activities while the 

opposite is true when they engaged in communication and 

interactive learning activities.      

5. DISCUSSION  

5.1 Implications 
Firstly, our longitudinal visualization at a static level of LD 

suggested that teachers designed learning activities differently over 

time. In line with our previous work [45, 48], assimilative and 

assessment activities accounted for the majority of learning 

activities followed by productive activities, whereas experiential, 

interactive, communication, and finding information were less 

common. In line with basic principles of LD, more assimilative 

activities were employed at the beginning of the course: students 

were required to acquire and obtain new knowledge and 

information about a particular module, such as reading course 

syllabus, watching the introductory lecture, and so forth. Towards 

the end of a module, fewer assimilative activities were used, 

whereas more formative and summative assessments were made to 

evaluate the understanding of learners [6]. Multiple peaks in 

assessment activities also indicated that the learning process was 

continuously assessed over time, rather than relying solely on a 

large final exam. Continuous formative and summative assessment 

plays a very important role in distance courses, since small and 

constant assessments can both motivate learners and provide an 

accurate evaluation of their understanding over time, in order to 

intervene in time [46].  

Secondly, our dynamic inspection on the LD of each module over 

time revealed that the use of LD varied considerably across 

modules and disciplines. A balanced approach of LD can be seen 

in module 2 in the Business and Law faculty, in which it consists 

of six out of seven LDs with equally distributed workloads for each 

activity and each week. When there was an assessment, the 

workload on other activities were reduced to avoid the 

overwhelming workload on students (see Figure 4). This is a very 

important remark for teachers and course designers since learners 

can be sensitive to peaks and troughs in workload, which in turn 

may damage their learning experience. Such example could be 

observed in module 1 in Art and Social Science discipline, in which 

there was a huge surge in the workload in week 10, which was more 

than 20 hours for all learning activities, compared with the average 

of 9 hours per week. Another example of a potentially unbalanced 

design was module 3 in the Faculty of Education and Language 

studies, which only used three types of LD throughout the course 

(i.e., assimilative, assessment, and productive). Evidently, we do 

not judge which design is good or bad, but this dynamic 

visualization of LD across modules can help educators reflect on 

their LD to anticipate whether their design best serves the learning 

objectives and learner experience.  

Thirdly, using social network analysis we were able to observe how 

different learning activities were connected to each other. Our 

results suggested that if we concentrate on a single component of 

learning design in isolation, we might omit the complexity and 

critical features of the instructional dynamic. By adopting the view 

of system of practice [21], our empirical evidence strengthened the 

view of Hora and Ferrare [24] which indicated that teachers 

perceive certain learning activities as being meant for each other 

(i.e. assimilative & productive, communication & experiential) and 

these perceptions varies across disciplines. Interestingly, even 

though certain disciplines exhibited favorable practice towards a 

particular learning activity, each module utilized it with other 

learning activities in different ways. For example, it is apparent that 

assimilative activities were the most common learning activities in 

all three exemplar modules. However, the repertoire of practice in 

module 1 (assimilative, information, and productive) was different 

from module 2’s (assimilative, information, communication, 

experiential, and productive) and module 3’s (assimilative, 

assessment, and productive). Overall, learning activities are best 

viewed in relation to one another in multiple dimensions 

throughout time. 

Our final takeaway is by taking into account the context of learning 

across 38 modules, learning designs were strong predictors of the 

time spent on VLE platform. Even though significant effects of 

certain learning activities on VLE activities were identified in our 

analysis, we advise readers to interpret them with cautions. As 

discussed above, learning activities should be perceived in relation 

with one another rather than in isolation. For example, our results 

showed that students spent less time on VLE when they engaged in 

productive activities. However, this did not imply that by simply 

cutting down productive activities, students will be more likely to 

engage. It is because each module employed productive activities 

in relation with different learning activities in different ways at 

different points in time. Students who engaged in productive 

activities which include building, constructing, and creating a 

‘knowledge nugget’ may work offline. If they are required to share 

these ‘knowledge nuggets’ with other students then a rise in 

communication activities is expected as they post their thoughts 

and creations to the discussion forum.  

From a researcher’s perspective, by acknowledging the distinctive 

features of each discipline, we can considerably increase the 



accuracy of predicting student engagement in VLE. From a 

practitioner’s point of view, our results highlighted the need to 

appropriately balance learning design that fit with specific learning 

outcomes and disciplinary practice.  

5.2 Limitations 
First, the measurements of the average time spent on VLE were 

crude indicators. Capturing the time spent on actual learning 

activities while control for which VLE activities are dedicated to 

which learning activities, and other unobservable non-studying 

activities is difficult. This problem has also been addressed in 

LAK15 in which Joksimovi?, Gaševi?, Loughin, Kovanovi? and 

Hatala [26] confirmed that the choice of the time-on-task 

estimations (assignment, reading, discussion, adding a post, or 

updating post) played an important role in the overall model fit and 

subsequent model interpretation.  

Second, in a time-series model, time lag issue may occur [12]. For 

example, students who anticipated an assessment in week 10 would 

start preparing in week 9. Thus, assessment should be discounted 

one week in order to accurately reflect its effect on VLE activities. 

However, determining time lag is challenging given the variances 

of LD and the inconsistencies across modules.   

Third, the LD taxonomy has certain limitations. On one hand, it 

could be over-simplify the actual LD since there are sub-categories 

in each types of learning activities (i.e. there are many kinds of 

assessment such as tutor-marked assessment, and computer-based 

assessment). On the other hand, some learning activities are overlap 

between different categories (i.e. watching a lecture while replying 

to the chat could be both assimilative and communication).  

Finally, at the time this paper was written, the OU does not model 

learning designs across a programme or curriculum perspective. 

Therefore, we are limited to what we can actually conceptually 

define and empirically test LD at a program level.  

6. CONCLUSION AND FUTURE WORK 
This study investigated how learning designs are configured over 

time and its effect on student activities in VLE by analyzing 38 

modules over 30 weeks at the Open University UK. By visualizing 

how learning design changed over time, teachers can explicitly 

reflect on their practice as well as compare and contrast with others. 

Using social network analysis, we illustrated how different learning 

activities interact with each other and which repertoire of practice 

was frequently adopted. Our results indicated a wide variance in the 

number of learning activities was used as well as the workload 

balance across modules. When the workload is unbalanced 

according to the OULDI framework, teachers can anticipate 

potential problems in their design to make informed interventions.  

Moreover, our panel data analysis on the effect of learning designs 

on VLE activities indicated that by controlling for the differences 

across modules, learning designs proved to be strong indicators of 

student activities. In particular, communication and interactive 

activities had a positive effect on VLE engagement whereas 

productive and experiential were associated with lower levels of 

VLE activities. Our findings reinforced and provided new 

empirical evidence of the importance of understanding pedagogical 

context in LA in order to translate the findings to sensible actions.  

Our research contributes to the existing literature in LA & LD by 

providing visualizations of elements of LD, and empirically 

examining the actual student learning behaviors in relation with the 

teachers’ pedagogical intentions. By analyzing the actual learning 

behaviors of students across a large number of online modules, our 

work addressed the issue of ecological validity of experimental 

studies in LD while enhanced the external validity of the findings. 

By connecting the LD (input) with students’ learning behaviors 

(progress), our work also supports previous LA findings which 

were mainly based on students’ learning behavior (progress) and 

learning outcomes (output).  

Future scholars are recommended to consider the inter-

relationships between learning activities in doing research on. For 

instance, social network metrics of LD can be incorporated in the 

prediction models. When more fine-grained data (i.e. how much 

time students are expected to spend on writing essays, watching 

video, listening to audio, etc.) become available, researchers can 

unfold the complexity of LD in a more specific manner. Multi-level 

analysis can be conducted on a large scale study to account for the 

heterogeneity across faculties, levels of study, modules, and 

configurations of learning design.      

7. REFERENCES 
[1] AUTCLearningDesign Predict–Observe–Explain: Designer’s Voice–

Context. Retrieved 9 Jan, 2016, from 
http://www.learningdesigns.uow.edu.au/exemplars/info/LD44/more/03Co

ntext.html 

[2] Bakharia, A. and Dawson, S. 2011. SNAPP: a bird's-eye view of 
temporal participant interaction. In Proceedings of the Proceedings of the 

1st international conference on learning analytics and knowledge, 2011. 

ACM, New York, NY, USA, 2011, 168-173. 

[3] Bakharia, A., Corrin, L., de Barba, P., Kennedy, G., Gaševi?, D., 

Mulder, R., Williams, D., Dawson, S. and Lockyer, L. 2016. A conceptual 
framework linking learning design with learning analytics. In Proceedings 

of the Sixth International Conference on Learning Analytics & 

Knowledge, 2016. ACM, New York, NY, USA, 2016, 329-338. 

[4] Bennett, S., Agostinho, S. and Lockyer, L. 2015. Technology tools to 

support learning design: Implications derived from an investigation of 
university teachers' design practices. Computers & Education, 81. 211-

220. 

[5] Borgatti, S. P., Everett, M. G. and Freeman, L. C. 2002. Ucinet for 
Windows: Software for social network analysis. 

[6] Boud, D. and Falchikov, N. 2006. Aligning assessment with long?term 

learning. Assessment & Evaluation in Higher Education, 31 (4). 399-413. 

[7] Buckingham Shum, S. and Crick, R. D. 2012. Learning dispositions 

and transferable competencies: pedagogy, modelling and learning 
analytics. In Proceedings of the 2nd International Conference on Learning 

Analytics and Knowledge, 2012. ACM, New York, NY, USA, 2012, 92-

101. 

[8] Cela, K. L., Sicilia, M. Á. and Sánchez, S. 2015. Social network 

analysis in e-learning environments: A Preliminary systematic review. 
Educational Psychology Review, 27 (1). 219-246. 

[9] Chen, B., Chen, X. and Xing, W. 2015. Twitter archeology of learning 

analytics and knowledge conferences. In Proceedings of the Fifth 
International Conference on Learning Analytics And Knowledge, 2015. 

ACM, New York, NY, USA, 2015, 340-349. 

[10] Clow, D. 2013. An overview of learning analytics. Teaching in 

Higher Education, 18 (6). 683-695. 

[11] Conole, G. Designing for learning in an open world. Springer 
Science & Business Media, 2012 

[12] Cromwell, J. B. Multivariate tests for time series models. Sage, 1994 

[13] Cross, S., Galley, R., Brasher, A. and Weller, M. Final Project Report 

of the OULDI-JISC Project: Challenge and Change in Curriculum Design 
Process, Communities, Visualisation and Practice. York: JISC. Retrieved 

October 16th, 2016, from http://www.open.ac.uk/blogs/OULDI/wp-

content/uploads/2010/11/OULDI_Final_Report_Final.pdf 

[14] Dalziel, J. 2003. Implementing learning design: The learning activity 

management system (LAMS). In Proceedings of the 20th Annual 

Conference of the Australian Society for Computers in Learning in 
Tertiary Education, Adelaide, 2003. University of Adelaide, 593-596. 

[15] Dalziel, J. Learning design: Conceptualizing a framework for 
teaching and learning online. Routledge, New York, NY, USA, 2015 



[16] Dalziel, J., Conole, G., Wills, S., Walker, S., Bennett, S., Dobozy, E., 

Cameron, L., Badilescu-Buga, E. and Bower, M. 2016. The Larnaca 
declaration on learning design. Journal of Interactive Media in Education, 

2016 (1). 1-24. 

[17] Ferguson, R. 2012. Learning analytics: drivers, developments and 

challenges. International Journal of Technology Enhanced Learning, 4 (5-

6). 304-317. 

[18] Gaševi?, D., Dawson, S. and Siemens, G. 2015. Let’s not forget: 

Learning analytics are about learning. TechTrends, 59 (1). 64-71. 

[19] Gaševi?, D., Dawson, S., Rogers, T. and Gasevic, D. 2016. Learning 

analytics should not promote one size fits all: The effects of instructional 

conditions in predicting academic success. The Internet and Higher 
Education, 28. 68-84. 

[20] Goodyear, P. 2015. Teaching as design. HERDSA Review of Higher 

Education, 2. 27-50. 

[21] Halverson, R. R. 2003. Systems of practice: How leaders use artifacts 

to create professional community in schools. Education Policy Analysis 
Archives, 11 (37). 1-35. 

[22] Hausman, J. A. 1978. Specification tests in econometrics. 

Econometrica: Journal of the Econometric Society, 46 (6). 1251-1271. 

[23] Hecking, T., Chounta, I.-A. and Hoppe, H. U. 2016. Investigating 

social and semantic user roles in MOOC discussion forums. In 
Proceedings of the Proceedings of the Sixth International Conference on 

Learning Analytics & Knowledge, 2016. ACM, New York, NY, USA, 

2016, 198-207. 

[24] Hora, M. T. and Ferrare, J. J. 2013. Instructional systems of practice: 

A multidimensional analysis of math and science undergraduate course 
planning and classroom teaching. Journal of the Learning Sciences, 22 

(2). 212-257. 

[25] Joksimovi?, S., Dowell, N., Skrypnyk, O., Kovanovi?, V., Gaševi?, 
D., Dawson, S. and Graesser, A. C. 2015. How do you connect?: Analysis 

of social capital accumulation in connectivist MOOCs. In Proceedings of 

the Fifth International Conference on Learning Analytics And Knowledge, 
2015. ACM, New York, NY, USA, 2015, 64-68. 

[26] Joksimovi?, S., Gaševi?, D., Loughin, T. M., Kovanovi?, V. and 
Hatala, M. 2015. Learning at distance: Effects of interaction traces on 

academic achievement. Computers & Education, 87. 204-217. 

[27] Joksimovi?, S., Kovanovi?, V., Jovanovi?, J., Zouaq, A., Gaševi?, D. 
and Hatala, M. 2015. What do cMOOC participants talk about in social 

media?: a topic analysis of discourse in a cMOOC. In Proceedings of the 

Fifth International Conference on Learning Analytics And Knowledge, 
2015. ACM, New York, NY, USA, 2015, 156-165. 

[28] Kirschner, P. Keynote: Learning Analytics: Utopia or Dystopia. 
Retrieved October 10th, 2016, from http://lak16.solaresearch.org/wp-

content/uploads/2016/05/lak16keynotelearninganalytics-utopiaofdystopia-

160428103734.pdf 

[29] Koper, R. and Manderveld, J. 2004. Educational modelling language: 

modelling reusable, interoperable, rich and personalised units of learning. 

British Journal of Educational Technology, 35 (5). 537-551. 

[30] Kuzilek, J., Hlosta, M., Herrmannova, D., Zdrahal, Z. and Wolff, A. 

2015. OU Analyse: analysing at-risk students at The Open University. 
Learning Analytics Review. 1-16. 

[31] Laurillard, D. and McAndrew, P. 2001. Virtual Teaching Tools: 

Bringing academics closer to the design of e-learning. In Proceedings of 

the Third International Conference on Networked Learning, Sheffield, 

England, 2001, 11-16. 

[32] Lockyer, L., Bennett, S., Agostinho, S., Harper, B., Lockyer, L., 

Bennett, S., Agostinho, S. and Harper, B. Handbook of Research on 

Learning Design and Learning Objects: Issues, Applications and 
Technologies. IGI Global, New York, NY, USA, 2008 

[33] Lockyer, L. and Dawson, S. 2011. Learning designs and learning 
analytics. In Proceedings of the 1st international conference on learning 

analytics and knowledge, 2011. ACM, New York, NY, USA, 2011, 153-

156. 

[34] Lockyer, L., Heathcote, E. and Dawson, S. 2013. Informing 

pedagogical action: Aligning learning analytics with learning design. 
American Behavioral Scientist, 57 (10). 1439 - 1459. 

[35] Macfadyen, L. P. and Dawson, S. 2010. Mining LMS data to develop 
an “early warning system” for educators: A proof of concept. Computers 

& education, 54 (2). 588-599. 

[36] MacLean, P. and Scott, B. 2011. Competencies for learning design: A 
review of the literature and a proposed framework. British Journal of 

Educational Technology, 42 (4). 557-572. 

[37] Mor, Y., Ferguson, R. and Wasson, B. 2015. Editorial: Learning 

design, teacher inquiry into student learning and learning analytics: A call 

for action. British Journal of Educational Technology, 46 (2). 221-229. 

[38] Newman, M. E. 2001. Scientific collaboration networks. II. Shortest 

paths, weighted networks, and centrality. Physical review E, 64 (1). 

016132. 

[39] Nguyen, Q., Tempelaar, D. T., Rienties, B. and Giesbers, B. 2016. 

What learning analytics based prediction models tell us about feedback 
preferences of students. Quarterly Review of Distance Education, 17 (3). 

13-33. 

[40] Persico, D. and Pozzi, F. 2015. Informing learning design with 
learning analytics to improve teacher inquiry. British Journal of 

Educational Technology, 46 (2). 230-248. 

[41] Poquet, O. and Dawson, S. 2016. Untangling MOOC learner 

networks. In Proceedings of the Sixth International Conference on 

Learning Analytics & Knowledge, 2016. ACM, New York, NY, USA, 
2016, 208-212. 

[42] Rienties, B., Toetenel, L. and Bryan, A. 2015. Scaling up learning 
design: impact of learning design activities on lms behavior and 

performance. In Proceedings of the Fifth International Conference on 

Learning Analytics And Knowledge, 2015. ACM, New York, NY, USA, 
2015, 315-319. 

[43] Rienties, B., Boroowa, A., Cross, S., Kubiak, C., Mayles, K. and 

Murphy, S. 2016. Analytics4Action Evaluation Framework: A Review of 
Evidence-Based Learning Analytics Interventions at the Open University 

UK. Journal of Interactive Media in Education, 2016 (1). 

[44] Rienties, B. and Toetenel, L. 2016. The impact of 151 learning 

designs on student satisfaction and performance: social learning 

(analytics) matters. In Proceedings of the Sixth International Conference 
on Learning Analytics & Knowledge, Edinburgh, United Kingdom, 2016. 

ACM, New York, NY, USA, 2016, 339-343. 

[45] Rienties, B. and Toetenel, L. 2016. The impact of learning design on 
student behaviour, satisfaction and performance: A cross-institutional 

comparison across 151 modules. Computers in Human Behavior, 60. 333-

341. 

[46] Tempelaar, D., Rienties, B. and Giesbers, B. 2015. In search for the 

most informative data for feedback generation: Learning Analytics in a 
data-rich context. Computers in Human Behavior, 47. 157-167. 

[47] Toetenel, L. and Rienties, B. 2016. Learning Design–creative design 

to visualise learning activities. Open Learning: The Journal of Open, 
Distance and e-learning, 31 (3). 233-244. 

[48] Toetenel, L. and Rienties, B. 2016. Analysing 157 learning designs 
using learning analytic approaches as a means to evaluate the impact of 

pedagogical decision making. British Journal of Educational Technology. 

[49] Whitelock, D., Twiner, A., Richardson, J. T., Field, D. and Pulman, 

S. 2015. OpenEssayist: a supply and demand learning analytics tool for 

drafting academic essays. In Proceedings of the Fifth International 
Conference on Learning Analytics And Knowledge, 2015. ACM, 208-212. 

[50] Wise, A. F. and Shaffer, D. W. 2015. Why theory matters more than 

ever in the age of big data. Journal of Learning Analytics, 2 (2). 5-13. 

[51] Wise, A. F., Cui, Y., Jin, W. and Vytasek, J. 2017. Mining for gold: 

Identifying content-related MOOC discussion threads across domains 
through linguistic modeling. The Internet and Higher Education, 32. 11-28. 

[52] Zhu, M. and Feng, G. 2015. An exploratory study using social 

network analysis to model eye movements in mathematics problem 
solving. In Proceedings of the Fifth International Conference on Learning 

Analytics And Knowledge, 2015. ACM, 383-387. 



