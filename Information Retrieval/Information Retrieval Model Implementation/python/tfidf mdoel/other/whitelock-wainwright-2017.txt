
1 

 

What do students want? Towards an instrument for 

students’ evaluation of quality of learning analytics 

services
Alexander Whitelock-Wainwright 

School of Psychology 

The University of Liverpool 

Liverpool, UK 

A.Wainwright@liverpool.ac.uk 

Dragan Gaševi? 
Moray House School of Education and  

School of Informatics 

The University of Edinburgh 

Edinburgh, UK 

dragan.gasevic@ed.ac.uk 

Ricardo Tejeiro 
School of Psychology 

The University of Liverpool 

Liverpool, UK 

tejeiro@liverpool.ac.uk 

 

ABSTRACT 
Quality assurance in any organization is important for ensuring that 

service users are satisfied with the service offered. For higher edu-

cation institutes, the use of service quality measures allows for 

ideological gaps to be both identified and resolved. The learning 

analytic community, however, has rarely addressed the concept of 

service quality. A potential outcome of this is the provision of a 

learning analytics service that only meets the expectations of cer-

tain stakeholders (e.g., managers), whilst overlooking those who 

are most important (e.g., students). In order to resolve this issue, 

we outline a framework and our current progress towards develop-

ing a scale to assess student expectations and perceptions of 

learning analytics as a service. 

CCS Concepts 

• Human-centered computing ? Accessibility design and 

evaluation methods  

Keywords 

Service quality; action research; learning analytics 

1. INTRODUCTION 
In recent years, Learning Analytics (LA) movements within the 

domain of higher education have been growing. As a result, stu-

dents are becoming immersed in a new culture where greater 

feedback and insight into their learning are readily available. From 

small-scale LA implementations designed to monitor fluctuations 

in emotions [29], to large-scale LA initiatives at institutions such 

as the Open University UK that aim to improve retention rates [5], 

there is an underlying commitment to “optimize learning and the 

environments in which it occurs” [1]. In other words, LA can be 

conceptualized as a service that aims to provide students with edu-

cational support during their learning. Thus, by thinking of LA in 

this way, it thereby creates a commitment for higher education 

institutes (HEIs), as service providers, to meet student 

expectations. 

Up until now, however, any research concerned with student 

expectations has focused mainly upon ethical and legal issues [8, 

12, 14, 28, 31, 40]. Nevertheless, these authors have recognized 

important issues that can facilitate the development of an evalua-

tion framework for future LA services. On the other hand, a 

student’s expectation of a LA service will not simply relate to 

whether ethical practice is followed. Rather, ethical and privacy 

issues will only reflect a handful of expectations, in what could be 

considered a plethora of expectations that students’ will hold to-

wards LA as a service. The latter is exemplified by [40], who stress 

the challenge of creating LA outputs that are meaningful to the 

student population. If an issue such as the latter arose in a large-

scale implementation of LA, how would this be fed back so that the 

system could be redesigned to meet students’ expectations of and 

improve their experience with LA? Without a resolution, this 

missing link between the running of a LA service and the 

incorporation of student feedback could jeopardize what could 

effectively be quality analytics. 

It would be an oversight to suggest that evaluation procedures 

have not received sufficient attention in LA research [3, 27]. 

Existing studies have sought to acquire feedback on LA tools that 

have been introduced. However, for a continuous improvement of 

institutional LA services, a broader approach needs to be 

undertaken that explores student expectations and experiences with 

the LA services on offer. In so doing, discrepancies in student 

expectations and actual service provision can be readily 

acknowledged and solutions introduced, which should lead to an 

improved quality of service for students. 

In this short paper, we intend to outline our on-going work 

towards the development of a scale that aims to assess students’ 

expectations and perceptions of LA services. Section one briefly 

discusses the topic of expectations and service quality. Section two 

focuses on the current gap within the LA literature of only a limited 

amount of attention being paid towards service evaluation. In sec-

tion three, we introduce our hypothesized model of how service 

quality can be included within the continual evolution and devel-

opment of LA systems. Finally, section four provides readers with 

a summary of our progress towards creating a LA service quality 

scale. 

2. SERVICE QUALITY 
Judgments of service quality are believed to be based on a user’s 

subjective assessment of the extent to which their needs or expecta-

tions were met [24, 41]. As a result, organizations become reliant 

upon providing a good quality of service, as it can be the pivotal 

factor towards enticing service users to utilize their service over 

and above those offered by competitors [26]. Furthermore, these 

Permission to make digital or hard copies of all or part of this work for 

personal or classroom use is granted without fee provided that copies are 

not made or distributed for profit or commercial advantage and that 

copies bear this notice and the full citation on the first page. Copyrights 

for components of this work owned by others than ACM must be 

honored. Abstracting with credit is permitted. To copy otherwise, or 

republish, to post on servers or to redistribute to lists, requires prior 

specific permission and/or a fee. Request permissions from 

Permissions@acm.org. 

LAK '17, March 13-17, 2017, Vancouver, BC, Canada  

© 2017 ACM. ISBN 978-1-4503-4870-6/17/03…$15.00 

DOI: http://dx.doi.org/10.1145/3027385.3027419 

 



2 

 

user evaluations of service quality are not only constrained to face-

to-face organizational settings, but extend to online service provi-

sion [25, 41], and even the online or offline services offered by 

HEIs [2, 16]. 

Student evaluations of teaching and general student experi-

ence surveys are commonly used as approaches to collecting 

students’ feedback on quality of service in higher education [33]. 

Although such surveys are commonly tailored for individual insti-

tutions, in some countries there are national initiatives. For 

example, in the UK, the National Student Survey (NSS) has be-

come an important measure of service quality that allows final year 

undergraduate students to provide feedback as to their overall satis-

faction with a course [13]. In conjunction with helping prospective 

students make informed decisions about where they choose to 

study, these results also assist HEIs in bringing about positive 

changes. Thus, HEIs are thereby engaging in the process of quality 

assurance to make sure that standards in their provision of educa-

tion are continually met. 

The process of exploring students’ expectations and percep-

tions of HEIs services is important. Without such procedures in 

place, there is a possibility that an ideological gap could persist 

[18]. This particular gap can be thought as a clear separation be-

tween student expectations of the university’s service and what the 

HEI believe the service they are providing should be [2, 19]. With-

out identification or resolution of such gaps, it can be readily 

assumed that dissatisfaction with the provided service will entail.  

Going beyond using only institutional and national surveys to 

understand student opinions of the service delivered by a HEI, 

researchers have sought to explore potential discrepancies between 

stakeholders by using alternative measures. A popular approach 

taken has been the use of SERVQUAL [26], which measures ser-

vice quality across five dimensions of Tangibles, Reliability, 

Responsiveness, Assurance, and Empathy. This scale can, depend-

ing upon the time between assessments [20], explore expectations 

and perceptions of a service [42]. Perceived quality is then assumed 

to be the difference between expectation and perception scores 

[26]. Alternatively, the perception-only scale of SERVPERF has 

shown to capture more of the variance in service quality, whilst 

also supporting the view of service quality as being an attitude [9]. 

Commonly, in practice, organizations apply the expectation discon-

firmation theory (EDT) to assess user satisfaction [21, 38], through 

the use of SERVQUAL. In other words, a user’s level and direction 

of satisfaction with a service is based upon whether the perfor-

mance aligned with their initial expectations or not [22].  

When applied in HEI settings, both the aforementioned varia-

tions in scales (i.e., expectations and perceptions together, or 

perceptions alone) have shown utility in measuring service quality 

[36, 42].  As [36] found SERVQUAL enabled them to go beyond 

simply identifying issues with the syllabus. Rather, this scale al-

lowed the authors to emphasize the importance of abstract features 

in education that often go overlooked (e.g., teaching staff being 

more responsive to the need of students). Thus, stakeholders within 

HEIs can utilize the findings from such measures to effectively 

identify service issues that may remain unnoticed, and introduce 

strategies designed to provide resolutions to any gaps identified. 

Therefore, it can be expected that by improving the services of-

fered, HEIs could improve overall student satisfaction. Conversely, 

without implementing the practice of assuring quality in a service, 

dissatisfaction in the student population may ensue.  

In the case of LA, it too can be regarded as a service provided 

by a HEI, as the underlying foundation of the field is to support 

students during their learning. As a result of it being conceptualized 

in this way, LA as a service should then be subject to quality assur-

ance measures. Otherwise, it is plausible to assume that without a 

continual evaluation of the LA services that are in place, problem-

atic features may endure without resolution. This will certainly 

result in HEIs ignoring ideological gaps that would effectively 

jeopardize the quality of the LA service they are providing. 

3. SERVICE QUALITY AND LEARNING 
ANALYTICS 
Expectations can be defined in terms of the user’s pre-trial beliefs 

about a service [23]. In the context of LA, those who use the ser-

vice can be considered as part of the following stakeholders groups: 

learners, teachers, managers, and policymakers [7]. Each of these 

respective stakeholder groups will have different expectations of 

LA as a service. For example, a teacher may expect to be provided 

with real-time updates on how their students are performing in a 

course. Whereas, a manager would expect feedback on how a hand-

ful of modules are running. A potential outcome of this variability 

in needs across stakeholders is a LA service that mainly satisfies 

the expectations of one group above the rest. 

Perceptions, in contrast, are defined as the user’s judgement of 

how the service performed in reference to their prior expectations 

[26]. This post-usage comparison reflects what is known as discon-

firmation, where the user determines whether performance 

exceeded, met, or fell below what was expected [21]. The outcome 

of a user assessing whether performance aligns with expectations 

or not can subsequently determine their level of satisfaction with 

the service provided [21]. This conceptualization of service quality 

through the perspective of EDT has been important in health care 

settings. For example, [4] assessed whether patient’s expectations 

of health care had been met. Through the use of a pre-visit ques-

tionnaire composed of expectation items, and a post-visit 

questionnaire made up of perception items, this enabled researchers 

to identify discrepancies in the health service provided. With these 

findings, organizations are able to assess the quality of service and 

gain valuable information as to how they can make improvements. 

The LA community does consider the student population to be 

the most important stakeholders in any form of LA service [10]. 

Research efforts, as well, have kept students in mind, from design-

ing visualization to monitor progress [15, 27], to creating software 

aimed at regulating metacognitive abilities [39]. In spite of how 

beneficial this research will be for students, LA has seemingly 

overlooked the importance of student expectations and perceptions 

of the service provided. Instead, LA services have seemingly been 

implemented in a top-down fashion directed by the beliefs of re-

searchers, managers, and policymakers without much consideration 

of what students expect from such a service. Thus, LA is not facili-

tating the evolution and development of the services it could offer 

to students, as evaluation processes are rarely being implemented.  

It would, however, be incorrect to suggest that evaluations of 

LA tools in general have been overlooked, as [30] developed a 

framework of quality indicators by working with experts in the 

field. Although, the limitation here is that these indicators are help-

ing to establish a standard by which LA tools should be measured, 

as opposed to investigating what students expect from a service. In 

the same way, when various research efforts have incorporated 

evaluation procedures, these have been directed towards an as-

sessment of a tool’s utility and value [3, 27]. Therefore, to some 

extent the LA community has not ignored the importance that eval-

uative processes can have in the design and implementation of LA 



3 

 

tools and services. On the other hand, these are not addressing how 

students perceived the quality of the LA service provided to be.  

As it stands, we posit that a movement towards developing 

and understanding how LA services can be regulated in a quality 

assurance framework should be supported. Irrespective of size 

(e.g., university-wide LA service, or a LA service confined to one 

module/course/degree programme), a LA service cannot be naively 

assumed as being unproblematic. Inevitably, there will be gaps 

between what students expect from and perceive the service to be. 

It then becomes the responsibility for the LA community to intro-

duce measures that will identify such issues and suggest resolutions 

that will facilitate re-developments of a LA service that is consid-

ered high in quality. Without the adoption of this approach, it can 

be assumed that students could become dissatisfied with the service 

as their needs are not met, and inevitably decrease the usage of LA 

output in their learning. 

4. CONCEPTUAL MODEL  
To address the issues discussed in the previous section, we propose 

an evaluation framework that can allow for student perceptions to 

feedback into the continuous evolution and development of LA as a 

service. To achieve this objective of assimilating evaluations of 

service quality into the development and implementation of LA 

service, we posit that an action research approach should be under-

taken. This was decided upon as the evaluative process are carried 

out by those involved in the LA service (e.g., teaching staff, man-

agers, etc.) with an aim to improve the service offered [6]. In 

addition, the process should be cyclical in nature, going from plan-

ning, acting, observing, and reflecting [6]. Put differently, we view 

the practice of assuring quality in LA services as an enquiry that 

encourages LA practitioners to engage in a process of investigating 

and evaluating their work [17].    

To illustrate this approach in practice, take a LA service of-

fered to students that provides real-time updates about their studies 

through a dashboard. Those LA practitioners who designed and 

setup this service may hold a preconceived belief that this 

dashboard would address all students’ needs and help improve 

academic performance. Log data can be used by the LA practition-

ers to investigate whether students are making use out of this 

particular tool. However, this approach naively assumes that the 

LA service is catering to the needs and expectations of the students. 

When, in actuality, students’ may be expecting less frequent up-

dates as to prevent themselves from being overloaded by 

information. Thus, this method of introducing a LA service from a 

top-down perspective can effectively create an ideological gap. As 

practitioners are holding beliefs about what they think a service 

should be, without acknowledging what students expect the service 

to be like. This both perpetuates the ideological gap and creates a 

risk that students could become dissatisfied with the LA service. 

Under our proposed action research approach, the practition-

ers would follow the procedure of planning and implementing the 

LA service. Following this, there is a need to observe of how the 

service is running, which can be accomplished through the use of a 

service quality self-report measure. Theoretically, this should allow 

the LA practitioners to gain an insight into student views about a 

service en masse. As up to now, evaluations taken from small 

groups of students are not going to reflect the divergent opinion 

towards a service held by the general population of students. These 

results can then be fed back to the practitioners, who can then re-

flect and decide how the LA service can be altered to meet the 

expectations of the student population. This then brings the process 

back round to start the cycle again. 

As previously mentioned, there are scales available that can 

measure service quality [2, 26]. In the case of SERVQUAL, the 

wording can be adapted to fit a particular environment. However, 

as LA is a relatively new field, and there has been no research to 

explore student expectations of LA as a service, we first explored 

how issues discussed in the literature relate can relate to service 

quality. On completion of this step, we felt it was more appropriate 

to create a new scale that could measure service quality in LA. 

5. SCALE DEVELOPMENT 
For the application of an action research approach aimed at 

assuring quality in LA services, a scale is required that can measure 

service quality. Researchers have incorporated evaluation processes 

into their methodologies [3, 27], but these were limited to small 

groups testing out new LA tools. What is required is a scale that 

can assess student expectations and perceptions of LA as a service 

offered by an institution. In doing so, it can provide the foundation 

for HEIs adoption of LA to be subject to regulatory measures. 

The initial step taken to create a new scale that would meas-

ure LA service quality was to conduct a review of the LA literature. 

Attention was paid towards articles discussing ethical and legal 

issues in the field of LA [14, 31, 40], or frameworks on how to 

assimilate LA into HEIs [32, 38].  

A number of themes emerged from this review that we believe 

would be important for measuring service quality. These themes 

can be grouped as followed: Ethics and Privacy, Meaningfulness, 

Agency, and Intervention. Ethics and Privacy expectations cover a 

multitude of topics that have been continually debated within LA, 

such as reassuring students that their data is kept securely. Mean-

ingfulness expectations center on the need for LA feedback to be 

both relatable and clear so it can be effectively incorporated into 

students’ learning. Agency expectations are the degree to which LA 

should be student-centered, so that students themselves decide how 

to interpret and use any feedback provided. Finally, Intervention 

expectations are concerned with what students expect LA interven-

tions to be aimed at (e.g., the development of academic skills, or 

emotional support). 

The design of the items for this scale was motivated by the 

EDT model proposed by [21]. As the objective of this scale was to 

measure disconfirmation between expectations and perceptions, the 

questions were phrased to reflect these. It is important to note, 

however, that we have only considered expectations as reflecting 

what a user wants from a service (e.g., the level of service I expect 

to happen in reality). The issue here, according to [37], is that this 

conceptualization of expectation creates a situation whereby user 

satisfaction would result from a poor service if they expected this 

to occur. To compensate for this problem, the authors deconstruct-

ed expectations into predictive (i.e., a user’s pre-usage belief of 

what they anticipated the service would achieve) and desired (i.e., a 

user’s pre-usage belief of what they wanted in a service; e.g., I 

hope for this ideally). In doing this, they were able to extend the 

EDT by showing that satisfaction was the result of desired expecta-

tions being met. Feelings of indifference, however, were caused by 

predictive expectations being met; whereas, dissatisfaction occurs 

under circumstances when these predictive expectations are not 

met. This approach of using predicted and desired expectations has 

been used by [4] in the context of health care settings. They found 

that this deconstruction of expectations offered more explanatory 

power than using one type of expectation alone. With this in mind, 

we believed using desired and predictive expectations would be 

more beneficial for our LA service quality scale. 

Taking into account the abovementioned findings and identi-



4 

 

fied themes, we developed a scale containing 79 items. These items 

were broken down into two subscales (e.g., predictive and desire 

expectations). This questionnaire was then subject to peer review 

by two LA experts. The opinion of these experts centered on mak-

ing the questions more focused and reducing the number of items 

as there were instances of overlapping topics. These comments into 

consideration, we refined the number of items in the questionnaire 

to 37 (Appendix), each of which contains two subscales (e.g., pre-

dictive and desire expectations).  

In the next methodological step, we will be running a pilot 

study on a small group of students. This pilot will allow for stu-

dents to provide feedback about the questionnaire to identify any 

issues with it (e.g., the clarity of wording used). Next, the pilot 

questionnaire will be distributed to a larger sample. The collected 

results will then be subject to reliability analysis, with items being 

removed if an improvement in coefficient alpha is possible. A fac-

tor analysis will then be run on the remaining items to extract the 

underlying factor structure. This should then leave us with a final 

questionnaire, which will be distributed across various universities 

so we can explore student expectations and perceptions of LA as a 

service. 

The overarching aim of this research is to develop a frame-

work by which students’ perspective of service quality and 

satisfaction in LA services can be assessed. Various EDT meas-

urement methods have been proposed to calculate quality of a 

service [11]. The most prominent of which has been exemplified by 

SERVQUAL, where perceived quality is the difference between the 

perception and expectation ratings of each item [26]. As an ap-

proach, however, it is problematic due to not measuring 

disconfirmation directly [35]. Thus, a decision was made to adopt 

the additive difference model as used by [34]. The first step in this 

method is to calculate the average score across the items of three 

scales (i.e., predictive and desired expectations, and perceived per-

formance; Appendix). Next, participants answer questions 

measuring desires congruency (e.g., difference between what I de-

sired and what I received) and expectations congruency (e.g., how 

good or bad is this difference?). For each item on this scale, 

individuals are asked to make a subjective assessment of whether 

the performance aligned with the desired/predictive expectation, 

and if this difference was good or bad. The scores on these two 

respective scales for the desired/predictive expectation items are 

multiplied for each item and then averaged, which provides average 

congruency scores (i.e., desires congruency and expectations dis-

confirmation). As in [34] research, we will also introduce items 

relating to satisfaction (e.g., overall, how do you feel about the 

learning analytic services you received?), and overall service quali-

ty (e.g., overall, what is the level of service quality you received 

from learning analytics services?). A decision to incorporate these 

items were based upon the findings that desires are predictive of 

satisfaction, whilst predictive expectations indirectly affect 

judgements of service quality through perceived performance [34]. 

Thus, the abovementioned points provides a foundation to develop 

a model that will enable practitioners to understand how expecta-

tions and performance affect students’ satisfaction with LA 

services and their judgements of overall LA service quality. 

6. CONCLUSION 
LA is a valuable service in education, from being a tool to improve 

a HEIs retention rate, to guiding students down the optimum learn-

ing pathway [32]. An examination of the literature, however, does 

show an important gap within the field of LA itself, which is to 

develop measures of quality assurance. As with any other service, 

there is a need to meet the needs and expectations of users. If these 

are regularly overlooked, it may perpetuate ideological gaps where 

the service providers are promoting their expectations over and 

above their users’ needs. 

As HEIs are increasing their use of LA services, it becomes 

imperative for practitioners to start considering how quality assur-

ance can be guaranteed. To resolve this issue, we have suggested 

the use of an action research framework that allows for a continual 

evaluation of the LA service in place. In addition, we outline our 

current progression towards developing a scale that will measure 

the expectations and perceptions of LA as a service. It is intended 

that this will deter complacency and stress the importance of 

engaging in continual evaluations and re-developments of the LA 

tools and services in place. 

7. APPENDIX 
Pilot Questionnaire Link: http://bit.ly/LASQE  

8. REFERENCES 

[1] 1st International Conf. on Learning Analytics and Knowledge 

2011 | Connecting the technical, pedagogical, and social 

dimensions of learning analytics: 2011. 

https://tekri.athabascau.ca/analytics/. Accessed: 2016-01-19. 

[2] Abdullah, F. 2006. The development of HEdPERF: a new 

measuring instrument of service quality for the higher educa-

tion sector. International J. of Consumer Studies. 30, 6 

(2006), 569–581. 

[3] Ali, L., Hatala, M., Gaševi?, D. and Jovanovi?, J. 2012. A 

qualitative evaluation of evolution of a learning analytics tool. 

Computers & Education. 58, 1 (2012), 470–489. 

[4] Bowling, A., Rowe, G., Lambert, N., Waddington, M., 

Mahtani, K., Kenten, C., Howe, A. and Francis, S. 2012. The 

measurement of patients’ expectations for health care: a re-

view and psychometric testing of a measure of patients’ 

expectations. Health Technology Assessment. 16, 30 (2012), 

1–532. 

[5] Calvert, C.E. 2014. Developing a model and applications for 

probabilities of student success: a case study of predictive an-

alytics. Open Learning: The J. of Open, Distance and e-

Learning. 29, 2 (2014), 160–173. 

[6] Carr, W. and Kemmis, S. 1986. Becoming critical education, 

knowledge, and action research. Falmer Press. 

[7] Clow, D. 2012. The learning analytics cycle. Proc. of the 2nd 

International Conf. on Learning Analytics and Knowledge - 

LAK ’12 (New York, New York, USA, 2012), 134. 

[8] Cormack, A. 2016. A data protection framework for learning 

analytics. J. of Learning Analytics. 3, 1 (2016), 91–106. 

[9] Cronin, J.J. and Taylor, S.A. 1992. Measuring Service Quali-

ty: A Reexamination and Extension. J. of Marketing. 56, 3 

(1992), 55. 

[10] Drachsler, H. and Greller, W. 2012. The pulse of learning 

analytics understandings and expectations from the stakehold-

ers. Proc. of the 2nd International Conf. on Learning 

Analytics and Knowledge - LAK ’12 (New York, New York, 

USA, 2012), 120. 

[11] Elkhani, N. and Bakri, A. 2012. Review on “expectancy dis-

confirmation theory” (EDT) Model in B2C E-Commerce. J. of 

Information Systems Research and Innovation. 2, 12 (2012), 

95–102. 

http://bit.ly/LASQE


5 

 

[12] Ferguson, R., Hoel, T., Scheffel, M. and Drachsler, H. 2016. 

Guest editorial: Ethics and privacy in learning analytics. J. of 

Learning Analytics. 3, 1 (2016), 5–15. 

[13] Higher Education Funding Council for England: 2016. 

http://www.hefce.ac.uk/. Accessed: 2016-10-14. 

[14] Ifenthaler, D. and Schumacher, C. 2016. Student perceptions 

of privacy principles for learning analytics. Educational 

Technology Research and Development. 64, 5 (2016), 923-

938. 

[15] Kim, J., Jo, I.-H. and Park, Y. 2015. Effects of learning ana-

lytics dashboard: analyzing the relations among dashboard 

utilization, satisfaction, and learning achievement. Asia Pacif-

ic Education Review. 17, 1 (2015), 13–24. 

[16] Kim-Soon, N., Rahman, A. and Ahmed, M. 2014. E-Service 

Quality in Higher Education and Frequency of Use of the Ser-

vice. International Education Studies. 7, 3 (2014), 1–10. 

[17] McNiff, J. and Whitehead, J. 2011. All You Need to Know 

About Action Research. SAGE. 

[18] Ng, I.C.L. and Forbes, J. 2009. Education as Service: The 

Understanding of University Experience Through the Service 

Logic. J. of Marketing for Higher Education. 19, 1 (2009), 

38–64. 

[19] Nguyen, A. and Rosetti, J. 2013. Overcoming potential nega-

tive consequences of customer orientation in higher education: 

closing the ideological gap. J. of Marketing for Higher 

Education. 23, 2 (2013), 155–174. 

[20] Oldfield, B.M. and Baron, S. 2000. Student perceptions of 

service quality in a UK university business and management 

faculty. Quality Assurance in Education. 8, 2 (2000), 85–95. 

[21] Oliver, R.L. 1980. A Cognitive Model of the Antecedents and 

Consequences of Satisfaction Decisions. J. of Marketing 

Research. 17, 4 (1980), 460. 

[22] Oliver, R.L. 1989. Processing of the satisfaction response in 

consumption: A suggested framework and research proposi-

tions. J. of Consumer Satisfaction, Dissatisfaction, and 

Complaining Behaviour. 2, (1989), 1–16. 

[23] Olson, J.C. and Dover, P. 1976. Effects of Expectation Crea-

tion and Disconfirmation on Belief Elements of Cognitive 

Structure. In NA - Advances in Consumer Research Volume 

03. B.B. Anderson, eds. Cincinnati, OH: Association for 

Consumer Research. 168–175. 

[24] Parasuraman, A. 2005. E-S-QUAL: A Multiple-Item Scale for 

Assessing Electronic Service Quality. J. of Service Research. 

7, 3 (2005), 213–233. 

[25] Parasuraman, A., Zeithaml, V.A. and Berry, L.L. 1985. A 

conceptual model of service quality and its implications for 

future research. J. of Marketing. 49, 4 (1985), 41–50. 

[26] Parasuraman, A., Zeithaml, V.A. and Berry, L.L. 1988. 

SERVQUAL: A Multiple-Item Scale for Measuring Consum-

er Perceptions of Service Quality. J. of Retailing. 64, 1 

(1988), 12–40. 

[27] Park, Y. and Jo, I.-H. 2015. Development of the Learning 

Analytics Dashboard to Support Students’ Learning Perfor-

mance. J. UCS. 21, 1 (2015), 110–133. 

[28] Prinsloo, P. and Slade, S. 2016. Student vulnerability, agency, 

and learning analytics: An exploration. J. of Learning 

Analytics. 3, 1 (2016), 159–182. 

[29] Ruiz, S., Charleer, S., Urretavizcaya, M., Klerkx, J., Fernán-

dez-Castro, I. and Duval, E. 2016. Supporting learning by 

considering emotions: tracking and visualization a case study. 

In Proc. of the Sixth International Conf. on Learning 

Analytics & Knowledge (LAK ’16), 254–263. 

[30] Scheffel, M., Drachsler, H., Stoyanov, S. and Specht, M. 

2014. Quality indicators for learing analytics. Educational 

Technology and Society. 17, 4 (2014), 117–132. 

[31] Sclater, N. 2016. Developing a code of practice for learning 

analytics. J. of Learning Analytics. 3, 1 (2016), 16–42. 

[32] Sclater, N., Peasgood, A. and Mullan, J. 2016. Learning Ana-

lytics in Higher Education: A Review of UK and International 

Practice. Jisc, Bristol. 

https://www.jisc.ac.uk/sites/default/files/learning-analytics-in-

he-v3.pdf 

[33] Spooren, P., Brockx, B. and Mortelmans, D. 2013. On the 

Validity of Student Evaluation of Teaching The State of the 

Art. Review of Educational Research. 83, 4 (2013), 598–

642. 

[34] Spreng, R.A. and Mackoy, R.D. 1996. An empirical examina-

tion of a model of perceived service quality and satisfaction. J. 

of retailing. 72, 2 (1996), 201–214. 

[35] Spreng, R.A. and Page, T.J. 2003. A test of alternative 

measures of disconfirmation. Decision Sciences. 34, 1 

(2003), 31–62. 

[36] Stodnick, M. and Rogers, P. 2008. Using SERVQUAL to 

measure the quality of the classroom experience. Decision 

Sciences J. of Innovative Education. 6, 1 (2008), 115–133. 

[37] Swan, E.I. and Trawick, F. 1980. Satisfaction related to pre-

dictive vs. desired expectations. Refining concepts and 

measures of consumer satisfaction and complaining 

behavior. H.K. Hunt and R.L. Day, eds. Bloomington: School 

of Business, Indiana University. 7–12. 

[38] Tse, D.K., Nicosia, F.M. and Wilton, P.C. 1990. Consumer 

Satisfaction as a Process. Psychology & Marketing (1986-

1998). 7, 3 (1990), 177. 

[39] West, D., Heath, D. and Huijser, H. 2016. Let’s Talk Learning 

Analytics: A Framework for Implementation in Relation to 

Student Retention. Online Learning. 20, 2 (2016), 150–170. 

[40] Winne, P.H. and Hadwin, A.F. 2013. nStudy: Tracing and 

Supporting Self-Regulated Learning in the Internet. Interna-

tional Handbook of Metacognition and Learning 

Technologies. R. Azevedo and V. Aleven, eds. Springer Inter-

national Handbooks of Education. 293–308. 

[41] Xu, D., Benbasat, I. and Cenfetelli, R.T. 2013. Integrating 

Service Quality with System and Information Quality: An 

Empirical Test in the E-Service Context1. MIS Quarterly. 37, 

3 (2013), 777–794. 

[42] Yooyen, A., Pirani, M. and Mujtaba, B.G. 2011. Expectations 

versus realities of higher education: gap analysis and universi-

ty service examination. Contemporary Issues in Education 

Research. 4, 10 (2011), 25. 



