
Understanding the relationship between technology use
and cognitive presence in MOOCs

Vitomir Kovanovic? Srec?ko Joksimovic? Oleksandra Poquet
The University of Edinburgh The University of Edinburgh University of South Australia
Edinburgh, United Kingdom Edinburgh, United Kingdom Adelaide, Australia
v.kovanovic@ed.ac.uk s.joksimovic@ed.ac.uk sspoquet@gmail.com

Thieme Hennis Shane Dawson Dragan Gaševic?
Delft University of Technology University of South Australia The University of Edinburgh

Delft, Netherlands Adelaide, Australia Edinburgh, United Kingdom
thieme@hennis.nl Shane.Dawson@unisa.edu.au dgasevic@acm.org

ABSTRACT
In this poster, we present the results of the study which exam-
ined the relationship between student differences in their use of the
available technology and their perceived levels of cognitive pres-
ence within the MOOC context. The cognitive presence is a con-
struct used to measure the level of practical inquiry in the Commu-
nities of Inquiry model. Our results revealed the existence of three
clusters based on student technology use. The clusters significantly
differed in terms of their levels of cognitive presence, most notably
they differed on the levels of problem resolution.

CCS Concepts
•Applied computing ? Distance learning; •Information sys-
tems? Clustering;

Keywords
Community of Inquiry model, MOOCs, Student clustering.

1. INTRODUCTION
The goal of this study is to replicate previously published work

by Kovanovic? et al. [3], examining the association between stu-
dent technology use and the development of cognitive presence.
Specifically, the study aimed at investigating to what extent pat-
terns of the association between student technology use and de-
velopment of cognitive presence identified in the traditional online
settings hold in the context of learning in MOOCs. The theoreti-
cal foundation is given by the Community of Inquiry (CoI) frame-
work [2], a popular model of online learning which defines three
key dimensions of online learning experience: cognitive presence,
social presence, and teaching presence. The CoI model defines cog-
nitive presence as focusing on the development of critical and deep
thinking skills through sustained communication [2] and consisting
of four phases: 1) triggering event phase (problem identified), 2)
exploration (brainstorming of ideas and solutions), 3) integration
(synthesis of relevant knowledge), and 4) resolution (application
and testing of the new knowledge). The original study Kovanovic?
et al. [3] found six clusters of students, significantly different in
terms of the technology use and the levels of cognitive presence, in
particular, exploration and integration phrases. The previous study
also unveiled several “successful” interaction approaches, and the
importance of the quality of student interactions, rather than their
quantity, on their development of cognitive presence.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

LAK ’17 March 13-17, 2017, Vancouver, BC, Canada
c© 2017 Copyright held by the owner/author(s).

ACM ISBN 978-1-4503-4870-6/17/03.

DOI: http://dx.doi.org/10.1145/3027385.3029471

2. METHODS
The data for this study comes from the Fall 2014 offering of the

“Introduction to Functional Programming,” an eight-weeks MOOC
offered by the Delft University of Technology on the edX platform.
The MOOC focused on introducing functional programming us-
ing the Haskell programming language. Overall, 38,029 students
enrolled, out of which 1,968 obtained the certificate. At the end
of the course, we administered the CoI questionnaire, a survey in-
strument with 34 five-point Likert scale items, designed to measure
the perceived levels of three presences within online learning envi-
ronments. In addition to the survey responses (818 responses), the
data used in this study also included edX trace logs and discussion
forum data about 23,648 students.

To identify different groups of students based on their technology
use, we extracted 29 measures of student engagement, grouped into
several groups of related variables:
1) Course component access: Number of times a particular course

page(s) were accessed (i.e., course homepage, wiki, progress,
syllabus, or module/module item1)

2) Graded assignments: Number of times an action related to
quizzes (viewing a quiz or checking answers) was performed.

3) Video lectures: Number of times each video related action (load,
play, pause, change speed, show subtitles) was performed.

4) Course navigation: Number of times a navigational action (pre-
vious/next module, jump to a module), was executed.

5) Discussion access: Number of times an action related to view-
ing a discussion was executed (opening a discussion, opening
discussion on a learning page, or searching discussions).

6) Discussion contribution: Number of discussions contributions
made, including the number of threads created, the number of
QA threads created2, the number of comments written, the av-
erage number of letters written when creating a thread, and the
average number of letters written when posting a comment.

7) Discussion engagement: Additional measures assessing the level
of discussion engagement, such as the number of replies re-
ceived, the number of message upvotes given/received, and the
number of endorsements received/given.

We also extracted four measures related to the perceived levels
of cognitive presence (i.e., perceived level of triggering event, ex-
ploration, integration, and resolution) in the course by averaging
the responses (from 1: strongly disagree to 5: strongly agree)3.

The analysis procedure followed the approach used by Kovanovic?
et al. [3]. We first conducted a hierarchical cluster analysis using 29
extracted measures of students’ technology use, with the Euclidean
distance and Ward’s agglomeration method. The optimal number
of three clusters is identified through the analysis of clustering den-

1 In the edX terminology, term ’courseware’ is used to denote
learning modules.
2 Questions are particular type of threads where one answer is rated
as the “correct” answer.
3 In the CoI instrument, each cognitive presence phase has three
questions designated to measure its perceived levels.

http://dx.doi.org/10.1145/3027385.3029471


C1 C2 C3

Cluster 1

Cluster 2

Cluster 3

Figure 1: Clustering dendogram.

drogram (Fig. 1) based on the height of the merging steps.
After the student clusters had been identified, we performed the

multivariate analysis of variance (MANOVA) procedure to examine
the significance of the differences among the identified clusters in
terms of their technology use, and also levels of cognitive presence.
Each significant MANOVA was followed by a robust MANOVA
analysis, and univariate analysis of variance (ANOVA) and Lev-
ene’s test for the homogeneity of variance. In cases where there
was no homogeneity of variance, we used non-parametric Kruskal-
Wallis tests. Each significant ANOVA analysis was also followed
by Tukey pairwise comparisons, and each significant Kruskal-Wallis
test with a pairwise comparison. In all cases where multiple com-
parisons were performed, we used the Bonferroni correction.

3. RESULTS AND DISCUSSION
The hierarchical clustering indicated the existence of three sep-

arate student clusters (Fig. 1). Based on the differences between
cluster centroids (Fig. 2) we labeled three clusters as:
• Cluster 1: Passive users (15,868 students), that are character-

ized by the overall low engagement, with the main focus on
watching videos without working on graded assignments.

• Cluster 2: Task-focused users (3,532 students), which focus on
obtaining certificate and have balanced use of the whole system.

• Cluster 3: Highly active users (4,248 students) that have a high
engagement in the course, with an active use of the discussions
as well as submitting graded assignments.
To analyze the differences in technology use between clusters,

we conducted MANOVA analysis using 29 clustering measures. A
statistically significant MANOVA was observed, Pillai’s Trace =
0.86, F(58,4,724) = 614.4, p < 0.0001, with multivariate effect
size ?2 = 0.43 which is considered a large effect size4. Results
indicate that 43% of the variance in the canonically derived depen-
dent variable can be accounted by the cluster assignment. Given the
significant departure from the homogeneity of variance for all clus-
tering measures, we used Kruskal-Wallis test, with all comparisons
being statistically significant, p < 0.001. The pairwise compar-
isons indicated significant differences among three clusters for all
course content-related measures and three variables related to ac-
cess to discussions (i.e., ThreadAccess, Threadaccessinline,
and DiscussionSearch). For the remaining discussion-related
variables, the differences between clusters one and three, and two
and three were significant, but not between clusters one and two.

As the next step of the analysis, we conducted a multivariate
analysis of variance (MANOVA) to examine the differences among
clusters in terms of the perceived levels of cognitive presence. Sta-
tistically, significant MANOVA results were obtained, Pillai’s Trace
= 0.02, F(8,1626) = 2.05, p = 0.038. The results were confirmed
by a robust rank-based MANOVA, which also yielded significant
results (Wills ?rank = 0.98,?2(8) = 15.85, p = 0.044). The multi-
variate effect size of ?2 = 0.0099 was obtained which is considered
4 Commonly adopted ?2 effect size ranges [1]: 0.01 small, 0.06
medium, and 0.14 large effect size.

C
o
m
m
e
n
t
s
E
n
d
.
R
e
c
.

C
o
m
m
e
n
t
s
E
n
d
.
G
i
v
e
n

U
p
v
o
t
e
s
R
e
c
e
i
v
e
d

U
p
v
o
t
e
s
G
i
v
e
n

C
o
m
m
e
n
t
s
R
e
c
e
i
v
e
d

C
o
m
m
e
n
t
s
C
h
a
r
s
A
v
g

T
h
r
e
a
d
s
C
h
a
r
s
A
v
g

C
o
m
m
e
n
t
s
W
r
i
t
t
e
n

Q
u
e
s
t
i
o
n
s
S
t
a
r
t
e
d

D
i
s
c
u
s
s
i
o
n
s
S
t
a
r
t
e
d

D
i
s
c
u
s
s
i
o
n
S
e
a
r
c
h

T
h
r
e
a
d
A
c
c
e
s
s
I
n
l
i
n
e

T
h
r
e
a
d
A
c
c
e
s
s

M
o
d
u
l
e
P
r
e
v

M
o
d
u
l
e
N
e
x
t

M
o
d
u
l
e
J
u
m
p

V
i
d
e
o
S
h
o
w
S
u
b
s

V
i
d
e
o
L
o
a
d

V
i
d
e
o
C
h
a
n
g
e
S
p
e
e
d

V
i
d
e
o
P
a
u
s
e

V
i
d
e
o
P
l
a
y

P
r
o
b
l
e
m
S
h
o
w

P
r
o
b
l
e
m
G
r
a
d
e

O
p
e
n
C
o
u
r
s
e
w
a
r
e
I
t
e
m

O
p
e
n
S
y
l
l
a
b
u
s

O
p
e
n
C
o
u
r
s
e
P
r
o
g
r
e
s
s

O
p
e
n
W
i
k
i

O
p
e
n
C
o
u
r
s
e
w
a
r
e

O
p
e
n
H
o
m
e

?
0

.5
0

.0
0

.5
1

.0
1

.5

z
?

s
c
o

re
s

C
lu

s
te

r 
1

C
lu

s
te

r 
2

C
lu

s
te

r 
3

Figure 2: Centroids of identified cluster centers. Shaded area
corresponds to discussion-related clustering features.

a small effect size. The follow-up ANOVA analysis indicated sig-
nificant differences in terms of resolution, F(2,815) = 5.86, p =
0.0029, with effect size ?2 = 0.014 which is also considered a
small effect size. Follow-up Tukey pairwise analyses revealed sig-
nificant differences between clusters one and two (0.21 SD), and
one and three (0.20 SD), with students from cluster one having
lower perceived levels of resolution in both cases.

Our results yield several compelling findings, showing the sim-
ilarities and differences between MOOCs and traditional online
courses in terms of technology use and cognitive presence devel-
opment. The results of this study are aligned with our previous
work [3], where we also observed a considerable number of stu-
dents being disengaged. However, in MOOC context this is even
more emphasized. Still, the observed differences in the levels of
cognitive presence were much smaller than it was found in a for-
credit online course by Kovanovic? et al. [3]. The smaller observed
differences are likely the result of the use of self-reported survey
instrument which has a severe self-selection bias, and much lower
discriminatory power than content analysis method used in Ko-
vanovic? et al. [3] study. Finally, the observed difference regarding
the resolution is aligned with the [4] study, which also observed
unique dynamics of reaching resolution phase within MOOC con-
texts. The observed differences, and the challenges with the self-
reported assessment of cognitive presence also further emphasize
the strong need for automated ways of assessing student levels of
cognitive presence through analysis discussion transcripts, in par-
ticular within the MOOC context [5].

4. ADDITIONAL AUTHORS
Pieter de Vries, Delft University of Technology, Netherlands

(Pieter.deVries@tudelft.nl). Marek Hatala, Simon Fraser
University, Canada (mhatala@sfu.ca). George Siemens, Univer-
sity of Texas at Arlington, USA (gsiemens@uta.edu).

5. REFERENCES
[1] J. Cohen. The Analysis of Variance. In Statistical power analysis for

the behavioral sciences, pages 273–406. L. Erlbaum Associates, Hills-
dale, N.J., 1988.

[2] D. R. Garrison, T. Anderson, and W. Archer. Critical Inquiry in a Text-
Based Environment: Computer Conferencing in Higher Education. The
Internet and Higher Education, 2(2–3):87–105, 1999.

[3] V. Kovanovic?, D. Gaševic?, S. Joksimovic?, M. Hatala, and O. Adesope.
Analytics of communities of inquiry: Effects of learning technology
use on cognitive presence in asynchronous online discussions. The In-
ternet and Higher Education, 27:74–89, Oct. 2015.

[4] V. Kovanovic?, S. Joksimovic?, O. Poquet, T. Hennis, I. C?ukic?, P. d.
Vries, M. Hatala, S. Dawson, G. Siemens, and D. Gaševic?. Exploring
Communities of Inquiry in Massive Open Online Courses. Manuscript
submitted for publication, 2016.

[5] V. Kovanovic?, S. Joksimovic?, Z. Waters, D. Gaševic?, K. Kitto,
M. Hatala, and G. Siemens. Towards automated content analysis of dis-
cussion transcripts: A cognitive presence case. In Proceedings of the
Sixth International Conference on Learning Analytics & Knowledge,
LAK ’16, pages 15–24, New York, NY, USA, 2016. ACM.


	Introduction
	Methods
	Results and discussion
	Additional Authors
	REFERENCES


