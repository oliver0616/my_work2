
ATCE - An Analytics Tool to Trace the Creation and 
Evaluation of Inclusive and Accessible Open Educational 

Resources 
Cecilia Avila 

Institute of Informatics and Applications 
University of Girona 

Spain 
cecilia.avila@udg.edu 

Ramon Fabregat 
Institute of Informatics and Applications 

University of Girona 
Spain 

ramon.fabregat@udg.edu 

Silvia Baldiris 
Direction of Research, Innovation and Social Projection 

Fundación Universitaria Tecnológico Comfenalco 
Colombia 

sbaldiris@tecnologicocomfenalco.edu.co 
Sabine Graf 

School of Computing and Information Systems 
Athabasca University 

Canada 
sabineg@athabascau.ca 

  
ABSTRACT 
The creation of Inclusive and Accessible Open Educational 
Resources (IA-OERs) is a challenge for teachers because they 
have to invest time and effort to create learning contents 
considering students’ learning needs and preferences. An IA-OER 
is characterized by its alignment with the Universal Design 
Learning (UDL) principles, the quality on its contents and the web 
accessibility as a way to address the diversity of students. 
Creating an IA-OER with these characteristics is not a 
straightforward task, especially when teachers do not have enough 
information/feedback to make decisions on how to improve the 
learning contents. In this paper we introduce ATCE – an 
Analytics Tool to Trace the Creation and Evaluation of IA-OERs. 
This tool focuses in particular on the accessibility and quality of 
the IA-OERs. ATCE was developed as a module within the 
ATutor Learning Management System (LMS). An analytics 
dashboard with visualizations related to the teachers’ competences 
in the creation and evaluation of IA-OERs was included as part of 
the tool. This paper also presents a use case of the visualizations 
obtained from the creation and evaluation of one IA-OER after 
using our analytics tool.  

CCS Concepts 
• Human-centered computing ?? Accessibility design and 
evaluation methods • Applied computing ?? Education 
• Information systems ?? Data analytics. 

Keywords 
Learning Analytics; Open Educational Resources; Web 
Accessibility; Quality; Teachers; Competences. 

1. INTRODUCTION 
One of the statements declared in the guidelines for Open 
Educational Resources (OERs) in higher education promotes the 
use and adoption of tools to “facilitate the creation and use of 
adaptable, inclusively designed educational resources” [6] to 
address students’ diverse needs. However, according to Prasad 
and Usagawa [13] inadequate trainings, uncertainties about 
copyright, and difficulties to select appropriate and high-quality 
educational resources, are some of the teachers’ perceived barriers 
to use OERs. Thereby, teachers should develop competences for 
addressing the diverse needs and preferences of their students. 
Based upon these ideas, one of the challenges for today’s teachers 
is to create inclusive and accessible learning contents to support 
students in the development of all tasks requested by the learning 
process itself and to address the diversity of students’ learning 
needs and preferences. Consequently, teachers are expected to be 
able to create Inclusive and Accessible Open Educational 
Resources (IA-OERs) which are digital resources with a 
pedagogical purpose that are accessible and available so that all 
students independent of their learning needs can use them and that 
other authors can revise, reuse, remix o redistribute them [4]. 

When it comes to create an IA-OER, that addresses the diversity 
of students’ learning needs and preferences, teachers should 
consider characteristics such as the accessibility to avoid content 
access barriers, and the quality to make learning contents more 
appropriate to the learning context for which the IA-OER is 
intended. Such creation process also entails an evaluation process 
to ensure that an IA-OER meets such characteristics. Both the 
creation and evaluation are part of the teacher’s competences. 
Since IA-OERs can be created as virtual courses which include 
web pages with different HTML elements, the main issue in this 
context is that teachers require technical knowledge to identify 
problems on the accessibility and quality of the IA-OERs. 
Thereby, teachers require tools that support them in the creation 
and evaluation of IA-OERs and also in the decision-making 
process to improve their learning contents. Thus, in our approach 
the creation and evaluation processes are focused on the web 
accessibility and the quality of the IA-OERs which are described 
as follows. 

Web accessibility means that all people can perceive, understand, 
navigate, and interact with the web [1]. Thus, international 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than the author(s) must be 
honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. Request permissions from 
permissions@acm.org. 
LAK '17, March 13 - 17, 2017, Vancouver, BC, Canada 
Copyright is held by the owner/author(s). Publication rights licensed to 
ACM.  
ACM 978-1-4503-4870-6/17/03…$15.00 
DOI: http://dx.doi.org/10.1145/3027385.3027413 



accessibility recommendations such as the Web Content 
Accessibility Guidelines 2.0 (WCAG 2.0) 1provide a set of best 
practices to make web contents more accessible for all people. 
Thereby, IA-OERs should be created taking into account a 
standard such as the WCAG 2.0. 

Quality is referred to as the set of inner properties of a product or 
service which enables to meet the design specifications [9] and 
thus it makes IA-OERs appropriated for a learning context. To 
evaluate the quality of IA-OERs we adopted LORI (Learning 
Object Review Instrument) [11] which has been widely used to 
evaluate the quality of learning objects [15]. 

In a previous exploratory study, we reported results in the creation 
and evaluation of 72 IA-OERs [2]. The creation process was 
carried out using a model for the Co-Creation of IA-OERs named 
CO-CREARIA [4], and the evaluation process was carried out 
using a tool which included two web instruments (online 
questionnaires) we developed to evaluate the web accessibility 
and the quality of each IA-OER. Moreover, the tool provided 
teachers with some feedback with the answers given by evaluators 
and some visualizations related to the accessibility of the IA-
OERs. Findings from that study emphasized the need of extending 
our tool to do learning analytics on the teachers’ competences in 
the creation and evaluation of IA-OERs.  

In this paper we introduce ATCE – an Analytics Tool that traces 
the activities done by teachers when they create and evaluate IA-
OERs. Preliminary results on the information delivered by the 
analytics tool are described by means of a use case of one of the 
IA-OERs created and evaluated with ATCE.  

The remainder of the paper is organized as follows: section 2 
gives an overview of learning analytics in the creation of 
educational resources, section 3 presents our approach in the 
creation and evaluation of IA-OERs and an overview of ATCE, 
section 4 describes the functionalities of ATCE, and section 5 
presents an use case to describe some of the visualizations 
obtained after using ATCE. 

2. LEARNING ANALYTICS IN THE 
CREATION OF EDUCATIONAL 
RESOURCES 
Most of the research in learning analytics solutions is carried out 
to support teachers in the analysis of student behavior data (e.g. 
[5, 7, 8]). There are some studies into learning analytics or 
monitoring solutions focused on supporting the decision-making 
of teachers at the creation or design time but in the field of 
learning design and not particularly in the creation of OERs. 

For instance, Lockyer, Heathcote and Dawson [12] analyze how 
checkpoint and process analytics can facilitate pedagogical action 
on learning designs. Such analytics provide teachers information 
about student engagement which is used to redesign the learning 
contents when they run a course again or in a different context. 
Rodríguez et al. [14] propose a script-aware monitoring process to 
support teachers in defining scripts enriched with monitoring 
information related to the behaviors of students in the enactment 
of learning contents. As a result, teachers could identify 
improvements for their learning designs. Karkalas, Mavrikis and 
Labs [10] designed and developed a Reflective Designer 
Analytics Platform (RDAP) aimed to support authors on the 
design of their learning materials (e-books) and on how they meet 
                                                                    
1 https://www.w3.org/TR/WCAG20/ 

the original learning objectives. This tool has a dashboard aimed 
to increase authors' awareness so that they can redesign and 
improve the e-books. Bakharia et al. [3] presented a learning 
analytics conceptual framework aimed to support teachers in the 
evaluation of learning designs. This framework was implemented 
in a tool called Loop which includes a dashboard with temporal, 
comparative, tool specific, cohort dynamics and contingency 
analytics of the learning events so that teachers adapt the learning 
designs by analyzing learning interactions. 

Overall, these studies highlight the need for providing teachers 
with information that help them to improve their learning contents 
or activities. However, there is a lack of research on how learning 
analytics solutions support teachers in creating educational 
resources to address students’ needs and preferences. We found 
learning analytics as an opportunity to provide teachers with a 
supporting tool in the creation but also in the evaluation of IA-
OERs considering the accessibility and quality characteristics. 

3. TRACING THE CREATION AND 
EVALUATION OF IA-OERs 
3.1 Creation and Evaluation of IA-OERs 
As mentioned in the introduction, in a previous exploratory study 
we reported results in the creation and evaluation of 72 IA-OERs 
created by teachers. Teachers participated in a training course 
about the creation of IA-OERs including topics such as: inclusive 
learning, Universal Design for Learning (UDL), OERs, the CO-
CREARIA model, and web accessibility. Then, teachers created 
the IA-OERs by following the phases of CO-CREARIA 
(Analysis, Design, Development, Evaluation and Implementation) 
[4]. In the development phase each teacher took the role of author 
to create an IA-OER in the form of a virtual course upon the 
ATutor LMS. Moreover, the content editor of ATutor was used by 
teachers to add web pages in the IA-OER by including different 
HTML elements (texts, images, links, videos, etc.). For the 
evaluation phase, we defined an evaluation model which involves 
a peer-review process to promote collaboration among teachers. 
Thereby, each teacher took the role of evaluator to evaluate an IA-
OER created by other teacher. To do so teachers used a tool we 
developed with two web instruments to evaluate the IA-OERs in 
terms of web accessibility (21 questions based on the WCAG 2.0) 
and quality (8 questions based on the LORI) (for more detail on 
the questions included in those instruments see [2]). Once the IA-
OERs were evaluated, teachers could access to a text-based report 
with the answers given by evaluators and some visualizations 
related only to the accessibility of the IA-OER.  

As a result of using the tool in that exploratory scenario, we found 
that it would be important to extent the tool to do learning 
analytics on the teachers’ competences in the creation and 
evaluation of IA-OERs. Thus, we developed ATCE – an 
Analytics Tool that Traces the activities done by teachers when 
they create and evaluate IA-OERs. 

3.2 ATCE: Analytics Tool to Trace the 
Creation and Evaluation of IA-OERs 
In line with recent research on monitoring solutions to support 
teachers at the creation or design time, as discussed in section 2, 
we developed ATCE an analytics tool that supports teachers in the 
creation and evaluation of IA-OERs taking into account their 
accessibility and quality characteristics.  

The main purposes of ATCE are: 1) to store actions performed by 
teachers during the creation and the evaluation of IA-OERs and 2) 



to inform teachers about the accessibility and quality of the IA-
OERs created (as indicators of the creation competence) as well as 
the competence level reached in the evaluation competence.  

On the other hand, the interpretation of visualizations depends on 
an understanding of the context in which the data are collected 
[12]. In ATCE, visualizations and text-based reports are 
connected with the creation and evaluation of IA-OERs and have 
a simple design to facilitate the interpretation of the data collected 
and analyzed. Moreover, as ATCE was developed as a module in 
ATutor, it can be deactivated without affecting the other 
functionalities ATutor and it was also translated into two 
languages (English and Spanish). ATCE was designed thinking of 
teachers as the main users; however the tool can be used by 
anyone interested in creating or evaluating IA-OERs. Following 
descriptions correspond with the roles of the users who interact 
with ATCE. 

• Author: person who adds web pages in an IA-OER.  
• Evaluator: person who carries out the manual evaluation of 

an IA-OER in terms of web accessibility and quality. 
• Expert: person who verifies if the answers given by 

evaluators in the evaluation process are correct or not. An 
expert can also take the role of evaluator. 

• Tool administrator: person managing the management 
options in the analytics tool. 

4. FUNCTIONALITIES OF ATCE 
ATCE is being used in the context of a training course in which 
teachers learn how to create and evaluate IA-OERs. In that 
context, ATCE is used when teachers create IA-OERs in the form 
of virtual courses in the ATutor LMS and also when the IA-OERs 
are evaluated. Each IA-OER is formed by a set of web pages that 
can be edited with the web editor of ATutor. Figure 1 shows the 
main steps on the life cycle of the analytics tool considering the 
actions taken by each role.  

 
Figure 1. Life cycle of the analytics tool 

The four main steps are: 

1. Gathering raw data: data related to the HTML elements added 
through the web editor, the answers and the clickstream 
obtained from the manual accessibility and quality 
instruments, and answers given by experts in the verification 
instrument are gathered as raw data.  

2. Storing: all data related to the actions in the use of the web 
editor, the accessibility and quality evaluation instruments and 
the verification instrument are stored in a database. The user 
who is performing the action and the timestamp are also 
stored for subsequent analysis of actions in a specific date.  

3. Analysis: data collected from the actions done by the different 
roles are analysed to generate the information that is included 
in the reports presented in the visualizations. 

4. Visualization: an analytics dashboard is provided to inform 
authors and evaluators about the accessibility and quality of 
the IA-OERs (as indicators of the creation competence) and 
the level reached in the evaluation competence. 

Following subsections describe main functionalities of ATCE. 

4.1 Management options 
The tool administrator can access to the following options: 
assignment of evaluators, assignment of experts and an option to 
change the states of an IA-OER. Possible states are: edit (by 
author), evaluate (by an evaluator), and verify (by an expert). 

4.2 Storing of HTML elements 
To generate some of the visualizations of the dashboard, ATCE 
needs to store the HTML elements. This is an automatic process 
in which the HTML elements added in each web page of the IA-
OERs (text, links, images, videos, etc.) are stored in the database. 
This process is executed once the author saves the changes in the 
web page. Although the web editor stores the whole HTML code 
in a single record, ATCE takes that source code and then by using 
the Document Object Model (DOM) structure of the page it 
extracts and stores the types of elements and the position of each 
HTML element in such structure. Information stored contains the 
detail of the elements added including the username of the author 
and the modification date. Each HTML element is controlled by 
means of four states: created, modified, moved, or deleted. 

4.3 Automatic accessibility evaluation 
The purpose of the automatic accessibility evaluation is to 
automatically identify as many accessibility failures as possible 
(without the intervention of a human evaluator) to save time and 
work. In this context, ATCE takes advantage of the AChecker2 
which is an automatic accessibility validator. After the storing of 
the HTML elements, ATCE carries out an automatic accessibility 
evaluation which is performed by the AChecker API. Each 
accessibility failure is related to an accessibility question and it is 
therefore stored in the database as an automatic answer to the 
corresponding accessibility evaluation question. 

4.4 Manual evaluation 
The purpose of the manual evaluation is that a human evaluator 
answers questions related to the accessibility (questions that 
cannot be answered in the automatic way) and questions related to 
the quality of the IA-OER. The manual evaluation is carried out 
once an IA-OER is assigned to an evaluator.  

In the manual accessibility evaluation the evaluator answers 
different questions according to the type of HTML elements 
added in the each page of an IA-OER (images, videos, links, etc.). 
Possible answers are: Yes, No or Not applicable and a space to 
add comments on each question. As mentioned in section 3.1, 
accessibility questions are based on the WCAG 2.0 principles. 
Questions appear in a tooltip with a button to see the information 
                                                                    
2 http://achecker.ca/ 

Web	editor

Actions	
DB

Analytics	
process

Data	sources

A

Actions	in	the	use	of	the	tool

Analytics	Dashboard	views

Accessibility	Evaluation	Instrument

1.	Gathering	raw	data

2.	Storing

3.	Analysis

4.	Visualization

Quality	Evaluation	Instrument

Verification	Instrument

EV

EX

Evaluator

Roles

LMS

Author

Evaluator

Expert

Accessibility Quality



of the element revised and a button to access help information of 
each question. 

In the quality evaluation the evaluator uses the quality instrument. 
As mentioned in section 3.1, the quality instrument is based on 
LORI. It evaluates nine aspects including accessibility, but as we 
have a separate instrument for accessibility our quality instrument 
considers: content quality, learning goal alignment, feedback and 
adaptation, motivation, presentation and design, usability, 
reusability, and standards compliance. An evaluator assigns from 
one to five stars to each category and adds comments on each one.  

The clickstream in the use of those instruments together with the 
verification of an expert on the answers given by an evaluator, are 
used to analyze and obtain the level reached by an evaluator in the 
evaluation competency. 

4.5 Dashboard and visualizations 
The dashboard informs teachers about the accessibility and quality 
of the IA-OERs created (as indicators of the creation competency) 
as well as the competence level reached in the evaluation 
competency. The dashboard has three views: 

• The Accessibility view presents information about the 
teacher’s activity and the accessibility of the IA-OERs 
created. 

• The Quality view presents information about the quality of 
the IA-OERs created; and  

• The Evaluator view presents information about the 
competency in the evaluation of IA-OERs evaluated. 

Results in the Accessibility and Quality views are indicators of the 
creation competence. This information is used by the teacher to 
make decisions of improvements in the IA-OERs. Results in the 
Evaluator view are indicators of the evaluation competence. 
Moreover, the Accessibility and the Quality views present 
information from all or one specific IA-OER. Visualizations 
obtained after using ATCE are described in Section 5. 

5. USE CASE: THE KINGDOMS OF THE 
NATURE 
This use case is intended to describe some of the visualizations 
obtained in the dashboard after using the ATCE tool. 

The kingdoms of the nature is an IA-OER created as a virtual 
course in ATutor using ATCE. This IA-OER has three web pages: 
1) Introduction, 2) Kingdoms (biology), and 3) Activities. Those 
pages include different HTML elements (texts, headings, images, 
etc.). This IA-OER was created in four stages: a) A teacher 
(author), who took the training course in the creation of IA-OERs, 
created the IA-OER. Meanwhile, the tool stored all HTML 
elements added by the author and carried out the automatic 
evaluation. b) Another teacher (evaluator) evaluated the 
accessibility and quality of the IA-OER (manual evaluation). c) 
An expert verifies the evaluation. And d) the author used the 
information provided in the Accessibility and Quality views of the 
dashboard to improve the IA-OER. Likewise the evaluator 
received feedback through the Evaluator view. Visualizations 
described below correspond to the Accessibility view. 

The Accessibility view includes: elements added in the IA-OER 
(Your activity – Figure 2); general accessibility and accessibility 
by type of element (General – Figure 3); an IA-OER’s 
accessibility history (Accessibility history – Figure 4); and the 
accessibility by WCAG principles (WCAG principles – Figure 5). 

Each principle in the WCAG principles tab has an information 
button which allows author to go into the detail of the 
accessibility failures identified in the IA-OER. 

 
Figure 2. Accessibility view - Your activity 

 
Figure 3. Accessibility view- General 

 
Figure 4. Accessibility view - Accessibility history 

As can be seen in the visualizations, the IA-OER has different 
types of HTML elements (paragraphs, links, images, tables, 
elements list) (Figure 2); the level of accessibility of this IA-OER 
is about 80% (Figure 3); the chart of the accessibility history 
indicates that this IA-OER has improved over time (Figure 4); and 



it has some failures in the Perceivable, Operable and 
Understandable WCAG principles (Figure 5). 

 
Figure 5. Accessibility view- WCAG principles 

6. CONCLUSIONS 
In this paper we introduced ATCE – an Analytics Tool to Trace 
the Creation and Evaluation of IA-OERs. Main functionalities of 
the tool include: the automatic storing of HTML elements added 
by authors, an automatic accessibility evaluation, options to 
support the manual evaluation of the accessibility and quality of 
the IA-OERs (including the clickstream in the manual evaluation 
and the input of the experts who verify the answers given by 
evaluators). With the data collected (HTML elements stored with 
the web editor, results from the accessibility and quality 
evaluation, and results from the verification done by experts on 
the answers given by an evaluator), the tool performs an analytics 
process to generate the visualizations to be included in the 
dashboard. Some of the visualizations were described by means of 
a use case of one IA-OER created and evaluated using ATCE. 
This tool takes advantage of the learning analytics approach to 
support teachers in decision-making at the creation time as well as 
to provide information related with their competences in the 
creation and evaluation of IA-OERs. Further research needs to be 
conducted on the support that this tool provides to teachers and 
the acceptance of the tool. 

7. ACKNOWLEDGMENTS 
This work is supported in part by the Spanish Ministry of 
Economy and Competitiveness (Open Co-creation Project – 
TIN2014-53082-R). Cecilia, Silvia, and Ramon belong to the 
BCDS group (ref. GRCT40) University of Girona (UdG) - DURSI 
(CSI – SGR-1469). Thanks to the UdG MPCUdG2016. Cecilia 
has financial support under BR2014 - UdG. Silvia thanks to the 
CO-CREEMOS project (Res. 386, Feb. 2016 – Tecnológico de 
Comfenalco). Sabine acknowledges the support from NSERC. 

8. REFERENCES 
[1] W3C. 2005. Introduction to Web Accessibility. Retrieved 

August 30, 2016 from 
https://www.w3.org/WAI/intro/accessibility.php 

[2] Avila, C., Baldiris, S., Fabregat, R. and Graf, S. 2016. 
Cocreation and Evaluation of Inclusive and Accessible Open 
Educational Resources: A Mapping Toward the IMS Caliper. 
IEEE RITA. 11, 3 (Aug. 2016), 167–176. 

[3] Bakharia, A., Corrin, L., de Barba, P., Kennedy, G., Gaševi?, 
D., Mulder, R., Williams, D., Dawson, S. and Lockyer, L. 
2016. A conceptual framework linking learning design with 

learning analytics. Proceedings of the Sixth International 
Conference on Learning Analytics & Knowledge (New York, 
New York, USA, 2016), 329–338. 

[4] Baldiris, S., Avila, C., Fabregat, R., Potes, E., Cuesta, J., 
Muñoz, T. and Cardona, S. 2015. CO-CREARIA: Modelo de 
Co-Creación de REA Inclusivos y Accesibles. La experiencia 
de la maestría en TIC de la Universidad Pontificia 
Bolivariana. Revista Ingeniería E Innovación. 3, 2 (2015), 
37–47. 

[5] Brooks, C., Erickson, G., Greer, J. and Gutwin, C. 2014. 
Modelling and quantifying the behaviours of students in 
lecture capture environments. Computers & Education. 75, 
(Jun. 2014), 282–292. 

[6] Commonwealth of Learning. 2011. Guidelines for open 
educational resources (OER) in higher education. 
Commonwealth of Learning, Vancouver and UNESCO. 

[7] Dawson, S. and Siemens, G. 2014. Analytics to literacies: 
The development of a learning analytics framework for 
multiliteracies assessment. The International Review of 
Research in Open and Distance Learning. 15, 4 (Aug. 2014), 
284–305. 

[8] Graf, S., Ives, C., Rahman, N. and Ferri, A. 2011. AAT – A 
Tool for Accessing and Analysing Students ’ Behaviour Data 
in Learning Systems. 1st International Conference on 
Learning Analytics and Knowledge (2011), 174–179. 

[9] Hoyer, R.W. and Hoyer, B.B.Y. 2001. What Is Quality? 
Quality progress. 34, 7 (2001), 53–62. 

[10] Karkalas, S., Mavrikis, M. and Labs, O. 2016. Towards 
analytics for educational interactive e-books. Proceedings of 
the Sixth International Conference on Learning Analytics & 
Knowledge (New York, New York, USA, 2016), 143–147. 

[11] Leacock, T.L. and Nesbit, J.C. 2007. A Framework for 
Evaluating the Quality of Multimedia Learning Resources. 
Journal of Educational Technology & Society. 10, 2 (2007), 
44–59. 

[12] Lockyer, L., Heathcote, E. and Dawson, S. 2013. Informing 
pedagogical action: Aligning learning analytics with learning 
design. American Behavioral Scientist. 57, 10 (2013), 1439–
1459. 

[13] Prasad, D. and Usagawa, T. 2014. Towards development of 
OER derived custom-built open textbooks: A baseline survey 
of university teachers at the University of the South Pacific. 
The International Review of Research in Open and Distance 
Learning. 15, 4 (Aug. 2014), 226–247. 

[14] Rodríguez-Triana, M.J., Martínez-Monés, A., Asensio-Pérez, 
J.I. and Dimitriadis, Y. 2015. Scripting and monitoring meet 
each other: Aligning learning analytics and learning design to 
support teachers in orchestrating CSCL situations. British 
Journal of Educational Technology. 46, 2 (Mar. 2015), 330–
343. 

[15] Yuan, M. and Recker, M. 2015. Not All Rubrics Are Equal: 
A Review of Rubrics for Evaluating the Quality of Open 
Educational Resources. The International Review of 
Research in Open and Distributed Learning. 16, 5 (Sep. 
2015).  

 



