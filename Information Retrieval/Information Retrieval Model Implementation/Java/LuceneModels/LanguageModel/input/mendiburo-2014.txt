
Interaction Design for Improved Analytics

Maria Mendiburo
Vanderbilt University

Nashville, TN 37203, U.S.A.
maria.mendiburo
@vanderbilt.edu

Brian Sulcer
Vanderbilt University

Nashville, TN 37203, U.S.A.
brian.sulcer

@vanderbilt.edu

Ted Hasselbring
Vanderbilt University

Nashville, TN 37203, U.S.A.
t.hasselbring

@vanderbilt.edu

ABSTRACT
In this paper, we explain a portion of the design research
process that we used to develop the learning analytics for a
manipulative-based fractions intervention program. In par-
ticular, we highlight a set of qualitative interviews that we
conducted with individual students after a short study in
which students in three classes at the same school learned
to use virtual manipulatives to compare pairs of proper frac-
tions and order groups of 3 proper fractions. These quali-
tative interviews provided us with considerable information
that helped us improve the interactions students have with
the virtual manipulatives and produce more sophisticated
and informative analytics. We emphasize the importance of
using mixed-methods during the iterative cycles of develop-
ment that define design research.

Categories and Subject Descriptors
K.3.1 [Computers and Educatation]: Computer Uses in
Education

General Terms
Measurement, Design, Human Factors

Keywords
virtual manipulatives, fractions, design research

1. INTRODUCTION
In June of 2010, we began an IES-funded development

project called Helping At-risk students Learn Fractions (HALF).
The long-term goal of HALF is to develop and pilot test a
complete, technology-based fractions intervention program
for fifth- and sixth-grade students. Research suggests that
manipulatives (e.g. fractions circles, fractions strips) pos-
itively impact students’ conceptual and procedural under-
standing of fractions without impeding their ability to com-
plete algorithmic procedures involving fractions [3]. There-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
LAK’14, March 24 - 28 2014, Indianapolis, IN, USA
Copyright 2014 ACM 978-1-4503-2664-3/14/03 ...$15.00.
http://dx.doi.org/10.1145/2567574.2567628.

fore, the HALF intervention program fully integrates vir-
tual manipulatives into all aspects of instruction, and stu-
dents learn to create and interpret visual models of fractions
problems.

The complete HALF intervention program combines indi-
vidualized instruction facilitated by a technology-enhanced
learning environment (TELE) with small-group and whole-
class instruction facilitated by a teacher. We intend for
teachers who implement HALF in their classrooms to use
data generated as students work individually on the TELE
to inform all the instructional decisions that they make. We
developed the entire HALF intervention program, which in-
cludes the analytics about the manipulatives that are em-
bedded in the TELE, using a design research methodology
that involves iterative cycles of design, enactment, analysis,
theory refinement, and redesign [2]. Although the learning
analytics for our program are still under development, by
our own analysis, the quality of the quantitative data we
have been able to collect via the TELE about student learn-
ing with manipulatives has improved dramatically since the
beginning of the project. In addition, the extent to which
we are able to translate the data collected by the TELE
into actionable instructional recommendations for teachers
rather than static results that teachers must still interpret
before applying them to their instructional practice has also
improved dramatically. We believe these improvements will
ultimately have a positive impact on the effectiveness as well
as the scalability of the intervention program.

In this paper, we explain part of the design research pro-
cess that we used to develop the learning analytics for the
HALF program for the purpose of sharing effective design re-
search methods with the broader community of learning an-
alytics researchers. In particular, we highlight a set of quali-
tative interviews that we conducted with individual students
after a short study in which students in three classes at the
same school learned to use the virtual manipulatives to com-
pare pairs of proper fractions and order groups of 3 proper
fractions. The purpose of this is to demonstrate how the
results of qualitative research provided insights into design
changes for the interactions students have with the virtual
manipulatives, which in turn improved the quality of the
analytics produced by the TELE. We also emphasize the
importance of using mixed-methods during the iterative cy-
cles of development that define design research.

2. CLASS LEARNING ZONES
The theory behind the Zone of Proximal Development

(ZPD) [7] is that an individual learns the most when the

78



skill or knowledge required for a task is not proportionally
too great for the individual’s ability. If the task is too easy
relative to ability, there is little for the individual to learn;
if the task is too hard, the individual does not have a suffi-
cient foundation and cannot be successful. Evaluation data
helps a teacher know where each student’s current skill level
lies in order to know the best place to start instruction, but
a number of factors can contribute to if and how teachers
integrate data on student performance into lesson planning
and instruction. A teacher’s level of content knowledge and
data literacy skills can preclude understanding of the pro-
vided data and prevent it from being utilized [5]. Teach-
ers may have personal biases against data, believing instead
that personal experience and intuition are better sources of
decision-making [4]. Time also plays a role; teachers have
an increasing number of demands on their time during the
school day, so there is not always enough time to properly
evaluate empirical information and use it to generate in-
structional content [4].

While it is not reasonable to expect a classroom teacher to
use evaluation data drawn from 20-35 students to specifically
tailor every lesson they teach to exactly match each student’s
individual ZPD, Murata and Fuson’s [6] work shows that
teachers can create a Class Learning Zone (CLZ) that ad-
vances students through their individual Zones while work-
ing as a class unit. The idea of CLZ is used to conceptualize
how one teacher can enact the ZPD framework for an entire
class of students. Although a class may have 20-35 students,
when a teacher introduces new content to students, the ma-
jority of students use 3 to 6 solution strategies to solve prob-
lems related to that content. Students who fail to master
the content on the first try are likely to make one of a small
set of predictable errors. Of course, random errors exist and
it is always possible for a student to use a novel method of
solving a problem, but CLZ suggests that classroom teach-
ers and specialized technologies can work synergistically to
provide students with the adaptive instruction that they re-
quire.

2.1 Long-Term Learning Analytics Goals
Rather than reporting only summaries of student perfor-

mance, our long-term goal for the analytics that we develop
for the HALF intervention program is for the system to be
able to aggregate data gathered by the TELE into summa-
tive reports that impact teachers’ ability to enact CLZ. We
aim for the system to aggregate information about individ-
ual students and then organize the reports in a summary
format that groups together students with similar learning
trajectories. We also aim for the system to deliver actionable
instructional recommendation for each group that teachers
can use to inform their pedagogy as they plan lessons before
class and then implement those lessons during class. The
study reported in this paper describes only one small step
towards these ambitious long-term goals, but it represents
how we learned to use qualitative research as an important
component of the design research process.

3. DESCRIPTION OF THE SYSTEM
The TELE used in this intervention was implemented as a

web application hosted on a portable server situated in the
classroom. The TELE presented two distinct interfaces: an
interactive learning environment for students and a report-
ing interface for teachers. Both interfaces were implemented

Figure 1: The student workspace interface.

Figure 2: The summary report interface.

in the browser using modern web standards and were de-
signed for touch screen interaction. Students experienced
the TELE via iPads.

The user interface for the virtual manipulatives (see Fig-
ure 1) includes a large, rectangular box called the workspace
on the left side of the screen. Students create models of frac-
tions in the workspace using a set of denominator buttons.
Students “shade” the number of pieces that corresponded to
the numerator of each fraction by tapping individual pieces
or swiping across a group of pieces. Practice problems ap-
peared on the right side of the screen. For the purpose of
answering research questions about feedback that are not
the focus of this paper, students assigned to different treat-
ment conditions received different types of feedback from the
system. Traces of student activity generated by the system
are stored in a relational database by the web application
for further use.

The teacher reporting interface presents two views that
are updated dynamically as the students work with the TELE.
The summary view (see Figure 2) gives an overview of the
students’ progress on the current activity. Each cell contains
two accuracy indicators: question response (upper-left) and
model (lower-right). The detail view (see Figure 3) is acces-
sible by clicking a student’s name or identifier in the sum-
mary view. This view allows the teacher to examine indi-

79



Figure 3: Sample entries from the student detail report.

vidual problem responses for a student. Each entry displays
both the student’s response to the problem and the model
they constructed.

4. METHODS
The study took place in a public charter school located

in the state of Tennessee (USA) that served primarily low-
achieving, minority students from low-income families. The
sample for the intervention included 41 students (26 female
and 15 male) drawn from 3 intact, fifth-grade mathematics
classes (10-11 years old). On the first day of the interven-
tion, a researcher led a general class discussion about frac-
tions, introduced the virtual manipulatives to the students,
and then briefly demonstrated how to use the manipulatives
to compare the relative size of two fractions. The students
then used iPads to complete a 10-question, multiple-choice,
pre-test. Five of the problems asked students to compare
two fractions and indicate which fraction was larger or indi-
cate that the two fractions were equal. The remaining five
problems asked students to order a group of three fractions
from smallest to largest. The system required students to
build a model of each problem before submitting the answer
to the problem. The system recorded the accuracy of the
students’ models along with the accuracy of their responses.
On the second day of the intervention, the researcher con-
ducted a lesson that included a review of the concepts stu-
dents learned the previous day, two class discussions, and
two practice sessions. During each pratice session, students
answered as many practice problems as they were able to
answer during the time limit, and they received feedback
about their answers from the system. Students assigned
to different treatment conditions received different types of
feedback. On the third day of the intervention, the students
completed a 10-question post-test that was identical to the
pre-test. A research assistant took field notes in each class
on every day of the study.

After the intervention, two researchers visually analyzed
the dynamic web reports generated for each class and for
each individual student for the purpose of choosing students
from each class to interview individually. The researchers
ultimately chose to interview 7 students (3 male and 4 fe-
male). The data for each student chosen for an interview
differed from the data of their classmates who were not cho-
sen for interviews in various ways, but all of the students
who were interviewed achieved low scores on the post-test.
Prior to the interview with each student, the 2 researchers
compiled notes that summarized their observations of each
student from the dynamic web reports and the field notes

taken during the intervention and set goals for what data
to collect during the interview. One researcher then inter-
viewed all 7 students. These interviews were audio recorded.

The researcher who conducted the interviews began each
one by asking the students general questions about their reg-
ular mathematics class and their experiences during the in-
tervention. She then asked students to “talk aloud” as they
solved problems from the post-test. The researcher used
each student’s answers along with the summative notes to
determine whether the evidence suggested that any specific
misconceptions or incomplete understandings could explain
that student’s low scores on the post-test; if so, she used var-
ious pedagogical strategies to attempt to address the mis-
conceptions and/or incomplete understandings. If the re-
searcher could not identify any particular misconception or
incomplete understanding, she asked probing questions that
were designed to determine what other factors might explain
why the student achieved a low score. Each interview was
audio recorded and lasted between 10 and 20 minutes. The
researcher who conducted the interviews wrote notes that
summarized her findings immediately after each interview.
A research assistant transcribed all of the audio recordings
from the interviews. Three researchers individually coded
and analyzed the qualitative data and then compared the re-
sults of their analysis. The qualitative results reported below
represent the summative findings of all three researchers.

5. RESULTS
On average, the ability of students who participated in

the intervention to compare pairs of proper fractions and to
order groups of 3 fractions using virtual manipulatives im-
proved between pre-test (M = 0.67, SD = 0.28) and post-
test (M = 0.79, SD = 0.27). The effect size of the difference
was moderate (d = 0.52), and a paired t-test showed that
it was statistically significant (t = 3.34, p < 0.01). How-
ever, the response accuracy for the 7 students we interviewed
was substantially lower than their classmates at post-test
(M = 0.34, SD = 0.24). The students’ scores at pre-test
and their performance during the practice activities varied,
but in general, the students we interviewed achieved less
than their classmates who were not interviewed on these
performance indicators. The qualitative data we collected
from all these 7 students impacted our understanding of
how to improve the learning analytics for the HALF inter-
vention program. We provide more specific detail about 3
cases in the sections below. We chose to highlight these
cases because the quantitative data that we collected about
these students during the practice activities and the visual

80



inspections of the summary and detail reports for each stu-
dent yielded mostly inconclusive or misleading results while
the qualitative data we collected provided particularly infor-
mative insights that led to design changes in future versions
of the system.

5.1 Student 1
The interview with Student 1 revealed that although the

student successfully learned how to build models of fractions
problems, the student completely ignored the model when
answering problems and instead relied on a procedural strat-
egy for ordering fractions that we did not teach or discuss
at any point during the intervention. The strategy involved
converting each fraction to an equivalent number that he
memorized that presumably should have been the decimal
equivalent of each fraction. However, the student applied
the strategy incorrectly, which is shown in the excerpt be-
low:

Student 1 : Um, I think one half would go first.

Researcher : Okay, how come?

Student 1 : Because it’s one-half equals five

Researcher : One half equals five, what do you
mean though, one half equals five?

Student 1 : Because it’s lower than any number,
um, than one third

Researcher : Okay, how is it lower?

Student 1 : Because one fifth equals, um, twenty-
five, and one-third equals three hundred thirty-
three

Researcher : Okay. Oh, when you convert it into
a decimal?

Student 1 : Hmm hmm

Researcher : Oh, okay, so you were converting
them into a decimal in your head?

Student 1 : Hmm hmm

Researcher : Okay, got it. So what do you think
the answer [to this problem] would be?

Student 1 : Um, one half, one third, and one fifth

We learned after the interview that Student 1’s teacher
required his students to memorize the decimal equivalents
of common unit fractions. Student 1’s answer suggests that
he memorized the digits of each decimal equivalent without
understanding what type of number (decimal versus whole
number) the digits represent. It also suggests that he does
not understand place value of decimals. However, the stu-
dent still chose to use a procedural strategy involving dec-
imals to solve the problem rather than applying the strat-
egy that we taught of using virtual manipulatives to build
a model of proper fractions and then using the models to
compare the relative size of the fractions. When answering
other problems, the student used the whole number domi-
nance strategy, which is a very common incorrect strategy in
which students compare fractions by making separate com-
parisons of the numerators and the denominators using the
ordering of whole numbers [1].

To address these misconceptions, the researcher showed
the student several additional models of groups of proper

fractions. Rather than using word names or symbolic rep-
resentations to refer to the fractions that the models rep-
resented, she referred to each fraction by the color of the
strip that was used to represent it in the model (e.g. the
red fraction, the blue fraction) and asked the student to in-
terpret the relative size of each part of the model rather
than order a group of three proper fractions (e.g. Which
fraction has more area shaded? The red fraction or the
blue fraction?). This tactic seemed to improve the student’s
ability to interpret the models and correctly answer prob-
lems by the end of the interview. The data collected during
this interview helped us develop a method of scaffolding the
problem solving process in which we remove any symbols
from the problems and require the student to answer the
problems by interpreting visual models. This new method
of scaffolding helps address the very common whole number
dominance strategy as well as any other incorrect strategy in
which students build correct models but then answer prob-
lems without interpreting the models that they have built.

5.2 Student 2
Despite the fact that Student 2 learned to build correct

models of fractions during our intervention, she demonstrated
almost no understanding of fractions during the follow-up
interview. In the excerpt below, the student struggles to
correctly name a set of three proper fractions that includes
4
8
, 2

3
, and 3

12
.

Researcher : So, what’s this one say?

Student 2 : Uh, eight fourth, comma, three half

Researcher : And then say the last one

Student 2 : And twelve thirds

Researcher : Okay, well when we read a fraction,
the top number is the numerator and the bottom
number is the denominator, so we read it the
other way. Can you try that? So what would
that be? Instead of eight-fourths, what would it
be?

Student 2 : Uh, eight... I’m confused

Researcher : Would it be four-eighths, maybe?
No? How come?

Student 2 : Because the four is on the top and
the eight is at the bottom

In the next excerpt, the student explains why she chose
four eighths as the largest fraction.

Student 2 : Because four is like the biggest, four is
like right there on the biggest number and then
three is like, like in the middle to be like the
highest, but it’s still like low. And then you’ve
got two that’s low, real low. And twelve, it would
be it, but it’s under three, so it can’t be it. It
would be good if it was under four.

The quantitative data from the post-test results showed
that Student 2 had the ability to translate symbolic repre-
sentations of fractions into visual models, but the qualitative
data collected during the interview confounded the quantita-
tive results by showing that Student 2 lacked understanding
of the symbolic representations themselves. It also showed

81



that the student lacked the ability to translate symbolic rep-
resentations of fractions into word names. We determined
that any additional instruction about fractions for Student
2 should assume that the student is at a very early devel-
opmental stage of understanding fractions. Data from this
interview contributed to our decision to directly assess stu-
dents’ ability to interpret symbolic representations of frac-
tions and their ability to translate symbolic representations
of fractions into word names. We built additional lessons
for the TELE that are intended to help students develop
these skills and then make connections between symbolic
representations, word names, and visual models of fractions.
If students fail to master these lessons via the TELE, we
now recommend that they receive direct intervention from
a teacher prior to moving on to the more advanced lessons
that require students to interpret models of fractions.

5.3 Student 3
During the interview with Student 3, the researcher noted

that the student seemed overwhelmed by the task of using
the virtual manipulatives to compare two fractions and order
groups of fractions, especially when she answered problems
incorrectly and was required to interpret feedback messages
from the TELE and revise her answers. The researcher also
noted that Student 3 strugled with multi-step processing.
For example, when asked to order groups of three fractions,
she seemed to have less difficulty interpreting the model if
the strips were already ordered visually and could be read
from top to bottom or bottom to top. However, if she needed
to mentally keep track of the correct order, she seemed to
have more trouble interpreting the model. Like Student 2,
this student seemed to have difficulty translating between
symbolic notation, word names, and visual representations,
but her answers to the interview questions suggested this
may be explained at least in part by a difficulty with multi-
step processing and/or interpreting verbal directions rather
than a lack of understanding of basic fractions concepts.
When the researcher asked the student to talk about frac-
tions only in terms of the color of each fraction strip, she
saw noticeable improvement.

The school’s records did not indicate that this student
had a learning disability, but the qualitative results of our
research suggest the Student 3’s low performance on the
post-test may be explained at least in part by factors that
are related to learning difficulties that extend beyond the
content taught during the intervention. These results con-
tributed to our decision to add a feature to the virtual ma-
nipulatives that allows students to easily rearrange the or-
der of the fractions strips. They also contributed to the
development of a new scaffolding framework that we em-
bedded in the system. When students are struggling, the
system automatically reduces the complexity of the prob-
lem and then gradually increases it when performance im-
proves. This method of scaffolding makes the system more
accessible to struggling students and students with learning
disabilities. It also provides more sophisticated data that
allows teachers to more accurately diagnose the source of
difficulty for individual students.

6. CONCLUSIONS
This paper explains a portion of the design research pro-

cess we used to develop the type of learning analytics that we
need to meet our long-term goals, and it highlights the im-

portance of collecting qualitative data about student users.
The qualitative interviews we conducted with 7 individual
students captured valuable information about the process
students used to solve fractions problems with manipula-
tives that we had previously been unable to capture. Once
we better understood these students’ process, we were able
to redesign the interaction students had with the TELE in
ways that provided more useful quantitative data. Because
the meaning of interaction traces is defined by the designs of
the interactions, refinement of analytics and user interaction
with a TELE go hand in hand.

7. ACKNOWLEDGMENTS
This research was supported by the Institute of Educa-

tion Sciences, U.S. Department of Education, through Grant
R305A100110 to Vanderbilt University. The opinions ex-
pressed are those of the authors and do not represent views
of the Institute or the U.S. Department of Education.

8. REFERENCES
[1] M. Behr, I. Wachsmuth, T. Post, and R. Lesh. Order

and equivalence of rational numbers: A clinical
teaching experiment. Journal for Research in
Mathematics Education, 15(5):323–341, 1984.

[2] P. Cobb, J. Confrey, A. Disessa, R. Lehrer, and
L. Schauble. Design experiments in educational
research. Educational Researcher, 32(1):9–13, 2003.

[3] K. A. Cramer, T. R. Post, and R. C. delMas. Initial
fraction learning by fourth- and fifth-grade students: A
comparison of the effects of using commercial curricula
with the effects of using the rational number project
curriculum. Journal for Research in Mathematics
Education, 33(2), 2002.

[4] D. Ingram, K. S. Louis, and R. G. Schroeder.
Accountability policies and teacher decision making:
Barriers to the use of data to improve practice.
Teachers College Record, 106(6):1258–1287, 2004.

[5] E. Mandinach, M. Honey, D. Light, and C. Brunner. A
conceptual framework for data-driven decision making.
In E. Mandinach and M. Honey, editors, Data-driven
school improvement: Linking data and learning, pages
13–31. Teachers College Press, New York, NY, 2008.

[6] A. Murata and K. Fuson. Teaching as assisting
individual constructive paths within an interdependent
class learning zone: Japanese first graders learning to
add using 10. Journal for Research in Mathematics
Education, 37(5):421–456, 2006.

[7] L. L. S. Vygotsky. Mind in society: The development of
higher psychological processes. Harvard University
Press, 1978.

82





