
A Gaze-based Learning Analytics Model: In-Video Visual
Feedback to Improve Learner’s Attention in MOOCs

Kshitij Sharma
CHILI Lab, EPFL

RLC D1 740, Station 20
1015 Lausanne, Switzerland
kshitij.sharma@epfl.ch

Hamed S. Alavi
CHILI Lab, EPFL

RLC D1 740, Station 20
1015 Lausanne, Switzerland

hamed.alavi@epfl.ch

Patrick Jermann
CEDE, EPFL

RLC D1 740, Station 20
1015 Lausanne, Switzerland
patrick.jermann@epfl.ch

Pierre Dillenbourg
CHILI Lab, EPFL

RLC D1 740, Station 20
1015 Lausanne, Switzerland

pierre.dillenbourg@epfl.ch

ABSTRACT

In the context of MOOCs, “With-me-ness” refers to the ex-
tent to which the learner succeeds in following the teacher,
specifically in terms of looking at the area in the video that
the teacher is explaining. In our previous works, we em-
ployed eye-tracking methods to quantify learners’ With-me-
ness and showed that it is positively correlated with their
learning gains. In this contribution, we describe a tool that
is designed to improve With-me-ness by providing a visual-
aid superimposed on the video. The position of the visual-
aid is suggested by the teachers’ dialogue and deixis, and it
is displayed when the learner’s With-me-ness is under the
average value, which is computed from the other students’
gaze behavior. We report on a user-study that examines
the effectiveness of the proposed tool. The results show
that it significantly improves the learning gain and it sig-
nificantly increases the extent to which the students follow
the teacher. Finally, we demonstrate how With-me-ness can
create a complete theoretical framework for conducting gaze-
based learning analytics in the context of MOOCs.

Categories and Subject Descriptors

K.3.1 [Computers and Education]: Computer Uses in
Education—Collaborative learning

Keywords

Eye-tracking, video based learning, MOOCs, Student atten-
tion

1. INTRODUCTION
The new wave of online learning – Massive Open Online

Courses – has brought with it new challenges as well as new

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

LAK ’16, April 25 - 29, 2016, Edinburgh, United Kingdom

c? 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00

DOI: http://dx.doi.org/10.1145/2883851.2883927

opportunities for learning analytics on a population of learn-
ers with high diversity of profiles. We specifically look at the
teacher-student interaction in MOOCs, and frame it from a
dyadic interaction point of view: “How can we measure and
improve students’ attention in MOOC lectures?” The so-
lution we propose in this contribution works independent
of the learning topics or the students’ backgrounds. In ad-
dition, it can stay at the periphery, keeping the students
focused on the learning content.

In our previous works, we have shown that, with eye-
tracking methods one can get insight into the learner’s per-
formance irrespective of their background or the learning
content [13]. Grounded in those results, this contribution
presents a tool that, through visual feedback, drives the stu-
dent’s attention to the part of the displayed content that is
being explained by the teacher.

In two previous studies, we captured students’ attention
as a response to what the teacher was saying. We addressed
this situation from the teacher’s perspective: “How much the
student is with me?” We called this gaze-measure With-
me-ness which identifies the extent to which the student
was “following” the lecturer, i.e. paying attention to the
parts of the display that correspond to the instant behaviour
of the teacher. We selected two aspects of teacher’s be-
haviour that could influence the students’ attention: the
teacher’s dialogue and the deictic references.

In this paper, we build on our previous results and present
a gaze-aware feedback tool to notify the learners about their
levels of With-me-ness, while they watch a MOOC lecture.
In addition, we construct a discussion that is structured
within the Learning Analytics (LA) models with three phases
[3, 9] [2]: 1) collecting data from the learners, 2) analysing
different variables and their relation with learning processes
and outcomes, and 3) providing the feedback to the learners
and further study the change in their behaviour, the learn-
ing processes and outcomes. We show how the proposed
tool contributes to the third phase and completes the dif-
ferent LA loops. Given the results of our previous studies
coupled with the results of the study presented in this pa-
per, we will argue that the notion of With-me-ness
can create a complete theoretical framework for con-
ducting gaze-based learning analytics in the context
of MOOCs.



2. IN-VIDEO VISUAL FEEDBACK

2.1 Background
In our previous studies we analyzed the learner’s gaze be-

havior while watching MOOC videos and developed the no-
tions of perceptual and conceptual With-me-ness as follow-
ing:

Perceptual With-me-ness: Is the extent to which the
learner succeeds in following the teacher’s explicit deictic
gestures. It is defined as combination of three components:
1) entry time is the temporal lag between the times a refer-
ring pointer appeared on the screen and stops at the referred
site (x,y); and the time student first looked at (x,y); 2) first
fixation duration is how long the student gaze stopped at
the referred site for the first time; 3) revisits are the number
of times the student’s gaze came back to the referred site.

Conceptual With-me-ness: Is the extent to which the
learner succeeds in following the content that is being ex-
plained through other channels such as dialogue. The teacher
may also verbally refer to the different objects on the dis-
play. We measured how often a student looked at the object
(or the set of objects) verbally referred to by the teacher
during the whole course of time (the complete video dura-
tion). In order to have a consistent measure of conceptual
“With-me-ness” we normalised the time a student looked at
the overlapping content (the verbal reference and the slide
content) by slide duration.

User studies have shown that both the levels of
With-me-ness are significantly correlated (positive)
with the learning gain.

2.2 Visual feedback tool
The visual feedback that we developed consists of a set

of red rectangular wire-frames highlighting the area of the
screen which the teacher was talking about (Figure 1). This
is made visible to the learner only when her With-me-ness
levels went below the baseline.

The baseline was calculated as the average of the “With-
me-ness” levels of the participants from our previous exper-
iment (with the same video content). This baseline was cal-
culated for each second of the video lecture. To calculate the
baseline we took the participants from the previous experi-
ment whose leaning gains were between 33 and 66 percentiles
of the overall learning gains of the previous experiment. The
reason for selecting this range of scores was that we wanted
to give the feedback based on the typical behaviour of the
students. In the remaining part of this paper this group is
called the “baseline group”. The learning gains of the two
groups are comparable as they had the same pretest and
posttest. We considered only a subset of this group to de-
fine our baseline, however, to compare the learning gains we
will use the complete set (with 50 students).

2.3 The present Study
We conducted an eye-tracking study where 27 participants

attended a MOOC lecture and received the visual feedback
as described in the previous subsection. Students received
the feedback whenever their With-me-ness was less than
the baseline at any given point of time in the video. The
hypothesis is that the gaze-aware feedback increases stu-
dents’ With-me-ness; and thus their learning gains. Pre-
cisely, through this study we address the following research
questions:

Figure 1: Example of the feedback used in the ex-
periment. The circumscribing red rectangle were
shown if the With-me-ness of the participant went
below the baseline With-me-ness at any given in-
stant during the video.

1. How does the gaze-aware feedback affect the gaze pat-
terns (With-me-ness) while watching the video?

2. How does the gaze-aware feedback affect learning gains?

2.4 Experiment

2.4.1 Participants and procedure
There were 27 bachelor students from E?cole Polytech-

nique Fe?de?rale de Lausanne, Switzerland participating in
the present study. There were 6 females among the partici-
pants. The participants were compensated with an equiva-
lent of $30 for their participation in the study.

Upon their arrival in the laboratory, the participants signed
a consent form. Then the participants took a pretest about
the video content. Then the participants watched two videos
about “Resting membrane potential”. Finally, they took a
posttest. The videos were taken from“Khan Academy”. The
total length of the videos was 17 minutes and 5 seconds.
It is worth mentioning that the teacher was not physically
present in the video. The participants were told that the
feedback would appear only when they were not paying at-
tention to what the teacher was saying or writing.

2.4.2 Dependent variables
Learning Gain: The learning gain was calculated as the

difference between the individual pretest and posttest scores.
The minimum for each test was 0, and the maximum for the
pretest was 9 and for the posttest was 10.

With-me-ness: We used the same method as described
Section 2.1, to calculate students’ With-me-ness levels, in
this experiment, in real time.

2.5 Results and interpretation
Feedback and Learning Gain: We observed a sig-

nificant improvement in learning gain for the experimental
group over that for the baseline group (t (df = 49.88) = -
2.50, p = .02, figure 2). The two populations (in the baseline
and the experimental) were largely similar (the participant
recruitment was done using the same university channel, and
there was no drastic changes in student populations) in the
two conditions.

Immediate effect of feedback on gaze: We observed
a significant improvement in With-me-ness levels for par-



ticipants (within the experimental group) before (mean =
0.31, sd = 0.08) and after (mean = 0.57, sd = 0.16) display-
ing the feedback (F [1, 26] = 310, p < .001, figure 3). The
duration of each instance of the displayed feedback was min-
imum 2 seconds. The With-me-ness levels were significantly
higher after showing the feedback than before showing the
feedback. It can be explained by the salient nature of the
feedback: since the red rectangles appeared as a salient vi-
sual feature for the participants, their attention was drawn
towards the feedback.

0
.3

5
0
.4

0
0
.4

5
0
.5

0

Experimental conditions

L
e
a
r
n
in

g
 g

a
in

 

(n
o
r
m

a
li
s
e
d
 b

e
tw

e
e
n
 0

 a
n
d
 1

)

Baseline

group

Experimental 

group

n=50 n=27

Figure 2: Learning gain for the experimental and
baseline conditions.

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

Feedback timing

W
it
h
?

m
e
?

n
e
s
s
 l
e
v
e
ls

1.Before 

feedBack

2.After

feedBack

n=27 n=27

Figure 3: Immediate effect of feedback on With-me-
ness.

Overall effect of feedback on gaze: In order to find
the overall effect of the feedback on the participants’ gaze,
we divided the whole video in one minute episodes. Results
from a linear mixed effect model showed that on average,
participants’ With-me-ness increased by 1% every minute.
This improvement was significant over time (F [1, 26] =
32.60, p < .0001). Figure 4 shows the temporal evolution
for the difference between the observed mean With-me-ness
and the baseline With-me-ness for the participants; and the
average number of times the feedback was shown to the par-
ticipants. We can see in Figure 4 that, towards the end of
the video, the difference increased and the number of feed-
back displayed decreased. This showes that the participants
became more aware of the fact that they should follow the

0.0

0.2

0.4

0.6

0 5 10 15

Time (minutes)

Figure 4: Overall effect of feedback on the gaze. The
whole video was divided into one minute episodes.
The red curve shows the difference between the ob-
served and baseline With-me-ness (smoothed using
a two minute rolling window). The bars denote
the number of required feedback per participant per
minute.

teacher in an efficient manner.
The significant long term effect on the With-me-ness in-

dicates that the feedback had an effect on participants’ at-
tention in the terms of “how well they follow the teacher
in both the deictic and dialogue spaces”. One plausible in-
terpretation of increase in With-me-ness over time could be
that the participants internalized the fact that following the
teacher during the MOOC video is important to understand
the content and thus they started following the teacher more
closely than before. This effect is also evident from Figure
4, where we can see that the difference between the baseline
With-me-ness and the observed With-me-ness was higher
during the second half of the video.

3. RELATED WORK
In this section, we present the previous studies demon-

strating the relation between 1) gaze and dialogues, 2) gaze-
awareness, and 3) learning analytics models proposed in the
literature.

3.1 Gaze in communication and referencing
Gaze and speech are coupled. Mayer et. al.,[11] showed

that the time duration between looking at an object and
naming it is between 430 and 510 milliseconds. [7] showed
that there exists an eye-voice span of about 900 millisec-
onds. The eye-voice span denotes the time between looking
at a picture and start to provide a short explanation to it.
Allopenna et. al.,[1] showed that the mean delay between
hearing a verbal reference and looking at the object of ref-
erence (the listeners’ voice-eye span) was between 500 and
1000 ms.

Richardson et. al.,[4] proposed the eye-eye span as the
difference between the time when the speakers started look-
ing at the referent and the time when listeners looked at
the referred object. This time lag was termed as the cross-
recurrence between the participants. The results show that



the cross recurrence was correlated with the correctness of
the answers given by the listeners in a comprehension quiz.
The average cross-recurrence was found to be between 1200
and 1400 milliseconds.

Jermann and Nu?ssli[10] extended the concept of cross-
recurrence in a pair programming task, by enabling the re-
mote collaborators to share their selections on the screen.
Results showed that the cross-recurrence levels were higher
when there was a selection present on the screen than the
times when there was no selections on the screen. Moreover,
the cross-recurrence was higher, in the case, when a selection
was followed by a verbal explanation.

Gergle et. al.,[6] conducted a dual eye-tracking study
where the participants completed a collaborative reference
elicitation task. The participants were given four replicas
for the same sculpture. The key task for the participants
was to find the correct replica. The authors found that the
gaze overlap between the partners was lowest when the ref-
erences were local as compared to when the references had
location modifiers.

The notion of With-me-ness builds upon the combined
notion of gaze-speech and gaze-deixis coupling. The two
levels of With-me-ness, perceptual and conceptual capture
the gaze-speech and gaze-deixis couplings respectively, for a
teacher-student dyad.

3.2 Gaze-awareness
Gaze-awareness had been used to build intelligent tutor-

ing systems [5, 15, 8], online collaboration support [12, 14].
D’Mello et. al., [5] used students’ real time gaze informa-
tion to inform the tutor about the boredom and engagement
levels for selecting the dialogue moves for the virtual tutor
accordingly. The authors found that the gaze-aware tutor
was more effective in terms of both maintaining a higher en-
gagement level and achieving a higher learning gain. Wang
wt. al, [15] used students’ gaze information to infer the tu-
tor’s strategy in terms of the instruction and feedback to be
given, and the emotions of the tutor. The authors also used
gaze as the interaction modality for students to interact with
the system. In a preliminary usability test the authors found
that such a feedback improved students’ involvement with
the learning processes. Gaze-awareness was also shown to
be effective in improving the quality of online collaboration
[12, 14].

The gaze-aware feedback tool that we propose, gives real-
time feedback to the students based on their gaze. The key
difference from the reviewed similar work is that we give the
feedback directly to the students rather than providing it
to a tutor. The system computes students’ With-me-ness
levels and gives them a visual feedback on the video lecture,
if their With-me-ness levels fall below a certain threshold.

3.3 Learning analytics models
Clow [3], proposed a learning analytics cycle as a four-step

loop: 1) identifying the learner population, 2) generating
and capturing the data from the learners, 3) analysing the
data to get insights about the learning processes, and 4)
providing interventions to the learners based on the insights
acquired.

Jermann [9], proposed learning analytics as a cybernetic
control with four phases: 1) learner data collection, 2) se-
lecting one or more indicators to represent the current state
of learner, 3) diagnosis of the current state by comparing it

to a desired state, and 4) proposition for a remedial action.
Chatti et. al., [2], proposed learning analytic process as

a loop consisting of three phases: 1) learner data collection
and pre-processing (finding patterns in the data), 2) data
exploration based on the learning analytics objectives and
taking appropriate actions, such as, prediction, assessment
and recommendation, and 3) post-processing the newly ac-
quired data.

4. GENERAL DISCUSSION
In this section, we revisit the learning analytics models

summarized in Section 3.3 and use the results of the pre-
sented study, to argue that the notion of With-me-ness can
create a complete theoretical framework for conducting gaze-
based learning analytics in the context of MOOCs.

First, we consider the model proposed by Clow [3]. The
example model for a gaze-based LA model is shown in Fig-
ure 5. The learner population is a subset of MOOC students
(step 1). In our previous two experiments, we collected stu-
dents’ gaze data while they watched the MOOC videos (step
2). Further, we developed With-me-ness as a gaze-based in-
dicator and we found that it is positively correlated to the
learning gain (step 3). The experiment described in this pa-
per uses With-me-ness as an indicator for building a gaze-
aware feedback tool to notify students about their attention
level (step 4). The results show that the feedback tool not
only helped students to learn more but also improved their
attention levels.

Figure 5: A gaze-based learning analytics model in
compliance with the model proposed by Clow [3].

Second, we consider the model proposed by Jermann [9].
The gaze-based cybernetic control system is shown in Fig-
ure 6. We collect the gaze data of the students while they
watch the videos (phase 1). We define With-me-ness as an
indicator of the students’ current state (phase 2). We create
the desired learner state using the With-me-ness levels of
the students from our previous studies (phase 3). Finally,
the proposed gaze-aware feedback tool provides the feed-
back to the students about which part of the display that
students should look at, if the measured With-me-ness is
lower than desired With-me-ness at any instant (phase 4).
The gaze-aware feedback tool acts as the key element in this
gaze-based cybernetic control system.

Finally, we consider the LA model proposed by Chatti and
colleagues [2]. The gaze-based LA model is shown in Figure
7. We collect the gaze data of the students and measure their
With-me-ness (step 1). We found that students’ With-me-
ness is positively correlated with their learning gains (step
2). The gaze-aware feedback tool, we presented, monitors
the With-me-ness levels of the students and give them the
feedback about how much attention they are paying to the
teacher (step 3).



Figure 6: A gaze-based learning analytics cybernetic
control model in compliance with the model pro-
posed by Jermann [9].

Figure 7: A gaze-based learning analytics model in
compliance with the model proposed by Chatti and
colleagues [2].

Succinctly, With-me-ness as a gaze-based variable can be
used as a accurate learning analytic indicator for two pur-
poses: 1) to quantify learners’ attention, and 2) to design
intervention tools to provide feedback to the learners to in-
crease their attention levels as well as learning outcome.
Moreover, the feedback tool completes the LA loops by in-
tervening the learning process during the moments of lack
of attention.

5. CONCLUSIONS
In a nutshell, the gaze-aware intervention in the learn-

ing process of the students had a positive effect on their
attention. Provided that such a feedback is used during reg-
ular MOOC studies, this might have a long term impact on
students’ overall attention. Regarding our general research
question about “how to improve the attention of the stu-
dents during MOOC videos”; gaze-aware feedback emerged
as a influencing tool for intervention.

Finally, we propose, as future work, to create the visual-
feedback directly from the heat-map of students’ gaze pat-
tern, rather than eliciting it from the teachers’ dialogue
and deixis (as implemented in the presented system). This
can construct a reliable method especially in the context of
MOOCs as the number of students increases.

6. REFERENCES
[1] P. Allopenna, J. Magnuson, and M. Tanenhaus.

Tracking the time course of spoken word recognition
using eye movements: Evidence for continuous
mapping models* 1,* 2,* 3,* 4,* 5. Journal of memory
and language, 38(4), 1998.

[2] M. A. Chatti, A. L. Dyckhoff, U. Schroeder, and
H. Thu?s. A reference model for learning analytics.
International Journal of Technology Enhanced
Learning, 4(5-6):318–331, 2012.

[3] D. Clow. The learning analytics cycle: closing the loop
effectively. In Proceedings of the 2nd international
conference on learning analytics and knowledge, pages
134–138. ACM, 2012.

[4] R. D. D.C. Richardson and N. Kirkham. The art of
conversation is coordination. Psychological Science,
18(5):407–413, 2007.

[5] S. D’Mello, A. Olney, C. Williams, and P. Hays. Gaze
tutor: A gaze-reactive intelligent tutoring system.
International Journal of human-computer studies,
70(5):377–398, 2012.

[6] D. Gergle and A. T. Clark. See what i’m saying?
using dyadic mobile eye tracking to study
collaborative reference. In In Proceedings of the ACM
2011 conference on Computer supported cooperative
work (pp. 435-444). ACM., 2011.

[7] Z. Griffin and K. Bock. What the eyes say about
speaking. Psychological science, 11(4), 2000.

[8] N. Jaques, C. Conati, J. M. Harley, and R. Azevedo.
Predicting affect from gaze data during interaction
with an intelligent tutoring system. In Intelligent
Tutoring Systems, pages 29–38. Springer, 2014.

[9] P. Jermann. Computer support for interaction
regulation in collaborative problem-solving.
Unpublished Ph. D. thesis, University of Geneva,
Switzerland, 2004.

[10] P. Jermann and M.-A. Nussli. Effects of sharing text
selections on gaze cross-recurrence and interaction
quality in a pair programming task. In In Proceedings
of Computer Supported Collaborative Work 2012, 2012.

[11] A. S. Meyer, A. M. Sleiderink, and W. J. Levelt.
Viewing and naming objects: Eye movements during
noun phrase production. Cognition, 66(2):B25–B33,
1998.

[12] A. Oh, H. Fox, M. Van Kleek, A. Adler, K. Gajos,
L.-P. Morency, and T. Darrell. Evaluating look-to-talk:
a gaze-aware interface in a collaborative environment.
In CHI’02 Extended Abstracts on Human Factors in
Computing Systems, pages 650–651. ACM, 2002.

[13] K. Sharma. Gaze analysis methods for learning
analytics. PhD thesis, Ecole Polytechnique Federale de
Lausanne, 2015.

[14] K.-H. Tan, I. Robinson, R. Samadani, B. Lee, D. Gelb,
A. Vorbau, B. Culbertson, and J. Apostolopoulos.
Connectboard: A remote collaboration system that
supports gaze-aware interaction and sharing. In
Multimedia Signal Processing, 2009. MMSP’09. IEEE
International Workshop on, pages 1–6. IEEE, 2009.

[15] H. Wang, M. Chignell, and M. Ishizuka. Empathic
tutoring software agents using real-time eye tracking.
In Proceedings of the 2006 symposium on Eye tracking
research & applications, pages 73–78. ACM, 2006.



