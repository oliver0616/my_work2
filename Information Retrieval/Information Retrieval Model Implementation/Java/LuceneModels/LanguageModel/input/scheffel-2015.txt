
Developing an Evaluation Framework
of Quality Indicators for Learning Analytics

Maren Scheffel
Open Universiteit Nederland

Valkenburgerweg 177
6419 AT Heerlen,
The Netherlands

maren.scheffel@ou.nl

Hendrik Drachsler
Open Universiteit Nederland

Valkenburgerweg 177
6419 AT Heerlen,
The Netherlands

hendrik.drachsler@ou.nl

Marcus Specht
Open Universiteit Nederland

Valkenburgerweg 177
6419 AT Heerlen,
The Netherlands

marcus.specht@ou.nl

ABSTRACT
This paper presents results from the continuous process of
developing an evaluation framework of quality indicators
for learning analytics (LA). Building on a previous study,
a group concept mapping approach that uses multidimen-
sional scaling and hierarchical clustering, the study pre-
sented here applies the framework to a collection of LA tools
in order to evaluate the framework. Using the quantitative
and qualitative results of this study, the first version of the
framework was revisited so as to allow work towards an im-
proved version of the evaluation framework of quality indi-
cators for LA.

Categories and Subject Descriptors
I.6.4 [Computing Methodologies]: Model Validation and
Analysis; J.1 [Computer Applications]: Administrative
Data Processing—Education; K.3.1 [Computers and Ed-
ucation]: Computer Uses in Education

General Terms
Design, Standardization, Verification

Keywords
evaluation framework, assessment of learning analytics tools,
quality indicators, group concept mapping

1. INTRODUCTION
Over the years that learning analytics (LA) have become

more and more prominent, the number of tools and appli-
cations using such techniques as well as publications about
them has grown rapidly. And although the added value of
LA for learners as well as for educators has clearly been
recognised in the last few years [12], research on the compa-
rability of empirical LA studies is sparse. The comparison
of LA approaches, i.e. their measures, algorithms, results,

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
LAK’15, March 16 - 20, 2015, Poughkeepsie, NY, USA
Copyright 2015 ACM 978-1-4503-3417-4/15/03 ...$15.00
http://dx.doi.org/10.1145/2723576.2723629.

effects, etc., is hardly possible due to the lack of a compre-
hensive knowledge base about what makes a good, effective,
efficient, useful LA tool in a given situation.

We therefore developed a framework of quality indica-
tors (QIs) for LA to help standardise the evaluation of LA
tools [16]. The framework comprises five criteria (Objectives,
Learning Support, Learning Measures and Output, Data As-
pects, and Organisational Aspects) with four QIs each (see
Figure 1). In order to ensure an organically grown and ac-
cepted evaluation framework (EF), stakeholders active in
the domain of LA have been involved in the development
process of the EF using a group concept mapping (GCM)
approach.

The aim of the evaluation study presented in this paper is
to find out whether the EF developed is applicable to eval-
uate LA tools or whether it needs to be further adapted,
changed, restructured or defined differently for another eval-
uation cycle. This paper is thus building on the results of
the previous study [16], as well as drawing on the experience
of Drachsler et al. [6] who have successfully created an EF
specifically for data competitions in TEL.

The rest of this paper is structured as follows: Section
2 presents the methodology to evaluate the framework, fol-
lowed by the presentation of quantitative as well as quali-
tative study results. Section 3 then presents ways to work
towards an improvement of the framework for the next eval-
uation cycle. Section 4 concludes the paper.

2. FRAMEWORK EVALUATION STUDY

2.1 Methodology
For the evaluation of the framework two things had to be

done: on the one hand the EF needed to be turned into an
applicable tool itself and on the other hand a collection of
LA tools to validate the EF against had to be compiled. As
a first step, the EF’s criteria and QIs were therefore trans-
formed into a questionnaire using Google Forms1. For every
quality indicator the questionnaire asked (1) whether that
QI was present in/supported by a tool or not or whether
it was not applicable, (2) in what way that QI was present
in/supported by the tool, and (3) how difficult or easy (on a
scale of 1 (very difficult) to 5 (very easy)) it was to judge that
QI. At the end of each criterion section participants were of-
fered an open text box asking for any additional comments.2

1http://forms.google.com
2The complete questionnaire can be accessed at
http://bit.ly/EFqiLA.

16



Figure 1: First version of the framework of quality indicators for learning analytics

To find suitable LA tool candidates the submissions to
the previous Learning Analytics and Knowledge conferences
as well as a number of existing tools from previous project
partners were browsed. Eight prominent LA tools were then
randomly selected to be used for the evaluation of the frame-
work: Blackboard Learn 9.1 Retention Centre3, CourseSig-
nals4 [3, 2], EnquiryBlogger5 [8, 5], the LeMo project6 [7,
11], SNAPP7 [4], StepUp! [14, 15], Student Activity Meter
[9, 10] and Student Explorer [1, 13]. The study was con-
ducted with members from the LACE project8 consortium
and its associated partners. Each of the eight participants
evaluated two of the eight tools, which in turn meant that
each of the eight tools was evaluated twice.

Due to the nature of the study, i.e. the evaluation of the
framework of quality indicators for LA, outcomes dealing
with individual tools are not addressed. Instead the focus is
entirely on the setup and applicability of the EF’s criteria
and quality indicators.

2.2 Quantitative Results
To get an overview of the results for all QIs Table 1 shows

how many yes, no and not applicable every QI received. The
highest scoring instance for yes, no and not applicable are
highlighted. Table 2 summarises the rating values of all QIs
and also lists their average rating. The highest average rat-
ing is achieved by the QI awareness, i.e. 4.3, while the lowest
average is achieved by efficiency, i.e. 2.6. These two QIs are
also the ones with the lowest (awareness) and highest (effi-
ciency) non-applicability.

The data shows that the QIs of the first criterion, i.e.
Objectives, are often present in/supported by the tools anal-

3https://help.blackboard.com/en-
us/Learn/9.1 2014 04/Instructor/130 Student Performance
4http://www.itap.purdue.edu/learning/tools/signals/ and
http://www.itap.purdue.edu/studio/signals/
5http://learningemergence.net/tools/enquiryblogger/
6http://www.lemo-projekt.de
7http://www.snappvis.org
8http://www.laceproject.eu

ysed. Also, the amount of non-applicability of these QIs
is rather low compared to that of other criteria. The QI
of awareness has the highest score of yes, followed closely
by that of behavioural change. Non-applicability of QIs is
quite low in this criterion which in reverse means that they
are applicable and thus suitable QIs when evaluating LA
tools. Motivation seems to be the most controversial QI as
it has the most diverse results. Looking a the ratings for the
Objectives criterion this view is supported as most study
participants found it easy or very easy to judge the QIs of
this criterion.

The non-applicability of the QIs in the criterion Learning
Support is similarly low as that of the Objectives criterion.
Although they are applicable, however, they are not present
in/supported by the tools as often as the QIs of the first
criterion. Especially recommendation and activity classifi-
cation seem not to be as common in LA tools. The ratings
for the QIs in the Learning Support criterion are not as ten-
dentious as the previous ones. There are still many easy
and very easy ratings. However, the number of difficult and
very difficult ratings is notably higher. Especially activity
classification was deemed a difficult to evaluate QI by the
study participants.

Looking at the ratings for the QIs in the Learning Mea-
sures and Output criterion we can see that they are almost
evenly spread over the scale. No clear tendency of either dif-
ficulty or ease can be identified. Also, the non-applicability
of the QIs is quite a bit higher than that in the first two
criteria. In 50% of the cases efficiency was not applicable
while effectiveness was not applicable in 38% of the anal-
ysed cases. All QIs in this criterion, however, have rather
low (or even none) no values. It thus seems that QIs here
tend to be either present in/supported by the LA tools or
not applicable.

The QIs with the most no values are those of the Data As-
pects criterion, i.e. they are often not present in/supported
by the analysed LA tools. Non-applicability is on a medium
level of about a third for this criterion while the yes values
vary from low to medium levels. Study participants tended
to be rather positively confident when rating the QIs of this

17



Table 1: Presence (yes/no) or non-applicability of
quality indicators in a tool

yes no not applicable

awareness 15 - 1
reflection 12 2 2

motivation 9 4 3
behavioural change 14 1 1

perceived usefulness 14 - 2
recommendation 8 6 2

activity classification 6 8 2
det. of students at risk 12 3 1

comparability 12 1 3
effectiveness 9 1 6

efficiency 4 4 8
helpfulness 14 - 2

transparency 9 5 2
data standards 5 6 5
data ownership 1 10 5

privacy 9 2 5
availability 7 3 6

implementation 6 3 7
training of stakeholders 7 1 8

organisational change 8 5 3

criterion. Although there are hardly any very easy ratings,
the number of easy ratings is quite high.

The most clear and obvious rating tendency was given to
the QIs of the Organisational Aspects criterion. In three
quarters of the cases the QIs have either been rated as easy
or very easy to judge by the study participants. The non-
applicability of the QIs is the highest for this criterion while
yes and no values vary.

2.3 Qualitative Results
Apart from collecting quantitative feedback about the QIs,

study participants were also offered the opportunity to de-
scribe the application of the QI and to add comments.

Generally, participants thought that it was rather easy
to judge the QIs of the Objectives criterion. The resources
they used to evaluate the tool often provided information
about whether it supported awareness, reflection, motiva-
tion and behavioural change. One issue raised by partici-
pants was the distinction between a tool intending to foster
something and actually being successful in doing so. Based
on the fact that in many cases only the actual user of a tool
can assess whether awareness, reflection, motivation or be-
havioural change was fostered, they suggest to ask whether
a tool intends to do something when evaluating it. Another
issue raised was that the main user type of a tool should be
identified before evaluating it as some tools might cater to
learners, other to teachers, etc. A third issue mentioned by
the participants was that of direct or indirect fostering (or
better the intention to do so) of the QIs.

For the QIs of the Learning Support criterion participants
also stressed that taking the user type into account when
evaluating a tool is important. Participants also mentioned
that there are two types of QIs in this criterion. While
usefulness can be deemed an intended goal of a tool, the
QIs of recommendation, activity classification and detection
of students at risk are features / functionalities of a tool.
Although both types of QIs are valid to be used to evaluate

Table 2: Overview of 1(very difficult)-to-5(very
easy) scale ratings plus average rating for all QIs

1 2 3 4 5 avg.

awareness - 1 1 7 7 4.3
reflection 1 - 2 6 7 4.1

motivation 1 3 3 4 5 3.6
behavioural change - 3 5 5 3 3.5

perceived usefulness 2 - 1 7 6 3.9
recommendation 1 1 3 4 7 3.9

activity classification 4 3 3 1 5 3.0
det. of students at risk - 1 3 6 6 4.1

comparability - 6 2 5 3 3.3
effectiveness 2 5 4 4 1 2.8

efficiency 4 3 5 3 1 2.6
helpfulness 2 4 2 5 3 3.2

transparency - 4 6 5 1 3.2
data standards 3 2 2 5 4 3.3
data ownership 3 3 3 6 1 2.9

privacy - 3 2 8 3 3.7
availability 2 1 1 3 9 4.0

implementation 2 1 2 2 9 3.9
training of stakeholders 2 - 1 8 5 3.9

organisational change 2 - 1 12 1 3.6

a LA tool, an EF should benefit from using only one type
of QI per criterion. It was also noted that for some QIs it
might not suffice to say whether a tool does something or
not in order for it to be deemed a good tool, e.g. too many
recommendations might be worse than no recommendations.
The QI that caused most trouble to the study participants
is that of activity classification. Participants found it rather
difficult to judge this QI as they did not fully understand
what it meant while participants of the GCM study most
likely had a clear concept in mind, i.e. that LA tools ”know”
what their users are doing automatically. It was therefore
suggested to rephrase or redefine the QI.

The criterion Learning Measures and Output was an over-
all difficult one to judge for the participants. They not only
had difficulties judging some of the QIs but the criterion
title increased this difficulty even more. They were unsure
whether to relate a QI to the measures or the output of a
tool, to the processes or the tool itself and thus suggested
to define a better, clearer name and concept for this crite-
rion. The QI comparability was quite difficult for partici-
pants to apply as they were not completely sure about what
was to be comparable. In the GCM study the LA experts
had identified comparability in relation to the measures and
outcomes of an analytics tool, e.g. that effects of one tool
could be compared to those of another tool. From the re-
sponses of this study’s participants, however, it is clear that
some participants assumed the comparability to be for users
within one tool. This misunderstanding clearly needs to be
addressed by a better definition of the QI and possibly a
rephrasing. For the QIs effectiveness and efficiency it was
suggested to distinguish between the intention of a tool and
the fulfillment of that intention. Also, participants would
have liked to see clear definitions in order to better distin-
guish them from another. They also suggested to clearly
indicate the type of user of a tool, too, when applying these
QIs. The same applies to the QI helpfulness. They also
suggested to clearly distinguish this QI from the one about

18



perceived usefulness by giving a clear definition to both.
In the GCM study the LA experts had identified the QIs

dealing with Data Aspects as the most important and as
the most feasible ones. This time, however, the QIs of this
criterion were often either not supported by a tool or not
applicable. The main reason given for either saying no or not
applicable was that they had not used the tool themselves
but had to rely on the resources describing the tools. It was
thus suggested to add an I don’t know -option. Here, the
QI data ownership was deemed the most difficult to rate.
Some participants were not able to fully grasp and apply
the concept to a given tool and therefore suggested a more
detailed definition of the QI. Again, they would have liked
to see the type of user in focus mentioned when doing the
evaluation. For the QIs transparency and privacy the issue
of differentiation and a clearer definition was raised. It was
also mentioned that in the case of transparency, two types
could be present in a tool: a tool supports transparency if
users know what data about them is collected and stored
but also if one user can see information about other users.

The criterion on Organisational Aspects was by far the
easiest to rate for the participants. It is also the one with
the most not applicable values. Many participants reported
that this was due to many of the tools being prototypical
implementations that had only been used within one course
or as a small test bed study. Another reason given was the
lack of information provided by the resources used for the
evaluation about anything related to Organisational Aspects
and not being able to use the tool. The difference between
the QIs of availability and implementation was not clear to a
number of participants. They thus suggested to either define
the QIs more clearly or merge them into one.

3. WORKING TOWARDS AN IMPROVED
EVALUATION FRAMEWORK VERSION

The results of this framework evaluation study allow us
to identify several issues with the EF that need to be ad-
dressed in order to work towards an improved EF for the
next evaluation cycle. The issues identified can be divided
into the following categories: (1) concept definitions, (2) dif-
ferentiations, (3) framework structure, and (4) questionnaire
adaption.

The first category, concept definitions, relates to any case
where it was expressed that either a criterion or a QI needs to
be rephrased or defined more clearly in order to be properly
applied to a tool evaluation. One criterion and three QIs
where this is the case were particularly mentioned: Learning
Measures and Output, activity classification, comparability,
and data ownership. Renaming, and thus redefining, a whole
criterion also influences how the QIs of that criterion are
interpreted. When constructing the next version of the EF,
this will have to be taken into account.

The issues of the second category, differentiations, are
closely related to those of the first category. Participants
identified some QIs, or better pairs of QIs, that needed to
be defined more clearly and supported by some distinct ex-
ample so as to be able to properly distinguish between them.
Otherwise users of the EF might misunderstand them and
thus distort the results of a tool evaluation. The QIs men-
tioned by the study participants are usefulness vs. helpful-
ness, effectiveness vs. efficiency, transparency vs. privacy,
and availability vs. implementation.

The third category, framework structure, deals with the
issue of inter-criterion homogeneity of QI types. It was sug-
gested to ensure that the types of QIs within one criterion
are the same in order to improve the applicability of the
whole criterion. Generally, QIs should tend to be concept
rather than feature driven. Participants identified this is-
sue in the criterion Learning Support but all other criteria
should be inspected as well so as to avoid this issue from
appearing again in the next evaluation cycle.

The fourth category, questionnaire adaption, comprises is-
sues that need to be addressed when setting up the next ver-
sion of the EF’s questionnaire or better the next practical,
applicable and executable version of the EF. Several aspects
were noted that would highly improve the applicability of
the QIs. For many QIs the answers would differ depending
on the user type addressed. This should thus be clarified for
each questionnaire and thus lead to specific instances of the
EF for different stakeholders. Questions for the QIs should
best ask about the intention of a tool as this is something
that can be answered much more easily than a tool’s actual
impact on a user. This is especially true if the evaluator
has no access to the tool but has to work with descriptive
resources. The third issue related to questionnaire adaption
is the possible addition of answer options. Several partici-
pants of the evaluation study remarked that they would have
liked to see an I don’t know -option or a too much-option as
information for some QIs might be too sparse.

An issue that is not related to any of the categories and
that cannot be improved by us is the sparsity of information
provided in the resources about LA tools. While addressing
the issues mentioned above will make it easier for externals
to evaluate a tool, the most complete evaluations will be
those of the actual users or creators of a tool. In those
cases where users or creators apply the EF to their own
tool, however, the results might be biased which has to be
taken into account as well.

4. CONCLUSION
This paper builds on the findings of a group concept map-

ping study that empirically identified criteria and quality
indicators for LA tools to form an EF. We then conducted
a second study with members of the LACE project to apply
the EF to a number of tools in order to evaluate it. With
the feedback from the participants we were able to iden-
tify problematic issues and have collected suggestions how
to overcome the issues and improve the framework.

Figure 2 shows which criteria and quality indicators have
been identified with category 1 (solid), category 2 (dashed)
or no issues (dotted). The outcomes of the evaluation study
will be carefully analysed and discussed within the LACE
consortium to develop an improved version of the EF. Apart
from the theoretical framework set up, the structure of the
related evaluation instrument will also be improved as dif-
ferent stakeholders might require different versions of the
instrument. The improved EF as well as its implementation
will then form the basis of another evaluation cycle. The re-
sults of the tool analyses of these studies will be fed into an
Evidence Hub9, a knowledge base of evidence created and
curated by LACE that captures evidence for the effective-
ness and the relative desirability of the outcomes resulting
from use of various tools and techniques.

9http://evidence.laceproject.eu

19



Figure 2: Indicators and criteria with category 1 (solid) and category 2 (dashed) and no issues (dotted)

5. ACKNOWLEDGMENTS
This work was partly funded by the LACE project (GA

No. 619424) under the Seventh Framework Programme of
the European Commission.

6. REFERENCES
[1] S. Aguilar, S. Lonn, and S. D. Teasley. Perceptions

and use of an early warning system during a higher
education transition program. In Proc. of the 4th Int.
Conf. on Learning Analytics And Knowledge, LAK
’14, pages 113–117, New York, NY, USA, 2014. ACM.

[2] K. E. Arnold. Signals: Applying academic analytics.
EDUCAUSE Quarterly, 33(1), March 2010.

[3] K. E. Arnold and M. D. Pistilli. Course signals at
purdue: Using learning analytics to increase student
success. In Proc. of the 2nd Int. Conf. on Learning
Analytics and Knowledge, LAK ’12, pages 267–270,
New York, NY, USA, 2012. ACM.

[4] J. Baron and S. Jayaprakash. Snapp (social network
analysis & pedagogical practices) for sakai cle v2.8x &
v2.9x.
https://confluence.sakaiproject.org/x/MYEPBQ,
August 2014.

[5] S. Buckhingham Shum, R. Ferguson, and R. Deakin
Crick. Enquiryblogger: Blog-based learning analytics
for learning power & authentic enquiry.
http://de.slideshare.net/sbs/

enquirybloggeranalyticscalrg2012, June 2012.

[6] H. Drachsler, S. Stoyanov, Mathieud’Aquin,
E. Herder, M. Guy, and S. Dietze. An evaluation
framework for data competitions in tel. In EC-TEL
2014, LNCS 8719, pages 70–83. Springer, 2014.

[7] M. Elkina, A. Fortenbacher, and A. Merceron. The
learning analytics application lemo - rationals and first
results. International Journal of Computing,
12(3):226–234, 2013.

[8] R. Ferguson, S. Buckhingham Shum, and R. Deakin
Crick. Discussion paper: Enquiryblogger - using

widgets to support awareness and reflection in a ple
setting. In Workshop on Awareness and Reflection in
Personal Learning Environments, PLE Conference
2011, 2011.

[9] S. Govaerts, K. Verbert, and E. Duval. Evaluating the
student activity meter: Two case studies. In H. Leung,
E. Popescu, Y. Cao, R. Lau, and W. Nejdl, editors,
Advances in Web-Based Learning - ICWL 2011,
volume 7048 of Lecture Notes in Computer Science,
pages 188–197. Springer Berlin Heidelberg, 2011.

[10] S. Govaerts, K. Verbert, E. Duval, and A. Pardo. The
student activity meter for awareness and
self-reflection. In CHI ’12 Extended Abstracts on
Human Factors in Computing Systems, CHI EA ’12,
pages 869–884, New York, NY, USA, 2012. ACM.

[11] LeMoTeam. LeMo Handbuch fu?r Anwender und
Administratoren. http://lemo.htw-berlin.de/
public/doc/LeMo_HB_final_a.pdf.

[12] P. Long and G. Siemens. Penetrating the fog:
Analytics in learning and education. Educause Review,
46(5):31–40, 2011.

[13] S. Lonn and S. D. Teasley. Student explorer: A tool for
supporting academic advising at scale. In Proc. of the
1st ACM Conf. on Learning @ Scale Conference, L@S
’14, pages 175–176, New York, NY, USA, 2014. ACM.

[14] J. L. Santos, K. Verbert, and E. Duval. Empowering
students to reflect on their activity with stepup!: two
case studies with engineering students. In Proc. of the
2nd workshop on Awareness and Reflection, ARTEL
’12, Saarbru?cken, Germany, 2012. CEUR.

[15] J. L. Santos, K. Verbert, S. Govaerts, and E. Duval.
Addressing learner issues with stepup!: An evaluation.
In Proc. of the 3rd Int. Conf. on Learning Analytics
and Knowledge, LAK ’13, pages 14–22, New York,
NY, USA, 2013. ACM.

[16] M. Scheffel, H. Drachsler, S. Stoyanov, and M. Specht.
Quality indicators for learning analytics. Educational
Technology and Society, 17(4), 2014.

20

https://confluence.sakaiproject.org/x/MYEPBQ
http://de.slideshare.net/sbs/enquirybloggeranalyticscalrg2012
http://de.slideshare.net/sbs/enquirybloggeranalyticscalrg2012
http://lemo.htw-berlin.de/public/doc/LeMo_HB_final_a.pdf
http://lemo.htw-berlin.de/public/doc/LeMo_HB_final_a.pdf




