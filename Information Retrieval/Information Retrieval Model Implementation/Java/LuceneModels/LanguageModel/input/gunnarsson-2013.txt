
Understanding Promotions in a case study of student
blogging.

Bjorn Levi Gunnarsson
Brandeis University

Computer Science Department
Waltham, MA 02454 USA

bjornlevi@cs.brandeis.edu

Richard Alterman
Brandeis University

Computer Science Department
Waltam, MA 02454 USA

alterman@cs.brandeis.edu

ABSTRACT
Promoting blog content is a social activity; it is a means of
communicating one student’s appreciation of another stu-
dent’s work. This paper explores the feasibility of using
student promotions of content, in a blogosphere, to identify
quality content, and implications for instructors. We show
that students actively and voluntarily promote content, use
promotion data to select which posts to read, and with con-
siderable accuracy identify quality material. We explore the
benefits of knowing which students are good and poor pre-
dictors of quality content, and what instructors can do with
this information in terms of feedback and guidance.

Keywords
Blogging, Social Blogging, Liking, Promoting, Assessment,
Quality content, Learning analytics, Knowledge community

1. INTRODUCTION
The research reported on in this paper analyzes learner

created content within a single class of 107 students. The
class is a blended course, with lectures twice per week, and
ongoing participation in an online knowledge community
was a requirement. Improvements in learning are visible
in the online data, but for a class of our size there is not go-
ing to be as much data as there is for institution-sized data
sets or MOOCS (massively open online courses). Given the
smaller data set size, the type of generalizations that can
be made from the data is limited, but in some ways more
important, because they represent the current conditions of
many learning environments.

In a traditional classroom a student typically only has
access to verified high quality content like books, articles,
and lectures supplied by the instructor. Online there might
be videos, more articles, and tutorials by other profession-
als. Other sources of content are peers. Offline peer content
can be produced by study groups, group projects, or class-
room discussions. Online peer content can be produced in a
knowledge community, which uses technologies that enable

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, to republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee.
LAK’13, April 8 - 12 2013, Leuven, Belgium.
Copyright 2013 ACM 978-1-4503-1785-6/13/04 ... $15.00

content creation and distribution online (e.g., wikis, blogs,
forums, and chats).

Online content is an important resource for several rea-
sons – giving students access to each other’s contributions
is the basis for a knowledge community [30], with each con-
tribution representing a different, but valid, viewpoint that
provides “scaffolding” for others to build upon and improve.
Different contributions and examples on the same subject
are multiple representations, which are valuable for learn-
ing [1], and can lead to higher order thinking [28, 14]. De-
pending on the technology that mediates peer collaboration,
there will be different benefits. For example, a blogosphere
is especially valuable as resource for co-reflection amongst
the students, which is very helpful for learning [9].

Students in our case study blogged throughout the semester,
each producing over 1000 words a week. This is a tremen-
dous amount of content for the students to navigate and for
the instructor to assess, which is why three graduate graders
and three undergraduate graders did the assessments; how-
ever, these kind of resources are not typical for most classes.

The students produce content of mixed quality. Ideally,
the best content is foregrounded in some way. One solution
is to have the instructor and graders identify the quality
content for the rest of the class. However, as the size of
the class increases, the cost of finding good content, without
any automated assistance, becomes quite labor intensive –
perhaps prohibitively so. An alternative solution is to have
students themselves locate and label important content as
they read. If the students are willing to do so on a regular
basis and can accurately identify the relevant material, the
cost-reduction would be tremendous.

Social feedback is common in social networks, where par-
ticipants can share, like, pin, upvote, or give a “+”. This is
classified as transactional content and is an exchange of “af-
fect” [35], e.g., one student “liking” another students work.
We call this kind of social feedback in a knowledge building
community a promotion — the reader is labeling content for
the rest of the community, potentially reducing the amount
of material everybody has to read to accomplish their learn-
ing goals, and providing instructors with valuable informa-
tion on what the students consider to be quality content. In
the study presented in this paper, students promote content
by attaching badges and“likes”to each other’s contributions.
Promoted material, by definition, is content the reader con-
siders noteworthy and wants to publicly label it as such for
the community.

What it means to promote content can vary in different
applications. Social feedback can be a subjective evaluation

57

suthers
Rectangle



of content given the setting, for example sharing cute pic-
tures or funny jokes. In other applications it is used to ap-
prove of solutions to a given problem (stackoverflow.com).
In such a setting the different proposed solutions can be
rated for usefulness and also tested against the given prob-
lem and verified, giving objective results. In our environ-
ment, the purpose is to identify high quality content using
a subjective promotion tool to give objective results. The
premise that noteworthy content is of high quality is not a
given. This paper explores the viability of using subjective
evaluation by peers in a knowledge building environment as
a method for identifying high quality content.

Exploring the learner created promotions within the scope
of the field of learning analytics is a perfect match. By defi-
nition learning analytics apply analytical methods to learner
created content [32]. Within the field researchers have cre-
ated sense-making visualizations for instructors and admin-
istrators [8], explored knowledge building in online social
settings [16], and data mined institution-sized data sets [33,
5]. Learning analytics gives the exact framework to explore
the potential of promotions.

Our primary concern is filtering for high quality content.
Given the amount of content created in the blogosphere, be-
ing able to sort out the important posts is valuable for every-
body involved. Direct access to the highest quality content
means a leaner and more efficient knowledge community.
With the addition of some learning analytic methods, the
potential exist, for converting student promotions into valu-
able commodities for both instructors and students. For ex-
ample, student promotions could be input to a mechanism
that highlights quality material. These highlights could be
used to reduce the amount of grading work and could also
be used as a template for providing feedback to students.

In this paper, we analyze the the effect of promoting posts
in a blogosphere, measure the overall value of student pro-
motions as a form of assessment of quality, and identify stu-
dents that act as reliable predictors of quality based on the
posts they promote. This leads to a more accurate sorting of
the quality content, beneficial for both the students and the
instructor. The main result is that leaning analytic meth-
ods verify that student promotions can be used as a basis
for identifying high quality material in the blogosphere.

2. BACKGROUND
Blogs are widely used as a learning tool in numerous disci-

plines and for various purposes [9, 12, 3]. The most frequent
use of student blogging is as an open journal [40] or a tool of
reflection [9, 26]. The core features of any student blogging
system is the capability to create and edit posts. Blogs can
either be a community blog (where the whole class shares
a single blog) [39] or a collection of blogs – with each stu-
dent “owning” and posting to his/her blog. The blogs can
be collected into a single application, open to the class but
not the public [4], or students can use a standard blog site
like wordpress.com, where students connect to each others
blogs via a Really Simple Syndication (RSS) feed [13].

Several researchers have reported student blogging has
positive effects on, for example: higher order thinking skills
[28, 14], knowledge sharing and reflection [25, 19, 11], the
learning process beyond the class room [15], sense of own-
ership [19], sense of community as measured by the com-
munity dimension [9], and identity development [24]. Some
have cautioned that student blogging does not automatically

guarantee positive learning outcomes [23, 10].
Blogs have a low technical threshold and a simple intu-

itive delivery system (click a button to write and then click
another button to post). Blogs are versatile, used in many
different settings and for different purposes [26]. Various
methods of measuring content quality have been applied
to blogs [37, 21], such as search and clustering algorithms
[37], automated and voting methods based on various quality
measures [22], classification of different types of blog posts
using machine learning techniques [27], and using natural
language processing [29]. New features have been added to
blogs attempting to enhance learning, e.g., topic maps [20]
and awareness graphs [17].

Given the widespread use of educational blogs, a good
source of learner created content, a success story about ap-
plying learning analytic methods to student blogging data
will have significant value. The text of posts and comments
are not the only data that can be analyzed. The number of
contributions and reads can be counted, as well as any trans-
actional content (i.e., promotions) that is produced by the
students. Examples of data other than the raw text that has
analytic value includes: counting reads [18], counting com-
ments, counting external links, login data, load time, and
many other features as listed in [22].

When analyzing blogs, the social aspect can not be ig-
nored. A blog is meant to be read by others. Social learning
analytics [16] offers great potential to enhance the learners
experience. In this paper, we analyze promotions, an impor-
tant social feature, as a basis for determining and filtering
quality content.

3. CASE STUDY
The paper presents a case study of a class on Internet &

Society. During the semester, the students read four books
(Here Comes Everybody: The Power of Organizing Without
Organizations; by Clay Shirky, Convergence Culture: Where
Old and New Media Collide; by Henry Jenkins, The Social
Life of Information; by John Seely Brown and Paul Duguid,
and Remix: Making Art and Commerce Thrive in the Hy-
brid Economy; by Lawrence Lessig) on the social impact of
the Internet on society. During the semester the students
had weekly writing assignments that were to be completed
in the blogosphere.

The online part of the course functions as a knowledge
community [31, 30] and a discourse community [6, 38]. A
place for students to question, negotiate, share, construct,
explore, and develop their understanding of the course mate-
rial, assignments, and concepts. Online they can participate
at their leisure in an environment where everyone have a
common learning goal [34]. Student contributions to the bl-
ogosphere have a social orientation [7]. Each contribution
further develops their online identity [24] within the knowl-
edge community and ownership [19, 15] of content.

Each student had her own blog, which was composed of
multiple posts she authored. In addition to the 11 writing
assignments, the student also had weekly assignments to ‘of-
ficially’ provide peer comments and assessment on the posts
of other students. Students were also encouraged to read
freely throughout the semester in the blogosphere – joining
conversations that struck their interest.

Students were encouraged to leverage each other’s work,
using the blogosphere as a resource to improve their own
posts. Just browsing in the blogosphere and looking at

58



other students’ work has tremendous educational value [2].
While working on an assignment, it was allowed to review
the posted work of other students. It was also permissible
to revise posts, and revise again, up until the deadline. In
this manner, the blogosphere is a platform for peer tutoring,
peer assessment, and collaborative learning [36].

For each book, the students had to write both an edito-
rial post and a book review post. Each post was required
to be between 600-750 words in length; in practice many
of the posts were longer than that. The editorials required
the student to review an issue raised by the book and then
take a position, either expanding on the argument of the
book with examples or presenting counterarguments. Each
review explicated the central argument of the entire book.
Students explained the key points, referenced the editorial
posts of fellow students as support for their analysis, pro-
vided additional examples, and argued for (or against) the
core argument of the book.

Each student was required to write two comments and two
peer assessments each week (an additional 400 words). After
the assignment due date, two posts were randomly assigned
to each student on which she would ‘officially’ comment, and
another two for which she was responsible for giving peer as-
sessment using a 3 point questionnaire form. The peer com-
ment was posted under the student’s user name but the peer
assessment was anonymous. We instructed that comments
and assessments were to be both thoughtful and judicious.
To ensure quality, both forms of feedback were evaluated.
Students were also encouraged to do additional comments,
respond to comments on their own posts, and promote posts
by giving them “likes” and badges. The additional activities
were not directly graded but were taken into consideration
when assessing overall participation in the blogosphere.

In the later part of the semester, the students were re-
quired to write three reflections that synthesized material
from two or more books and referenced posts written on the
books. A requirement for each reflection was to reference at
least three editorials or reviews that were written by other
students. The same comment and assessment process also
applied to the reflections.

Because of the specific requirement of the blogging assign-
ment, the students got plenty of feedback on their contri-
butions to the blogosphere. The comments and peer assess-
ments produced by other students were one form of feedback.
Grades and publicly displayed“gold stars” for excellent work
were given out by the TA as another form of feedback. The
peer promotions (badges and “likes”) were a third significant
form of feedback.

3.1 Student promotions in the blogosphere
While browsing in the blogosphere, students could pro-

mote posts by assigning them merit badges and “likes”. Lik-
ing a post means exactly that – the student supposedly liked
it. Merit badges are a more specific type of promoting a post.
Instead of clicking a button to say you “like” the post, you
can click a button to say that you liked it because it was,
for example, “nicely written”. The badges were:

Good example
The post contains an interesting example or case as a
basis for its argument.

Good question
The post raises and interesting issue.

Nicely written
The post is well written.

Good argument
The post makes a persuasive argument.

Good references
The references are interesting and relevant.

Good summary
The post provides an accurate and succinct summary
of an issue or the book.

There was incentive to identify high quality posts in the
blogosphere – earlier posts were referenced in later assign-
ments. By finding high quality posts that clearly explained
a topic in one of the books, the student could more easily
build her argument when reviewing or reflecting on the cen-
tral issue of the book. Reading poor quality posts would not
be as helpful. Searching for good quality posts takes work,
but also depends on a student’s ability to identify what is
a good post. The data will show that not all students were
good at assessing quality.

The gold stars that were provided by the TA were one re-
liable source of information for identifying quality content.
The data shows that each “gold star” post was read on av-
erage 44 times during the semester – more frequently than
posts that did not get gold stars. Promoted posts got, on
average, 30.1 reads and the average for all posts was 22.9
times per post. The data shows the number of promotions a
post received was directly related to how much it was read
(see Figure 1).

Figure 1: Number of promotions vs. number of
reads

There are three parts to the question of whether promo-
tions were a useful tool for identifying good content. Do
students actually promote content (this is explored in detail
in Section 4.2) and if so do they act on it? As can be seen in
Figure 1, the students use the promotions as a way to select
which posts to read. The third part is whether, or not, the
promoted content was actually of good quality, which is also
explored later (see Section 4.3).

59



Our analysis will show that the class can be divided into
good, average, and poor predictors of quality content. With
this refinement, it becomes possible to automatically iden-
tify, with a great deal of accuracy, both good and poor con-
tributions. By coupling this analysis with a highlighting
mechanism, it would be possible to foreground quality posts
in the blogosphere. This could either be a replacement for
the existing gold star mechanism or an add-on that fore-
grounds material in a much more timely fashion. Also it
has great potential for supporting grading as a preliminary
sorting of student work.

3.2 The technology
The blogging environment was developed at Brandeis Uni-

versity over a number of years; the current version is a recent
rebuild by the first author of this paper. It has already been
used in several classes.

Students could browse content in the blogosphere using
several different views. Regardless of the view, posts were
always listed in reverse chronological order, with the most
recently update posts at the top of the page. For each post
in a list, there is a header that includes the name of the
author of the post, the title of the post, the type of post
(editorial, review, or reflection), the book(s) the post dis-
cussed, the number of comments the post had accrued, and
the number of promotions it had received. Students could
preview the first paragraph of a post by hovering over it’s
title, and click on title to view the post in its entirety, as
well as any comments that had been made on the post.

Most of the time, students would start a session by view-
ing the front page, which was a list of the most recent posts.
From there, a student could switch to an alternate view. Ex-
amples of available views are: all posts on a specific book,
all posts that have a specific type of promotion, all posts
that had received a gold star, all posts the student had
bookmarked within the system, and all posts the student
had commented upon. It was also possible to do a keyword
search to retrieve a list of all posts that contained the search
term.

We have sometimes referred to the kind of blogging the
students do in our environment as co-blogging. It is co-
blogging as opposed to blogging, because it is a closed “col-
laborative blogging” community. In this case study, the
students still collaboratively blog, but the blogging envi-
ronment includes additional social features (like the promo-
tions). In this paper we will refer to this species of student
blogging as “social blogging”.

3.3 Grading
There were six graders for the Internet & Society class.

Posts were graded differently than comments and peer as-
sessments. The posts were graded by three graduate stu-
dents; a third of the posts were randomly assigned to each
of the graders for grading after the due date of each home-
work assignment. The grading was done by filling out a
questionnaire of six questions. Each homework type (edi-
torial, review, and reflection) had different questions. For
example, grading questionnaire for an editorial post was:

1. The issue is clearly explained.

2. The opinion is interesting and substantial.

3. The references are relevant to the argument.

4. The post is well written.

5. The post demonstrates understanding of the subject
of the book.

6. Overall grade for this post?

The purpose of the “overall grade” question was to be
an approximation factor. Clearly not all of the items are
of equal “value”. A well written post might be more rele-
vant than a post with good references. Generally the overall
grade followed a trend set by the previous questions, but
for some borderline cases the overall grade would add or
deduct points based on over which border the post fit into
as a whole.

Each of the questions could be given a grade between ‘0’
and ‘3’.

‘0’ means that part was not completed.

‘1’ is not good.

‘2’ is good work.

‘3’ exceeds expectations.

For example, no references in the post would yield a ‘0’ for
that part of the questionnaire. An exceptionally clear and
thoughtful introduction to the issue would get a ‘3’ while a
post that did not conclude with a logical and original opinion
on the issue would receive a ‘1’. A post with a outstanding
introduction, no references and a simple “this is what the
book said” opinion would in turn get a total of ‘4’ for those
parts of the questionnaire. Most commonly a good intro-
duction was followed by a well written account with relevant
references, understanding, and opinion of the stated issue.
A poor introduction followed a similar pattern.

Students were told to expect a ‘2’ to be a good passing
grade. Grading was done inside the blogosphere but in a
special grading view where only the questionnaire and the
text of the blog post were visible. The graders could not see
the blog post’s comments, “likes” or badges without explic-
itly browsing to that blog post in the standard view. This
was done to minimize any bias the post’s comments and
promotions could have on the grader.

The comments were graded by three undergraduate stu-
dents. The comment and peer review forms were simpler –
just asking if the comment or peer review met expectations
– the graders could give a ‘0’ for not completed, ‘1’ for not
good and ‘2’ for good work.

The grading process was quality controlled in several phases.
First in weekly meetings between the graders and instructor
where random posts were graded together. Then, individu-
ally, each grader got randomly assigned a batch of posts to
grade and the head TA was then responsible for reviewing
the grade submissions. The third phase, again for the head
TA, was to reply to grade inquiries by students, check the
post and grades and if needed contact the original grader
for comments. Finally all high quality graded posts were
reviewed by the head TA as a part of the gold star process.

The grading data we use in our analysis was generated by
the graduate level graders who were on their second semester
of grading blogosphere posts together. While not every sin-
gle grade was reviewed by the head TA the graders were
consistent and the grading scheme simple enough to pre-
vent major discrepancies in grading. All high quality graded
posts were officially reviewed and approved.

60



We evaluated of how much work per week it took to grade
the posts, comments, and peer evaluations. For each assign-
ment, the instructor would meet weekly with all six graders
(three graduate students and three undergraduates) to give
an overview of how to grade the most recent assignment –
recall there was one assignment per week. These meetings
typically lasted around an hour (totaling ten person hours
per week). From interviews with the graduate graders (the
ones who graded the posts), we learned that the average time
spent on grading posts was around four hours per grader
(twelve hours per week total). The average time spent grad-
ing comments and assessments about three hours per grader
(nine hours per week total). The average number of person
hours spent grading each week was 31 hours and over the en-
tire semester roughly 350 hours. Clearly, this is an enormous
amount of work and many courses will not have available re-
sources sufficient to commit to this level of grading. Being
able to automate some of the process of identifying good
content, providing feedback, and evaluating student work,
would have tremendous value.

4. EVALUATION
The last assignment of the semester is left out of the data,

as it is an outlier. Students’ participation in the last as-
signment was very different than in the previous ten. This
is most likely due to the fact that the semester was end-
ing and students were busy finishing up their work in many
classes. The data also shows declining promotion activity in
assignment 9 and 10, they were sufficiently numerous to be
contributors to the data.

The assignments were (numbers used in figures): (1) edi-
torial on Here Comes Everybody, (2) review of Here Comes
Everybody, (3) editorial on Convergence Culture, (4) review
on Convergence Culture, (5) editorial on The Social Life of
Information, (6) review of The Social Life of Information,
(7) editorial on Remix, (8) review of Remix, (9) reflection
on a cross cutting theme from two or more books, and (10)
another reflection on a different cross cutting theme from
two or more books.

The data compiles the promotion statistics of 92 students.
The 15 students filtered out did not participate fully in the
blogosphere for various reasons; for example, because they
dropped the class.

4.1 Outline of the argument
An outline of the evidence presented in the evaluation is

shown in Figure 2. The basic idea is to convert student pro-
motions into highlights in the blogosphere or preliminary
assessments for graders. The scheme will only work if stu-
dents are actively promoting content during the semester
(see point 1). In the case study, the raw data shows that
the students were producing a lot of promotions: students
voluntarily produced merit badges and “likes” on average at
the rate of 108 per assignment. Not every post received a
promotion, and those that received promotions sometimes
received many.

1. Students promote posts as they read in the blogo-
sphere. See Section 4.2.

2. The promoted posts are on average are of higher qual-
ity than the average blogosphere post. See Section 4.3.

3. Evaluating the reliability of predictors. See Sec-
tion 4.4.
(a) Some students are better promoters than others.
(b) Some students were poor predictors of quality ma-

terial. In fact, there were students who tended to
promote poor material!

(c) As the semester progressed, all students became
better predictors of quality content.

4. High quality content can be predicted based on who
promotes it. See Section 4.5.

Figure 2: Summary of evidence

A second issue is whether the promotions were warranted.
The data shows that the students tended to promote high
quality material (2), but some students were better than
other students at this activity (point 3a & point 3b). As
the semester progressed, the students became more accu-
rate with their promotions (point 3c). All of this evidence
bodes well for the possibility that student promotions can
converted into highlights for quality material and prelimi-
nary assessments for graders (point 4).

4.2 Promoting content
For the entire class, 89.7% of students used the promotion

feature at least once. Students did not promote a post in
every assignment. In the first assignment, many students
used the feature, but the number of students that promoted
declined in later assignments (see Figure 3). On average,
roughly a third of the students promoted content in each
assignment.

Figure 3: Ratio of class giving likes

During the whole semester, the average student promoted
6.77 posts from 3.75 assignments. The number of students
that gave promotions dropped considerably from the first
assignment (78%) to the last assignment (16%). The mid-
part of the semester was stable with only a drop in promo-

61



tion activity from 50% to 34% from assignment two through
assignment eight.

Based on these numbers, and the argument below, having
a third of the community promote content seems sufficient
to filter content. Information from the initial promotion
“frenzy” could be used to encourage more promotions from
the portion of the community who are good at it and give
feedback to the portion of the community who are bad at
promoting.

Promotions have a significant influence on a which posts
students choose to read as shown on Figure 1. Simply put,
the more promotions a post gets, the more participation that
post represents in the community. Reading a post created
by another student is considered to be the most basic defi-
nition of participating because of the so called “lurkers” —
community members that only read.

Obviously there is a participation feedback loop. If a post
gets a promotion, it gets more readers. These additional
readers can give even more promotions which lead to more
reads, and so on. In this way the highest quality posts (as
deemed so by the community) have also reached the largest
audience and had the greatest impact.

4.3 Quality of promoted content
What was the quality, in terms of grades, of the posts that

received promotion? There are three parts to the answer:
1. Higher quality posts got more promotions. (number of

promotions per post)
2. Higher quality posts were more likely to be promoted.

(promotion hit rate)
3. All types of promotions were useful (different types of

promotions)

The number of promotions per post.
Figure 4 shows the total number of posts that were pro-

moted and also the total number of posts that did not get
promoted, both grouped by grade. It also shows the number
of total promotions for posts of each grade. It is important
to note that each post can we awarded multiple promotions,
so in the case of posts that received, a grade between 14 and
18 (see shaded region), received more promotions than there
were posts. The graph shows that there was an interesting
increase in the number of promotions on content with high
grades (again see the shaded region in figure 4).

Figure 4: Posts and promotions per grade.

In other words, all posts can receive promotions, but posts
with high grades get significantly (p-value: 0.002045) more
promotions than posts with a lower grade (see Figure 5).

Figure 5: Average promotions per grade

The promotion hit rate.
Were high quality posts more likely to get promoted than

lower quality posts? Only 36% of low quality posts (graded
8 or lower) were promoted. This means low quality posts
have a 36% promotion “hit rate”. The hit rate significantly
increased to 45% for posts of average grade (graded between
8 and 13), and high quality posts (graded 14 or higher) had
a hit rate of 54%, which is again a significant increase.

Different types of promotions.
As mentioned before, we counted all promotions (both

“likes” and badges) as the same activity of promoting. If
we dig a bit deeper into the data and explore each type of
promotion we get an interesting picture (see Figure 6).

Figure 6: Average grade per type of like

The average grade for all posts during the semester was
10.7 (see horizontal line for average class grade). With one
exception, all types of promoted content had significantly
higher grades than the average class grade. Posts that had

62



been promoted by the “like” feature had by far the highest
average grade. Posts that received more specific kinds of
merit badges were graded on average with a slightly lower
grade, but were still, as a whole, significantly above the aver-
age grade for a post. The exception was posts that received
a “good question” badge; these posts received an above av-
erage grade, but not significantly so.

Summary.
Combining all of these results – high quality posts get

more promotions, the promotion hit rate is higher for qual-
ity content, and all types of promotions are useful – means
promotions are a powerful method for identifying quality
material: students can successfully identify high quality con-
tent.

4.4 Promotion predictions
Are some students better at promoting quality material

than others? We wanted to know which students were con-
tributing the reliable promotion data and which ones were
not – they might be reliably providing promotions to poor
quality material. Figure 7 shows us that some students
tended to promote high quality posts, other students tended
to promote low quality posts, but the majority of students
are less predictable. About 25% of students regularly pro-
mote post with high grades and 8% regularly promote posts
with poor grades.

In Figure 7, “good predictors” refers to students that on
average promoted posts that received a good grade (on or
above the top dividing line), and “poor predictors” are the
students that promoted posts that received a poor grade (on
or below the bottom dividing line). Obviously, the posts
“good predictors” promote can be used to highlight content
in the blogosphere.

In terms of assessment, the users classified as“poor predic-
tors”present an interesting case. First of all, getting them to
stop promoting content will immediately increase the aver-
age quality of promoted content. Secondly, this information
can be used by the instructor as a teaching opportunity –
these users lack an important critical reading skill.

Figure 7: Average grades vs average liked grades

The distribution is interesting in that the grades of stu-
dents and the grades of the posts they like do not corre-
late. We would have expected the students who promote
high quality content to also able to identify quality content.

However, some students that get high grades only promote
average or low quality content. Perhaps, even more surpris-
ingly, there are students that receive low grades on the posts
they produce, but are very good at promoting high quality
content. This suggests that students are developing two dif-
ferent skills: (1) writing and (2) identifying quality content.
A student can be good at both, either, or neither. One of the
benefits of the promotion data for instructors is that it can
be used to measure the writing and critical reading abilities
of each student.

Figure 8: Number of predictors per assignment

As the semester progressed, the students became better at
identifying good blogosphere material (see Figure 8). The
number of “good predictors” increased and the number of
“poor predictors” decreased. This is an indication that the
students are learning.

4.5 Summary of analysis
The analysis above provides evidence that it is possible to

filter content for the purposes of either highlighting or pre-
liminary assessment. Students identify high quality content
using promotions. High quality content is more likely to be
promoted – have a higher hit rate. Grading promoted con-
tent enables the instructor to sort out the students who are
good or bad predictors, thus providing another dimension
to evaluation and an opportunity for instruction.

5. CONCLUDING REMARKS
A large online knowledge community can be a confus-

ing place for a student, but she is not alone. Together the
students can make sense of everything (but not necessarily
good sense). The promotion mechanism is a social feedback
feature that allows students to collectively identify content
that is interesting or of particular value. In this paper we
found evidence that the promotion mechanism is a signifi-
cant source of information for the knowledge community and
a powerful tool for assessment of quality.

In our class, students get feedback through grades, com-
ments, peer assessments and promotions. Future work will
evaluate the effectiveness of the promotion activity without
the rigorous grading done in this class. Would student pro-
motion improve over the course of the semester as the data

63



has shown, if instructors were to grade fewer posts? What
if the only posts graded were the promoted ones?

Consider, as a possible grading scheme, a plan for the in-
structor to grade only promoted posts. This reduces work
for the instructor. Under this grading scheme it would be
possible to ascertain which students are good predictors and
which ones are bad ones, which students are less predictable,
and which students don’t participate. This level of effort on
the instructor part is sufficient to develop some kind of high-
lighting mechanism. The obvious downside of this scheme
its that some students would not be receiving feedback or
grades from the instructor (or the graders). A potential
fix is to randomly sample, for grading purposes, the con-
tributions of students who never get promoted. With this
addition work, the instructor would be able to monitor the
progress of students from assignment to assignment, both
as readers and contributors. With regards to feedback, the
students will be receiving regular feedback from the instruc-
tor (but not every week). They would also be receiving peer
feedback in the form of comments and promotions, and have
access to highlighted content to compare to their own work
or to build upon.

A good-sized sample of highlighted quality content should
be sufficient for scaffolding purposes. But what about the
high quality content that did not get promoted? In the
above scheme, some of those posts would be highlighted by
the instructor after random grading. Whether or not stu-
dents would be discovered as a result of their posts being
highlighted, and thereafter be more likely to receive promo-
tions from their peers, is an interesting question.

Alternative scenarios for promotions are possible. One
where next to no high quality content is promoted at all.
Analyzing the promotions in such a setting would be invalu-
able for the instructor, perhaps indicating that the content
created by the community is on average very poor or the
readers are unable to find the actual high quality content.
Both situations provide a path towards a solution. Another
scenario could be where participants of the learning commu-
nity are promoting content not created by themselves. Ex-
ploring how well learners identify and promote high quality
content that is beyond their current knowledge level would
be very interesting.

Considerable amount of content was created in our knowl-
edge community, created by a number of participants. Other
possible scenarios for a knowledge community could be one
where there are few users and little content, few users and a
lot of content, or a lot of users and not much content. Each
of these knowledge communities could have different prob-
lems with using the promotion feature. For example, a lot of
users with little content could result in over-exposure of the
promotion feature; or few users with a lot of content could
result in many high quality contributions being overlooked.

Because promoted content is, on average, of higher quality
than non-promoted content, it is intuitive to assume that if
someone “likes” a post, it is probable that another student
will like it too. If we take into account that students dis-
played skill at identifying quality content at the start of the
semester, before any feedback had been given, we conclude
that the promotion mechanism will prove useful in most set-
tings. What the number of promotions might mean, for a
post, will vary from one setting to another, but the promo-
tion feature will always have social value.

6. REFERENCES
[1] S. Ainsworth. Deft: A conceptual framework for

considering learning with multiple representations.
Learning and Instruction, 16(3):183–198, 2006.

[2] R. Alterman and B. Gunnarsson. The blogosphere as
representational space. Technical Report CS-12-282,
Brandeis University, 2012.

[3] R. Alterman and J. Larusson. Collaborative
sensemaking in the blogosphere. Technical report,
Technical Report CS-09-272, Brandeis University,
Department of Computer Science, 2009.

[4] R. Alterman and J. Larusson. Student producing thick
descriptions. In Proceedings of the 2011 conference on
Computer support for collaborative learning.
International Society of the Learning Sciences, 2011.

[5] K. Arnold. Signals: Applying academic analytics.
Educause Quarterly, 33(1):n1, 2010.

[6] A. Brown, D. Ash, M. Rutherford, K. Nakagawa,
A. Gordon, and J. Campione. Distributed expertise in
the classroom. Distributed cognitions: Psychological
and educational considerations, pages 188–228, 1993.

[7] D. Cameron and T. Anderson. Comparing weblogs to
threaded discussion tools. 2006.

[8] S. Dawson. ’seeing’ the learning community: An
exploration of the development of a resource for
monitoring online student networking. British Journal
of Educational Technology, 41(5):736–752, 2009.

[9] L. Deng and A. Yuen. Towards a framework for
educational affordances of blogs. Computers &
education, 56(2):441–451, 2011.

[10] M. Divitini, O. Haugalokken, and E. Morken. Blog to
support learning in the field: lessons learned from a
fiasco. In Advanced Learning Technologies, 2005.
ICALT 2005. Fifth IEEE International Conference on,
pages 219–221. IEEE, 2005.

[11] H. Du and C. Wagner. Weblog success: Exploring the
role of technology. International Journal of
Human-Computer Studies, 64(9):789–798, 2006.

[12] L. Ducate and L. Lomicka. Exploring the blogosphere:
Use of web logs in the foreign language classroom.
Foreign language annals, 38(3):410–421, 2005.

[13] P. Duffy and A. Bruns. The use of blogs, wikis and rss
in education: A conversation of possibilities. 2006.

[14] N. Ellison and Y. Wu. Blogging in the classroom: A
preliminary exploration of student attitudes and
impact on comprehension. Journal of Educational
Multimedia and Hypermedia, 17(1):99–122, 2008.

[15] R. Ferdig and K. Trammell. Content delivery in
the’blogosphere’. The Journal, 31(7):12–20, 2004.

[16] R. Ferguson and S. Shum. Social learning analytics:
five approaches. In Proceedings of the 2nd
International Conference on Learning Analytics and
Knowledge, pages 23–33. ACM, 2012.

[17] R. Ferguson, S. Shum, and R. Crick. Discussion paper:
Enquiryblogger–using widgets to support awareness
and reflection in a ple setting.

[18] B. Gunnarsson and R. Alterman. Predicting failure: a
case study in co-blogging. In Proceedings of the 2nd
International Conference on Learning Analytics and
Knowledge, pages 263–266. ACM, 2012.

[19] W. Hong. Exploring educational use of blogs in us

64



education. Online Submission, 2008.

[20] T. Huang. Creating a knowledge development model
for blog-based learning.

[21] T. Huang, S. Cheng, and Y. Huang. A blog article
recommendation generating mechanism using an
sbacpso algorithm. Expert Systems with Applications,
36(7):10388–10396, 2009.

[22] M. Kargar and F. Azimzadeh. A framework for
ranking quality of information on weblog. World
Academy of Science, Engineering and Technology,
56:690–695, 2009.

[23] S. Krause. When blogging goes bad: A cautionary tale
about blogs, email lists, discussion, and interaction.
Kairos, 9(1), 2004.

[24] A. Luehmann. Using blogging in support of teacher
professional identity development: A case study. The
Journal of the Learning Sciences, 17(3):287–337, 2008.

[25] A. Luehmann and R. MacBride. Classroom blogging
in the service of student-centered pedagogy: Two high
school teachers’ use blogs. THEN: Technology,
Humanities, Education, & Narrative, 6:5–36, 2009.

[26] B. Nardi, D. Schiano, M. Gumbrecht, and L. Swartz.
Why we blog. Communications of the ACM,
47(12):41–46, 2004.

[27] X. Ni, G. Xue, X. Ling, Y. Yu, and Q. Yang.
Exploring in the weblog space by detecting
informative and affective articles. In Proceedings of the
16th international conference on World Wide Web,
pages 281–290. ACM, 2007.

[28] R. Philip and J. Nicholls. Group blogs: Documenting
collaborative drama processes. Australasian Journal of
Educational Technology, 25(5):683–699, 2009.

[29] V. Rubin and E. Liddy. Assessing credibility of
weblogs. In Proceedings of the AAAI spring
symposium: Computational approaches to analyzing
weblogs (CAAW), pages 7–21, 2006.

[30] M. Scardamalia and C. Bereiter. Computer support
for knowledge-building communities. The journal of
the learning sciences, 3(3):265–283, 1994.

[31] M. Scardamalia and C. Bereiter. Knowledge building:
Theory, pedagogy, and technology. The Cambridge
handbook of the learning sciences, pages 97–115, 2006.

[32] G. Siemens. Learning analytics: a foundation for
informed change in higher education, 2011.

[33] D. Smolin and S. Butakov. Applying artificial
intelligence to the educational data: an example of
syllabus quality analysis. In Proceedings of the 2nd
International Conference on Learning Analytics and
Knowledge, pages 164–169. ACM, 2012.

[34] J. Swales. Approaching the concept of discourse
community. 1987.

[35] N. Tichy, M. Tushman, and C. Fombrun. Social
network analysis for organizations. Academy of
Management Review, pages 507–519, 1979.

[36] K. Topping. Trends in peer learning. Educational
Psychology, 25(6):631–645, 2005.

[37] B. Ulicny, K. Baclawski, and A. Magnus. New metrics
for blog mining. Technical report, DTIC Document,
2007.

[38] J. Wertsch and J. Wertsch. Voices of the mind:
Sociocultural approach to mediated action. Harvard

University Press, 1991.

[39] J. Williams and J. Jacobs. Exploring the use of blogs
as learning spaces in the higher education sector.
Australasian Journal of Educational Technology,
20(2):232–247, 2004.

[40] J. Zagal and A. Bruckman. Gamelog: fostering
reflective gameplaying for learning. In Proceedings of
the 2007 ACM SIGGRAPH symposium on Video
games, pages 31–38. ACM, 2007.

65





