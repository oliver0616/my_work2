
ReaderBench: An Integrated Tool Supporting both 
Individual and Collaborative Learning 

Mihai Dascalu,  
Larise L. Stavarache,  
Stefan Trausan-Matu 

University Politehnica of Bucharest 
313 Splaiul Indepententei 

Bucharest, Romania 
mihai.dascalu@cs.pub.ro 

larise.stavarache@ro.ibm.com 
stefan.trausan@cs.pub.ro 

Philippe Dessus, 
Maryse Bianco 

 
Univ. Grenoble Alpes, LSE, 
F38000 Grenoble, France 

philippe.dessus@upmf-grenoble.fr 
maryse.bianco@upmf-grenoble.fr 

Danielle S. McNamara 
 
 

Arizona State University 
Tempe, Arizona 85287-2111 

dsmcnama@asu.edu 
 

 
 

ABSTRACT 
The core of our ReaderBench software framework exposes a 
unified vision for predicting and assessing comprehension in both 
individual and collaborative learning scenarios. ReaderBench 
aims to improve both the quality and the classification of the 
analyzed documents by using an expanded range of criteria such 
as: morphology, semantics, discourse analysis with emphasis on 
polyphony and dialogism, thus providing reliable support for both 
tutors and students across a range of educational settings. 
ReaderBench uses a unitary cohesion-based representation of 
discourse applied into three major directions, all tightly connected 
by the underlying model and the Natural Language Processing 
(NLP) computations: reading strategies, textual complexity, and 
collaboration evaluation in Computer Supported Collaborative 
Learning (CSCL) conversations. 

Categories and Subject Descriptors 
I.2.7 [Natural Language Processing], K.3.1 [Computer Uses in 
Education] 

General Terms 
Algorithms, Measurement 

Keywords 
Discourse Analysis, Learning Analytics, Reading Strategies, 
Textual Complexity, Participation/Collaboration Assessment. 

1. INTRODUCTION 
ReaderBench is an educational software that uses natural language 
processing and text mining technologies to allow both tutors and 
learners to regularly track learning progress, providing multi-
lingual support for English, French, and partially for Italian. The 
main areas covered by our system are derived from a unitary 

cohesion-based representation of discourse [1] used to: a) identify 
reading strategies [2], b) evaluate textual complexity [2], and 
c) assess participation and collaboration [3]. Overall, 
ReaderBench is a flexible and easy-to-use tool that can be 
integrated with a student’s learning path to enable real-time 
progress tracking. In a nutshell, ReaderBench’s evaluation process 
starts with the setup of an initial baseline of the textual complexity 
scores for the targeted educational materials, continues with the 
detection of the reading strategies used by learners self-explaining 
the given texts, and finally assesses participation and 
collaboration in CSCL conversations that encourage creativity. 

2. OVERVIEW 
The benefits of using an automatic system to evaluate students’ 
learning progress during an educational activity (e.g., courses, 
evaluations, exams, research activities) adds valuable 
enhancements to the tutor-learner relationship, particularly with 
regards to communication, real-time feedback, and, last but not 
least, aligning expectations for both sides. If we consider the 
tutor-learner relationship as a whole, then we can clearly see the 
importance of having immediate and relevant feedback on the 
learning and understanding processes, which on the tutor’s side 
can be both time consuming and potentially subjective. 

ReaderBench is a fully functional framework which uses text 
mining techniques based on advanced natural language processing 
and machine learning algorithms to design and deliver summative 
and formative assessments using multiple sets of data (e.g., textual 
materials, behavior tracks, meta-cognitive explanations). The 
cohesion-based integrated approach used within ReaderBench 
addresses multiple facets of learners’ comprehension processes. 

 
Figure 1 ReaderBench General processing pipeline [1] 

 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be 
honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. Request permissions from 
Permissions@acm.org. 
LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA Copyright 2015 
ACM 978-1-4503-3417-4/15/03...$15.00 
http://dx.doi.org/10.1145/2723576.2723647 



More specifically, in terms of educational data mining, 
ReaderBench encompasses a wide variety of techniques, besides 
the natural language processing pipeline applied in the pre-
processing phase including [1]: semantic distances based on 
ontologies, Latent Semantic Analysis (LSA) and Latent Dirichlet 
Allocation (LDA) for evaluating textual cohesion, as well as 
Social Network Analysis for modeling the interactions among 
participants. In addition, specific internal processes deduced from 
a cohesion-based underlying discourse structure are focused on 
(see Figure 1)[1]: a) topic extraction for determining the relevance 
of keywords, b) extractive unsupervised summarization using the 
bottom-up scoring mechanism, c) identification of reading 
strategies employed by learners in their verbalizations, d) textual 
complexity assessment by enforcing Support Vector Machines 
(SVM) classification on top a wide variety of metrics ranging 
from surface factors to morphology, semantics and discourse 
analysis, as well as e) collaboration evaluation derived from the 
cohesion-based and voice inter-animation automatic models. In 
sum, our integrated tool is applicable across multiple situations: 
tutor course preparation, essay scoring, vocabulary structure 
analysis, evaluation, or student assessment during a course. 

Moreover, ReaderBench can be used as a Personal Learning 
Environment (PLE), enabling two kinds of work-loops in terms of 
individual learning: a) a reading loop, in which learners can get 
information about the textual organization and structure of reading 
materials from ReaderBench, and b) a writing loop that provides 
learners the opportunity to develop at length what they understood 
from the text (e.g., reading strategies applied on self-explanations 
or summaries). In addition, the tutor can use ReaderBench to 
select appropriate textual materials suited for the learners’ level. 
From a collaborative perspective, ReaderBench can be used to 
support both learners and tutors in the learning process. Within 
the reading loop, learners are focused on other peers’ 
contributions, as well as on an overview of the entire 
conversation. The writing loop emphasizes the active participation 
and collaboration of each member by considering learners’ 
productions in the ongoing conversations. 

3. COMPARISON TO SIMILAR SYSTEMS 
The design of ReaderBench is focused on multiple views centered 
on discourse cohesion used for comprehension assessments and 
text analysis. In comparison to existing tools such as Coh-Metrix 
[5], DMeasure [4] or iSTART [6], ReaderBench encompasses their 
major functionalities and extends the individual learning 
perspective with collaborative specific CSCL analyses to support 
both tutors and learners. 

In comparison to Coh-Metrix, ReaderBench uses different textual 
analysis factors and employs SVM to enhance prediction 
accuracy. ReaderBench also incorporates multi-hierarchical 
classes obtained through the aggregation of factors from common 
text analysis systems [7], as well as specific factors focused on 
semantics and cohesion-based discourse analysis [2]. Coh-Metrix, 
by contrast, provides a more detailed evaluation and a broader 
range of integrated factors by also covering more usage scenarios. 
If we consider automated software tools for French, DMeasure is 
also a good alternative for assessing textual complexity, but it 
lacks a broader set of semantic and discourse factors, as well as a 
friendly interface for common usage. 
In comparison to iSTART, a comprehension strategy tutoring 
system, whereas iSTART is principally appropriate for high school 
and adult students, ReaderBench also targets primary school 
students, and provides multi-lingual support. 

4. CONCLUSIONS 
ReaderBench provides large range computations and data 
measurements, which have been previously validated and 
compared to human scores [1-3]. Moreover, ReaderBench can be 
integrated across a variety of pedagogical scenarios and can be 
easily extended for different purposes such as support for other 
languages and vocabularies, integration of additional semantic and 
cohesion-centered factors, as well as the possibility to visually 
display charts and reports to facilitate feedback. 

ReaderBench can be configured such that certain complexity 
factors, which do not apply to all vocabularies, are excluded from 
the final analysis report. On the one hand, tutors can select 
learning materials by using the multi-dimensional textual 
complexity model, can compare the learners’ productions to the 
automatically extracted features, and can evaluate self-
explanations while addressing the identified reading strategies. On 
the other hand, learners can take advantage of the document 
assessment facilities in order to better understand the structure, 
difficulty level and topics of the assigned material. Moreover, 
they can improve their own self-regulated processes through the 
system’s feedback, especially in the case of their self-explanations 
in terms of the identified reading strategies. In addition, 
collaboration with other peers can be an alternative for problem-
solving specific tasks. 

5. ACKNOWLEDGEMENTS 
This research was partially supported by the ANR DEVCOMP 
project, by the Sectorial Operational Program Human Resources 
Development 2007-2013 of the Ministry of European Funds 
through the Financial Agreements POSDRU/159/1.5/S/ 134398 
and 132397, by the LTfLL FP7 project, and by NSF grants 
1417997 and 1418378 to Arizona State University. 

6. REFERENCES 
[1] Dascalu, M., 2014. Analyzing discourse and text complexity 

for learning and collaborating, Studies in Computational 
Intelligence. Springer, Switzerland. 

[2] Dascalu, M., Dessus, P., Trausan-Matu, S., Bianco, M., and 
Nardy, A., 2013. ReaderBench, an environment for analyzing 
text complexity and reading strategies. In AIED 2013, H.C. 
Lane, K. Yacef, J. Mostow and P. Pavlik Eds. Springer, 
Memphis, USA, 379–388. 

[3] Dascalu, M., Trausan-Matu, S., and Dessus, P., 2014. 
Validating the Automated Assessment of Participation and of 
Collaboration in Chat Conversations. In ITS 2014, S. 
Trausan-Matu, K.E. Boyer, M. Crosby and K. Panourgia 
Eds. Springer, Honolulu, USA, 230–235. 

[4] François, T. and Miltsakaki, E., 2012. Do NLP and machine 
learning improve traditional readability formulas? In 
PITR2012 ACL, Montreal, Canada, 49–57. 

[5] McNamara, D.S., Graesser, A.C., McCarthy, P., and Cai, Z., 
2014. Automated evaluation of text and discourse with Coh-
Metrix. Cambridge University Press, Cambridge. 

[6] McNamara, D.S., O'Reilly, T.P., Rowe, M., Boonthum, C., 
and Levinstein, I.B., 2007. iSTART: A web-based tutor that 
teaches self-explanation and metacognitive reading 
strategies. In Reading comprehension strategies, D.S. 
McNamara Ed. Erlbaum, Mahwah, NJ, 397–420. 

[7] Nelson, J., Perfetti, C., Liben, D., and Liben, M., 2012. 
Measures of text difficulty: Testing their predictive value for 
grade levels and student performance. Council of Chief State 
School Officers. 



