
The LATUX Workflow: Designing and Deploying Awareness 
Tools in Technology-Enabled Learning Settings 

Roberto Martinez-Maldonado1,3, Abelardo Pardo2, Negin Mirriahi4,  
Kalina Yacef3, Judy Kay3 and Andrew Clayphan3,4  

1Faculty of Education and Social Work, 2School of Electrical & Information Engineering, 
 3School of Information Technologies, The University of Sydney, 2006, Australia. 

4 School of Education and Learning & Teaching Unit, University of New South Wales, 2052, Australia. 
{roberto.martinezmaldonado, abelardo.pardo, kalina.yacef, judy.kay, 

andrew.clayphan}@sydney.edu.au, negin.mirriahi@unsw.edu.au 
  

ABSTRACT 
Designing, deploying and validating learning analytics tools for 
instructors or students is a challenge requiring techniques and 
methods from different disciplines, such as software engineering, 
human-computer interaction, educational design and psychology. 
Whilst each of these disciplines has consolidated design 
methodologies, there is a need for more specific methodological 
frameworks within the cross-disciplinary space defined by 
learning analytics. In particular there is no systematic workflow 
for producing learning analytics tools that are both 
technologically feasible and truly underpin the learning 
experience. In this paper, we present the LATUX workflow, a 
five-stage workflow to design, deploy and validate awareness 
tools in technology-enabled learning environments. LATUX is 
grounded on a well-established design process for creating, testing 
and re-designing user interfaces. We extend this process by 
integrating the pedagogical requirements to generate visual 
analytics to inform instructors’ pedagogical decisions or 
intervention strategies. The workflow is illustrated with a case 
study in which collaborative activities were deployed in a real 
classroom.  

Categories and Subject Descriptors 
J.1 Administrative Data Processing: Education; K.3.1 Computer 
Uses in Education: Collaborative learning, Distance learning.  

General Terms 
Design, Experimentation, Human Factors, Standardisation. 

Keywords 
Design, groupware; visualisations; design; dashboard; awareness. 

1. INTRODUCTION 
The field of learning analytics (LA) has emerged in recent years as 
a multidisciplinary research area with the aim to improve the 

overall learning experience for instructors and students. Several 
authors have highlighted the potential impact of this discipline to 
enhance teaching practice and student learning [12, 29, 30, 33] 
but at the same time, there is also a recognition of the importance 
of establishing a connection with other research fields such as the 
Learning Sciences [29] and Human-Computer Interaction (HCI) 
[33]. As a consequence, designing tools in this space demands 
challenging interdisciplinary processes [2] so as to satisfy 
multiple goals and stakeholder requirements and leverage the 
expertise provided from the different disciplines. There is 
substantial research demonstrating the value of LA [12, 30], 
describing the basic requirements that the LA tools can target 
[11], envisioning the future of this field [29], and providing 
general analytical solutions for specific contexts [11]. However, 
little attention has been paid to exploring frameworks that can 
help designers to systematically develop, evaluate, and deploy 
effective LA tools.  

The area of Software Engineering (SE) provides established 
methodologies to help designers recognise processes, understand 
the requirements for interactive systems and iteratively evaluate 
these as they are built [31]. Similarly, the HCI and User 
Experience (UX) disciplines have a large array of methods for 
establishing user needs, and designing and evaluating interfaces 
[15]. However, these methodologies alone are not sufficient for 
building LA tools because they do not consider the learning 
context. Dillenbourg et al. [8] stated that traditional usability 
testing (mostly focused on individual or group interactions with 
computers) is not enough to prepare designs for the complexity of 
a learning context. The same authors suggested considering 
‘usability at the classroom level’. For example, in a classroom, 
instructors typically have to deal with constraints and 
contingencies such as time limitations, curriculum alignment, 
lesson plans, physical space constraints, unexpected events, and 
tool usage. This complexity can also be found in blended and 
online learning settings [27] where LA tools are commonly used. 
Similarly, Siemens [29] proposed a holistic view of LA that 
includes such practical issues, but additional aspects related to the 
data such as openness, accessibility, and ethics; and the particular 
pedagogical goals and context of usage of the LA tools (e.g. 
detecting at-risk students, supporting self-awareness or enhancing 
instructors’ awareness).  
These complexities suggest the need for new design 
methodologies for LA tools encompassing a pedagogical 
underpinning and considering the different actors (e.g. instructors 
and students), the dimensions of usability in learning contexts [8] 
(individuals, groups of students, and the classroom), the learning 
goals, data sources, and the tasks to be accomplished. This paper 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with 
credit is permitted. To copy otherwise, or republish, to post on servers or to 
redistribute to lists, requires prior specific permission and/or a fee. Request 
permissions from Permissions@acm.org.  
LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA 
Copyright is held by the owner/author(s). Publication rights licensed to 
ACM. 
ACM 978-1-4503-3417-4/15/03…$15.00  
http://dx.doi.org/10.1145/2723576.2723583 
 
 

1

mailto:Permissions@acm.org
http://dx.doi.org/10.1145/2723576.2723583


proposes LATUX (Learning Awareness Tools – User 
eXperience), a workflow for designing and deploying awareness 
tools for technology-enabled learning settings. By awareness 
tools we refer to those that provide a user (e.g. the instructor) with 
an enhanced level of awareness of what other actors (e.g. 
students) are doing in the learning space.  
The proposed workflow extends well-established design processes 
for developing user interfaces (UI’s) by integrating the 
pedagogical requirements, collecting the relevant data from 
students, and evaluating the awareness tools. The evidence 
supporting the design of the LATUX workflow is illustrated with 
a case study of the iterative development of visualisations for in-
classroom technology-enabled group activities to assist instructors 
in monitoring student collaboration and progress [23].  
The rest of the paper is organised as follows. The next section 
provides an overview of the current related LA methods followed 
by a brief description of the most relevant methods for interface 
development and evaluation used in SE and HCI. Then, we define 
the LATUX workflow, with its four principles and five stages. 
The workflow is illustrated in the final section with the 
description of the iterations of design, evaluation and the 
deployment of visualisations of student group activity in our case-
study.  

2. RELATED WORK 
2.1 Learning Analytics Methods for 
Pedagogical Intervention 
Technology-enabled learning environments, whether online or 
face-to-face in the classroom, have the advantage of capturing 
copious amounts of data left behind from students’ digital 
footprints [10, 14] that when made accessible to instructors can 
provide valuable objective insight into students’ learning. Using 
data mining techniques, the data captured from students’ 
interactions can be aggregated and analysed by instructors to 
make informed pedagogical decisions. For example, data on 
students’ use of a Learning Management System (LMS) can 
reveal patterns regarding student engagement with certain tools 
[3, 19]. However, these patterns are best understood by instructors 
when made available and visualised through user-friendly and 
intuitive LA tools. The challenge for these tools is to offer a low 
adoption threshold for non-technical users while offering 
additional functionality for more advanced users [29].  
The target audience of LA tools must be considered during the 
design process to ensure the insights gleaned from the data 
become actionable [26]. In the case of course-level analytics, 
visualisation tools should highlight the intended goal identified by 
the instructor and whether achieved [13], or even allow 
instructors to apply the performance indicators appropriate for 
their learning context [11]. The ability to observe whether 
students are engaging with a learning tool or progressing as 
intended, allows instructors to identify when to intervene, in order 
to support the students, which is specially challenging in online 
learning environments [11] and in large classes. For example, 
Social Networks Adapting Pedagogical Practice (SNAPP) 
represents student interactions within an online discussion forum 
to help instructors identify when to consult with students who are 
not engaging or when to change the nature of the activity [18]. 
Other learning analytics dashboards, such as Purdue University’s 
Course signals, provide data visualisations (e.g. traffic signals) to 
monitor performance and trigger early intervention when students 

may be at risk of poor performance or dropping out of a course 
[1]. While there is no shortage of evidence to support the 
argument that LA tools to visualise students’ activity at the 
course-level are needed to inform instructors about their students’ 
learning behaviour, there is limited research on effective 
workflows for designing such tools, particularly when focusing on 
data collected from face-to-face learning environments. This paper 
addresses this gap by proposing a design workflow that combines 
the rigor of HCI paradigms with respect to the user experience 
with a solid pedagogical underpinning. 

2.2 Designing User Interfaces  
A variety of methodologies have evolved over the years, to 
facilitate the process of software development and, specifically, 
user interface design. From a general SE perspective, these 
methodologies are commonly known as lifecycles [31] which for 
example, may include waterfall, spiral development, prototyping, 
and extreme programming approaches. The purpose of the 
lifecycles in HCI is to help designers develop more efficiently 
and, for the case of user interfaces, to enhance the user 
experience. In other words, the lifecycles provide best practices to 
other developers and designers.  
The lifecycles in both SE and HCI disciplines have similar 
structure and activities, such as requirement identification, design, 
and evaluation. However, the methodologies for designing user 
interfaces in SE and HCI have been formulated almost 
independently [15]. There exists a number of specific HCI 
lifecycles, such as: Mayhew’s usability lifecycle; the Star 
lifecycle; the LUCID framework of interaction design; and the 
Wheel model [15]. The latter is a more recent elaboration of the 
iterative process model for usability engineering which provides a 
general framework into which designers can fit existing or new 
techniques or methods to apply “best user experience practices” 
[16]. This is particularly relevant as a foundation for the workflow 
we propose in this paper. Overall, most of the lifecycles 
mentioned above have four generic stages (analysis, design, 
implementation, and evaluation) that can be performed iteratively, 
with potential for each stage to feed back to refine an earlier one. 
This iterative process for interface design can lead to a higher 
quality user experience.  
Adding to the challenge for LA, Bevan [4] argued for greater use 
of usability standards, but acknowledges that generic standards 
may fail to meet all the needs of designers in specific areas (e.g. 
for LA tools design).  A widely accepted standard that is relevant 
for developing interfaces for LA is the “Human-centred design for 
interactive systems” standard (ISO 9241-210 [9]). Its six 
principles are: 1) the design should be based upon an explicit 
understanding of users, tasks and environments; 2) users should 
be involved throughout the design; 3) the design should be driven 
by user-centred evaluation; 4) the process is iterative; 5) the 
design should address the whole user experience; and 6) the 
design team should be multidisciplinary in terms of skills and 
perspectives.  
The workflow proposed in this paper is based on this standard, 
and the evaluation-centred UX model (the Wheel) lifecycle 
concept [15]. Its novelty resides in the fact that it instantiates the 
user experience lifecycle with specific evaluation stages and 
requirements for designing awareness tools aimed at improving 
the learning experience, using data captured from students’ 
activity with learning technologies. 

2



3. The LATUX Workflow 
The LATUX workflow is defined by a) the principles on which it 
is grounded, and b) the evaluation-oriented stages that compose it 
(see Figure 1 on pg. 4). The entire workflow is evaluation-
oriented. Hence, each stage evaluates different parts of an 
awareness tool. The workflow has one problem identification 
stage and four evaluation stages. These principles and the stages 
are described in the rest of this section.  

3.1 LATUX Principles 
Based on LA literature and our first-hand experience, we 
identified a set of principles to take the workflow beyond generic 
development lifecycles. These principles are:   
Principle 1: The development process in LA has a strong research 
component. In traditional software design, development is 
commonly based on requirements elicitation, with iterative 
refinement. The classic UX approach consists in exploring the 
design space and gaining feedback iteratively, with careful 
evaluation in each iteration. By contrast, developing effective 
awareness tools for learning settings also requires carrying out 
research to explore the new possibilities that learner data can offer 
for supporting instructors. As with every field that has been 
revolutionised by technology, LA tools can have a 
transformational power in education, in addition to improving its 
processes. Research needs to be carried out concurrently to 
understand how these changes can take place. At the current state-
of-the-art, there is a prominent gap between research and practice 
in LA [29] indicating the importance of this principle in the 
proposed workflow. This opportunity to close the gap is what 
Duval [10] has highlighted as “Data-based Research on 
Learning”.  
Principle 2: The impact of authenticity; where the term authentic 
captures the nature of existing learning contexts, without LA 
tools. In line with recognised UX practice, we need to distil the 
nature of learning contexts and how these impact the needs of 
each stakeholder and act as drivers for interface design. This point 
highlights the need to go beyond standard usability. Usability 
testing with a small number of users is a general method to 
evaluate interfaces and user experience. However, for awareness 
tools, the contingencies that may occur in real learning scenarios 
may affect and be crucial for the adoption of technology [8]. As 
also noted by Rodriguez-Triana et al. [28], we need to align the 
design of the learning activities (tools usage, tasks, pedagogical 
strategies) and the learning analytics solutions to support 
instructors more effectively. In our workflow, each evaluation 
stage aims to increase the level of compliance of the awareness 
tool with the demands of authentic learning contexts while 
advancing towards the deployment of the tool in-the-wild (e.g. the 
classroom).   
Principle 3: Holistic design and evaluation. In the same way that 
the LA field is multi-disciplinary and focused on understanding 
systems as a whole [30], the design process to build LA tools 
must consider the complexity of the learning environments. This 
includes the features of the learning tools, the adopted 
pedagogical approach, the physical space, the student social roles, 
etc. Additionally, the evaluation of the tools should also be 
holistic. For example, rather than evaluating the effectiveness of 
the tool itself (in isolation with the purpose of generalising 
findings), the evaluation should consider the tool in the context of 
the learning setting and its further impact on the actual learning-
related activities (e.g. student retention, change in learning 

behaviour [33] or instructor’s attention [23]). This holistic view 
translates into a close relationship with Learning Sciences [30].  
Principle 4: Evaluation is oriented on data and its 
meaningfulness. In addition to the evaluation of the UI, the design 
of LA tools contains an additional dimension for evaluation which 
is making sense of the student activity data or interpreting it in 
order to inform pedagogical decisions. A crucial part of the design 
and evaluation process includes the ways to distil data, present it, 
and validate it accordingly to the pedagogical elements of learning 
and instruction [17] including representation, purpose, timing, 
context, learning goals, teaching goals, epistemic beliefs, and 
assessment approaches. For example, a specific data visualisation 
may enlighten instructors while they reflect on student progress 
over the semester, but may be useless in the classroom, or in other 
learning contexts. Thus, the design process should not only 
respond to the questions about what data and how the data should 
be presented, but also where and how it will be used. While the 
evaluation in terms of usability and user experience still applies 
for LA tools, the workflow presented in this paper is not a 
software development or user experience lifecycle. It is a design 
that incorporates and draws on the body of knowledge about 
learning contexts such as the learning cycle, where the students’ 
learning activity or instruction is at the focus of attention. Further, 
the design and evaluation of LA tools should be connected to 
both, the context of the learning activity and the context of usage.  

3.2 LATUX Stages  
Figure 1 illustrates the conceptual representation of the stages 
featured in our proposed iterative workflow for designing and 
deploying awareness tools in the classroom. The problem 
definition is the first stage of recommended practice in HCI and 
SE. The figure also shows the four evaluation-oriented stages that 
compose iterations towards deployment.  

3.2.1 Stage 1 – Problem definition 
Most literature presented above, concerning LA, has explored the 
different elements of the problem definition [29, 30, 33]. The first 
stage aims to identify the requirements for LA and user interface 
design. This process offers an opportunity to identify possible 
new and radical features that can be offered by the data to address 
stakeholder needs, but where the stakeholders may not realise this. 
Other elements of the problem definition that have been addressed 
by LA research as discussed below are the following: stakeholder 
identification, data sources, types of data logging, features of the 
learning setting and design for evaluation.  
Stakeholder identification. Although LA has mostly focused on 
informing and empowering instructors and learners [30], Siemens 
[29] identified other relevant stakeholders including faculties, 
departments, the whole institution, researchers, corporations, and 
government funders. From a SE perspective, stakeholder 
identification is a basic step for successful design [31]. Hence, the 
design of LA tools should adopt this, carefully identifying the 
target stakeholders.  
Data sources. According to Verbert et al. [33], to date, research 
on LA has mostly focused on using student data linked to resource 
usage, interaction time, number of artefacts created, collaborative 
interactions, and task results. By contrast, assessing the impact of 
the data displayed in awareness tools has not been deeply 
explored. In addition, there are many additional data sources that 
can offer new insights into student learning [5], but have been 
under-explored in the LA field such as gesture sensing, wearable 

3



bio sensors, head trackers, infrared imaging, depth imaging and 
eye tracking. Even manual tracking of activity has not been widely 
used in LA [33].  
Data Logging: As a continuation of the data source identification, 
Verbert et al. [33] also indicated that the design of LA tools 
should include an analysis of the feasibility and trade-offs of 
considering alternative data sensors used to capture more 
complete student data. Currently, most LA solutions are based on 
application logs and forms of audio and video capture. The 
problem definition should include the analysis of the implications 
of introducing additional sensors in terms of affecting the 
authenticity of the learning setting.  
Features of the learning setting: Verbert et al. [33] further 
identified the need for more research to identify the settings where 
particular LA approaches work well, and also to identify 
limitations and alternatives for learning environments where some 
LA solutions have been unsuccessful. This suggests a strong link 
between the development of LA tools and the context as a whole; 
and the risk of over-generalising LA solutions.   
Design for Evaluation: Finally, it is important to define how to 
evaluate the effectiveness of the use of LA tools in the target 
learning setting. This means, designing the evaluation of the user 
experience according to the principles this workflow is grounded 
upon (e.g. considering the authenticity of data orientation, 
meaningfulness, etc). Different measurement techniques should be 
used, as perception may not be an accurate way to evaluate the 
impact of a tool [15]. Measuring users’ actions and the impact of 
such actions are also an indirect measure of the effectiveness of 
the learning analytics tool. These include: usability testing; 
measuring the impact on teaching; learning gains; reduction in 
misconceptions; improved collaboration; enhanced instructor 
awareness; reduced load on the teacher; enhanced management of 
instructor time; etc. In addition, the conditions in which the 
evaluation will be performed (e.g. in the classroom, with 
simulated class conditions, simulated data, etc.) are required. 
Further details on this point will be discussed in the following 
stages of the proposed workflow.  

3.2.2 Stage 2 – Low-fidelity prototyping 
The idea of prototyping in early stages of the development is to 
provide a quick and flexible preliminary vision of the proposed 
system [15]. To accomplish this, a prototype must contain some 
key elements of the system without actually building the system. 
Low-fidelity prototypes are initial high-level representations of 
the intended design. They are appropriate when the details of the 
tool have not been defined or are likely to change, hence the 
construction of prototypes should involve little effort. In this 
stage, we propose the construction of low-fidelity prototypes with 
the purpose of: a) testing a wide range of possible visualisations 
that can be part of the intended awareness tool; b) testing usability 
and data sense making in the early stages of the design; and c) 
evaluating and triggering dialogue with stakeholders before 
building the LA tool. To achieve these Figure 1 lists a set of 
questions that may guide a designer to decide how to generate and 
evaluate low-fidelity prototypes of awareness tools. They include, 
for example, the selection of the indicators of the learning activity 
that will be visualised by the tool. The designer may also want to 
consider either sketching or using graphical tools to generate 
visualisations according to the LA goals. 
In LA, low-fidelity prototyping may focus on either the evaluation 
of the UI (user interface oriented prototype, e.g. a paper-based 
version of the interface where some visualisations can be drafted 
with simulated data) or the actual information conveyed in the 
visualisations (data oriented prototype e.g. visualisations of data 
captured by the actual learning system or from shared datasets). 
The first case is closer to a typical HCI evaluation, while data 
oriented prototyping is perhaps more relevant (and useful) in LA. 
We will provide an example of this in the Case Study described in 
the next section.  
In either case, low-fidelity prototypes are commonly paper-based 
representations of parts of a system or tool. They have a low cost 
and are highly effective to inform the design compared with 
subsequent versions of the tool [32] (see Table 1, column 1, first 
row). They also allow evaluations with multiple users and under 
controlled lab conditions (e.g. it is much easier to evaluate paper-
based versions of a dashboard in one hour interviews with 
multiple instructors than when the actual system is used). 

Figure 1. Conceptual Representation of the LATUX Workflow for Designing and Deploying Awareness Tools. 
 

2- Low-fidelity prototyping

3- Higher fidelity prototyping 
4- Pilot studies

5- Classroom use

1- Problem identification

What candidate visualisations should be generated?
How to mock up/sketch visualisations?
How would the visuals be generated?
What data is required?
How would the evaluation mimic real usage?
Example: paper-based visualisations

How can visualisations be dynamically generated?
How can the data be generated/processed?
What conditions of the setting can be simulated?
How can  the interventions be simulated?
How can interaction be supported?
How can the impact of the LA tools be evaluated?
Example:  Simulated class

What are the requirements?
What are the (unexplored) possibilities?
Who are the stakeholders?
What are the available data sources ?
How can the data be captured?
What is the learning context?
What are the learning tools?
What is the intended pedagogy?
How to evaluate the LA tools?

What visualisations can be generated in real-time?
How can interactivity be supported? 
How can the impact of the tool be tested?
How can contingencies be addressed?
How can interventions and change of behaviour be tested?
Example: Usage of the tool in a few authentic sessions

What limitations are imposed by the in-the-wild conditions?
What ethical issues can emerge from evaluating tools?

4



By contrast, low-fidelity prototypes in LA have many limitations 
(see Table 1, column 1, second row). They cannot replicate the 
connection between what occurs in an environment and the 
generation of data in that precise instant. Also these evaluations 
completely ignore the effect of interventions (e.g. instructors’ 
actions or provision of feedback) or changes in student behaviour. 
They also afford very limited understanding of the user experience 
under authentic learning conditions (see Table 1, column 1, rows 
3 and 4).  

3.2.3 Stage 3 – Higher-fidelity prototyping  
The next stage in the workflow is to enhance the fidelity of the 
prototypes by generating a programmed version of the system. 
From a HCI perspective, a high-fidelity prototype is a 
representation of the designed tool that is more detailed and 
realistic than a low-fidelity prototype [15]. It may include details 
of either the appearance or interaction behaviour needed to further 
evaluate how users interact with the tool or the data. High-fidelity 
prototypes require more effort and time, but the evaluation of such 
prototypes are still less expensive, faster to produce, and more 
flexible than developing the final tool.   
For LA awareness tools, specifically, we discuss the simulation of 
generation of student data and the conditions of the learning 
setting. This adds some degree of authenticity to the evaluation 
since the simulation may involve real student data and real-time 
usage. As indicated in the first row of Table 1, column 2, using 
simulation prototypes allows the inclusion of the time factor and, 
therefore, the evaluation of the decision making process on the fly 
(e.g. it is possible to analyse what an instructor would decide to 
do, right after looking at the tool). Similar to low-fidelity 
prototypes, evaluations with more users can be performed under 
lab conditions compared with more authentic but costly pilot 
studies. In addition, some aspects of interaction design can be 
evaluated (e.g. analysing how users would explore the data).    
More complex experiments can be designed to evaluate the user 
experience with a simulation tool since subjects can entirely focus 
on the details of the visualisations while the tasks can be 

replicated for multiple people in less time. However, as noted in 
the second row of Table 1, column 2, the impact of students’ or 
instructors’ actions is still ignored. Hence, the simulation of a 
tools’ usage does not reflect what would actually happen in a real 
environment where students or the instructors can actively 
respond to the data presented via the awareness tool. Therefore, 
the effect of the tool on instructors’ or students’ actions cannot be 
tested. As a result, the designer should consider the trade-off 
between the low degree of authenticity with this type of 
prototyping and the effort that is needed to build it. Figure 1 
presents some questions that can help the designer identify the 
conditions of the learning setting that can be simulated to evaluate 
the awareness tool, and how to address or simulate interventions 
or changes in behaviour in the design of the evaluation, or how 
the impact of the tool can be evaluated without actually measuring 
learning or behavioural change.  

3.2.4 Stage 4 – Pilot studies - Classroom use 
The fourth proposed stage before deploying an awareness tool in-
the-wild is the evaluation of the user experience in pilot studies. A 
pilot study is commonly accomplished under conditions that are 
similar to a real deployment but at a smaller scale (e.g. in a limited 
number of classroom sessions or during a limited period of time in 
a course). A pilot study can help prove a concept and observe the 
live usage of the tool in an authentic scenario. This is 
recommended as a part of the design workflow to minimise the 
risk of deploying an LA tool at a large scale while still performing 
the necessary research and evaluation.  
Key additional knowledge can be gained from pilot studies 
including testing interactivity and evaluation, which can include 
the analysis of the impact of the interventions and change of 
behaviours as a result of using the tool. In pilot studies, it is also 
possible to test the effect of the tool and the unexpected events 
that may occur on learning and instruction (see Table 1, column 3, 
first row). These are issues that the designer should inquire for a 
successful accomplishment of pilot studies (see Figure 1-4).  
However, pilot studies are typically more expensive in terms of 

Table 1. Strengths and limitations of the evaluation-oriented stages of the workflow. 
 
 

Low-fidelity 
prototyping

Higher-fidelity 
prototyping Pilot studies Validation in-the-wild

Strengths

•Testing usability and 
sense making in early 
stages
•Testing with multiple 
people 
•Very cheap

•Simulated decision making 
on the fly 
•Testing with multiple 
people 
•Deeper evaluation by 
users
•Interactivity can be tested 
•Cheap 

•Authentic –live usage of the tool
•Interactivity can be tested
•Interventions affect the use of the tool
•Impact of the tool on users’ can be tested
•Contingencies considered
•Cheaper than full deployment •More generalisable

•Testing with multiple people 
•Reduced ‘novelty-effect’

Limitations

•Time factor missing
•Interventions cannot 
be tested
•Interactivity cannot be 
easily tested

•Interventions cannot be 
tested
•More expensive than 
static prototypes

•More expensive than 
prototyping
•Testing with less people  
•Less generalisable

•The most expensive
•Contingencies
•Limitedexperimental 
conditions

Controlled 
conditions

Authenticity

5



effort and coordination, compared with evaluating low or high 
fidelity prototypes [15]. Added to that, results obtained from pilot 
studies are not necessarily generalisable. Setting controlled 
variables can be restrictive; depending on how authentic the 
learning conditions are (e.g. pilot studies in real schools may 
impose limitations on the range of items that can be evaluated). In 
addition, the amount of evidence and subjects can be more limited 
than performing shorter but more numerous evaluations with 
prototypes under lab conditions (see limitations in Table 1, 
column 3, second row).   

3.2.5 Stage 5 – Validation in-the-wild - Classroom use 
Evaluating in-the-wild has become a standard method within HCI 
testing [6]. In-the-wild testing consists of field trials of 
experimental systems with groups of subjects in relatively 
unconstrained settings outside of the laboratory, in a larger scale 
and for a longer duration than in pilot studies. For LA research 
and practice, this can help designers recognise the actual 
deployments as another evaluation stage of user experience. 
Similar to a pilot study, deployments in-the-wild provide an 
authentic test-bed to analyse not only usability and user 
experience, but also the usage and the impact of awareness tools 
in unpredictable learning environment and within the context of 
the pedagogical requirements and goals (see Table 1, column 3, 
first row).  
The usage of the awareness tool can be tested considering the 
possible contingencies that may occur. As the authenticity of the 
setting is higher and the duration of the study can be longer than a 
pilot study, the results of the evaluation of the awareness tool can 
be more generalisable to real world contexts and use. Also the 
longer-term usage of an awareness tool can minimise the ‘novelty-
effect’ inherent in introducing new technology. This effect cannot 
be easily analysed in smaller pilot studies or in prototype testing. 
This aspect is important for designing and testing LA tools since 
the ultimate goal of a successful deployment is the sustained 
usage of the tool for the learning or teaching goal that was 
envisaged for.  
In addition, testing the tool in-the-wild may require more 
challenging learning management, ethical and privacy issues to be 
addressed given the larger scale of the tools usage compared with 
pilot studies (Table 1, column 3, second row); these are aspects 
that may impact the design of an awareness tool and that the 
designer should be aware of (Figure 1-5). At the same time, 
evaluating LA tools in authentic deployments is the most 
expensive stage in a design workflow. Testing under authentic 
conditions can also affect the use of the tool and the evaluation as 
they offer less flexibility in setting control-experimental 
conditions (Table 1, column 4, second row). 

3.2.6 Iterative Design-Deployment-Evaluation 
Finally, bridging the gap between practice and research calls for 
communication of what happens in the deployment to inform the 
possible tuning or re-design of the tools. This invites the 
definition of iterations of design to either develop the LA tools in 
small incremental steps, or to improve certain elements of the UI 
or data processing. Mendiburo et al.[25] argues the importance of 
iterative interaction design to generate their LA tool, but other 
than this example, iterative design in LA design has not been 
explored and reported in depth. The designer may want to respond 
to some questions once the system, or part of it, has already been 
deployed, such as: what evaluation stages may be needed in an 

iteration (e.g. some functionality of the awareness tool may not 
require all types of prototyping testing or pilot studies)? What is 
the goal of the current iteration? What knowledge should be 
gained from a research perspective? How much iteration is 
required? As depicted in Table 1, rows 3 and 4, each stage within 
an iteration of the proposed workflow features a trade-off between 
the authenticity and degree of control of the learning setting 
condition.  

4. Case Study  
We illustrate our proposed workflow’s effectiveness by describing 
the iterative development of the MTDashboard, which has proven 
effective in enhancing instructor awareness and drove the 
provision of informed feedback in higher education technology-
enabled classroom studies with >500 students. 

4.1 Stage 1 - Problem identification  
We describe the context of the study in terms of the problem 
definition as a starting point of our design workflow. The 
objective of the project is to design, evaluate and deploy a number 
of visualisations of student group work in a classroom. The 
purpose of this is to enhance instructor awareness of students’ 
small-group collaboration and task progress. The group activity 
was supported either in the lab or in the classroom through the use 
of interactive tabletops. These are shared multi-touch devices that 
allow students to manipulate digital media while communicating 
face-to-face.  
Figure 2 (top-left, in the next page) shows three students working 
on a collaborative activity. The main requirement, from a LA 
point of view, was to generate an awareness tool (like the one 
shown in Figure 2, bottom-right), in the form of an instructor 
dashboard that would inform the instructor of student activity and 
progress while students work in small groups using multiple 
tabletops (Figure 2, top-right).  
Data sources and Data logging. Two main sources of student data 
were targeted in the project: the physical manipulation of virtual 
objects on the tabletop and quantitative dimensions of student 
speech (e.g. presence and duration of utterances). In regards to the 
tabletop technology, there has not been much research on sensing 
technology to capture the above student data. Part of the project 
included the production of the technology to differentiate touches 
on the tabletops and synchronise those with the detection of audio 
activity. More information about how the speech and touch 
activity can be automatically captured in this face-to-face setting 
can be found in [23]. 
Features of the learning setting. For the prototyping stages and 
pilot studies, most students were posed with problem solving and 
case-based resolution tasks to be solved with the construction of a 
concept map. Collaborative concept mapping is representative of 
a group learning activity that requires students to share their 
perspectives, visually represent their ideas, and agree on a group 
solution [7]. For the in-the-wild deployment, two additional 
learning applications were used: a brainstorming application and a 
tool for supporting project management meetings [24]. 
Evaluation. In this project, classroom instructors were the main 
stakeholders and source of requirements for the tool. Therefore, 
their role was crucial in the evaluations discussed below.  

4.2 Series of studies  
The design process of the case study included the following series 
of smaller studies, which support stages 2 to 5 respectively of our 

6



proposed workflow:  the first explored candidate visualisations 
based on paper prototypes [20]; the second consisted of trials in 
the lab with real data but simulated classroom sessions [21]; the 
third included two pilot studies conducted in two different 
semesters [22]; and the fourth was a longitudinal class experience 
with four instructors [24].  

4.2.1 Stage 2 – Validation of prototypes  
Description of the study: The first study focused on exploring 
visualisations of small-group face-to-face work that could be 
generated from the data captured from a tabletop device in a lab 
setting such as the one shown in Figure 2. An initial set of 
visualisations was designed to provide key indicators of 
accountability of students’ contributions to the group solution, the 
progress of the task, and how egalitarian the activity was amongst 
learners. A total of five visualisations were generated using a 
computer drawing editor. Details of these visualisations can be 
found in [20]. Two sets of the visualisations were printed on 
paper, which were generated based on real data obtained from the 
video recordings of two groups of three students each 
collaborating at the tabletop on a concept mapping task.   
Evaluation: The usability and meaningfulness of the visualisations 
were tested using these prototypes with five experienced 
instructors. Regarding usability, an instrument was first applied to 
investigate if all the facilitators were able to easily understand the 
visualisations. Then, a second instrument addressed a series of 

hypotheses that questioned if the set of visualisations revealed 
some facets of the group process to the instructors (e.g. about 
equity and amount of participation, overall collaboration, equity 
of intellectual contribution and the creation process.  
Main lessons learnt: In accordance with UX literature, [32] this 
inexpensive paper-based prototyping allowed the exploration of 
initial user experience to drive the design of the awareness tool. 
For example, the evaluation showed that instructors identified 
visualisations they would use in class and those they would prefer 
to use for post-hoc reflection. This drove the goals of the 
following studies and the iterative design of the awareness tool in 
general. Results from this evaluation also helped to refine the 
design of the visual aspects of the group indicators as well as to 
detect what other visualisations would be useful to explore.   

4.2.2 Stage 3 – Simulations with real instructors  
Description of the study: Based on the results of the evaluation of 
low-fidelity prototypes described above, in this study, a high-
fidelity prototype of a dashboard was built. This summarised 
student data captured from four group sessions recorded in the 
lab. The result was a dashboard with two levels of detail: 1) the 
class level, that showed minimal information of multiple groups at 
the same time (with three visualisations per group); and ii) the 
detailed group level, that allowed instructors to drill down to 
more specific information about a select group when required 
(with five visualisations in a timeline, e.g. Figure 2 - bottom-left). 

Figure 2. Case-study learning environment: Top-left: a three-member group acomplishing a collaborative learning task in the 
lab; Bottom-left: a high-fidelity prototype of an instructor dashboard; Top-right: a multi-tabletop classroom setting; and 

Bottom-right: an instructor using an awareness tool in-class. 
 
 

7



More details of the study can be found in [21]. 
Evaluation: This study evaluated how instructors used the 
dashboard to intervene in a group and what visualisations were 
most effective in conveying key information to inform their 
decisions. Controlled trials were conducted in a lab with real data 
but simulated classroom sessions involved eight instructors (all 
different from the previous evaluation stage). This high-fidelity 
prototype went beyond providing functionality by simulating the 
real-time generation of data for each instructor, as if he or she was 
monitoring up to three groups for 30 minutes. In parallel, each 
group video was manually analysed by an external person to 
diagnose a groups’ collaboration. The evaluation recreated the 
classroom orchestration loop documented by Dillenbourg et. al. 
[8], where instructors monitor the classroom (at the class level of 
the dashboard), compare it to some desirable state, and intervene 
to adapt the scenario (selecting the key visualisations that helped 
in the decision making process and drilling down to the detailed 
group level of the dashboard). If the instructors decided to 
intervene, they had to wait at least two minutes in the detailed 
group level simulating the time taken to talk with the group. 
Instructors followed this loop throughout the duration of the 
trials. Data captured from the instructors’ use of the dashboard, a 
questionnaire, and interviews were used to understand the 
instructors’ experience with the tool. 
Main lessons learnt: Even though the construction of a 
programmed simulator is a more expensive task, it provided a 
more realistic user experience than using static paper-based 
prototypes [15]. It forced instructors to experience the 
visualisations and analyse student data on-the-fly. This helped 
them think if they would realistically use the awareness tool in the 
classroom. For example, in this study instructors were able to 
identify the key features that gave them clues of groups 
encountering collaboration problems. The simulator also provided 
an opportunity to test a visualisation generated from a complex 
data mining algorithm that processed student data on the fly. 
Results from this evaluation showed that those visualisations 
which provided less processed data facilitated more effectively the 
management of instructors’ attention and interventions in 
comparison with the use of visualisations generated using more 
sophisticated data processing (e.g. a graph showing levels of 

collaboration as detected by a data 
mining algorithm The detailed 
group level, showing 
chronological information, was 
considered effective for assessing 
task progress after class. 

4.2.3 Stage 4 – Pilot studies 
Description of the study: Two 
pilot studies in the classroom were 
conducted in two different weeks 
for two courses with a single 
instructor conducting 22 classes 
(14 and 8 with 236 and 140 
students in each, respectively). 
Similarly to the setting shown in 
Figure 2 (top-right), the classroom 
featured four interconnected 
interactive tabletops logging 
student’s actions to a central 
repository. The instructor was 
provided with a handheld device 

where a new version of the dashboard was deployed. This showed 
real-time selected visualisations of either 1) group task progress, 
or 2) equity of student’s participation, within each group.  
Evaluation: In this case, to evaluate the impact of the tool in the 
classroom we collected information from a number of sources to 
triangulate evidence. These sources included: automated capture 
of the tabletops, notes from an external observer focused on 
instructor’s actions, notes from a second external observer 
focused on assessing each small group’s work. The evaluation 
focused on investigating if the instructor attended the ‘less 
achieving’ groups from the information provided in the dashboard 
and then if the instructor’s intervention accomplished in this way 
had some effect on student’s learning or their task. Further details 
of this evaluation process can be found in [22] and [23]. 
Main lessons learnt: This study made it evident how important it 
is to evaluate the tools in a real classroom environment. The 
instructor had to cope with many constraints (e.g. time limits, 
answering student’s questions, aligning to the curriculum, 
student’s arriving late, organising the activity and orchestrating 
the whole class), and the provision of the awareness tool added 
complexity to the already multifaceted instruction activity. This 
study also proved that instructors can greatly benefit from having 
quick access to information that is not easily visible in class time, 
for example, aspects of the quality of student’s solutions. The 
realism of this study also made it difficult to test experimental 
conditions (e.g. alternating visualisations in the awareness tools or 
comparing the usage of the tool with not using it at all) due to 
practical or ethical issues (e.g. all students had to have similar 
opportunities of learning). Additionally, the evaluation of pilot 
studies allowed testing the impact of the use of the awareness tool 
in pedagogical and learning terms beyond usability. This could 
not be done in the previous two studies. 

4.2.4 Stage 5 – Classroom use  
Description of the study: this study consisted of a  longitudinal 
class experience running across two full university courses (1 
semester) with four instructors (all different from the previous 
evaluations stage), three different learning tasks and 145 students 
[24]. In this study, a full version of the awareness tool was 

C

D

E

A

B

Figure 3. One final deployed teacher’s dashboard featuring: A and B: Control functions;    
C: Runtime script visualisation; D: Small group visualisations and E: Notifications [24]. 

 
 

8



provided. As shown in Figure 
3, it featured a number of 
functions so the instructor 
could control the learning 
technology (Figure 3-A and 
B). It also showed one 
visualisation per group 
(Figure 3-D) and it included 
the provision of notifications 
(rounded squares around 
visualisations – Figure 3-E) 
that were triggered when 
misconceptions were detected.  
Evaluation: Similar to the 
previous study, testing the 
awareness tool in-the-wild 
allowed understanding of the 
impact of the tool on learning 
and instruction. The visualised 
data reflects the change of 
student behaviour or the interventions performed by the 
instructor. For example, if the instructor attends to a group that 
has a misconception, the dashboard will update in real-time when 
that misconception has been addressed by the students.  
Main lessons learnt: The evaluation of the awareness tool in-the-
wild in this larger study allowed a higher degree of generalisation 
of the results [6]. Although the visualisations provided in the 
dashboard were not necessarily final, they proved useful and 
meaningful not only for the concept mapping activity but for other 
learning activities as well (such as brainstorming and face-to-face 
meetings). Additionally, the in-the-wild study allowed for testing 
the tool with more instructors than the pilot studies and for a 
longer time, thus minimising the novelty effect.  

4.2.5 Iterative Design  
The design of effective awareness tools, similar to other user 
interfaces, can be addressed with an iterative process. The purpose 
of this case-study is not to recommend the details of the 
visualisations and dashboards themselves (for this there has been 
nascent but extensive work [33]), but on the process of designing 
and refining visual awareness tools that may be effective for a 
specific context. For example, Figure 4 shows the evolution of 
two visualisations that were deployed in a classroom and were 
initially evaluated with prototypes. Although they may be simple, 
they present useful information to the instructor. It can be 
observed that: 1) there was a tendency towards minimalism. For 
example, for the visualisations “radars of participation” (see the 
three visualisation shown in Figure 4, on the top row) the 
prototypes featured two triangles depicting equity of verbal (blue) 
and touch (red) participation with the interactive tabletops. In the 
classroom, given the challenges to capture clean speech data, the 
visualisation had to be simplified to only show students’ touch 
activity data from the tabletops and names of students rather than 
symbolic representations (e.g. coloured circles).  
Additionally, 2) there was an increased focus on higher level 
indicators and less on accountability. For example, for the 
“contribution charts” (see the three visualisation shown in Figure 
4, on the lower row), the pie chart used for the prototypes was 
simplified to indicate size of the solution (outer orange circle, in 
the rightmost visualisation) and the portion that matched essential 
elements expected by the instructor (the inner yellow circle), 

instead of individual contributions depicted by the slices of the 
pie chart. This project is continuing to explore other visualisations 
of group activity that can be useful by instructors or students. 
Therefore, it is important to have an iterative perspective from the 
beginning of the design process though to the end. 

5. Conclusions  
The design of effective LA tools requires the convergence of 
methodologies from multiple disciplines such as software 
development, human-computer interaction and education. Even 
though these disciplines provide guidelines and frameworks to 
guide designers towards effective applications, the LA community 
may benefit from paradigms that reflect its multidisciplinary 
approach. There is a need for systematic processes to tackle the 
critical challenge of designing effective interfaces for LA 
applications.  
In this paper, a workflow has been described to design LA 
awareness tools. It proposes an iterative process with five stages 
with the objective of producing robust tools suitable for large-
scale adoption, and based on the combination of well-established 
techniques to improve user experience while maintaining an 
explicit connection with the underpinning learning environment. 
Our proposed workflow draws on the substantial body of work 
from SE and UX disciplines but also considers the pedagogical 
requirements and challenges designing LA tools to support 
teaching and learning in technology-enabled learning 
environments.  
The workflow has been supported with a case study on the 
development of an awareness tool for instructors to observe small 
group interactions in a classroom. Even though the case study 
discussed involves rather leading edge technology (e.g. interactive 
tabletops), the core lessons learnt and the proposed workflow 
have broader applicability to both existing and future learning 
technology. Further work is needed to validate the application of 
the workflow in blended and online learning environments, and 
explore cases where awareness tools are given to students or other 
stakeholders. Future work is also needed to explore the time and 
effort required in order to put the proposed workflow into practice 
in other domains of application (e.g. into a production 
environment). We believe that this work is an initial step towards 
much research needed to provide methodologies for the design of 
LA awareness tools. We consider the area of design frameworks 

Figure 4. Evolution of two visualisations, from prototypes to deployment               
A) Radars of Participation and B) Contribution charts.  

9



as a crucial aspect to contribute to the holistic view of LA, foster 
its widespread adoption, and improve the overall learning and 
teaching experience.  

6. ACKNOWLEDGMENTS 
This research has been partially funded by Smart Services CRC, 
Australia, CONACYT, Mexico and Fundación Pablo García, 
Mexico. 

7. REFERENCES 
[1] Arnold, K. E. and Pistilli, M. D. Course Signals at Purdue: 

Using learning analytics to increase student success. In Proc. 
LAK 2012. ACM, (2012), 267-270. 

[2] Balacheff, N. and Lund, K. Multidisciplinarity vs. 
Multivocality, the case of "learning analytics". In Proc. LAK 
2013. ACM, (2013), 5-13.  

[3] Beer, C., Jones, D. and Clark, D. Analytics and complexity: 
Learning and leading for the future. In Proc. ASCILITE, 
2012, 78-87. 

[4] Bevan, N. International standards for usability should be more 
widely used. Journal of Usability Studies, 4, 3 (2009), 106-
113. 

[5] Blikstein, P. Multimodal learning analytics. In Proc. LAK 
2013. ACM, (2013), 102-106.  

[6] Brown, B., Reeves, S. and Sherwood, S. Into the wild: 
challenges and opportunities for field trial methods. In Proc. 
CHI'11. ACM,  (2011), 1657-1666. 

[7] Chaka, C. Collaborative Learning: Leveraging Concept 
Mapping and Cognitive Flexibility Theory. Handbook of 
Research on Collaborative Learning Using Concept 
Mapping. Birmingham, UK, 2010, 152-170. 

[8] Dillenbourg, P., Zufferey, G., Alavi, H., Jermann, P., Do-
Lenh, S. and Bonnard, Q. Classroom orchestration: The third 
circle of usability. In Proc. CSCL 2011, ISLS (2011), 510-
517.  

[9] ISO DIS. 9241-210: 2010. Ergonomics of human system 
interaction-Part 210: Human-centred design for interactive 
systems (formerly known as 13407). 2009.  

[10] Duval, E. Attention please!: learning analytics for 
visualization and recommendation. In Proc. LAK 2011. ACM, 
(2011), 9-17.  

[11] Dyckhoff, A. L., Zielke, D., Bültmann, M., Chatti, M. A. and 
Schroeder, U. Design and Implementation of a Learning 
Analytics Toolkit for Teachers. Educational Technology & 
Society, 15, 3 (2012), 58-76. 

[12] Ferguson, R. The State of Learning Analytics in 2012: A 
Review and Future Challenges a review and future challenges. 
Technical Report KMI-12-01, Knowledge Media Institute, 
The Open University, UK.,(March)(2012). 

[13] Gluga, R., Kay, J., Lister, R., Simon, S., Charleston, M., 
Harland, J. and Teague, D. M. A conceptual model for 
reflecting on expected learning vs. demonstrated student 
performance. In Proc. ACE 2013, ACS,  (2013), 77-86. 

[14] Greller, W. and Drachsler, H. Translating learning into 
numbers: A generic framework for learning analytics. 
Educational Technology & Society (2012) 42-57. 

[15] Hartson, R. and Pyla, P. S. The UX book: process and 
guidelines for ensuring a quality user experience. Elsevier, 
2012. 

[16] Helms, J. W., Arthur, J. D., Hix, D. and Hartson, H. R. A 
field study of the Wheel—a usability engineering process 

model. Journal of Systems and Software, 79, 6 (2006), 841-
858. 

[17] Knight, S., Shum, S. B. and Littleton, K. Epistemology, 
pedagogy, assessment and learning analytics. In Proc. LAK 
2013.  ACM, (2013), 75-84.  

[18] Lockyer, L., Heathcote, E. and Dawson, S. Informing 
pedagogical action: Aligning learning analytics with learning 
design. American Behavioral Scientist, 57, 10 (2013), 1439-
1459. 

[19] Macfadyen, L. P. and Dawson, S. Mining LMS data to 
develop an “early warning system” for educators: A proof of 
concept. Computers & Education, 54, 2 (2010), 588-599. 

[20] Martinez-Maldonado, R., Kay, J. and Yacef, K. 
Visualisations for longitudinal participation, contribution and 
progress of a collaborative task at the tabletop. In Proc. CSCL 
2011, ISLS (2011), 25-32.  

[21] Martinez-Maldonado, R., Yacef, K., Kay, J. and 
Schwendimann, B. An interactive teacher’s dashboard for 
monitoring multiple groups in a multi-tabletop learning 
environment. In Proc. ITS 2012, (2012), 482-492.  

[22] Martinez-Maldonado, R., Dimitriadis, Y., Kay, J., Yacef, K. 
and Edbauer, M.-T. MTClassroom and MTDashboard: 
supporting analysis of teacher attention in an orchestrated 
multi-tabletop classroom. In Proc. CSCL2013, ISLS (2013), 
119-128.  

[23] Martinez-Maldonado, R. Analysing, visualising and 
supporting collaborative learning using interactive tabletops. 
PhD thesis, The University of Sydney, 2014. 

[24] Martinez-Maldonado, R., Clayphan, A., Ackad, C. and Kay, 
J. Multi-touch Technology in a Higher Education Classroom: 
Lessons In-the-wild. In Proc.OzCHI'14, (2014), 189-192.  

[25] Mendiburo, M., Sulcer, B. and Hasselbring, T. Interaction 
design for improved analytics. In Proc. LAK 2014. 
ACM,(2014), 78-82.  

[26] Olmos, M. and Corrin, L. Academic analytics in a medical 
curriculum: Enabling educational excellence. Australasian 
Journal of Educational Technology, 28, 1 (2012), 1-15. 

[27] Prieto, L. P., Dlab, M. H., Gutiérrez, I., Abdulwahed, M. and 
Balid, W. Orchestrating technology enhanced learning: a 
literature review and a conceptual framework. International 
Journal of Technology Enhanced Learning, 3, 6 (2011), 583-
598. 

[28] Rodríguez Triana, M. J., Martínez Monés, A., Asensio Pérez, 
J. I. and Dimitriadis, Y. Scripting and monitoring meet each 
other: Aligning learning analytics and learning design to 
support teachers in orchestrating CSCL situations. British 
Journal of Educational Technology (2014) (In press). 

[29] Siemens, G. Learning analytics: envisioning a research 
discipline and a domain of practice. In Proc. LAK 2012. 
ACM,(2012), 4-8. 

[30] Siemens, G. and Baker, R. S. J. d. Learning analytics and 
educational data mining: towards communication and 
collaboration. In Proc. LAK 2012. ACM,(2012), 252-254.  

[31] Sommerville, I. Software Engineering. Pearson, 2011. 
[32] Tohidi, M., Buxton, W., Baecker, R. and Sellen, A. Getting 

the right design and the design right. In Proc. CHI'06. ACM,  
(2006), 1243-1252. 

[33] Verbert, K., Govaerts, S., Duval, E., Santos, J., Assche, F., 
Parra, G. and Klerkx, J. Learning dashboards: an overview 
and future research opportunities. Personal and Ubiquitous 
Computing (2013), 1-16. 

10





