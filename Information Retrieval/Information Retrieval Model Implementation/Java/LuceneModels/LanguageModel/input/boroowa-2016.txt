
Reviewing three case-studies of learning analytics 
interventions at the Open University UK 

Bart Rienties, Avinash Boroowa, Simon Cross, Lee Farrington-Flint, Christothea Herodotou, 

Lynda Prescott, Kevin Mayles, Tom Olney, Lisette Toetenel, John Woodthorpe 

Open University UK 

Milton Keynes, United Kingdom 

[First.Lastname]@open.ac.uk

ABSTRACT 
This study provides a conceptual framework how organizations 
may adopt evidence-based interventions at scale, and how 
institutions may evaluate the costs and benefits of such 
interventions. Building on a new conceptual model developed by 
the Open University UK (OU), we will analyse three case-studies 
of evidence-based interventions. By working with 90+ large-scale 
modules for a period of two years across the five faculties and 
disciplines within the OU, Analytics4Action provides a bottom-
up-approach for working together with key stakeholders within 
their respective contexts. Using principles of embedded case-
study approaches by Yin [1], by comparing the learning behavior, 
satisfaction and performance of 11079 learners the findings 
indicated that each of the three learning designs led to satisfied 
students and average to good student retention. In the second part 
we highlighted that the three module teams made in-presentation 
interventions based upon real-time analytics, whereby initial user 
data indicated VLE behaviour in line with expectations. In 2-5 
years, we hope that a rich, robust evidence-base will be presented 
to show how learning analytics can help teachers to make 
informed, timely and successful interventions that will help 
learners to achieve their learning outcomes. 

CCS Concepts 
• Applied computing~Distance learning   
• Applied computing~E-learning 
 

Keywords 
Online learning settings, Collaborative Learning, Distance 
Learning 

1. INTRODUCTION 
Increased availability of large datasets [2, 3], powerful analytics 
engines, and skillfully designed visualisations of analytics results 
[4] mean that institutions may now be able to use the experience 
of the past to create supportive, insightful models of primary (and 
even real-time) learning processes [5-7]. While substantial 
progress has been made in small-scale experimental and practical 
studies, various meta-reviews and our LACE evidence hub [6, 8] 
indicate that most learning analytics studies have provided limited 
empirical support to suggest that learning analytics tools are 
currently able to provide a rich personalised experience for each 
learner.  

We argue that one of the largest challenges for learning analytics 
research and practice still lies ahead of us, namely how to put the 
power of learning analytics into the hands of teachers and 
administrators. At the Open University UK, the Analytics4Action 
(A4A) project is building towards a comprehensive conceptual 
model called Analytics4Action Evaluation Framework (A4AEF), 
which is nested within a strong evidence-base. The A4AEF 
describes how teachers and administrators can use learning 
analytics to make successful interventions in their own practice [9, 
10].  

Building on work with the A4AEF [9, 10], we will use an 
embedded case-study approach developed by Yin [1] to map, 
unpack and develop a fine-grained understanding of the learning 
analytics experiences in three large-scale level 1 modules at the 
OU. A rich understanding of the functioning of learning analytics 
interventions within the OU will be explored across three modules 
that have intensively used a range of ICT tools (e.g., blogs, 
computer simulations, discussion forums, videoconferences, 
wikis). We hope that our study will contribute to a body of insight 
how learning analytics approach can maximise student retention 
using evidence-based interventions. 

2. ANALYTICS4ACTION 
2.1 Analytics4Action Evaluation Framework 
As argued by [9], the A4AEF provides an evidence-based 
framework for learning analytics with which students, researchers, 
educators, and policy makers can manage, evaluate, and make 
decisions about which types of interventions work well, under 
which conditions, and which do not. [9] indicated that an effective 
evidence-based framework in learning analytics needs to adhere to 
five conditions:  

1) accurately and reliably identify learners at-risk/needing for 
support;  

2) identify learning design improvements; 
3) deliver (personalised) intervention suggestions that work for 

both student and teacher;  
4) operate within the existing teaching and learning culture; and  
5) be cost-effective.  

2.2 Research questions and approach 
We used an embedded case-study design [1] to understand how 
teachers designed their modules and how students engaged in 
these three interactive modules: 

1) To what extent do existing OU learning analytics metrics and 
visualisations of student journeys provide an accurate picture 
of learning design, learning processes and outcomes across 
the three modules? 

2) To what extent can these learning analytics metrics and 
visualisations help teachers to implement effective 
interventions? 

Permission to make digital or hard copies of part or all of this work 
for personal or classroom use is granted without fee provided that 
copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for third-party components of this work must 
be honored. For all other uses, contact the Owner/Author. 
Copyright is held by the owner/author(s). 
LAK '16, April 25-29, 2016, Edinburgh, United Kingdom 
ACM 978-1-4503-4190-5/16/04. 
DOI: http://dx.doi.org/10.1145/2883851.2883886 



3. RESULTS  
Three trends can be identified from the Module Activity Chart 
visualisations in Figure 1. First of all, in all three modules the 
learning design led to a continuous engagement of the vast 
majority of students, as illustrated by the orange lines. A second 
clear trend is that assessment drives learning and learning 
engagement.  A final trend is that the VLE engagement patterns 
across the three modules followed subtly different trajectories. 

 
Figure 1: VLE engagement across the three modules. 

Note: Orange line = VLE Engagement per week; Blue line = registered 
students; Green dots = Submission rate of assessments  

Table 1: Student retention and satisfaction 

 

Average 
Level 1 SD Arts 

Psycho-
logy 

Tech-
nology 

Registered 
students 

666.62 861.77 2234 3181 2489 

After 25% 94.13 5.93 94.10 87.90 86.50 

% Passed 65.02 12.73 71.30 64.40 59.70 

z-score -0.19 2.87 3.80 -0.70 3.30 

 

As illustrated in Table 1, the retention (% Passed) in the Arts 
module was well above the average of level 1 modules, while the 
Psychology module and Technology module were within the 
range of the average of level 1 (n=114). The z-score, which takes 
into consideration the relative performance of the module given 
the positive/negative characteristics of the students, indicated that 
both Arts and Technology performed better than expected, 

whereby a score close to 4 indicated a strong performance for 
both modules. The psychology module was relatively 
underperforming given the characteristics of the students, 
although with a small effect.  

4. DISCUSSION 
An important lesson from research question 1 is that just 
focussing on basic learning outcomes measures might lead to ill-
informed decisions by managers and teachers alike. In terms of 
research question 2, at LAK 2016 we will elaborate on the follow-
up interventions that were initiated based upon the discussions 
with the Analytics4Action team in the three modules indicated 
that small changes in learning design could be traced effectively 
with current VLE data. By working together in interdisciplinary 
teams consisting of teachers, learning designers, learning analytics 
specialists, educational psychologists, data interpreters, IT 
specialists and multi-media designers, the OU aims to 
continuously refine the learning experiences of our large cohorts 
of learners to meet their specific learning needs in an evidence-
based manner. In the next 2–5 years, we hope that a rich, robust 
evidence-base will be available which will demonstrate how 
learning analytics approaches can help teachers and administrators 
around the globe to make informed, timely and successful 
interventions that will help each learner achieve their learning 
outcomes. 

REFERENCES 
[1] Yin, R. K. Case study research: Design and methods. Sage, 

2009. 
[2] Arbaugh, J. B. System, scholar, or students? Which most 

influences online MBA course effectiveness? Journal of 
Computer Assisted Learning, 30, 4 2014), 349-362. 

[3] Rienties, B., Toetenel, L. and Bryan, A. “Scaling up” 
learning design: impact of learning design activities on LMS 
behavior and performance. ACM, City, 2015. 

[4] González-Torres, A., García-Peñalvo, F. J. and Therón, R. 
Human–computer interaction in evolutionary visual software 
analytics. Computers in Human Behavior, 29, 2 (3// 2013), 
486-495. 

[5] Ferguson, R. and Buckingham Shum, S. Social learning 
analytics: five approaches. ACM, City, 2012. 

[6] Papamitsiou, Z. and Economides, A. Learning Analytics and 
Educational Data Mining in Practice: A Systematic Literature 
Review of Empirical Evidence. Educational Technology & 
Society, 17, 4 2014), 49–64. 

[7] Arnold, K. E. and Pistilli, M. D. Course signals at Purdue: 
using learning analytics to increase student success. ACM, 
City, 2012. 

[8] Clow, D., Cross, S., Ferguson, R. and Rienties, B. Evidence 
Hub Review. LACE Project, City, 2014. 

[9] Rienties, B., Boroowa, A., Cross, S., Kubiak, C., Mayles, K. 
and Murphy, S. Analytics4Action Evaluation Framework: a 
review of evidence-based learning analytics interventions at 
Open University UK. Journal of Interactive Media in 
Education, 1, 2 2016), 1-12. 

[10] Rienties, B., Cross, S. and Zdrahal, Z. Implementing a 
Learning Analytics Intervention and Evaluation Framework: 
what works? Springer, City, 2016. 

 



