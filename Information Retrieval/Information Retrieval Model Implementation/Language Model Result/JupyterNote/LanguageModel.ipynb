{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model <br>\n",
    "The purpose of this program is to calculate Term Frequency, Term Frequency inverse document frequency, and frequency collection; their formula are, $TF=raw count $, $tfidf=TF*IDF, IDF=log(1+(docNum/wordAppear))$, $(\\alpha*tfidf)+1-(\\alpha*cf)$ <br>\n",
    "## Imports <br>\n",
    "The program use nltk for most of the text processing such as extracting name entities, tokenization and normalization. Use numpy to improve performance of the program, and other import for minor things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions <br>\n",
    "### Term Frequency Functions: <br>\n",
    "NameEntity function return a tuple of name entities chunk and regular tokens chunk. The function use nltk to extract the name entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: input tokens\n",
    "#Return: tuple of name entities and other tokens that are not name entities\n",
    "def nameEntity(text):\n",
    "    #parsing in to parse tree with nltk pre build model\n",
    "    chunked = ne_chunk(pos_tag(text))\n",
    "    name_entities_chunk = []\n",
    "    regular_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            name_entities_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        else:\n",
    "            name,pos = i\n",
    "            regular_chunk.append(name)\n",
    "    return (name_entities_chunk,regular_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index text into unigram, tokenize the text into tokens, extract name entities and add to the very end, text normalization lower all tokens remove punctuations and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: raw text\n",
    "#Return: tokens that has been normalize and name entities extracted at the very end\n",
    "def indexingUni(inputString):\n",
    "    #normalization\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(inputString)\n",
    "    #extract name entities and combine with rest of the tokens\n",
    "    nameTokens, normalTokens = nameEntity(tokens)\n",
    "    #lower case all the words\n",
    "    lowerTokens = [word.lower() for word in normalTokens]\n",
    "    lowerNameTokens = [word.lower() for word in nameTokens]\n",
    "    #remove punctuation\n",
    "    stripped = [word for word in lowerTokens if word.isalpha()]\n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in stripped if not w in stop_words]\n",
    "    #combine name entities and tokens\n",
    "    words = words + lowerNameTokens\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract top 20 counts of all the words within this document, use nltk to do the frequnecy distribution and get the top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: tokens that has go through indexingUni function\n",
    "#Return: top 20 counts\n",
    "def searchUni(uniWords):\n",
    "    fd = nltk.FreqDist(uniWords)\n",
    "    top = fd.most_common(20)\n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index text into bigram, tokenize input string and normalize them. Also extract the name entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: raw text\n",
    "#Returns: list of bigram\n",
    "def indexingBi(inputString):\n",
    "    final =[]\n",
    "    finalNameTokens=[]\n",
    "    #normalization\n",
    "    #sentences segmentation\n",
    "    sentences = nltk.sent_tokenize(inputString)\n",
    "    for sentence in sentences:\n",
    "        #tokenize the sentence into tokens\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        #extract name entities and combine with rest of the tokens\n",
    "        nameTokens, normalTokens = nameEntity(tokens)\n",
    "        #lower case all words\n",
    "        lowerTokens = [word.lower()for word in normalTokens]\n",
    "        lowerNameTokens = [word.lower() for word in nameTokens]\n",
    "        #remove punctuation\n",
    "        stripped = [word for word in lowerTokens if word.isalpha()]\n",
    "        #remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in stripped if not w in stop_words]\n",
    "        \n",
    "        #create bigrams \n",
    "        bigram = nltk.bigrams(words)\n",
    "        listBigram = list(bigram)\n",
    "        tupleName = [(w,\"\") for w in lowerNameTokens]\n",
    "        if len(listBigram) ==0:\n",
    "            finalNameTokens=finalNameTokens+tupleName\n",
    "            continue\n",
    "        else:\n",
    "            final.append(listBigram)\n",
    "            finalNameTokens=finalNameTokens+tupleName\n",
    "    final.append(finalNameTokens)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the tokens from the setence structure first than use nltk frequency distribution to get counts for all words than find the top 20 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: Tokens that has indexed by indexingBi\n",
    "#Return: top 20 counts\n",
    "def searchBi(searchSent):\n",
    "    organize=[]\n",
    "    #deconsturct sentence structure\n",
    "    for sentence in searchSent:\n",
    "        for first, second in sentence:\n",
    "            organize.append((first,second))\n",
    "    fd = nltk.FreqDist(organize)\n",
    "    top = fd.most_common(20)\n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency <br>\n",
    "Extract all unique words from all documents for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters: path where file is, and user input allow program to know which form of the text to get, unigram or bigram.\n",
    "#Return: return list of unique words for unigram, list of unique tuple for bigram\n",
    "def getAllWords(inputPath,userInput):\n",
    "    words = []\n",
    "    #loop through files\n",
    "    for f in os.listdir(inputPath):\n",
    "        inputFileName = os.path.join(inputPath,f)\n",
    "        inputFile=open(inputFileName,\"r\")\n",
    "        inputString = inputFile.read()\n",
    "        inputString = inputString.replace(\"\\n\",\" \")\n",
    "        #add all tokenize words\n",
    "        if userInput == \"3\" or userInput == \"5\":\n",
    "            words = words+ indexingUni(inputString)\n",
    "        elif userInput == \"4\" or userInput == \"6\":\n",
    "            bi = indexingBi(inputString)\n",
    "            #deconsturct sentence structure\n",
    "            for each in bi:\n",
    "                words = words+ each\n",
    "        print(\"File \"+f+\" word has extracted\")\n",
    "\n",
    "    result = set(words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a idf dictinoary for later tfidf calculation use. Numpy allow mutiple index add one at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "    #allWords: all the words for all the documents\n",
    "    #indexDict: a dictionary that has name as key, index as value\n",
    "    #docNum: all documents counts\n",
    "    #inputPath: path where input documents at\n",
    "    #userInput: user selection for unigram or bigram\n",
    "#Return: a idf dictionary that idf has been calculated\n",
    "def constructDict(allWords,indexeDict,docNum, inputPath, userInput):\n",
    "    #initialize numpy array\n",
    "    word_idf_dict = np.zeros(len(allWords))\n",
    "    #loop through files and add one when word occur\n",
    "    for f in os.listdir(inputPath):\n",
    "        inputFileName = os.path.join(inputPath,f)\n",
    "        inputFile=open(inputFileName,\"r\")\n",
    "        inputString = inputFile.read()\n",
    "        inputString = inputString.replace(\"\\n\",\" \")\n",
    "        words = []\n",
    "        #find the word's index and add one\n",
    "        if userInput == \"3\" or userInput == \"5\":\n",
    "            words = set(indexingUni(inputString))\n",
    "            indexes = [indexDict[word] for word in words]\n",
    "            word_idf_dict[indexes] += 1.0\n",
    "        elif userInput == \"4\" or userInput == \"6\":\n",
    "            bi = indexingBi(inputString)\n",
    "            for each in bi:\n",
    "                words = words+ each\n",
    "            indexes = [indexDict[word] for word in words]\n",
    "            word_idf_dict[indexes] += 1.0\n",
    "    #calculate idf\n",
    "    word_idf_dict = np.log(1+(docNum / (word_idf_dict).astype(float)))\n",
    "    return word_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the term frequency * inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "    #idfDict: idf dictionary that is preconsturcted\n",
    "    #indexDict: index dictionary that has word as key index as value\n",
    "    #inputWords: current text that you are processing\n",
    "#Return: a list of tuple that has word and tfidf value\n",
    "def tfidf(idfDict,indexDict, inputWords):\n",
    "    result=[]\n",
    "    #tf\n",
    "    fd = nltk.FreqDist(inputWords)\n",
    "    fdList = list(fd)\n",
    "    \n",
    "    #idf\n",
    "    for each in fdList:\n",
    "        idf = idfDict[indexDict[each]]\n",
    "        tf = fd[each]\n",
    "        tfidfVal = tf*idf\n",
    "        result.append((each, tfidfVal))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Frequency <br>\n",
    "Construct a dictionary that has collection frequency of all words in all documents. Numpy allow adding the list to the corresponding array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "    #allWords: all the words for all documents\n",
    "    #indexDict: a dictionary that has name as key, index as value\n",
    "    #inputPath: path where input documents at\n",
    "    #userInput: user selection for unigram or bigram\n",
    "#Return:a collection frequency dictionary\n",
    "def freqDict(allWords,indexeDict, inputPath, userInput):\n",
    "    #initialize numpy array\n",
    "    collectionFreq = np.zeros(len(allWords))\n",
    "    #loop through files and add one when word occur\n",
    "    for f in os.listdir(inputPath):\n",
    "        inputFileName = os.path.join(inputPath,f)\n",
    "        inputFile=open(inputFileName,\"r\")\n",
    "        inputString = inputFile.read()\n",
    "        inputString = inputString.replace(\"\\n\",\" \")\n",
    "        words = []\n",
    "        #find the word's index and add one\n",
    "        if userInput == \"5\":\n",
    "            all_words = indexingUni(inputString)\n",
    "            freq_dist_words = nltk.FreqDist(all_words)\n",
    "            words =set(all_words)\n",
    "            indexes = [indexDict[word] for word in words]\n",
    "            counts = [freq_dist_words[word] for word in words]\n",
    "            collectionFreq[indexes] += counts\n",
    "        elif userInput == \"6\":\n",
    "            bi = indexingBi(inputString)\n",
    "            for each in bi:\n",
    "                all_words = words+ each\n",
    "            freq_dist_words = nltk.FreqDist(all_words)\n",
    "            words =set(all_words)\n",
    "            indexes = [indexDict[word] for word in words]\n",
    "            counts = [freq_dist_words[word] for word in words]\n",
    "            collectionFreq[indexes] += counts\n",
    "    return collectionFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function calculate $(\\alpha*tfidf)+1-(\\alpha*cf)$ and return result, alpha is default to 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "    #tfidfDict: a dictionary for tfidf value\n",
    "    #collectionFreqDict: a dictionary for collection frequency value\n",
    "    #indexDict: a dictionary that has name as key, index as value\n",
    "    #inputWords: current text that you are processing\n",
    "#Return: return collection frequnecy \n",
    "def cftfidf(tfidfDict,collectionFreqDict, indexDict, inputWords):\n",
    "    alpha = 0.3\n",
    "    uniqueWord = set(inputWords)\n",
    "    result = []\n",
    "    for each in uniqueWord:\n",
    "        tfidfValue = tfidfDict[each]\n",
    "        cfValue= collectionFreqDict[indexDict[each]]\n",
    "        cftfidfValue= (alpha*tfidfValue) + 1 - (alpha*cfValue)\n",
    "        result.append((each, cftfidfValue))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other functions\n",
    "Get the key function for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKey(item):\n",
    "    return item[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function <br>\n",
    "User select what kind of operation they would like to do, base on the selection the program would construct inverse document frequency dictionary or collection frequency dictionary. Also calculate out time taken for each operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choice operation\n",
    "userInput=input(\"TF-Unigram: 1, TF-Bigram: 2, TF-IDF-Unigram: 3, TF-IDF-Bigram: 4, Collection Frequnecy Unigram: 5, Collection Frequnecy Bigram: 6\")\n",
    "#Get current directory path, for input file and outpuf file use later on\n",
    "cwd = os.getcwd()\n",
    "inputPath = os.path.join(cwd,\"input\")\n",
    "tfOutputPath = os.path.join(cwd,\"output\",\"tf\")\n",
    "tfidfOutputPath = os.path.join(cwd,\"output\",\"tfidf\")\n",
    "cftfidfOutputPath = os.path.join(cwd,\"output\",\"cf-tfidf\")\n",
    "\n",
    "#if user want tfidf\n",
    "if userInput == \"3\" or userInput == \"4\":\n",
    "    #Get all the words, document counts, and word indexes\n",
    "    x = time.time()\n",
    "    allWords = getAllWords(inputPath, userInput)\n",
    "    y = time.time()\n",
    "    z = y - x\n",
    "    print(\"Get All Words Time: \"+str(z))\n",
    "    docNum = len(os.listdir(inputPath))\n",
    "    indexDict = {w: idx for idx, w in enumerate(allWords)}\n",
    "    #construct idf dictionary\n",
    "    a = time.time()\n",
    "    idfDict = constructDict(allWords, indexDict, docNum, inputPath,userInput)\n",
    "    b = time.time()\n",
    "    c = b - a\n",
    "    print(\"Done constructDict\")\n",
    "    print(\"ConstructDict Time: \",c)\n",
    "#if user want cftfidf\n",
    "elif userInput == \"5\" or userInput == \"6\":\n",
    "    #Get all the words, document counts, and word indexes\n",
    "    x = time.time()\n",
    "    allWords = getAllWords(inputPath, userInput)\n",
    "    y = time.time()\n",
    "    z = y - x\n",
    "    print(\"Get All Words Time: \"+str(z))\n",
    "    docNum = len(os.listdir(inputPath))\n",
    "    indexDict = {w: idx for idx, w in enumerate(allWords)}\n",
    "    #construct idf dictionary and collectionFreq dictionary\n",
    "    a = time.time()\n",
    "    idfDict = constructDict(allWords, indexDict, docNum, inputPath,userInput)\n",
    "    b = time.time()\n",
    "    c = b - a\n",
    "    print(\"Done constructDict\")\n",
    "    print(\"ConstructDict Time: \",c)\n",
    "    d = time.time()\n",
    "    collectionFreqDict = freqDict(allWords, indexDict, inputPath,userInput)\n",
    "    e = time.time()\n",
    "    f = e - d\n",
    "    print(\"Done collectionFreqDict\")\n",
    "    print(\"ConstructDict Time: \",f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caculation base on user selection,1 for TF unigram, 2 for TF bigram, 3 for TFIDF unigram, 4 for TFIDF bigram, 5 for CF-TFIDF unigram, 6 for CF-TFIDF bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculations(tf,tf-idf)\n",
    "for f in os.listdir(inputPath):\n",
    "    inputFileName = os.path.join(inputPath,f)\n",
    "    inputFile=open(inputFileName,\"r\")\n",
    "    inputString = inputFile.read()\n",
    "    inputString = inputString.replace(\"\\n\",\" \")\n",
    "    #TF\n",
    "    if userInput==\"1\":\n",
    "        #unigram\n",
    "        uniWords = indexingUni(inputString)\n",
    "        top = searchUni(uniWords)\n",
    "        \n",
    "    elif userInput == \"2\":\n",
    "        #bigram\n",
    "        biWords = indexingBi(inputString)\n",
    "        top = searchBi(biWords)\n",
    "    #TF-IDF\n",
    "    elif userInput == \"3\":\n",
    "        #unigram\n",
    "        uniWords = indexingUni(inputString)\n",
    "        tfidfList= tfidf(idfDict, indexDict, uniWords)\n",
    "        tfidfList.sort(key=getKey, reverse= True)\n",
    "    elif userInput == \"4\":\n",
    "        #bigram\n",
    "        words=[]\n",
    "        biWords = indexingBi(inputString)\n",
    "        for each in biWords:\n",
    "            words = words+ each\n",
    "        tfidfList= tfidf(idfDict, indexDict, words)\n",
    "        tfidfList.sort(key=getKey, reverse= True)\n",
    "    elif userInput == \"5\":\n",
    "        #unigram\n",
    "        uniWords = indexingUni(inputString)\n",
    "        tfidfList= tfidf(idfDict, indexDict, uniWords)\n",
    "        tfidfDict = dict((wo,i) for wo,i in tfidfList)\n",
    "        cftfidfList= cftfidf(tfidfDict,collectionFreqDict, indexDict, uniWords)\n",
    "        cftfidfList.sort(key=getKey, reverse= True)\n",
    "    elif userInput == \"6\":\n",
    "        #bigram\n",
    "        words = []\n",
    "        biWords = indexingBi(inputString)\n",
    "        for each in biWords:\n",
    "            words = words + each\n",
    "        tfidfList= tfidf(idfDict, indexDict, words)\n",
    "        tfidfDict = dict((wo,i) for wo,i in tfidfList)\n",
    "        cftfidfList= cftfidf(tfidfDict,collectionFreqDict, indexDict, words)\n",
    "        cftfidfList.sort(key=getKey, reverse= True)\n",
    "        #Writeing output files\n",
    "    #TF\n",
    "    if userInput == \"1\" or userInput == \"2\":\n",
    "        tfOutputFileName = os.path.join(tfOutputPath,f)\n",
    "        tfOutputFile=open(tfOutputFileName,\"w\")\n",
    "        tfOutputFile.write('%-30s %s\\n' %(\"Word\",\"Count\"))\n",
    "        tfOutputFile.write(\"-----------------------------------\\n\")\n",
    "        for word, count in top:\n",
    "            tfOutputFile.write('%-30s %s\\n' %(word,count))\n",
    "        tfOutputFile.close()\n",
    "    #TF-IDF\n",
    "    elif userInput == \"3\" or userInput == \"4\":\n",
    "        tfidfOutputFileName = os.path.join(tfidfOutputPath,f)\n",
    "        tfidfOutputFile = open(tfidfOutputFileName,\"w\")\n",
    "        tfidfOutputFile.write('%-30s %s\\n' %(\"Word\",\"TF-IDF\"))\n",
    "        tfidfOutputFile.write(\"-----------------------------------\\n\")\n",
    "        for word, count in tfidfList[:20]:\n",
    "            tfidfOutputFile.write('%-30s %s\\n' %(word,count))\n",
    "        tfidfOutputFile.close()\n",
    "    #CF TF-IDF\n",
    "    elif userInput == \"5\" or userInput == \"6\":\n",
    "        cftfidfOutputFileName = os.path.join(cftfidfOutputPath,f)\n",
    "        cftfidfOutputFile = open(cftfidfOutputFileName,\"w\")\n",
    "        cftfidfOutputFile.write('%-30s %s\\n' %(\"Word\",\"CF TF-IDF\"))\n",
    "        cftfidfOutputFile.write(\"-----------------------------------\\n\")\n",
    "        for word, count in cftfidfList[:20]:\n",
    "            cftfidfOutputFile.write('%-30s %s\\n' %(word,count))\n",
    "        cftfidfOutputFile.close()\n",
    "    #tracer\n",
    "    print(\"File \" + f +\" is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
